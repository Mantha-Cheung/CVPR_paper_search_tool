deep compositional captioning describing novel object categories without paired training data <eos> while recent deep neural network models achieved promising result image captioning task they rely largely availability corpora paired image sentence captions describe object context <eos> work propose deep compositional captioner dcc address task generating descriptions novel object present paired image sentence datasets <eos> method achieves leveraging large object recognition datasets external text corpora transferring knowledge between semantically similar concepts <eos> current deep caption models only describe object contained paired image sentence corpora despite fact they pre trained large object recognition datasets namely imagenet <eos> contrast model compose sentences describe novel object their interactions other object <eos> demonstrate model ability describe novel concepts empirically evaluating its performance mscoco show qualitative result imagenet image object no paired image caption data exist <eos> further extend approach generate descriptions object video clips <eos> result show dcc distinct advantages over existing image video captioning approaches generating descriptions new object context <eos> <eop> generation comprehension unambiguous object descriptions <eos> propose method generate unambiguous description known referring expression specific object region image also comprehend interpret such expression infer object being described <eos> show method outperforms previous method generate descriptions object without taking into account other potentially ambiguous object scene <eos> model inspired recent successes deep learning method image captioning but while image captioning difficult evaluate task allows easy objective evaluation <eos> also present new large scale dataset referring expressions based ms coco <eos> released dataset toolbox visualization evaluation see github <eos> com mjhucla google refexp toolbox <eos> <eop> stacked attention network image question answering <eos> paper presents stacked attention network sans learn answer natural language questions image <eos> sans use semantic representation question query search region image related answer <eos> argue image question answering qa often requires multiple steps reasoning <eos> thus develop multiple layer san query image multiple times infer answer progressively <eos> experiments conducted four image qa data set demonstrate proposed sans significantly outperform previous state art approaches <eos> visualization attention layer illustrates progress san locates relevant visual clues lead answer question layer layer <eos> <eop> image question answering using convolutional neural network dynamic parameter prediction <eos> tackle image question answering imageqa problem learning convolutional neural network cnn dynamic parameter layer whose weights determined adaptively based questions <eos> adaptive parameter prediction employ separate parameter prediction network consists gated recurrent unit gru taking question its input fully connected layer generating set candidate weights its output <eos> however challenging construct parameter prediction network large number parameters fully connected dynamic parameter layer cnn <eos> reduce complexity problem incorporating hashing technique candidate weights given parameter prediction network selected using predefined hash function determine individual weights dynamic parameter layer <eos> proposed network joint network cnn imageqa parameter prediction network trained end end through back propagation its weights initialized using pre trained cnn gru <eos> proposed algorithm illustrates state art performance all available public imageqa benchmarks <eos> <eop> neural module network <eos> visual question answering fundamentally compositional nature question like dog shares substructure questions like color dog cat paper seeks simultaneously exploit representational capacity deep network compositional linguistic structure questions <eos> describe procedure constructing learning neural module network compose collections jointly trained neural modules into deep network question answering <eos> approach decomposes questions into their linguistic substructures uses structures dynamically instantiate modular network reusable components recognizing dogs classifying colors etc <eos> resulting compound network jointly trained <eos> evaluate approach two challenging datasets visual question answering achieving state art result both vqa natural image dataset new dataset complex questions about abstract shapes <eos> <eop> learning deep representations fine grained visual descriptions <eos> state art method zero shot visual recognition formulate learning joint embedding problem image side information <eos> formulations current best complement visual feature attributes manually encoded vectors describing shared characteristics among categories <eos> despite good performance attributes limitations finer grained recognition requires commensurately more attributes attributes provide natural language interface <eos> propose overcome limitations training neural language models scratch <eos> without pre training only consuming words characters <eos> proposed models train end end align fine grained category specific content image <eos> natural language provides flexible compact way encoding only salient visual aspects distinguishing categories <eos> training raw text model inference raw text well providing humans familiar mode both annotation retrieval <eos> model achieves strong performance zero shot text based image retrieval significantly outperforms attribute based state art zero shot classification caltech ucsd birds dataset <eos> <eop> multi cue zero shot learning strong supervision <eos> scaling up visual category recognition large numbers classes remains challenging <eos> promising research direction zero shot learning require any training data recognize new classes but rather relies some form auxiliary information describing new classes <eos> ultimately may allow use textbook knowledge humans employ learn about new classes transferring knowledge classes they know well <eos> most successful zero shot learning approaches currently require particular type auxiliary information namely attribute annotations performed humans readily available most classes <eos> goal circumvent bottleneck substituting such annotations extracting multiple pieces information multiple unstructured text sources readily available web <eos> compensate weaker form auxiliary information incorporate stronger supervision form semantic part annotations classes transfer knowledge <eos> achieve goal joint embedding framework maps multiple text parts well multiple semantic parts into common space <eos> result consistently significantly improve state art zero short recognition retrieval <eos> <eop> latent embeddings zero shot classification <eos> present novel latent embedding model learning compatibility function between image class embeddings context zero shot classification <eos> proposed method augments state art bilinear compatibility model incorporating latent variables <eos> instead learning single bilinear map learns collection maps selection map use being latent variable current image class pair <eos> train model ranking based objective function penalizes incorrect rankings true class given image <eos> empirically demonstrate model improves state art various class embeddings consistently three challenging publicly available datasets zero shot setting <eos> moreover method leads visually highly interpretable result clear clusters different fine grained object properties correspond different latent variable maps <eos> <eop> one shot learning scene locations via feature trajectory transfer <eos> appearance outdoor scenes changes considerably strength certain transient attributes such rainy dark sunny <eos> obviously also affects representation image feature space <eos> activations certain cnn layer consequently impacts scene recognition performance <eos> work investigate variability transient attributes rich source information studying how image representations change function attribute strength <eos> particular leverage recently introduced dataset fine grain annotations estimate feature trajectories collection transient attributes then show how trajectories transferred new image representations <eos> enables synthesize new data along transferred trajectories respect dimensions space spanned transient attributes <eos> applicability concept demonstrated problem one shot recognition scene locations <eos> show data synthesized via feature trajectory transfer considerably boosts recognition performance respect baselines combination state art approaches one shot learning <eos> <eop> learning attributes equals multi source domain generalization <eos> attributes possess appealing properties benefit many computer vision problems such object recognition learning humans loop image retrieval <eos> whereas existing work mainly pursues utilizing attributes various computer vision problems contend most basic problem how accurately robustly detect attributes image left under explored <eos> especially existing work rarely explicitly tackles need attribute detectors should generalize well across different categories including previously unseen <eos> noting analogous objective multi source domain generalization if treat each category domain provide novel perspective attribute detection propose gear techniques multi source domain generalization purpose learning cross category generalizable attribute detectors <eos> validate understanding approach extensive experiments four challenging datasets three different problems <eos> <eop> anticipating visual representations unlabeled video <eos> anticipating actions object before they start appear difficult problem computer vision several real world applications <eos> task challenging partly because requires leveraging extensive knowledge world difficult write down <eos> believe promising resource efficiently learning knowledge through readily available unlabeled video <eos> present framework capitalizes temporal structure unlabeled video learn anticipate human actions object <eos> key idea behind approach train deep network predict visual representation image future <eos> visual representations promising prediction target because they encode image higher semantic level than pixels yet automatic compute <eos> then apply recognition algorithms predicted representation anticipate object actions <eos> experimentally validate idea two datasets anticipating actions one second future object five seconds future <eos> <eop> learning assign orientations feature point <eos> show how train convolutional neural network assign canonical orientation feature point given image patch centered feature point <eos> method improves feature point matching upon state art used conjunction any existing rotation sensitive descriptors <eos> avoid tedious almost impossible task finding target orientation learn propose use siamese network implicitly find optimal orientations during training <eos> also propose new type activation function neural network generalizes popular relu maxout prelu activation functions <eos> novel activation performs better task <eos> validate effectiveness method extensively four existing datasets including two non planar datasets well own dataset <eos> show outperform state art without need retraining each dataset <eos> <eop> learning dense correspondence via three dimensional guided cycle consistency <eos> discriminative deep learning approaches shown impressive result problems human labeled ground truth plentiful but about tasks labels difficult impossible obtain paper tackles one such problem establishing dense visual correspondence across different object instances <eos> task although know ground truth know should consistent across instances category <eos> exploit consistency supervisory signal train convolutional neural network predict cross instance correspondences between pairs image depicting object same category <eos> each pair training image find appropriate three dimensional cad model render two synthetic views link pair establishing correspondence flow cycle <eos> use ground truth synthetic synthetic correspondences provided rendering engine train convnet predict synthetic real real real real synthetic correspondences cycle consistent ground truth <eos> test time no cad models required <eos> demonstrate end end trained convnet supervised cycle consistency outperforms state art pairwise matching method correspondence related tasks <eos> <eop> global patch collider <eos> paper proposes novel extremely efficient fully parallelizable task specific algorithm computation global point wise correspondences image video <eos> algorithm global patch collider based detecting unique collisions between image point using collection learned tree structures act conditional hash functions <eos> contrast conventional approaches rely pairwise distance computation algorithm isolates distinctive pixel pairs hit same leaf during traversal through multiple learned tree structures <eos> split functions stored intermediate nodes trees trained ensure only visually similar patches their geometric photometric transformed versions fall into same leaf node <eos> matching process involves passing all pixel positions image under analysis through tree structures <eos> then compute matches isolating point uniquely collide each other ie <eos> fell same empty leaf multiple trees <eos> algorithm linear number pixels but made constant time parallel computation architecture tree traversal individual image point decoupled <eos> demonstrate efficacy method using perform optical flow matching stereo matching some challenging benchmarks <eos> experimental result show only method extremely computationally efficient but also able match outperform state art method much more complex <eos> <eop> joint probabilistic matching using best solutions <eos> matching between two set object typically approached finding object pairs collectively maximize joint matching score <eos> paper argue single solution necessarily lead optimal matching accuracy general one one assignment problems improved considering multiple hypotheses before computing final similarity measure <eos> end propose utilize marginal distributions each entity <eos> previously idea neglected mainly because exact marginalization intractable due combinatorial number all possible matching permutations <eos> here propose generic approach efficiently approximate marginal distributions exploiting best solutions original problem <eos> approach only improves matching solution but also provides more accurate ranking result because extra information included marginal distribution <eos> validate claim two distinct objectives person re identification temporal matching modelled integer linear program ii feature point matching using quadratic cost function <eos> experiments confirm marginalization indeed leads superior performance compared single nearly optimal solution yielding state art result both applications standard benchmarks <eos> <eop> face alignment across large poses three dimensional solution <eos> face alignment fits face model image extracts semantic meanings facial pixels important topic cv community <eos> however most algorithms designed faces small medium poses below degree lacking ability align faces large pose up degree <eos> challenges three fold firstly commonly used landmark based face model assumes all landmarks visible therefore suitable profile views <eos> secondly face appearance varies more dramatically large poses ranging frontal view profile view <eos> thirdly labelling landmarks large poses extremely challenging work since invisible landmarks guessed <eos> paper propose solution three problems new alignment framework called three dimensional dense face alignment ddfa dense three dimensional face model fitted image via convolutional neutral network cnn <eos> also propose method synthesize large scale training sample profile views solve third problem data labelling <eos> experiments challenging aflw database show approach achieves significant improvements over state art method <eos> <eop> interactive segmentation rgbd image via cue selection <eos> interactive image segmentation important problem computer vision many applications including image editing object recognition image retrieval <eos> most existing interactive segmentation method only operate color image <eos> until recently very few works proposed leverage depth information low cost sensors improve interactive segmentation <eos> while method achieve better result than color based method they still limited either using depth additional color channel simply combining depth color linear way <eos> propose novel interactive segmentation algorithm incorporate multiple feature cues like color depth normals unified graph cut framework leverage cues more effectively <eos> key contribution method automatically selects single cue used each pixel based intuition only one cue necessary determine segmentation label locally <eos> achieved optimizing over both segmentation labels cue labels using terms designed decide both segmentation label cues should change <eos> algorithm thus produces only segmentation mask but also cue label map indicates each cue contributes final result <eos> extensive experiments five large scale rgbd datasets show proposed algorithm performs significantly better than both other color based rgbd based algorithms reducing amount user inputs well increasing segmentation accuracy <eos> <eop> layered scene decomposition via occlusion crf <eos> paper addresses challenging problem perceiving hidden occluded geometry scene depicted any given rgbd image <eos> unlike other image labeling problems such image segmentation each pixel needs assigned single label layered decomposition requires assign multiple labels pixels <eos> propose novel occlusion crf model allows integration sophisticated priors regularize solution space enables automatic inference layer decomposition <eos> use generalization fusion move algorithm perform maximum posterior map inference model handle large label set needed represent multiple surface assignments each pixel <eos> evaluated proposed model inference algorithm many rgbd image cluttered indoor scenes <eos> experiments show only model able explain occlusions but also enables automatic inpainting occluded invisible surfaces <eos> <eop> affinity cnn learning pixel centric pairwise relations figure ground embedding <eos> spectral embedding provides framework solving perceptual organization problems including image segmentation figure ground organization <eos> affinity matrix describing pairwise relationships between pixels clusters pixels into region using complex valued extension orders pixels according layer <eos> train convolutional neural network cnn directly predict pairwise relationships define affinity matrix <eos> spectral embedding then resolves predictions into globally consistent segmentation figure ground organization scene <eos> experiments demonstrate significant benefit direct coupling compared prior works use explicit intermediate stages such edge detection pathway image affinities <eos> result suggest spectral embedding powerful alternative conditional random field crf based globalization schemes typically coupled deep neural network <eos> <eop> weakly supervised object boundaries <eos> state art learning based boundary detection method require extensive training data <eos> since labelling object boundaries one most expensive types annotations there need relax requirement carefully annotate image make both training more affordable extend amount training data <eos> paper propose technique generate weakly supervised annotations show bounding box annotations alone suffice reach high quality object boundaries without using any object specific boundary annotations <eos> proposed weak supervision techniques achieve top performance object boundary detection task outperforming large margin current fully supervised state art method <eos> <eop> object contour detection fully convolutional encoder decoder network <eos> develop deep learning algorithm contour detection fully convolutional encoder decoder network <eos> different previous low level edge detection algorithm focuses detecting higher level object contours <eos> network trained end end pascal voc refined ground truth inaccurate polygon annotations yielding much higher precision object contour detection than previous method <eos> find learned model generalizes well unseen object classes same supercategories ms coco match state art edge detection bsds fine tuning <eos> combining multiscale combinatorial grouping algorithm method generate high quality segmented object proposals significantly advance state art pascal voc improving average recall <eos> relatively small amount candidates per image <eos> <eop> value explicit high level concepts vision language problems <eos> much recent progress vision language problems achieved through combination convolutional neural network cnn recurrent neural network rnns <eos> approach explicitly represent high level semantic concepts but rather seeks progress directly image feature text <eos> paper investigate whether direct approach succeeds due despite fact avoids explicit representation high level information <eos> propose method incorporating high level concepts into successful cnn rnn approach show achieves significant improvement state art both image captioning visual question answering <eos> also show same mechanism used introduce external semantic information doing so further improves performance <eos> achieve best reported result both image captioning vqa several benchmark datasets provide analysis value explicit high level concepts problems <eos> <eop> fast detection curved edges low snr <eos> detecting edges fundamental problem computer vision many applications some involving very noisy image <eos> while most edge detection method fast they perform well only relatively clean image <eos> unfortunately sophisticated method robust high levels noise quite slow <eos> paper develop novel multiscale method detect curved edges noisy image <eos> even though algorithm searches edges over exponentially large set candidate curves its runtime nearly linear total number image pixels <eos> demonstrate experimentally algorithm orders magnitude faster than previous method designed deal high noise levels <eos> same time obtains comparable often superior result existing method variety challenging noisy image <eos> <eop> object skeleton extraction natural image fusing scale associated deep side outputs <eos> object skeleton useful cue object detection complementary object contour provides structural representation describe relationship among object parts <eos> while object skeleton extraction natural image very challenging problem requires extractor able capture both local global image context determine intrinsic scale each skeleton pixel <eos> existing method rely per pixel based multi scale feature computation result difficult modeling high time consumption <eos> paper present fully convolutional network multiple scale associated side outputs address problem <eos> observing relationship between receptive field sizes sequential stages network skeleton scales they capture introduce scale associated side output each stage <eos> impose supervision different stages guiding scale associated side outputs toward groundtruth skeletons different scales <eos> responses multiple scale associated side outputs then fused scale specific way localize skeleton pixels multiple scales effectively <eos> method achieves promising result two skeleton extraction datasets significantly outperforms other competitors <eos> <eop> learning relaxed deep supervision better edge detection <eos> propose using relaxed deep supervision rds within convolutional neural network edge detection <eos> conventional deep supervision utilizes general ground truth guide intermediate predictions <eos> instead build hierarchical supervisory signals additional relaxed labels consider diversities deep neural network <eos> begin capturing relaxed labels simple detectors <eos> then merge them general ground truth generate rds <eos> finally employ rds supervise edge network following coarse fine paradigm <eos> relaxed labels seen some false positives difficult classified <eos> consider false positives supervision able achieve high performance better edge detection <eos> compensate lack training image capturing coarse edge annotations large dataset image segmentations pretrain model <eos> extensive experiments demonstrate approach achieves state art performance well known bsds dataset ods score <eos> obtains superior cross dataset generalization result nyud dataset <eos> <eop> occlusion boundary detection via deep exploration context <eos> occlusion boundaries contain rich perceptual information about underlying scene structure <eos> they also provide important cues many visual perception tasks such scene understanding object recognition segmentation <eos> paper improve occlusion boundary detection via enhanced exploration contextual information <eos> local structural boundary patterns observations surrounding region temporal context doing so develop novel approach based convolutional neural network cnn conditional random fields crfs <eos> experimental result demonstrate detector significantly outperforms state art <eos> commonly used cmu benchmark <eos> last but least empirically assess roles several important components proposed detector so validate rationale behind approach <eos> <eop> semicontour semi supervised learning approach contour detection <eos> supervised contour detection method usually require many labeled training image obtain satisfactory performance <eos> however large set annotated data might unavailable extremely labor intensive <eos> paper investigate usage semi supervised learning ssl obtain competitive detection accuracy very limited training data three labeled image <eos> specifically propose semi supervised structured ensemble learning approach contour detection built structured random forests srf <eos> allow srf applicable unlabeled data present effective sparse representation approach capture inherent structure image patches finding compact discriminative low dimensional subspace representation unsupervised manner enabling incorporation abundant unlabeled patches their estimated structured labels help srf perform better node splitting <eos> re examine role sparsity propose novel fast sparse coding algorithm boost overall learning efficiency <eos> best knowledge first attempt apply ssl contour detection <eos> extensive experiments bsds segmentation dataset nyu depth dataset demonstrate superiority proposed method <eos> <eop> learning localize little landmarks <eos> interact everyday tiny object such door handle car light switch room <eos> little landmarks barely visible hard localize image <eos> describe method find such landmarks finding sequence latent landmarks each prediction model <eos> each latent landmark predicts next sequence last localizes target landmark <eos> example find door handle car method learns start latent landmark near wheel globally distinctive subsequent latent landmarks use context earlier ones get closer target <eos> method supervised solely location little landmark displays strong performance more difficult variants established tasks two new tasks <eos> <eop> interactive inter layer activeness propagation <eos> increasing number computer vision tasks tackled deep feature intermediate outputs pre trained convolutional neural network <eos> despite astonishing performance deep feature extracted low level neurons still below satisfaction arguably because they cannot access spatial context contained higher layer <eos> paper present interactive novel algorithm computes activeness neurons network connections <eos> activeness propagated through neural network top down manner carrying high level context improving descriptive power low level mid level neurons <eos> visualization indicates neuron activeness interpreted spatial weighted neuron responses <eos> achieve state art classification performance wide range image datasets <eos> <eop> exploit bounding box annotations multi label object recognition <eos> convolutional neural network cnn shown great performance general feature representations object recognition applications <eos> however multi label image contain multiple object different categories scales locations global cnn feature optimal <eos> paper incorporate local information enhance feature discriminative power <eos> particular first extract object proposals each image <eos> each image treated bag object proposals extracted treated instances transform multi label recognition problem into multi class multi instance learning problem <eos> then addition extracting typical cnn feature representation each proposal propose make use ground truth bounding box annotations strong labels add another level local information using nearest neighbor relationships local region form multi view pipeline <eos> proposed multi view multi instance framework utilizes both weak strong labels effectively more importantly generalization ability even boost performance unseen categories partial strong labels other categories <eos> framework extensively compared state art hand crafted feature based method cnn based method two multi label benchmark datasets <eos> experimental result validate discriminative power generalization ability proposed framework <eos> strong labels framework able achieve state art result both datasets <eos> <eop> ti pooling transformation invariant pooling feature learning convolutional neural network <eos> paper present deep neural network topology incorporates simple implement transformation invariant pooling operator ti pooling <eos> operator able efficiently handle prior knowledge nuisance variations data such rotation scale changes <eos> most current method usually make use dataset augmentation address issue but requires larger number model parameters more training data result significantly increased training time larger chance under overfitting <eos> main reason drawbacks learned model needs capture adequate feature all possible transformations input <eos> other hand formulate feature convolutional neural network transformation invariant <eos> achieve using parallel siamese architectures considered transformation set applying ti pooling operator their outputs before fully connected layer <eos> show topology internally finds most optimal canonical instance input image training therefore limits redundancy learned feature <eos> more efficient use training data result better performance popular benchmark datasets smaller number parameters when comparing standard convolutional neural network dataset augmentation other baselines <eos> <eop> fashion style floats joint ranking classification using weak data feature extraction <eos> propose novel approach learning feature weakly supervised data joint ranking classification <eos> order exploit data weak labels jointly train feature extraction network ranking loss classification network cross entropy loss <eos> obtain high quality compact discriminative feature few parameters learned relatively small datasets without additional annotations <eos> enables tackle tasks specialized image very similar more generic ones existing fully supervised datasets <eos> show resulting feature combination linear classifier surpass state art hipster wars dataset despite using feature only <eos> proposed feature significantly outperform obtained network trained imagenet despite being times smaller single precision floats trained noisy weakly labeled data using only <eos> <eop> equiangular kernel dictionary learning applications dynamic texture analysis <eos> most existing dictionary learning algorithms consider linear sparse model often cannot effectively characterize nonlinear properties present many types visual data <eos> dynamic texture dt <eos> such nonlinear properties exploited so called kernel sparse coding <eos> paper proposed equiangular kernel dictionary learning method optimal mutual coherence exploit nonlinear sparsity high dimensional visual data <eos> two main issues addressed proposed method coding stability redundant dictionary infinite dimensional space computational efficiency computing kernel matrix training sample high dimensional data <eos> proposed kernel sparse coding method applied dynamic texture analysis both local dt pattern extraction global dt pattern characterization <eos> experimental result showed its performance gain over existing method <eos> <eop> compact bilinear pooling <eos> bilinear models shown achieve impressive performance wide range visual tasks such semantic segmentation fine grained recognition face recognition <eos> however bilinear feature high dimensional typically order hundreds thousands few million makes them impractical subsequent analysis <eos> propose two compact bilinear representations same discriminative power full bilinear representation but only few thousand dimensions <eos> compact representations allow back propagation classification errors enabling end end optimization visual recognition system <eos> compact bilinear representations derived through novel kernelized analysis bilinear pooling provide insights into discriminative power bilinear pooling platform further research compact pooling method <eos> experimentation illustrate utility proposed representations image classification few shot learning across several datasets <eos> <eop> accumulated stability voting robust descriptor descriptors multiple scales <eos> paper proposes novel local descriptor through accumulated stability voting asv <eos> stability feature dimensions measured their differences across scales <eos> more robust noise stability further quantized thresholding <eos> principle maximum entropy utilized determining best thresholds maximizing discriminant power resultant descriptor <eos> accumulating stability renders real valued descriptor converted into binary descriptor additional thresholding process <eos> real valued descriptor attains high matching accuracy while binary descriptor makes good compromise between storage accuracy <eos> descriptors simple yet effective easy implement <eos> addition descriptors require no training <eos> experiments popular benchmarks demonstrate effectiveness descriptors their superiority state art descriptors <eos> <eop> comal good feature match object boundaries <eos> traditional feature detectors trackers use information aggregation patches detect match discriminative patches <eos> however information remain same object boundaries when there object motion against significantly varying background <eos> paper propose new approach feature detection tracking re detection gives significantly improved result object boundaries <eos> utilize level lines iso intensity curves often remain stable reliably detected even object boundaries they often trace <eos> stable portions long level lines detected point high curvature detected such curves corner detection <eos> further level line used separate portions belonging two object then used robust matching such point <eos> while such comal corners maximally stable level line segments point were found much more reliable object boundary region they perform comparably interior region well <eos> illustrated exhaustive experiments real world datasets <eos> <eop> progressive feature matching alternate descriptor selection correspondence enrichment <eos> address two difficulties establishing accurate system image matching <eos> first image matching relies descriptor feature extraction but optimal descriptor often varies image image even patch patch <eos> second conventional matching approaches carry out geometric checking small set correspondence candidates due concern efficiency <eos> may result restricted performance recall <eos> aim tackling two issues integrating adaptive descriptor selection progressive candidate enrichment into image matching <eos> consider two integrated components complementary high quality matching yielded adaptively selected descriptors helps exploring more plausible candidates while enriched candidate set serves better reference descriptor selection <eos> motivates formulate image matching joint optimization problem adaptive descriptor selection progressive correspondence enrichment alternately conducted <eos> approach comprehensively evaluated compared state art approaches two benchmarks <eos> promising result manifest its effectiveness <eos> <eop> new finsler minimal path model curvature penalization image segmentation closed contour detection <eos> paper propose new curvature penalized minimal path model image segmentation via closed contour detection based weighted euler elastica curves firstly introduced field computer vision <eos> image segmentation method extracts collection curvature penalized minimal geodesics concatenated form closed contour connecting set user specified point <eos> globally optimal minimal paths computed solving eikonal equation <eos> first order pde traditionally regarded unable penalize curvature related path acceleration active contour models <eos> introduce here new approach enables finding global minimum geodesic energy including curvature term <eos> achieve through use novel finsler metric adding image domain orientation extra space dimension <eos> metric non riemannian asymmetric defined orientation lifted space incorporating curvature penalty geodesic energy <eos> experiments show proposed finsler minimal path model indeed outperforms state art minimal path models both synthetic real image <eos> <eop> scale aware alignment hierarchical image segmentation <eos> image segmentation key component many computer vision systems recovering prominent spot literature method improve overcome their limitations <eos> outputs most recent algorithms form hierarchical segmentation provides segmentation different scales single tree like structure <eos> commonly hierarchical method start some low level feature aware scale information different region them <eos> such one might need work many different levels hierarchy find object scene <eos> work tries modify existing hierarchical algorithm improving their alignment trying modify depth region tree better couple depth scale <eos> so first train regressor predict scale region using mid level feature <eos> then define anchor slice set region better balance between over segmentation under segmentation <eos> output method improved hierarchy re aligned anchor slice <eos> demonstrate power method perform comprehensive experiments show method post processing step significantly improve quality hierarchical segmentation representations ease usage hierarchical image segmentation high level vision tasks such object segmentation <eos> also prove improvement generalizes well across different algorithms datasets low computational cost <eos> <eop> deep interactive object selection <eos> interactive object selection very important research problem many applications <eos> previous algorithms require substantial user interactions estimate foreground background distributions <eos> paper present novel deep learning based algorithm much better understanding objectness reduce user interactions just few clicks <eos> algorithm transforms user provided positive negative clicks into two euclidean distance maps then concatenated rbg channels image compose image user interactions pairs <eos> generate many such pairs combining several random sampling strategies model users click patterns use them finetune deep fully convolutional network fcns <eos> finally output probability maps fcn model integrated graph cut optimization refine boundary segments <eos> model trained pascal segmentation dataset evaluated other datasets different object classes <eos> experimental result both seen unseen object clearly demonstrate algorithm good generalization ability superior all existing interactive object selection approaches <eos> <eop> pull plug predicting if computers humans should segment image <eos> foreground object segmentation critical step many image analysis tasks <eos> while automated method produce high quality result their failures disappoint users need practical solutions <eos> propose resource allocation framework predicting how best allocate fixed budget human annotation effort order collect higher quality segmentations given batch image automated method <eos> framework based proposed prediction module estimates quality given algorithm drawn segmentations <eos> demonstrate value framework two novel tasks related pulling plug computer human annotators <eos> specifically implement two systems automatically decide batch image when replace humans computers create coarse segmentations required initialize segmentation tools computers humans create final fine grained segmentations <eos> experiments demonstrate advantage relying mix human computer efforts over relying either resource alone segmenting object three diverse datasets representing visible phase contrast microscopy fluorescence microscopy image <eos> <eop> shadows shape priors shine using occlusion improve multi region segmentation <eos> present new algorithm multi region segmentation image object may partially occlude each other <eos> algorithm based observation human performance task based both prior knowledge about plausible shapes taking into account presence occluding object whose shape already known once occluded region identified shape prior used guess shape missing part <eos> capture former aspect using deep learning model shape latter simultaneously minimize energy all region consider only unoccluded pixels data agreement <eos> existing algorithms incorporating object shape priors consider every object separately turn distinguish genuine deviation expected shape parts missing due occlusion <eos> show method significantly improves performance representative algorithm evaluated both preprocessed natural synthetic image <eos> furthermore synthetic image recover ground truth segmentation good accuracy <eos> <eop> convexity shape constraints image segmentation <eos> segmenting image into multiple components central task computer vision <eos> many practical scenarios prior knowledge about plausible components available <eos> incorporating such prior knowledge into models algorithms image segmentation highly desirable yet non trivial <eos> work introduce new approach allows first time constrain some all components segmentation convex shapes <eos> specifically extend minimum cost multicut problem class constraints enforce convexity <eos> solve instances np hard integer linear program optimality separate proposed constraints branch cut loop state art ilp solver <eos> result photographs micrographs demonstrate effectiveness approach well its advantages over state art heuristic <eos> <eop> mcmc shape sampling image segmentation nonparametric shape priors <eos> segmenting image low quality missing data challenging problem <eos> integrating statistical prior information about shapes segmented improve segmentation result significantly <eos> most shape based segmentation algorithms optimize energy functional find point estimate object segmented <eos> provide measure degree confidence result neither provide picture other probable solutions based data priors <eos> statistical view addressing issues would involve problem characterizing posterior densities shapes object segmented <eos> such characterization propose markov chain monte carlo mcmc sampling based image segmentation algorithm uses statistical shape priors <eos> addition better characterization statistical structure problem such approach would also potential address issues getting stuck local optima suffered existing shape based segmentation method <eos> approach able characterize posterior probability density space shapes through its sample return multiple solutions potentially different modes multimodal probability density would encountered <eos> segmenting object multiple shape classes <eos> present promising result variety data set <eos> also provide extension segmenting shapes object parts go through independent shape variations <eos> extension involves use local shape priors object parts provides robustness limitations shape training data size <eos> <eop> noise modeling blind image denoising <eos> traditional image denoising algorithms always assume noise homogeneous white gaussian distributed <eos> however noise real image much more complex empirically <eos> paper addresses problem proposes novel blind image denoising algorithm cope real world noisy image even when noise model provided <eos> realized modeling image noise mixture gaussian distribution mog approximate large varieties continuous distributions <eos> number components mog unknown practically work adopts bayesian nonparametric technique proposes novel low rank mog filter lr mog recover clean signals patches noisy ones contaminated mog noise <eos> based lr mog novel blind image denoising approach developed <eos> test proposed method study conducts extensive experiments synthesis real image <eos> method achieves state art performance consistently <eos> <eop> efficient robust color consistency community photo collections <eos> present efficient technique optimize color consistency collection image depicting common scene <eos> method first recovers sparse pixel correspondences input image stacks them into matrix many missing entries <eos> show matrix satisfies rank two constraint under simple color correction model <eos> parameters viewed pseudo white balance gamma correction parameters each input image <eos> present robust low rank matrix factorization method estimate unknown parameters model <eos> using them improve color consistency input image perform color transfer any input image source <eos> approach insensitive outliers pixel correspondences thereby precluding need complex pre processing steps <eos> demonstrate high quality color consistency result large photo collections popular tourist landmarks personal photo collections containing image people <eos> <eop> needle match reliable patch matching under high uncertainty <eos> reliable patch matching forms basis many algorithms super resolution denoising inpainting etc <eos> however when image quality deteriorates noise blur geometric distortions reliability patch matching deteriorates well <eos> matched patches degraded image necessarily imply similarity underlying patches unknown high quality image <eos> restricts applicability patch based method <eos> paper present patch representation called needle consists small multi scale versions patch its immediate surrounding region <eos> while patch finest image scale severely degraded degradation decreases dramatically coarser needle scales revealing reliable information matching <eos> show needle robust many types image degradations leads matches faithful underlying high quality patches improvement existing patch based method <eos> <eop> reconnet non iterative reconstruction image compressively sensed measurements <eos> goal paper present non iterative more importantly extremely fast algorithm reconstruct image compressively sensed cs random measurements <eos> end propose novel convolutional neural network cnn architecture takes cs measurements image input outputs intermediate reconstruction <eos> call network reconnet <eos> intermediate reconstruction fed into off shelf denoiser obtain final reconstructed image <eos> standard dataset image show significant improvements reconstruction result both terms psnr time complexity over state art iterative cs reconstruction algorithms various measurement rates <eos> further through qualitative experiments real data collected using block spc single pixel camera show network highly robust sensor noise recover visually better quality image than competitive algorithms extremely low sensing rates <eos> demonstrate algorithm recover semantically informative image even low measurement rate <eos> present very robust proof concept real time visual tracking application <eos> <eop> soft segmentation guided object motion deblurring <eos> object motion blur challenging problem foreground background scenes undergo different types image degradation due movements various directions speed <eos> most object motion deblurring method address problem segmenting blurred image into region different kernels estimated applied restoration <eos> segmentation blurred image difficult due ambiguous pixels between region but plays important role object motion deblurring <eos> address problems propose novel model object motion deblurring <eos> proposed model developed based maximum posterior formulation soft segmentation incorporated object layer estimation <eos> propose efficient algorithm jointly estimate object segmentation camera motion each layer deblurred well under guidance soft segmentation <eos> experimental result demonstrate proposed algorithm performs favorably against state art object motion deblurring method challenging scenarios <eos> <eop> two illuminant estimation user correction preference <eos> paper examines problem white balance correction when scene contains two illuminations <eos> two step process estimate two illuminants correct image <eos> existing method attempt estimate spatially varying illumination map however result error prone resulting illumination maps too low resolution used proper spatially varying white balance correction <eos> addition spatially varying nature method make them computationally intensive <eos> show problem effectively addressed attempting obtain spatially varying illumination map but instead performing illumination estimation large sub region image <eos> approach able detect when distinct illuminations present image accurately measure illuminants <eos> since proposed strategy suitable spatially varying image correction user study performed see if there preference how image should corrected when two illuminants present but only global correction applied <eos> user study shows when illuminations distinct there preference outdoor illumination corrected resulting warmer final result <eos> use collective findings demonstrate effective two illuminant estimation scheme produces corrected image users prefer <eos> <eop> deep contrast learning salient object detection <eos> salient object detection recently witnessed substantial progress due powerful feature extracted using deep convolutional neural network cnn <eos> however existing cnn based method operate patch level instead pixel level <eos> resulting saliency maps typically blurry especially near boundary salient object <eos> furthermore image patches treated independent sample even when they overlapping giving rise significant redundancy computation storage <eos> paper propose end end deep contrast network overcome aforementioned limitations <eos> deep network consists two complementary components pixel level fully convolutional stream segment wise spatial pooling stream <eos> first stream directly produces saliency map pixel level accuracy input image <eos> second stream extracts segment wise feature very efficiently better models saliency discontinuities along object boundaries <eos> finally fully connected crf model optionally incorporated improve spatial coherence contour localization fused result two streams <eos> experimental result demonstrate deep model significantly improves state art <eos> <eop> multiview image completion space structure propagation <eos> present multiview image completion method provides geometric consistency among different views propagating space structures <eos> since user specifies region completed one multiview photographs casually taken scene proposed method enables complete set photographs geometric consistency creating removing structures specified region <eos> proposed method incorporates photographs estimate dense depth maps <eos> initially complete color well depth view then facilitate two stages structure propagation structure guided completion <eos> structure propagation optimizes space topology scene across photographs while structure guide completion enhances completes local image structure both depth color multiple photographs structural coherence searching nearest neighbor fields relevant views <eos> demonstrate effectiveness proposed method completing multiview image <eos> <eop> composition preserving deep photo aesthetics assessment <eos> photo aesthetics assessment challenging <eos> deep convolutional neural network convnet method recently shown promising result aesthetics assessment <eos> performance deep convnet method however often compromised constraint neural network only takes fixed size input <eos> accommodate requirement input image need transformed via cropping scaling padding often damages image composition reduces image resolution causes image distortion thus compromising aesthetics original image <eos> paper present composition preserving deep convnet method directly learns aesthetics feature original input image without any image transformations <eos> specifically method adds adaptive spatial pooling layer upon regular convolution pooling layer directly handle input image original sizes aspect ratios <eos> allow multi scale feature extraction develop multi net adaptive spatial pooling convnet architecture consists multiple sub network different adaptive spatial pooling sizes leverage scene based aggregation layer effectively combine predictions multiple sub network <eos> experiments large scale aesthetics assessment benchmark ava demonstrate method significantly improve state art result photo aesthetics assessment <eos> <eop> automatic image cropping computational complexity study <eos> attention based automatic image cropping aims preserving most visually important region image <eos> common task kind method search smallest rectangle inside summed attention maximized <eos> demonstrate under appropriate formulations task achieved using efficient algorithms low computational complexity <eos> practically useful scenario aspect ratio cropping rectangle given problem solved computational complexity linear number image pixels <eos> also study possibility multiple rectangle cropping new model facilitating fully automated image cropping <eos> <eop> deeper look saliency feature contrast semantics beyond <eos> paper consider problem visual saliency modeling including both human gaze prediction salient object segmentation <eos> overarching goal paper identify high level considerations relevant deriving more sophisticated visual saliency models <eos> deep learning model based fully convolutional network fcns presented shows very favorable performance across wide variety benchmarks relative existing proposals <eos> also demonstrate manner training data selected ground truth treated critical resulting model behaviour <eos> recent efforts explored relationship between human gaze salient object also examine point further context fcns <eos> close examination proposed alternative models serves vehicle identifying problems important developing more comprehensive models going forward <eos> <eop> spatially binned roc comprehensive saliency metric <eos> recent trend saliency algorithm development large scale benchmarking algorithm ranking ground truth provided datasets human fixations <eos> order accommodate strong bias humans toward central fixations common replace traditional roc metrics shuffled roc metric uses randomly sampled fixations other image database negative set <eos> however shuffled roc introduces number problematic elements including fundamental assumption possible separate visual salience image spatial arrangement <eos> argue more informative directly measure effect spatial bias algorithm performance rather than try correct <eos> capture quantify known sources bias propose novel metric measuring saliency algorithm performance spatially binned roc sproc <eos> metric provides direct insight into spatial biases saliency algorithm without sacrificing intuitive raw performance evaluation traditional roc measurements <eos> quantitatively measuring bias saliency algorithms researchers will better equipped select optimize most appropriate algorithm given task <eos> use baseline measure inherent algorithm bias show adaptive whitening saliency aws attention information maximization aim dynamic visual attention dva provide least spatially biased result suiting them tasks there no information about underlying spatial bias stimuli whereas algorithms such graph based visual saliency gbvs context aware saliency cas significant inherent central bias <eos> <eop> grab visual saliency via novel graph model background priors <eos> propose unsupervised bottom up saliency detection approach exploiting novel graph structure background priors <eos> input image represented undirected graph superpixels nodes <eos> feature vectors extracted each node cover regional color contrast texture information <eos> novel graph model proposed effectively capture local global saliency cues <eos> obtain more accurate saliency estimations optimize saliency map using robust background measure <eos> comprehensive evaluations benchmark datasets indicate algorithm universally surpasses state art unsupervised solutions performs favorably against supervised approaches <eos> <eop> predicting when saliency maps accurate eye fixations consistent <eos> many computational models visual attention use image feature machine learning techniques predict eye fixation locations saliency maps <eos> recently success deep convolutional neural network dcnns object recognition opened new avenue computational models visual attention due tight link between visual attention object recognition <eos> paper show using feature dcnns object recognition make predictions enrich information provided saliency models <eos> namely estimate reliability saliency model raw image serves meta saliency measure may used select best saliency algorithm image <eos> analogously consistency eye fixations among subjects <eos> agreement between eye fixation locations different subjects also predicted used designer assess whether subjects reach consensus about salient image locations <eos> <eop> split match example based adaptive patch sampling unsupervised style transfer <eos> paper presents novel unsupervised method transfer style example image source image <eos> complex notion image style here considered local texture transfer eventually coupled global color transfer <eos> local texture transfer propose new method based adaptive patch partition captures style example image preserves structure source image <eos> more precisely example based partition predicts how well source patch matches example patch <eos> result various image show method outperforms most recent techniques <eos> <eop> detection accurate localization circular fiducials under highly challenging conditions <eos> using fiducial markers ensures reliable detection identification planar feature image <eos> fiducials used wide range applications especially when reliable visual reference needed <eos> track camera cluttered textureless environments <eos> marker designed such applications must robust partial occlusions varying distances angles view fast camera motions <eos> paper present robust highly accurate fiducial system whose markers consist concentric rings along its theoretical foundations <eos> relying projective properties allows robustly localize imaged marker accurately detect position image common circle center <eos> demonstrate system detect accurately localize circular fiducials under very challenging conditions experimental result reveal outperforms other recent fiducial systems <eos> <eop> scene recognition cnn object scales dataset bias <eos> since scenes composed part object accurate recognition scenes requires knowledge about both scenes object <eos> paper address two related problems scale induced dataset bias multi scale convolutional neural network cnn architectures how combine effectively scene centric object centric knowledge <eos> places imagenet cnn <eos> earlier attempt hybrid cnn showed incorporating imagenet did help much <eos> here propose alternative method taking scale into account resulting significant recognition gains <eos> analyzing response imagenet cnn places cnn different scales find both operate different scale ranges so using same network all scales induces dataset bias resulting limited performance <eos> thus adapting feature extractor each particular scale <eos> scale specific cnn crucial improve recognition since object scenes their specific range scales <eos> experimental result show recognition accuracy highly depends scale simple yet carefully chosen multi scale combinations imagenet cnn places cnn push state art recognition accuracy sun up <eos> deeper architectures comparable human performance <eos> <eop> learning action maps large environments via first person vision <eos> when people observe interact physical spaces they able associate functionality region environment <eos> goal automate functional understanding large spaces leveraging activity demonstrations recorded ego centric viewpoint <eos> method describe enables functionality estimation both large scenes people behaved well novel scenes no behaviors available <eos> method learns predicts action maps encode ability user perform activities various locations <eos> usage egocentric camera observe demonstrations method scales size scene without need mounting multiple static surveillance cameras well suited task observing activities up close <eos> demonstrate capturing appearance based attributes environment associating attributes activity demonstrations mathematical framework allows prediction action maps new environments <eos> additionally take preliminary look breadth applicability action maps demonstrating proof concept application they used concert activity detections perform localization <eos> <eop> single image crowd counting via multi column convolutional neural network <eos> paper aims develop method accurately estimate crowd count individual image arbitrary crowd density arbitrary perspective <eos> end proposed simple but effective multi column convolutional neural network mcnn architecture map image its crowd density map <eos> proposed mcnn allows input image arbitrary size resolution <eos> utilizing filters receptive fields different sizes feature learned each column cnn adaptive variations people head size due perspective effect image resolution <eos> furthermore true density map computed accurately based geometry adaptive kernels need knowing perspective map input image <eos> since exiting crowd counting datasets adequately cover all challenging situations considered work collected labelled large new dataset includes image about heads annotated <eos> challenging new dataset well all existing datasets conduct extensive experiments verify effectiveness proposed model method <eos> particular proposed simple mcnn model method outperforms all existing method <eos> addition experiments show model once trained one dataset readily transferred new dataset <eos> <eop> shallow deep convolutional network saliency prediction <eos> prediction salient areas image traditionally addressed hand crafted feature based neuroscience principles <eos> paper however addresses problem completely data driven approach training convolutional neural network convnet <eos> learning process formulated minimization loss function measures euclidean distance predicted saliency map provided ground truth <eos> recent publication large datasets saliency prediction provided enough data train end end architectures both fast accurate <eos> two designs proposed shallow convnet trained scratch another deeper solution whose first three layer adapted another network trained classification <eos> authors knowledge first end end cnn trained tested purpose saliency prediction <eos> <eop> sample filter nonparametric scene parsing via efficient filtering <eos> scene parsing attracted lot attention computer vision <eos> while parametric models proven effective task they cannot easily incorporate new training data <eos> contrast nonparametric approaches bypass any learning phase directly transfer labels training data query image readily exploit new labeled sample they become available <eos> unfortunately because computational cost their label transfer procedures state art nonparametric method typically filter out most training image only keep few relevant ones label query <eos> such method throw away many image still contain valuable information generally obtain unbalanced set labeled sample <eos> paper introduce nonparametric approach scene parsing follows sample filter strategy <eos> more specifically propose sample labeled superpixels according image similarity score allows obtain balanced set sample <eos> then formulate label transfer efficient filtering procedure lets exploit more labeled sample than existing techniques <eos> experiments evidence benefits approach over state art nonparametric method two benchmark datasets <eos> <eop> delay robust spatial layout estimation cluttered indoor scenes <eos> consider problem estimating spatial layout indoor scene monocular rgb image modeled projection three dimensional cuboid <eos> existing solutions problem often rely strongly hand engineered feature vanishing point detection prone failure presence clutter <eos> paper present method uses fully convolutional neural network fcnn conjunction novel optimization framework generating layout estimates <eos> demonstrate method robust presence clutter handles wide range highly challenging scenes <eos> evaluate method two standard benchmarks show achieves state art result outperforming previous method wide margin <eos> <eop> text detection system natural scenes convolutional feature learning cascaded classification <eos> propose system finds text natural scenes using variety cues <eos> novel data driven method incorporates coarse fine detection character pixels using convolutional feature text conv followed extracting connected components ccs characters using edge color feature finally performing graph based segmentation ccs into words word graph <eos> text conv initial detection based convolutional feature maps similar used convolutional neural network cnn but learned using convolutional means <eos> convolution masks defined local neighboring patch feature used improve detection accuracy <eos> word graph algorithm uses contextual information both improve word segmentation prune false character word detections <eos> different definitions foreground text region used train detection stages some based bounding box intersection others bounding box pixel intersection <eos> system obtains pixel character word detection measures <eos> respectively icdar robust reading focused scene text dataset out performing state art systems <eos> approach may work other detection targets homogenous color natural scenes <eos> <eop> reversible recursive instance level object segmentation <eos> work propose novel reversible recursive instance level object segmentation ios framework address challenging instance level object segmentation task <eos> ios consists reversible proposal refinement sub network predicts bounding box offsets refining object proposal locations instance level segmentation sub network generates foreground mask dominant object instance each proposal <eos> being recursive ios iteratively optimizes two sub network during joint training refined object proposals improved segmentation predictions alternately fed into each other progressively increase network capabilities <eos> being reversible proposal refinement sub network adaptively determines optimal number refinement iterations required each proposal during both training testing <eos> furthermore handle multiple overlapped instances within proposal instance aware denoising autoencoder introduced into segmentation sub network distinguish dominant object other distracting instances <eos> extensive experiments challenging pascal voc benchmark well demonstrate superiority ios over other state art method <eos> particular ap over classes <eos> significantly outperforms result <eos> <eop> coherent parametric contours interactive video object segmentation <eos> interactive video segmentation systems aim producing sub pixel level object boundaries visual effect applications <eos> recent approaches mainly focus using sparse user input <eos> scribbles efficient segmentation however quality final object boundaries satisfactory following reasons boundary each frame often accurate boundaries across adjacent frames wiggle around inconsistently causing temporal flickering there lack direct user control fine tuning <eos> propose coherent parametric contours novel video segmentation propagation framework addresses all above issues <eos> approach directly models object boundary using set parametric curves providing direct user controls manual adjustment <eos> spatio temporal optimization algorithm employed produce object boundaries spatially accurate temporally stable <eos> show existing evaluation datasets limited demonstrate new set cover common cases professional rotoscoping <eos> new metric evaluating temporal consistency proposed <eos> result show approach generates higher quality more coherent segmentation result than previous method <eos> <eop> manifold slic fast method compute content sensitive superpixels <eos> superpixels perceptually meaningful atomic region effectively capture image feature <eos> among various method computing uniform superpixels simple linear iterative clustering slic popular due its simplicity high performance <eos> paper extend slic compute content sensitive superpixels <eos> small superpixels content dense region <eos> high intensity color variation large superpixels content sparse region <eos> rather than conventional slic method clusters pixels map image dimensional manifold whose area elements good measure content density <eos> propose efficient method compute restricted centroidal voronoi tessellation rcvt uniform tessellation induces content sensitive superpixels <eos> unlike other algorithms characterize content sensitivity geodesic distances manifold slic tackles problem measuring areas voronoi cells computed very low cost <eos> result runs times faster than state art content sensitive superpixels algorithm <eos> evaluate manifold slic seven representative method bsds benchmark observe method outperforms existing method <eos> <eop> deep saliency encoded low level distance map high level feature <eos> recent advances saliency detection utilized deep learning obtain high level feature detect salient region scene <eos> they demonstrated superior result over previous works utilize hand crafted low level feature saliency detection <eos> paper demonstrate hand crafted feature provide complementary effects enhance performance saliency detection utilizes only high level feature <eos> method utilizes both high level low level feature saliency detection under unified deep learning framework <eos> high level feature extracted using vgg net low level feature compared other parts image form low level distance map <eos> low level distance map then encoded using cnn multiple convolutional relu layer <eos> concatenate encoded low level distance map high level feature connect them fully connected neural network classifier evaluate saliency query region <eos> experiments show method further improve performance state art deep learning based saliency detection method <eos> <eop> instance level segmentation autonomous driving deep densely connected mrfs <eos> aim provide pixel wise instance level labeling monocular image context autonomous driving <eos> build recent work zhang <eos> iccv trained convolutional neural net predict instance labeling local image patches extracted exhaustively stride image <eos> simple markov random field model using several heuristics was then proposed zhang <eos> iccv derive globally consistent instance labeling image <eos> paper formulate global labeling problem novel densely connected markov random field show how encode various intuitive potentials way amenable efficient mean field inference krahenbuhl <eos> potentials encode compatibility between global labeling patch level predictions contrast sensitive smoothness well fact separate region form different instances <eos> experiments challenging kitti benchmark geiger <eos> cvpr demonstrate method achieves significant performance boost over baseline zhang <eos> <eop> dhsnet deep hierarchical saliency network salient object detection <eos> traditional salient object detection models often use hand crafted feature formulate contrast various prior knowledge then combine them artificially <eos> work propose novel end end deep hierarchical saliency network dhsnet based convolutional neural network detecting salient object <eos> dhsnet first makes coarse global prediction automatically learning various global structured saliency cues including global contrast objectness compactness their optimal combination <eos> then novel hierarchical recurrent convolutional neural network hrcnn adopted further hierarchically progressively refine details saliency maps step step via integrating local context information <eos> whole architecture works global local coarse fine manner <eos> dhsnet directly trained using whole image corresponding ground truth saliency masks <eos> when testing saliency maps generated directly efficiently feedforwarding testing image through network without relying any other techniques <eos> evaluations four benchmark datasets comparisons other state art algorithms demonstrate dhsnet only shows its significant superiority terms performance but also achieves real time speed fps modern gpus <eos> <eop> object co segmentation via graph optimized flexible manifold ranking <eos> aiming automatically discovering common object contained set relevant image segmenting them foreground simultaneously object co segmentation become active research topic recent years <eos> although number approaches proposed address problem many them designed misleading assumption unscalable prior low flexibility thus still suffer certain limitations reduces their capability real world scenarios <eos> alleviate limitations propose novel two stage co segmentation framework introduces weak background prior establish globally close loop graph represent common object union background separately <eos> then novel graph optimized flexible manifold ranking algorithm proposed flexibly optimize graph connection node labels co segment common object <eos> experiments three image datasets demonstrate method outperforms other state art method <eos> <eop> primary object segmentation video via alternate convex optimization foreground background distributions <eos> unsupervised video object segmentation algorithm discovers primary object video sequence automatically proposed work <eos> introduce three energies terms foreground background probability distributions markov spatiotemporal antagonistic energies <eos> then minimize hybrid three energies separate primary object its background <eos> however hybrid energy nonconvex <eos> therefore develop alternate convex optimization aco scheme decomposes nonconvex optimization into two quadratic programs <eos> moreover propose forward backward strategy performs segmentation sequentially first last frames then vice versa exploit temporal correlations <eos> experimental result extensive datasets demonstrate proposed aco algorithm outperforms state art techniques significantly <eos> <eop> automatic fence segmentation video dynamic scenes <eos> present fully automatic approach detect segment fence like occluders video clip <eos> unlike previous approaches usually assume either static scenes cameras method capable handling both dynamic scenes moving cameras <eos> under bottom up framework first clusters pixels into coherent groups using color motion feature <eos> pixel groups then analyzed fully connected graph labeled either fence non fence using graph cut optimization <eos> finally solve dense conditional random filed crf constructed multiple frames enhance both spatial accuracy temporal coherence segmentation <eos> once segmented one use existing hole filling method generate fence free output <eos> extensive evaluation suggests method outperforms previous automatic interactive approaches complex examples captured mobile devices <eos> <eop> discovering physical parts articulated object class multiple video <eos> propose motion based method discover physical parts articulated object class <eos> head torso leg horse multiple video <eos> key find object region exhibit consistent motion relative rest object across multiple video <eos> then learn location model parts segment them accurately individual video using energy function also enforces temporal spatial consistency part motion <eos> unlike approach traditional method motion segmentation non rigid structure motion operate one video time <eos> hence they cannot discover part unless displays independent motion particular video <eos> evaluate method new dataset video tigers horses significantly outperform recent motion segmentation method task part discovery obtaining roughly twice accuracy <eos> <eop> benchmark dataset evaluation methodology video object segmentation <eos> over years datasets benchmarks proven their fundamental importance computer vision research enabling targeted progress objective comparisons many fields <eos> same time legacy datasets may impend evolution field due saturated algorithm performance lack contemporary high quality data <eos> work present new benchmark dataset evaluation methodology area video object segmentation <eos> dataset named davis densely annotated video segmentation consists fifty high quality full hd video sequences spanning multiple occurrences common video object segmentation challenges such occlusions motion blur appearance changes <eos> each video accompanied densely annotated pixel accurate per frame ground truth segmentation <eos> addition provide comprehensive analysis several state art segmentation approaches using three complementary metrics measure spatial extent segmentation accuracy silhouette contours temporal coherence <eos> result uncover strengths weaknesses current approaches opening up promising directions future works <eos> <eop> learning temporal regularity video sequences <eos> perceiving meaningful activities long video sequence challenging problem due ambiguous definition meaningfulness well clutters scene <eos> approach problem learning generative model regular motion patterns termed regularity using multiple sources very limited supervision <eos> specifically propose two method built upon autoencoders their ability work little no supervision <eos> first leverage conventional handcrafted spatio temporal local feature learn fully connected autoencoder them <eos> second build fully convolutional feed forward autoencoder learn both local feature classifiers end end learning framework <eos> model capture regularities multiple datasets <eos> evaluate method both qualitative quantitative ways showing learned regularity video various aspects demonstrating competitive performance anomaly detection datasets application <eos> <eop> bilateral space video segmentation <eos> work propose novel approach video segmentation operates bilateral space <eos> design new energy vertices regularly sampled spatio temporal bilateral grid solved efficiently using standard graph cut label assignment <eos> using bilateral formulation energy minimize implicitly approximates long range spatio temporal connections between pixels while still containing only small number variables only local graph edges <eos> compare number recent method show approach achieves state art result multiple benchmarks fraction runtime <eos> furthermore method scales linearly image size allowing interactive feedback real world high resolution video <eos> <eop> red sfa relation discovery based slow feature analysis trajectory clustering <eos> spectral embedding clustering still open problem how construct relation graph reflect intrinsic structures data <eos> paper proposed approach named relation discovery based slow feature analysis red sfa feature learning graph construction simultaneously <eos> given initial graph only few nearest but most reliable pairwise relations new reliable relations discovered assumption reliability preservation <eos> reliable relations will preserve their reliabilities learnt projection subspace <eos> formulate idea cross entropy ce minimization problem reduce discrepancy between two bernoulli distributions parameterized updated distances existing relation graph respectively <eos> furthermore overcome imbalanced distribution sample boosting like strategy proposed balance discovered relations over all clusters <eos> evaluate proposed method extensive experiments performed various trajectory clustering tasks including motion segmentation time series clustering crowd detection <eos> result demonstrate red sfa discover reliable intra cluster relations high precision competitive clustering performance achieved comparison state art <eos> <eop> training region based object detectors online hard example mining <eos> field object detection made significant advances riding wave region based convnets but their training procedure still includes many heuristics hyperparameters costly tune <eos> present simple yet surprisingly effective online hard example mining ohem algorithm training region based convnet detectors <eos> motivation same always detection datasets contain overwhelming number easy examples small number hard examples <eos> automatic selection hard examples make training more effective efficient <eos> ohem simple intuitive algorithm eliminates several heuristics hyperparameters common use <eos> but more importantly yields consistent significant boosts detection performance benchmarks like pascal voc <eos> its effectiveness increases datasets become larger more difficult demonstrated result ms coco dataset <eos> moreover combined complementary advances field ohem leads state art result <eos> map pascal voc respectively <eos> <eop> deep residual learning image recognition <eos> deeper neural network more difficult train <eos> present residual learning framework ease training network substantially deeper than used previously <eos> explicitly reformulate layer learning residual functions reference layer inputs instead learning unreferenced functions <eos> provide comprehensive empirical evidence showing residual network easier optimize gain accuracy considerably increased depth <eos> imagenet dataset evaluate residual nets depth up layer deeper than vgg nets but still having lower complexity <eos> ensemble residual nets achieves <eos> error imagenet test set <eos> result won st place ilsvrc classification task <eos> also present analysis cifar layer <eos> depth representations central importance many visual recognition tasks <eos> solely due extremely deep representations obtain relative improvement coco object detection dataset <eos> deep residual nets foundations submissions ilsvrc coco competitions also won st places tasks imagenet detection imagenet localization coco detection coco segmentation <eos> <eop> you only look once unified real time object detection <eos> present yolo new approach object detection <eos> prior work object detection repurposes classifiers perform detection <eos> instead frame object detection regression problem spatially separated bounding boxes associated class probabilities <eos> single neural network predicts bounding boxes class probabilities directly full image one evaluation <eos> since whole detection pipeline single network optimized end end directly detection performance <eos> unified architecture extremely fast <eos> base yolo model processes image real time frames per second <eos> smaller version network fast yolo processes astounding frames per second while still achieving double map other real time detectors <eos> compared state art detection systems yolo makes more localization errors but less likely predict false positives background <eos> finally yolo learns very general representations object <eos> outperforms other detection method including dpm cnn when generalizing natural image other domains like artwork <eos> <eop> locnet improving localization accuracy object detection <eos> propose novel object localization methodology purpose boosting localization accuracy state art object detection systems <eos> model given search region aims returning bounding box object interest inside region <eos> accomplish its goal relies assigning conditional probabilities each row column region probabilities provide useful information regarding location boundaries object inside search region allow accurate inference object bounding box under simple probabilistic framework <eos> implementing localization model make use convolutional neural network architecture properly adapted task called locnet <eos> show experimentally locnet achieves very significant improvement map high iou thresholds pascal voc test set very easily coupled recent state art object detection systems helping them boost their performance <eos> finally demonstrate detection approach achieve high detection accuracy even when given input set sliding windows thus proving independent box proposal method <eos> <eop> sketch me shoe <eos> investigate problem fine grained sketch based image retrieval sbir free hand human sketches used queries perform instance level retrieval image <eos> extremely challenging task because visual comparisons only need fine grained but also executed cross domain ii free hand finger sketches highly abstract making fine grained matching harder most importantly iii annotated cross domain sketch photo datasets required training scarce challenging many state art machine learning techniques <eos> paper first time address all challenges providing step towards capabilities would underpin commercial sketch based image retrieval application <eos> introduce new database sketch photo pairs two categories fine grained triplet ranking annotations <eos> then develop deep triplet ranking model instance level sbir novel data augmentation staged pre training strategy alleviate issue insufficient fine grained training data <eos> extensive experiments carried out contribute variety insights into challenges data sufficiency over fitting avoidance when training deep network fine grained cross domain ranking tasks <eos> <eop> deep sliding shapes amodal three dimensional object detection rgb image <eos> focus task amodal three dimensional object detection rgb image aims produce three dimensional bounding box object metric form its full extent <eos> introduce deep sliding shapes three dimensional convnet formulation takes three dimensional volumetric scene rgb image input outputs three dimensional object bounding boxes <eos> approach propose first three dimensional region proposal network rpn learn objectness geometric shapes first joint object recognition network orn extract geometric feature three dimensional color feature <eos> particular handle object various sizes training amodal rpn two different scales orn regress three dimensional bounding boxes <eos> experiments show algorithm outperforms state art <eos> map faster than original sliding shapes <eos> <eop> object detection video tubelets convolutional neural network <eos> deep convolution neural network cnn shown impressive performance various vision tasks such image classification object detection semantic segmentation <eos> object detection particularly still image performance significantly increased last year thanks powerful deep network <eos> googlenet detection frameworks <eos> region cnn feature cnn <eos> lately introduced imagenet task object detection video vid brings object detection task into video domain object locations each frame required annotated bounding boxes <eos> work introduce complete framework vid task based still image object detection general object tracking <eos> their relations contributions vid task thoroughly studied evaluated <eos> addition temporal convolution network proposed incorporate temporal information regularize detection result shows its effectiveness task <eos> <eop> learning side information through modality hallucination <eos> present modality hallucination architecture training rgb object detection model incorporates depth side information training time <eos> convolutional hallucination network learns new complementary rgb image representation taught mimic convolutional mid level feature depth network <eos> test time image processed jointly through rgb hallucination network produce improved detection performance <eos> thus method transfers information commonly extracted depth training data network extract information rgb counterpart <eos> present result standard nyudv dataset report improvement rgb detection task <eos> <eop> object proposal evaluation protocol gameable <eos> object proposals quickly become de facto pre processing step number vision pipelines object detection object discovery other tasks <eos> their performance usually evaluated partially annotated datasets <eos> paper argue choice using partially annotated dataset evaluation object proposals problematic demonstrate via thought experiment evaluation protocol gameable sense progress under protocol necessarily correspond better category independent object proposal algorithm <eos> alleviate problem introduce nearly fully annotated version pascal voc dataset serves test bed check if object proposal techniques overfitting particular list categories <eos> perform exhaustive evaluation object proposal method introduced nearly fully annotated pascal dataset perform cross dataset generalization experiments introduce diagnostic experiment detect bias capacity object proposal algorithm <eos> tool circumvents need collect densely annotated dataset expensive cumbersome collect <eos> finally released easy use toolbox combines various publicly available implementations object proposal algorithms standardizes proposal generation evaluation so new method added evaluated different datasets <eos> hope result presented paper will motivate community test category independence various object proposal method carefully choosing evaluation protocol <eos> <eop> hypernet towards accurate region proposal generation joint object detection <eos> almost all current top performing object detection network employ region proposals guide search object instances <eos> state art region proposal method usually need several thousand proposals get high recall thus hurting detection efficiency <eos> although latest region proposal network method gets promising detection accuracy several hundred proposals still struggles small size object detection precise localization <eos> large iou thresholds mainly due coarseness its feature maps <eos> paper present deep hierarchical network namely hypernet handling region proposal generation object detection jointly <eos> hypernet primarily based elaborately designed hyper feature aggregates hierarchical feature maps first then compresses them into uniform space <eos> hyper feature well incorporate deep but highly semantic intermediate but really complementary shallow but naturally high resolution feature image thus enabling construct hypernet sharing them both generating proposals detecting object via end end joint training strategy <eos> deep vgg model method achieves completely leading recall state art object detection accuracy pascal voc using only proposals per image <eos> runs speed fps including all steps gpu thus having potential real time processing <eos> <eop> don need no bounding boxes training object class detectors using only human verification <eos> training object class detectors typically requires large set image object annotated bounding boxes <eos> however manually drawing bounding boxes very time consuming <eos> propose new scheme training object detectors only requires annotators verify bounding boxes produced automatically learning algorithm <eos> scheme iterates between re training detector re localizing object training image human verification <eos> use verification signal both improve re training reduce search space re localisation makes steps different normally done weakly supervised setting <eos> extensive experiments pascal voc show using human verification update detectors reduce search space leads rapid production high quality bounding box annotations scheme delivers detectors performing almost good trained fully supervised setting without ever drawing any bounding box verification task very quick scheme substantially reduces total annotation time factor <eos> <eop> factors finetuning deep model object detection long tail distribution <eos> finetuning pretrained deep model found yield state art performance many vision tasks <eos> paper investigates many factors influence performance finetuning object detection <eos> there long tailed distribution sample numbers classes object detection <eos> analysis empirical result show classes more sample higher impact feature learning <eos> better make sample number more uniform across classes <eos> generic object detection considered multiple equally important tasks <eos> detection each class task <eos> classes tasks their individuality discriminative visual appearance representation <eos> taking individuality into account cluster object into visually similar class groups learn deep representations groups separately <eos> hierarchical feature learning scheme proposed <eos> scheme knowledge group large number classes transferred learning feature its sub groups <eos> finetuned googlenet model experimental result show <eos> absolute map improvement approach imagenet object detection dataset without increasing much computational cost testing stage <eos> <eop> information driven adaptive structured light scanners <eos> sensor planning active sensing long studied robotics adapt sensor positioning operation mode order maximize information gain <eos> while concepts often used reason about three dimensional sensors usually treated predefined black box component <eos> paper show how same principles used part three dimensional sensor <eos> describe relevant generative model structured light three dimensional scanning show how adaptive pattern selection maximize information gain open loop feedback manner <eos> then demonstrate how different choices relevant variable set corresponding subproblems locatization mapping lead different criteria pattern selection computed online fashion <eos> show result both subproblems several pattern dictionary choices demonstrate their usefulness pose estimation depth acquisition <eos> <eop> simultaneous optical flow intensity estimation event camera <eos> event cameras bio inspired vision sensors mimic retinas measure per pixel intensity change rather than outputting actual intensity image <eos> proposed paradigm shift away traditional frame cameras offers significant potential advantages namely avoiding high data rates dynamic range limitations motion blur <eos> unfortunately however established computer vision algorithms may all applied directly event cameras <eos> method proposed so far reconstruct image estimate optical flow track camera reconstruct scene come severe restrictions environment motion camera <eos> allowing only rotation <eos> here propose best knowledge first algorithm simultaneously recover motion field brightness image while camera undergoes generic motion through any scene <eos> approach employs minimisation cost function contains asynchronous event data well spatial temporal regularisation within sliding window time interval <eos> implementation relies gpu based optimisation runs near real time <eos> series examples demonstrate successful operation framework including situations conventional cameras heavily suffer dynamic range limitations motion blur <eos> <eop> macroscopic interferometry rethinking depth estimation frequency domain time flight <eos> form meter scale macroscopic interferometry proposed using conventional time flight tof sensors <eos> today tof sensors use phase based sampling phase delay between emitted received high frequency signals encodes distance <eos> paper examines alternative tof architecture inspired micron scale microscopic interferometry relies only frequency sampling refer proposed macroscopic technique frequency domain time flight fd tof <eos> proposed architecture offers several benefits over existing phase tof systems such robustness phase wrapping implicit resolution multi path interference all while capturing same number subframes <eos> prototype camera constructed demonstrate macroscopic interferometry meter scale <eos> <eop> asp vision optically computing first layer convolutional neural network using angle sensitive pixels <eos> deep learning using convolutional neural network cnn quickly becoming state art challenging computer vision applications <eos> however deep learning power consumption bandwidth requirements currently limit its application embedded mobile systems tight energy budgets <eos> paper explore energy savings optically computing first layer cnn <eos> so utilize bio inspired angle sensitive pixels asps custom cmos diffractive image sensors act similar gabor filter banks layer human visual cortex <eos> asps replace both image sensing first layer conventional cnn directly performing optical edge filtering saving sensing energy data bandwidth cnn flops compute <eos> experimental result both synthetic data hardware prototype variety vision tasks such digit recognition object recognition face identification demonstrate reduction image sensor power consumption reduction data bandwidth sensor cpu while achieving similar performance compared traditional deep learning pipelines <eos> <eop> computational imaging vlbi image reconstruction <eos> very long baseline interferometry vlbi technique imaging celestial radio emissions simultaneously observing source telescopes distributed across earth <eos> challenges reconstructing image fine angular resolution vlbi data immense <eos> data extremely sparse noisy thus requiring statistical image models such designed computer vision community <eos> paper present novel bayesian approach vlbi image reconstruction <eos> while other method often require careful tuning parameter selection different types data method chirp produces good result under different settings such low snr extended emission <eos> success method demonstrated realistic synthetic experiments well publicly available real data <eos> present problem way accessible members community provide dataset website vlbiimaging <eos> edu facilitates controlled comparisons across algorithms <eos> <eop> you lead exceed labor free video concept learning jointly exploiting web video image <eos> video concept learning often requires large set training sample <eos> practice however acquiring noise free training labels sufficient positive examples very expensive <eos> plausible solution training data collection sampling vast quantities image video web <eos> such solution motivated assumption retrieved image video highly correlated query <eos> still number challenges remain <eos> first web video often untrimmed <eos> thus only parts video relevant query <eos> second retrieved web image always highly relevant issued query <eos> however thoughtlessly utilizing image video domain may even hurt performance due well known semantic drift domain gap problems <eos> result valid question how web image video interact video concept learning <eos> paper propose lead exceed neural network lenn reinforces training web image video curriculum manner <eos> specifically training proceeds inputting frames web video obtain network <eos> web image then filtered learnt network selected image additionally fed into network enhance architecture further trim video <eos> addition long short term memory lstm applied trimmed video explore temporal information <eos> encouraging result reported ucf trecvid medtest context both action recognition event detection <eos> without using human annotated exemplars proposed lenn achieve <eos> accuracy ucf dataset <eos> <eop> track segment iterative unsupervised approach video object proposals <eos> present unsupervised approach generates diverse ranked set bounding box segmentation video object proposals spatio temporal tubes localize foreground object unannotated video <eos> contrast previous unsupervised method either track region initialized arbitrary frame train fixed model over cluster region instead discover set easy group instances object then iteratively update its appearance model gradually detect harder instances temporally adjacent frames <eos> method first generates set spatio temporal bounding box proposals then refines them obtain pixel wise segmentation proposals <eos> through extensive experiments demonstrate state art segmentation result segtrack dataset bounding box tracking result perform competitively state art supervised tracking method <eos> <eop> beyond local search tracking object everywhere instance specific proposals <eos> most tracking detection method employ local search window around predicted object location current frame assuming previous location accurate trajectory smooth computational capacity permits search radius accommodate maximum speed yet small enough reduce mismatches <eos> however may valid always particular fast irregularly moving object <eos> here present object tracker limited local search window ability probe efficiently entire frame <eos> method generates small number high quality proposals novel instance specific objectness measure evaluates them against object model adopted existing tracking detection approach core tracker <eos> during tracking process update object model concentrating hard false positives supplied proposals help suppressing distractors caused difficult background clutters learn how re rank proposals according object model <eos> since reduce significantly number hypotheses core tracker evaluates use richer object descriptors stronger detector <eos> method outperforms most recent state art trackers popular tracking benchmarks provides improved robustness fast moving object well ultra low frame rate video <eos> <eop> groupwise tracking crowded similar appearance targets low continuity image sequences <eos> automatic tracking large scale crowded targets particular importance many applications such crowded people vehicle tracking video surveillance fiber tracking materials science cell tracking biomedical imaging <eos> problem becomes very challenging when targets show similar appearance inter slice inter frame continuity low due sparse sampling camera motion target occlusion <eos> main challenge comes step association aims matching predictions observations multiple targets <eos> paper propose new groupwise method explore target group information employ within group correlations association tracking <eos> particular within group association modeled nonrigid thin plate transform sequence group shrinking group growing group merging operations then developed refine composition each group <eos> apply propose method track large scale fibers microscopy material image compare its performance against several other multi target tracking method <eos> also apply proposed method track crowded people video poor inter frame continuity <eos> <eop> social lstm human trajectory prediction crowded spaces <eos> humans navigate complex crowded environments based social conventions they respect personal space yielding right way avoid collisions <eos> work propose data driven approach learn human human interactions predicting their future trajectories <eos> contrast traditional approaches use hand crafted functions such social forces <eos> present new long short term memory lstm model jointly reasons across multiple individuals scene <eos> different conventional lstm share information between multiple lstms through new pooling layer <eos> layer pools hidden representation lstms corresponding neighboring trajectories capture interactions within neighborhood <eos> demonstrate performance method several public datasets <eos> model outperforms previous forecasting method more than <eos> also analyze trajectories predicted model demonstrate social behaviours such collision avoidance group movement learned model <eos> <eop> players ball physically constrained interaction modeling <eos> tracking ball critical video based analysis team sports <eos> however difficult especially low resolution image due small size ball its speed creates motion blur its often being occluded players <eos> paper propose generic principled approach modeling interaction between ball players while also imposing appropriate physical constraints ball trajectory <eos> show approach formulated terms mixed integer program more robust more accurate than several state art approaches real life volleyball basketball soccer sequences <eos> <eop> highlight detection pairwise deep ranking first person video summarization <eos> emergence wearable devices such portable cameras smart glasses makes possible record life logging first person video <eos> browsing such long unstructured video time consuming tedious <eos> paper studies discovery moments user major special interest <eos> highlights video generating summarization first person video <eos> specifically propose novel pairwise deep ranking model employs deep learning techniques learn relationship between highlight non highlight video segments <eos> two stream network structure representing video segments complementary information appearance video frames temporal dynamics across frames developed video highlight detection <eos> given long personal video equipped highlight detection model highlight score assigned each segment <eos> obtained highlight segments applied summarization two ways video timelapse video skimming <eos> former plays highlight non highlight segments low high speed rates while latter assembles sequence segments highest scores <eos> hours first person video unique sports categories highlight detection achieves improvement over state art ranksvm method <eos> moreover approaches produce video summary better quality user study human subjects <eos> <eop> direct prediction three dimensional body poses motion compensated sequences <eos> propose efficient approach exploiting motion information consecutive frames video sequence recover three dimensional pose people <eos> previous approaches typically compute candidate poses individual frames then link them post processing step resolve ambiguities <eos> contrast directly regress spatio temporal volume bounding boxes three dimensional pose central frame <eos> further show approach achieve its full potential essential compensate motion consecutive frames so subject remains centered <eos> then allows effectively overcome ambiguities improve upon state art large margin human <eos> humaneva kth multiview football three dimensional human pose estimation benchmarks <eos> <eop> video gif automatic generation animated gifs video <eos> introduce novel problem automatically generating animated gifs video <eos> gifs short looping video no sound perfect combination between image video really capture attention <eos> gifs tell story express emotion turn events into humorous moments new wave photojournalism <eos> pose question automate entirely manual elaborate process gif creation leveraging plethora user generated gif content propose robust deep ranknet given video generates ranked list its segments according their suitability gif <eos> train model learn visual content often selected gifs using over user generated gifs their corresponding video sources <eos> effectively deal noisy web data proposing novel adaptive huber loss ranking formulation <eos> show approach robust outliers picks up several patterns frequently present popular animated gifs <eos> new large scale benchmark dataset show advantage approach over several state art method <eos> <eop> ntu rgb large scale dataset three dimensional human activity analysis <eos> recent approaches depth based human activity analysis achieved outstanding performance proved effectiveness three dimensional representation classification action classes <eos> currently available depth based rgb based action recognition benchmarks number limitations including lack training sample distinct class labels camera views variety subjects <eos> paper introduce large scale dataset rgb human action recognition more than thousand video sample million frames collected distinct subjects <eos> dataset contains different action classes including daily mutual health related actions <eos> addition propose new recurrent neural network structure model long term temporal correlation feature each body part utilize them better action classification <eos> experimental result show advantages applying deep learning method over state art hand crafted feature suggested cross subject cross view evaluation criteria dataset <eos> introduction large scale dataset will enable community apply develop adapt various data hungry learning techniques task depth based rgb based human activity analysis <eos> <eop> progressively parsing interactional object fine grained action detection <eos> fine grained video action analysis often requires reliable detection tracking various interacting object human body parts denoted interactional object parsing <eos> however most previous method based either independent joint object detection might suffer high model complexity challenging image content <eos> illumination pose appearance scale variation motion occlusion etc <eos> work propose end end system based recursive neural network perform frame frame interactional object parsing alleviate difficulty through incremental manner <eos> key innovation instead jointly outputting all object detections once each frame use set long short term memory lstm nodes incrementally refine detections <eos> after passing each lstm node more object detections consolidated thus more contextual information could utilized determine more difficult object detections <eos> extensive experiments two benchmark fine grained activity datasets demonstrate proposed algorithm achieves better interacting object detection performance turn boosts action recognition performance over state art <eos> <eop> hierarchical recurrent neural encoder video representation application captioning <eos> recently deep learning approach especially deep convolutional neural network convnets achieved overwhelming accuracy fast processing speed image classification <eos> incorporating temporal structure deep convnets video representation becomes fundamental problem video content analysis <eos> paper propose new approach namely hierarchical recurrent neural encoder hrne exploit temporal information video <eos> compared recent video representation inference approaches paper makes following three contributions <eos> first hrne able efficiently exploit video temporal structure longer range reducing length input information flow compositing multiple consecutive inputs higher level <eos> second computation operations significantly lessened while attaining more non linearity <eos> third hrne able uncover temporal transitions between frame chunks different granularities <eos> model temporal transitions between frames well transitions between segments <eos> apply new method video captioning temporal information plays crucial role <eos> experiments demonstrate method outperforms state art video captioning benchmarks <eos> <eop> keyframes key object video summarization representative object proposal selection <eos> propose summarize video into few key object selecting representative object proposals generated video frames <eos> representative selection problem formulated sparse dictionary selection problem <eos> choosing few representatives object proposals reconstruct whole proposal pool <eos> compared existing sparse dictionary selection based representative selection method new formulation incorporate object proposal priors locality prior feature space when selecting representatives <eos> consequently better locate key object suppress outlier proposals <eos> convert optimization problem into proximal gradient problem solve fast iterative shrinkage thresholding algorithm fista <eos> experiments synthetic data real benchmark datasets show promising result key object summarization apporach video content mining search <eos> comparisons existing representative selection approaches such mediod sparse dictionary selection density based selection validate formulation better capture key video object despite appearance variations cluttered backgrounds camera motions <eos> <eop> temporal action localization untrimmed video via multi stage cnn <eos> address temporal action localization untrimmed long video <eos> important because video real applications usually unconstrained contain multiple action instances plus video content background scenes other activities <eos> address challenging issue exploit effectiveness deep network temporal action localization via three segment based three dimensional convnets proposal network identifies candidate segments long video may contain actions classification network learns one vs all action classification model serve initialization localization network localization network fine tunes learned classification network localize each action instance <eos> propose novel loss function localization network explicitly consider temporal overlap achieve high temporal localization accuracy <eos> end only proposal network localization network used during prediction <eos> two large scale benchmarks approach achieves significantly superior performances compared other state art systems map increases <eos> <eop> summary transfer exemplar based subset selection video summarization <eos> video summarization unprecedented importance help digest browse search today ever growing video collections <eos> propose novel subset selection technique leverages supervision form human created summaries perform automatic keyframe based video summarization <eos> main idea nonparametrically transfer summary structures annotated video unseen test video <eos> show how extend method exploit semantic side information about video category genre guide transfer process training video semantically consistent test input <eos> also show how generalize method subshot based summarization only reduces computational costs but also provides more flexible ways defining visual similarity across subshots spanning several frames <eos> conduct extensive evaluation several benchmarks demonstrate promising result outperforming existing method several settings <eos> <eop> pod discovering primary object video based evolutionary refinement object recurrence background primary object models <eos> primary object discovery pod algorithm video sequence proposed work capable discovering primary object well identifying noisy frames contain object <eos> first generate object proposals each frame <eos> then bisect each proposal into foreground background region extract feature each region <eos> superposing foreground background feature build object recurrence model background model primary object model <eos> develop iterative scheme refine each model evolutionary using information other models <eos> finally using evolved primary object model select candidate proposals locate bounding box primary object merging proposals selectively <eos> experimental result challenging dataset demonstrate proposed pod algorithm extract primary object accurately robustly <eos> <eop> if multiple video same action video action localization using web image <eos> paper tackles problem spatio temporal action localization video without assuming availability multiple video any prior annotations <eos> action localized employing image downloaded internet using action name <eos> given web image first mitigate image noise using random walk framework evade distracting backgrounds within image using image action proposals <eos> then given video generate multiple spatio temporal action proposals <eos> suppress camera background generated proposals exploiting optical flow gradients within proposal <eos> obtain most action representative proposal propose reconstruct action proposals video leveraging action proposal image <eos> moreover preserve temporal smoothness video introducing consensus regularization <eos> consensus regularization enforces consistency among coefficients vectors multiple frames within proposal <eos> reconstruct video action proposals image action proposals while enforcing consistency across coefficient vectors multiple frames consensus regularization <eos> finally video proposal lowest reconstruction cost motion salient considered final action localization <eos> extensive experiments trimmed well untrimmed datasets validate effectiveness proposed approach <eos> <eop> beyond formations determining social involvement free standing conversing groups static image <eos> paper present first attempt analyse differing levels social involvement free standing conversing groups so called formations static image <eos> addition enrich state art formation modelling learning frustum attention accounts spatial context <eos> formation configurations vary respect arrangement furniture non uniform crowdedness space during mingling scenarios <eos> majority prior works considered labelling conversing group objective task requiring only single annotator <eos> however show embracing subjectivity social involvement only generate richer model social interactions scene but also significantly improve formation detection <eos> carry out extensive experimental validation proposed approach collecting novel set multi annotator labels involvement publicly available idiap poster data only multi annotator labelled database free standing conversing groups currently available <eos> <eop> deepfashion powering robust clothes recognition retrieval rich annotations <eos> recent advances clothes recognition driven construction clothes datasets <eos> existing datasets limited amount annotations difficult cope various challenges real world applications <eos> work introduce deepfashion large scale clothes dataset comprehensive annotations <eos> contains over image richly annotated massive attributes clothing landmarks correspondence image taken under different scenarios including store street snapshot consumer <eos> such rich annotations enable development powerful algorithms clothes recognition facilitating future researches <eos> demonstrate advantages deepfashion propose new deep model namely fashionnet learns clothing feature jointly predicting clothing attributes landmarks <eos> estimated landmarks then employed pool gate learned feature <eos> optimized iterative manner <eos> extensive experiments demonstrate effectiveness fashionnet usefulness deepfashion <eos> <eop> sketchnet sketch classification web image <eos> study present weakly supervised approach discovers discriminative structures sketch image given pairs sketch image web image <eos> contrast traditional approaches use global appearance feature relay keypoint feature aim automatically learn shared latent structures exist between sketch image real image even when there significant appearance differences across its relevant real image <eos> accomplish propose deep convolutional neural network named sketchnet <eos> firstly develop triplet composed sketch positive negative real image input neural network <eos> discover coherent visual structures between sketch its positive pairs introduce softmax loss function <eos> then ranking mechanism introduced make positive pairs obtain higher score comparing over negative ones achieve robust representation <eos> finally formalize above mentioned constrains into unified objective function create ensemble feature representation describe sketch image <eos> experiments tu berlin sketch benchmark demonstrate effectiveness model show deep feature representation brings substantial improvements over other state art method sketch classification <eos> <eop> embedding label structures fine grained feature representation <eos> recent algorithms convolutional neural network cnn considerably advance fine grained image classification aims differentiate subtle differences among subordinate classes <eos> however previous studies rarely focused learning fined grained structured feature representation able locate relevant image different levels relevance <eos> discovering cars same make same model both require high precision <eos> paper propose two main contributions tackle problem <eos> multi task learning framework designed effectively learn fine grained feature representations jointly optimizing both classification similarity constraints <eos> model multi level relevance label structures such hierarchy shared attributes seamlessly embedded into framework generalizing triplet loss <eos> extensive thorough experiments conducted three fine grained datasets <eos> stanford car car food datasets contain either hierarchical labels shared attributes <eos> proposed method achieved very competitive performance <eos> among state art classification accuracy <eos> more importantly significantly outperforms previous fine grained feature representations image retrieval different levels relevance <eop> fine grained image classification exploring bipartite graph labels <eos> given food image fine grained object recognition engine tell restaurant dish food belongs such ultra fine grained image recognition key many applications like search image but very challenging because needs discern subtle difference between classes while dealing scarcity training data <eos> fortunately ultra fine granularity naturally brings rich relationships among object classes <eos> paper proposes novel approach exploit rich relationships through bipartite graph labels bgl <eos> show how model bgl overall convolutional neural network resulting system optimized through back propagation <eos> also show computationally efficient inference thanks bipartite structure <eos> facilitate study construct new food benchmark dataset consists food image collected restaurants totally menus <eos> experimental result new food three other datasets demonstrate bgl advances previous works fine grained object recognition <eos> online demo available www <eos> com fg demo <eos> <eop> picking deep filter responses fine grained image recognition <eos> recognizing fine grained sub categories such birds dogs extremely challenging due highly localized subtle differences some specific parts <eos> most previous works rely object part level annotations build part based representation demanding practical applications <eos> paper proposes automatic fine grained recognition approach free any object part annotation both training testing stages <eos> method explores unified framework based two steps deep filter response picking <eos> first picking step find distinctive filters respond specific patterns significantly consistently learn set part detectors via iteratively alternating between new positive sample mining part model retraining <eos> second picking step pool deep filter responses via spatially weighted combination fisher vectors <eos> conditionally pick deep filter responses encode them into final representation considers importance filter responses themselves <eos> integrating all techniques produces much more powerful framework experiments conducted cub stanford dogs demonstrate superiority proposed algorithm over existing method <eos> <eop> spda cnn unifying semantic part detection abstraction fine grained recognition <eos> most convolutional neural network cnn lack midlevel layer model semantic parts object <eos> limits cnn based method reaching their full potential detecting utilizing small semantic parts recognition <eos> introducing such mid level layer facilitate extraction part specific feature utilized better recognition performance <eos> particularly important domain fine grained recognition <eos> paper propose new cnn architecture integrates semantic part detection abstraction spda cnn fine grained classification <eos> proposed network two sub network one detection one recognition <eos> detection sub network novel top down proposal method generate small semantic part candidates detection <eos> classification sub network introduces novel part layer extract feature parts detected detection sub network combine them recognition <eos> result proposed architecture provides end end network performs detection localization multiple semantic parts whole object recognition within one framework shares computation convolutional filters <eos> method outperforms state art method large margin small parts detection <eos> vs best previous precision <eos> detecting head cub <eos> also compares favorably existing state art fine grained classification <eos> <eop> fine grained categorization dataset bootstrapping using deep metric learning humans loop <eos> existing fine grained visual categorization method often suffer three challenges lack training data large number fine grained categories high intra class vs <eos> low inter class variance <eos> work propose generic iterative framework fine grained categorization dataset bootstrapping handles three challenges <eos> using deep metric learning humans loop learn low dimensional feature embedding anchor point manifolds each category <eos> anchor point capture intra class variances remain discriminative between classes <eos> each round image high confidence scores model sent humans labeling <eos> comparing exemplar image labelers mark each candidate image either true positive false positive <eos> true positives added into current dataset false positives regarded hard negatives metric learning model <eos> then model re trained expanded dataset hard negatives next round <eos> demonstrate effectiveness proposed framework bootstrap fine grained flower dataset categories instagram image <eos> proposed deep metric learning scheme evaluated both dataset cub birds dataset <eos> experimental evaluations show significant performance gain using dataset bootstrapping demonstrate state art result achieved proposed deep metric learning method <eos> <eop> mining discriminative triplets patches fine grained classification <eos> fine grained classification involves distinguishing between similar sub categories based subtle differences highly localized region therefore accurate localization discriminative region remains major challenge <eos> describe patch based framework address problem <eos> introduce triplets patches geometric constraints improve accuracy patch localization automatically mine discriminative geometrically constrained triplets classification <eos> resulting approach only requires object bounding boxes <eos> its effectiveness demonstrated using four publicly available fine grained datasets outperforms obtains comparable result state art classification <eos> <eop> part stacked cnn fine grained visual categorization <eos> context fine grained visual categorization ability interpret models human understandable visual manuals sometimes important achieving high classification accuracy <eos> paper propose novel part stacked cnn architecture explicitly explains fine grained recognition process modeling subtle differences object parts <eos> based manually labeled strong part annotations proposed architecture consists fully convolutional network locate multiple object parts two stream classification network encodes object level part level cues simultaneously <eos> adopting set sharing strategies between computation multiple object parts proposed architecture very efficient running frames sec during inference <eos> experimental result cub dataset reveal effectiveness proposed architecture multiple perspectives classification accuracy model interpretability efficiency <eos> being able provide interpretable recognition result realtime proposed method believed effective practical applications <eos> <eop> learning compact binary descriptors unsupervised deep neural network <eos> paper propose new unsupervised deep learning approach called deepbit learn compact binary descriptor efficient visual object matching <eos> unlike most existing binary descriptors were designed random projections linear hash functions develop deep neural network learn binary descriptors unsupervised manner <eos> enforce three criterions binary codes learned top layer network minimal loss quantization evenly distributed codes uncorrelated bits <eos> then learn parameters network back propagation technique <eos> experimental result three different visual analysis tasks including image matching image retrieval object recognition clearly demonstrate effectiveness proposed approach <eos> <eop> solving small piece jigsaw puzzles growing consensus <eos> paper present novel computational puzzle solver square piece image jigsaw puzzles no prior information such piece orientation anchor pieces resulting dimension puzzle <eos> piece mean square dxd block pixels investigate pieces small pixels <eos> reconstruct such challenging puzzles aim search piece configurations maximize size consensus <eos> grid loop configurations represent geometric consensus agreement among pieces <eos> pieces considered addition existing assemblies if pieces increase size consensus configurations <eos> contrast previous puzzle solvers goal assemblies maximizing compatibility measures between all pairs pieces thus depend heavily pairwise compatibility measure used new approach reduces dependency pairwise compatibility measures become increasingly uninformative small scales instead exploits geometric agreement among pieces <eos> contribution also includes improved pairwise compatibility measure exploits directional derivative information along adjoining boundaries pieces <eos> challenging unknown orientation piece puzzles size pieces small reduce assembly error up compared previous algorithms standard datasets <eos> <eop> pairwise matching through max weight bipartite belief propagation <eos> feature matching key problem computer vision pattern recognition <eos> one way encode essential interdependence between potential feature matches cast problem inference graphical model though recently alternatives such spectral method approaches based convex concave procedure achieved state art <eos> here revisit use graphical models feature matching propose belief propagation scheme exhibits following advantages explicitly enforce one one matching constraints offer tighter relaxation original cost function than previous graphical model based approaches sub problems decompose into max weight bipartite matching solved efficiently leading orders magnitude reductions execution time <eos> experimental result show proposed algorithm produces result superior current state art <eos> <eop> structured feature similarity explicit feature map <eos> feature matching fundamental process variety computer vision tasks <eos> beyond standard metric various method measure similarity between feature proposed mainly assumption feature defined histogram form <eos> other hand field image quality assessment ssim produces effective similarity between image taking place metric <eos> paper propose feature similarity measurement method based ssim <eos> unlike previous method proposed method built histogram form but tensor structure feature array extracted such spatial grids order construct effective ssim based similarity measure high robustness key requirement feature matching <eos> addition provide explicit feature map such proposed similarity metric embedded dot product <eos> contributes significant speedup similarity measurement well feature transformation toward effective vector form linear classifiers directly applicable <eos> experiments various tasks proposed method exhibits favorable performance both feature matching classification <eos> <eop> temporal epipolar region <eos> dynamic events often photographed number people different viewpoints different times resulting unconstrained set image <eos> finding corresponding moving feature each image allows extract information about object interest scene <eos> computing correspondence moving feature such set image considerably more challenging than computing correspondence video due possible significant differences viewpoints inconsistent timing between image captures <eos> prediction method used video improving robustness efficiency applicable set still image <eos> paper propose novel method predict locations approximately linear moving feature point given small subset correspondences temporal order image captures <eos> method extends use epipolar geometry divide image into valid invalid region termed temporal epipolar region ters <eos> formally prove location feature new image restricted valid ters <eos> demonstrate effectiveness method reducing search space correspondence both synthetic challenging real world data show improved matching <eos> <eop> recurrent attention models depth based person identification <eos> present attention based model reasons human body shape motion dynamics identify individuals absence rgb information hence dark <eos> approach leverages unique spatio temporal signatures address identification problem across days <eos> formulated reinforcement learning task model based combination convolutional recurrent neural network goal identifying small discriminative region indicative human identity <eos> demonstrate model produces state art result several published datasets given only depth image <eos> further study robustness model towards viewpoint appearance volumetric changes <eos> finally share insights gleaned interpretable three dimensional visualizations model spatio temporal attention <eos> <eop> learning discriminative null space person re identification <eos> most existing person re identification re id method focus learning optimal distance metrics across camera views <eos> typically person appearance represented using feature thousands dimensions whilst only hundreds training sample available due difficulties collecting matched training image <eos> number training sample much smaller than feature dimension existing method thus face classic small sample size sss problem resort dimensionality reduction techniques matrix regularisation lead loss discriminative power <eos> work propose overcome sss problem re id distance metric learning matching people discriminative null space training data <eos> null space image same person collapsed into single point thus minimising within class scatter extreme maximising relative between class separation simultaneously <eos> importantly fixed dimension closed form solution very efficient compute <eos> extensive experiments carried out five person re identification benchmarks including viper prid cuhk cuhk market show such simple approach beats state art alternatives often big margin <eos> <eop> learning deep feature representations domain guided dropout person re identification <eos> learning generic robust feature representations data multiple domains same problem great value especially problems multiple datasets but none them large enough provide abundant data variations <eos> work present pipeline learning deep feature representations multiple domains convolutional neural network cnn <eos> when training cnn data all domains some neurons learn representations shared across several domains while some others effective only specific one <eos> based important observation propose domain guided dropout algorithm improve feature learning procedure <eos> experiments show effectiveness pipeline proposed algorithm <eos> method person re identification problem outperform state art method multiple datasets large margins <eos> <eop> how far solving pedestrian detection <eos> encouraged recent progress pedestrian detection investigate gap between current state art method perfect single frame detector <eos> enable analysis creating human baseline pedestrian detection over caltech dataset manually clustering recurrent errors top detector <eos> result characterise both localisation background versus foreground errors <eos> address localisation errors study impact training annotation noise detector performance show improve even small portion sanitised training data <eos> address background foreground discrimination study convnets pedestrian detection discuss factors affect their performance <eos> other than depth analysis report top performance caltech dataset provide new sanitised set training test annotations <eos> <eop> similarity learning spatial constraints person re identification <eos> pose variation remains one major factors adversely affect accuracy person re identification <eos> such variation arbitrary body parts <eos> head torso legs relative stable spatial distribution <eos> breaking down variability global appearance regarding spatial distribution potentially benefits person matching <eos> therefore learn novel similarity function consists multiple sub similarity measurements each taking charge subregion <eos> particular take advantage recently proposed polynomial feature map describe matching within each subregion inject all feature maps into unified framework <eos> framework only outputs similarity measurements different region but also makes better consistency among them <eos> framework collaborate local similarities well global similarity exploit their complementary strength <eos> flexible incorporate multiple visual cues further elevate performance <eos> experiments analyze effectiveness major components <eos> result four datasets show significant consistent improvements over state art method <eos> <eop> sample specific svm learning person re identification <eos> person re identification addresses problem matching people across disjoint camera views extensive efforts made seek either robust feature representation discriminative matching metrics <eos> however most existing approaches focus learning fixed distance metric all instance pairs while ignoring individuality each person <eos> paper formulate person re identification problem imbalanced classification problem learn classifier specifically each pedestrian such matching model highly tuned individual appearance <eos> establish correspondence between feature space classifier space propose least square semi coupled dictionary learning lsscdl algorithm learn pair dictionaries mapping function efficiently <eos> extensive experiments series challenging databases demonstrate proposed algorithm performs favorably against state art approaches especially rank recognition rate <eos> <eop> joint learning single image cross image representations person re identification <eos> person re identification usually solved either matching single image representation sir classification cross image representation cir <eos> work exploit connection between two categories method propose joint learning framework unify sir cir using convolutional neural network cnn <eos> specifically deep architecture contains one shared sub network together two sub network extract sirs given image cirs given image pairs respectively <eos> sir sub network required computed once each image both probe gallery set depth cir sub network required minimal reduce computational burden <eos> therefore two types representation jointly optimized pursuing better matching accuracy moderate computational cost <eos> furthermore representations learned pairwise comparison triplet comparison objectives combined improve matching performance <eos> experiments cuhk cuhk viper datasets show proposed method achieve favorable accuracy while compared state arts <eos> <eop> multi level contextual model person recognition photo albums <eos> work present new framework person recognition photo albums exploits contextual cues multiple levels spanning individual persons individual photos photo groups <eos> through experiments show information available each distinct contextual levels provides complementary cues person identities <eos> person level leverage clothing body appearance addition facial appearance compensate instances faces visible <eos> photo level leverage learned prior joint distribution identities same photo guide identity assignments <eos> going beyond single photo able infer natural groupings photos shared context unsupervised manner <eos> exploiting shared contextual information able reduce identity search space exploit higher intra personal appearance consistency within photo groups <eos> new framework enables efficient use complementary multi level contextual cues improve overall recognition rates photo album person recognition task demonstrated through state art result challenging public dataset <eos> result outperform competing method significant margin while being computationally efficient practical real world application <eos> <eop> unsupervised cross dataset transfer learning person re identification <eos> most existing person re identification re id approaches follow supervised learning framework large number labelled matching pairs required training <eos> severely limits their scalability real world applications <eos> overcome limitation develop novel cross dataset transfer learning approach learn discriminative representation <eos> unsupervised sense target dataset completely unlabelled <eos> specifically present multi task dictionary learning method able learn dataset shared but target data biased representation <eos> experimental result five benchmark datasets demonstrate method significantly outperforms state art <eos> <eop> pedestrian detection inspired appearance constancy shape symmetry <eos> discrimination simplicity feature very important effective efficient pedestrian detection <eos> however most state art method unable achieve good tradeoff between accuracy efficiency <eos> inspired some simple inherent attributes pedestrians <eos> appearance constancy shape symmetry propose two new types non neighboring feature nnf side inner difference feature sidf symmetrical similarity feature ssf <eos> sidf characterize difference between background pedestrian difference between pedestrian contour its inner part <eos> ssf capture symmetrical similarity pedestrian shape <eos> however difficult neighboring feature such above characterization abilities <eos> finally propose combine both non neighboring neighboring feature pedestrian detection <eos> found nonneighboring feature further decrease average miss rate <eos> experimental result inria caltech pedestrian datasets demonstrate effectiveness efficiency proposed method <eos> compared state ofthe art method without using cnn method achieves best detection performance caltech outperforming second best method <eos> <eop> recurrent convolutional network video based person re identification <eos> paper propose novel recurrent neural network architecture video based person re identification <eos> given video sequence person feature extracted each frame using convolutional neural network incorporates recurrent final layer allows information flow between time steps <eos> feature all time steps then combined using temporal pooling give overall appearance feature complete sequence <eos> convolutional network recurrent layer temporal pooling layer jointly trained act feature extractor video based re identification using siamese network architecture <eos> approach makes use colour optical flow information order capture appearance motion information useful video re identification <eos> experiments conduced ilids vid prid datasets show approach outperforms existing method video based re identification <eos> <eop> person re identification multi channel parts based cnn improved triplet loss function <eos> person re identification across cameras remains very challenging problem especially when there no overlapping fields view between cameras <eos> paper present novel multi channel parts based convolutional neural network cnn model under triplet framework person re identification <eos> specifically proposed cnn model consists multiple channels jointly learn both global full body local body parts feature input persons <eos> cnn model trained improved triplet loss function serves pull instances same person closer same time push instances belonging different persons farther each other learned feature space <eos> extensive comparative evaluations demonstrate proposed method significantly outperforms many state art approaches including both traditional deep network based ones challenging lids viper prid cuhk datasets <eos> <eop> top push video based person re identification <eos> most existing person re identification re id models focus matching still person image across disjoint camera views using setting either single shot multi shot <eos> since limited information exploited still image hard if impossible overcome occlusion pose camera view change lighting variation problems <eos> comparison video based re id method utilize extra space time information contains much more rich cues matching overcome mentioned problems <eos> however work find when using video based representation some inter class difference much more obscure than one when using still image based representation because different people could only similar appearance but also may similar motions actions hard align <eos> solve problem propose top push distance learning model tdl integrate top push constrain matching video feature persons <eos> top push constraint enforces optimization top rank matching re id so make matching model more effective towards selecting more discriminative feature distinguish different persons <eos> experiments show proposed video based re id framework outperforms state art video based re id method <eos> <eop> improving person re identification via pose aware multi shot matching <eos> person re identification problem recognizing people across image video non overlapping views <eos> although there much progress person re identification last decade still remains challenging task because severe appearance changes person due diverse camera viewpoints person poses <eos> paper propose novel framework person re identification analyzing camera viewpoints person poses so called pose aware multi shot matching pamm robustly estimates target poses efficiently conducts multi shot matching based target pose information <eos> experimental result using public person re identification dataset show proposed method promising person re identification under diverse viewpoints pose variances <eos> <eop> hierarchical gaussian descriptor person re identification <eos> describing color textural information person image one most crucial aspects person re identification <eos> paper present novel descriptor based hierarchical distribution pixel feature <eos> hierarchical covariance descriptor successfully applied image classification <eos> however mean information pixel feature absent covariance tends major discriminative information person image <eos> solve problem describe local region image via hierarchical gaussian distribution both means covariances included their parameters <eos> more specifically model region set multiple gaussian distributions each gaussian represents appearance local patch <eos> characteristics set gaussians again described another gaussian distribution <eos> both steps unlike hierarchical covariance descriptor proposed descriptor model both mean covariance information pixel feature properly <eos> result experiments conducted five databases indicate proposed descriptor exhibits remarkably high performance outperforms state art descriptors person re identification <eos> <eop> stct sequentially training convolutional network visual tracking <eos> due limited amount training sample fine tuning pre trained deep models online prone over fitting <eos> paper propose sequential training method convolutional neural network cnn effectively transfer pre trained deep feature online applications <eos> regard cnn ensemble each channel output feature map individual base learner <eos> each base learner trained using different loss criterions reduce correlation avoid over training <eos> achieve best ensemble online all base learners sequentially sampled into ensemble via important sampling <eos> further improve robustness each base learner propose train convolutional layer random binary masks serves regularization enforce each base learner focus different input feature <eos> proposed online training method applied visual tracking problem transferring deep feature trained massive annotated visual data shown significantly improve tracking performance <eos> extensive experiments conducted two challenging benchmark data set demonstrate tracking algorithm outperform state art method considerable margin <eos> <eop> determining occlusions space time image reconstructions <eos> problem localizing occlusions between consecutive frames video important but rarely tackled its own <eos> most works tightly interleaved computation accurate optical flows leads delicate chicken egg problem <eos> mind propose novel approach occlusion detection visibility point next frame formulated terms visual reconstruction <eos> key issue now determine how well pixel first image recon structed co located colors next image <eos> first exploit reasoning pixel level new detection criterion <eos> contrary ubiquitous displaced frame difference forward backward flow vector matching proposed alternative critically depend precomputed dense displacement field while being shown more effective <eos> then leverage local modeling within energy minimization framework delivers occlusion maps <eos> easy obtain collection parametric motion models exploited within energy provide required level motion information <eos> approach outperforms state art detection method challenging mpi sintel dataset <eos> <eop> online multi object tracking via structural constraint event aggregation <eos> multi object tracking mot becomes more challenging when object interest similar appearances <eos> case motion cues particularly useful discriminating multiple object <eos> however online mot scenes acquired moving cameras observable motion cues complicated global camera movements thus always smooth predictable <eos> deal such unexpected camera motion online mot structural motion constraint between object utilized thanks its robustness camera motion <eos> paper propose new data association method effectively exploits structural motion constraints presence large camera motion <eos> addition further improve robustness data association against mis detections clutters novel event aggregation approach developed integrate structural constraints assignment costs online mot <eos> experimental result large number datasets demonstrate effectiveness proposed algorithm online mot <eos> <eop> staple complementary learners real time tracking <eos> correlation filter based trackers recently achieved excellent performance showing great robustness challenging situations exhibiting motion blur illumination changes <eos> however since model they learn depends strongly spatial layout tracked object they notoriously sensitive deformation <eos> models based colour statistics complementary traits they cope well variation shape but suffer when illumination consistent throughout sequence <eos> moreover colour distributions alone insufficiently discriminative <eos> paper show simple tracker combining complementary cues ridge regression framework operate faster than fps outperform only all entries popular vot competition but also recent far more sophisticated trackers according multiple benchmarks <eos> <eop> robust optical flow estimation double layer image under transparency reflection <eos> paper deals challenging frequently encountered yet properly investigated problem two frame optical flow estimation <eos> input frames compounds two imaging layer one desired background layer scene one distracting possibly moving layer due transparency reflection <eos> situation conventional brightness constancy constraint cornerstone most existing optical flow method will no longer valid <eos> paper propose robust solution problem <eos> proposed method performs both optical flow estimation image layer separation <eos> exploits generalized double layer brightness consistency constraint connecting two tasks utilizes priors both them <eos> experiments both synthetic data real image confirmed efficacy proposed method <eos> best knowledge first attempt towards handling generic optical flow fields two frame image containing transparency reflection <eos> <eop> siamese instance search tracking <eos> paper present tracker radically different state art trackers apply no model updating no occlusion detection no combination trackers no geometric matching still deliver state art tracking performance demonstrated popular online tracking benchmark otb six very challenging youtube video <eos> presented tracker simply matches initial patch target first frame candidates new frame returns most similar patch learned matching function <eos> strength matching function comes being extensively trained generically <eos> without any data target using siamese deep neural network design tracking <eos> once learned matching function used without any adapting track previously unseen targets <eos> turns out learned matching function so powerful simple tracker built upon coined siamese instance search tracker sint only uses original observation target first frame suffices reach state art performance <eos> further show proposed tracker even allows target re identification after target was absent complete video shot <eos> <eop> adaptive decontamination training set unified formulation discriminative visual tracking <eos> tracking detection method demonstrated competitive performance recent years <eos> approaches tracking model heavily relies quality training set <eos> due limited amount labeled training data additional sample need extracted labeled tracker itself <eos> often leads inclusion corrupted training sample due occlusions misalignments other perturbations <eos> existing tracking detection method either ignore problem employ separate component managing training set <eos> propose novel generic approach alleviating problem corrupted training sample tracking detection frameworks <eos> approach dynamically manages training set estimating quality sample <eos> contrary existing approaches propose unified formulation minimizing single loss over both target appearance model sample quality weights <eos> joint formulation enables corrupted sample down weighted while increasing impact correct ones <eos> experiments performed three benchmarks otb video vot video temple color video <eos> otb unified formulation significantly improves baseline gain <eos> mean overlap precision <eos> finally method achieves state art result all three datasets <eos> <eop> part based sparse tracker automatic synchronization registration <eos> paper present part based sparse tracker particle filter framework both motion appearance model formulated three dimensional motion model adaptive directed according simple yet powerful occlusion handling paradigm intrinsically fused motion model <eos> also since three dimensional trackers sensitive synchronization registration noise rgb depth streams propose automated method solve two issues <eos> extensive experiments conducted popular rgbd tracking benchmark demonstrate tracker achieve superior result outperforming many other recent state art rgbd trackers <eos> <eop> recurrently target attending tracking <eos> robust visual tracking challenging task computer vision <eos> due accumulation propagation estimation error model drifting often occurs degrades tracking performance <eos> mitigate problem paper propose novel tracking method called recurrently target attending tracking rtt <eos> rtt attempts identify exploit reliable parts beneficial overall tracking process <eos> bypass occlusion discover reliable components multi directional recurrent neural network rnns employed rtt capture long range contextual cues traversing candidate spatial region multiple directions <eos> produced confidence maps rnns employed adaptively regularize learning discriminative correlation filters suppressing clutter background noises while making full use information reliable parts <eos> solve weighted correlation filters especially derive efficient closed form solution sharp reduction computation complexity <eos> extensive experiments demonstrate proposed rtt more competitive over correlation filter based method <eos> <eop> structured regression gradient boosting <eos> propose new way train structured output prediction model <eos> more specifically train nonlinear data terms gaussian conditional random field gcrf generalized version gradient boosting <eos> approach evaluated three challenging regression benchmarks vessel detection single image depth estimation image inpainting <eos> experiments suggest proposed boosting framework matches exceeds state art <eos> <eop> loss functions top error analysis insights <eos> order push performance realistic computer vision tasks number classes modern benchmark datasets significantly increased recent years <eos> increase number classes comes along increased ambiguity between class labels raising question if top error right performance measure <eos> paper provide extensive comparison evaluation established multiclass method comparing their top performance both practical well theoretical perspective <eos> moreover introduce novel top loss functions modifications softmax multiclass svm losses provide efficient optimization schemes them <eos> experiments compare various datasets all proposed established method top error optimization <eos> interesting insight paper softmax loss yields competitive top performance all simultaneously <eos> specific top error new top losses lead typically further improvements while being faster train than softmax <eos> <eop> metric learning convex combinations local models generalization guarantees <eos> over past ten years metric learning allowed improvement numerous machine learning approaches manipulate distances similarities <eos> field local metric learning shown very efficient especially take into account non linearities data better capture peculiarities application interest <eos> however well known local metric learning entail overfitting ii face difficulties compare two instances assigned two different local models <eos> paper address two issues introducing novel metric learning algorithm linearly combines local models lm <eos> starting partition space region model score function each region lm defines metric between point weighted combination models <eos> weight vector learned each pair region spatial regularization ensures weight vectors evolve smoothly nearby models favored combination <eos> proposed approach particularity working regression setting working implicitly different scales being generic enough so applicable similarities distances <eos> prove theoretical guarantees approach using framework algorithmic robustness <eos> carry out experiments datasets using both distances perceptual color distances using mahalanobis like distances similarities semantic word similarities using bilinear forms showing lm consistently improves regression accuracy even case amount training data small <eos> <eop> efficient training very deep neural network supervised hashing <eos> paper propose training very deep neural network dnns supervised learning hash codes <eos> existing method context train relatively shallow network limited issues arising back propagation <eos> vanishing gradients well computational efficiency <eos> propose novel efficient training algorithm inspired alternating direction method multipliers admm overcomes some limitations <eos> method decomposes training process into independent layer wise local updates through auxiliary variables <eos> empirically observe training algorithm always converges its computational complexity linearly proportional number edges network <eos> empirically manage train dnns hidden layer nodes per layer supervised hashing about hours using single gpu <eos> proposed very deep supervised hashing vdsh method significantly outperforms state art several benchmark datasets <eos> <eop> information bottleneck learning using privileged information visual recognition <eos> explore visual recognition problem main data view when auxiliary data view available during training <eos> important because allows improving training visual classifiers when paired additional data cheaply available improves recognition multi view data when there missing view testing time <eos> problem challenging because intrinsic asymmetry caused missing auxiliary view during testing <eos> account such view during training extending information bottleneck method combining risk minimization <eos> way establish information theoretic principle leaning any type visual classifier under particular setting <eos> use principle design large margin classifier efficient optimization primal space <eos> extensively compare method state art different visual recognition datasets different types auxiliary data show proposed framework very promising potential <eos> <eop> action recognition novel viewpoints <eos> propose human pose representation model transfers human poses acquired different unknown views view invariant high level space <eos> model deep convolutional neural network requires large corpus multiview training data very expensive acquire <eos> therefore propose method generate data fitting synthetic three dimensional human models real motion capture data rendering human poses numerous viewpoints <eos> while learning cnn model use action labels but only pose labels after clustering all training poses into clusters <eos> proposed model able generalize real depth image unseen poses without need re training fine tuning <eos> real depth video passed through model frame wise extract view invariant feature <eos> spatio temporal representation propose group sparse fourier temporal pyramid robustly encodes action specific most discriminative output feature proposed human pose model <eos> experiments two multiview three single view benchmark datasets show proposed method dramatically outperforms existing state art action recognition <eos> <eop> shape attributes <eos> paper investigate three dimensional attributes means understand shape object single image <eos> end make number contributions introduce define set three dimensional shape attributes including planarity symmetry occupied space ii show such properties successfully inferred single image using convolutional neural network cnn iii introduce image dataset sculptures works over artists training evaluating cnn iv show three dimensional attributes trained dataset generalize image other non sculpture object classes furthermore show cnn also provides shape embedding used match previously unseen sculptures largely independent viewpoint <eos> <eop> three dimensional object detection layout prediction using clouds oriented gradients <eos> develop new representations algorithms three dimensional three dimensional object detection spatial layout prediction cluttered indoor scenes <eos> rgb image traditionally described local geometric feature three dimensional point cloud <eos> propose cloud oriented gradient cog descriptor links appearance three dimensional pose object categories thus accurately models how perspective projection affects perceived image boundaries <eos> also propose manhattan voxel representation better captures three dimensional room layout geometry common indoor environments <eos> effective classification rules learned via structured prediction framework accounts intersection over union overlap hypothesized three dimensional cuboids human annotations well orientation estimation errors <eos> contextual relationships among categories layout captured via cascade classifiers leading holistic scene hypotheses improved accuracy <eos> model learned solely annotated rgb image without benefit cad models but nevertheless its performance substantially exceeds state art sun rgb database <eos> avoiding cad models allows easier learning detectors many object categories <eos> <eop> semantic parsing large scale indoor spaces <eos> paper propose method semantic parsing three dimensional point cloud entire building using hierarchical approach first raw data parsed into semantically meaningful spaces <eos> rooms etc aligned into canonical reference coordinate system <eos> second spaces parsed into their structural building elements <eos> walls columns etc <eos> performing strong notation global three dimensional space backbone method <eos> alignment first step injects strong three dimensional priors canonical coordinate system into second step dis covering elements <eos> allows diverse challenging scenarios man made indoor spaces often show recurrent geo metric patterns while appearance feature change drastically <eos> also argue identification structural elements indoor spaces essentially detection problem rather than segmentation commonly used <eos> evaluated method new dataset several buildings covered area over over million point demonstrating robust result readily useful practical applications <eos> <eop> dense human body correspondences using convolutional network <eos> propose deep learning approach finding dense correspondences between three dimensional scans people <eos> method requires only partial geometric information form two depth maps partial reconstructed surfaces works humans arbitrary poses wearing any clothing require two people scanned similar viewpoints runs real time <eos> use deep convolutional neural network train feature descriptor depth map pixels but crucially rather than training network solve shape correspondence problem directly train solve body region classification problem modified increase smoothness learned descriptors near region boundaries <eos> approach ensures nearby point human body nearby feature space vice versa rendering feature descriptor suitable computing dense correspondences between scans <eos> validate method real synthetic data both clothed unclothed humans show correspondences more robust than possible state art unsupervised method more accurate found using method require full watertight three dimensional geometry <eos> <eop> geometry informed material recognition <eos> goal recognize material categories using image geometry information <eos> many applications such construction management coarse geometry information available <eos> investigate how three dimensional geometry surface normals camera intrinsic extrinsic parameters used feature texture color improve material classification <eos> introduce new dataset geomat first provide both image geometry data form training testing patches were extracted different scales perspectives real world examples each material category ii large scale construction site scene includes image over hand labeled three dimensional point <eos> result show using three dimensional feature both jointly independently model materials improves classification accuracy across multiple scales viewing directions both material patches image large scale construction site scene <eos> <eop> towards open set deep network <eos> deep network produced significant gains various visual recognition problems leading high impact academic commercial applications <eos> recent work deep network highlighted easy generate image humans would never classify particular object class yet network classify such image high confidence given class deep network easily fooled image humans consider meaningful <eos> closed set nature deep network forces them choose one known classes leading such artifacts <eos> recognition real world open set <eos> recognition system should reject unknown unseen classes test time <eos> present methodology adapt deep network open set recognition introducing new model layer openmax estimates probability input being unknown class <eos> key element estimating unknown probability adapting meta recognition concepts activation patterns penultimate layer network <eos> openmax allows rejection fooling unrelated open set image presented system openmax greatly reduces number obvious errors made deep network <eos> prove openmax concept provides bounded open space risk thereby formally providing open set recognition solution <eos> evaluate resulting open set deep network using pre trained network caffe model zoo imagenet validation data thousands fooling open set image <eos> proposed openmax model significantly outperforms open set recognition accuracy basic deep network well deep network thresholding softmax probabilities <eos> <eop> wrong object identifying image unusual object modelling detection score distribution <eos> paper studies challenging problem identifying unusual instances known object image within open world setting <eos> aim find object members known class but typical class <eos> thus unusual object should distinguished both regular object other object <eos> such unusual object may interest many applications such surveillance quality control <eos> propose identify unusual object inspecting distribution object detection scores multiple image region <eos> key observation motivating approach regular object image unusual object image other object image exhibit different region level scores terms both score values spatial distributions <eos> model distributions propose use gaussian processes gp construct two separate generative models one regular object other other object <eos> more specifically design new covariance function simultaneously model detection score single location score dependencies between multiple region <eos> demonstrate proposed approach outperforms comparable method new large dataset constructed purpose <eos> <eop> large scale location recognition geometric burstiness problem <eos> visual location recognition task determining place depicted query image given database geo tagged image <eos> location recognition often cast image retrieval problem recent research almost exclusively focused improving chance relevant database image ranked high enough after retrieval <eos> implicit assumption number inliers found spatial verification used distinguish between related unrelated database photo high precision <eos> paper show assumption hold large datasets due appearance geometric bursts <eos> set visual elements appearing similar geometric configurations unrelated database photos <eos> propose algorithms detecting handling geometric bursts <eos> although conceptually simple using proposed weighting schemes dramatically improves recall achieved when high precision required compared standard re ranking based inlier count <eos> approach easy implement easily integrated into existing location recognition systems <eop> regularity driven facade matching between aerial street views <eos> present approach detecting matching building facades between aerial view street view image <eos> exploit regularity urban scene facades captured their lattice structures deduced median tiles shape context color texture spatial similarities <eos> experimental result demonstrate effective matching oblique partially occluded facades between aerial ground views <eos> quantitative comparisons automated urban scene facade matching three cities show superior performance method over baseline sift root sift more sophisticated scale selective self similarity binary coherent edge descriptors <eos> also illustrate regularity based applications occlusion removal street views higher resolution texture replacement aerial views <eos> <eop> computational models differ systematically human object perception <eos> recent advances neural network revolutionized computer vision but algorithms still outperformed humans <eos> could performance gap due systematic differences between object representations humans machines answer question collected large dataset perceived dissimilarity measurements visual object across human subjects used dataset train test leading computational models <eos> best model combination all models accounted explainable variance <eos> importantly all computational models showed systematic deviations perception they underestimated perceptual distances between object symmetry large area differences they overestimated perceptual distances between object shared feature <eos> result reveal critical elements missing computer vision algorithms point explicit encoding properties higher visual areas brain <eos> <eop> contour detection unstructured three dimensional point clouds <eos> describe method automatically detect contours <eos> lines along surface orientation sharply changes large scale outdoor point clouds <eos> contours important intermediate feature structuring point clouds converting them into high quality surface solid models extensively used graphics mapping applications <eos> yet detecting them unstructured inhomogeneous point clouds turns out surprisingly difficult existing line detection algorithms largely fail <eos> approach contour extraction two stage discriminative learning problem <eos> first stage contour score each individual point predicted binary classifier using set feature extracted point neighborhood <eos> contour scores serve basis construct overcomplete graph candidate contours <eos> second stage selects optimal set contours candidates <eos> amounts further binary classification higher order mrf whose cliques encode preference connected contours penalize loose ends <eos> method handle point clouds point couple minutes vastly outperforms baseline performs canny style edge detection range image representation point cloud <eos> <eop> unsupervised learning edges <eos> data driven approaches edge detection proven effective achieve top result modern benchmarks <eos> however all current data driven edge detectors require manual supervision training form hand labeled region segments object boundaries <eos> specifically human annotators mark semantically meaningful edges subsequently used training <eos> form strong high level supervision actually necessary learn accurately detect edges work present simple yet effective approach training edge detectors without human supervision <eos> end utilize motion more specifically only input method noisy semi dense matches between frames <eos> begin only rudimentary knowledge edges form image gradients alternate between improving motion estimation edge detection turn <eos> using large corpus video data show edge detectors trained using unsupervised scheme approach performance same method trained full supervision within <eos> finally show when using deep network edge detector approach provides novel pre training scheme object detection <eos> <eop> blind image deblurring using dark channel prior <eos> present simple effective blind image deblurring method based dark channel prior <eos> work inspired interesting observation dark channel blurred image less sparse <eos> while most image patches clean image contain some dark pixels pixels dark when averaged neighboring high intensity pixels during blur process <eos> analysis shows change sparsity dark channel inherent property blur process both theoretically empirically <eos> change sparsity dark channel inherent property blur process both prove mathematically validate using training data <eos> therefore enforcing sparsity dark channel helps blind deblurring various scenarios including natural face text low illumination image <eos> however sparsity dark channel introduces non convex non linear optimization problem <eos> introduce linear approximation min operator compute dark channel <eos> look up table based method converges fast practice directly extended non uniform deblurring <eos> extensive experiments show method achieves state art result deblurring natural image compares favorably method well engineered specific scenarios <eos> <eop> deeply recursive convolutional network image super resolution <eos> propose image super resolution method sr using deeply recursive convolutional network drcn <eos> network very deep recursive layer up recursions <eos> increasing recursion depth improve performance without introducing new parameters additional convolutions <eos> albeit advantages learning drcn very hard standard gradient descent method due exploding vanishing gradients <eos> ease difficulty training propose two extensions recursive supervision skip connection <eos> method outperforms previous method large margin <eos> <eop> accurate image super resolution using very deep convolutional network <eos> present highly accurate single image superresolution sr method <eos> method uses very deep convolutional network inspired vgg net used imagenet classification <eos> find increasing network depth shows significant improvement accuracy <eos> final model uses weight layer <eos> cascading small filters many times deep network structure contextual information over large image region exploited efficient way <eos> very deep network however convergence speed becomes critical issue during training <eos> propose simple yet effective training procedure <eos> learn residuals only use extremely high learning rates times higher than srcnn enabled adjustable gradient clipping <eos> proposed method performs better than existing method accuracy visual improvements result easily noticeable <eos> <eop> raw image reconstruction using self contained srgb jpeg image only kb overhead <eos> most camera image saved bit standard rgb srgb compressed jpegs <eos> even when jpeg compression set its highest quality encoded srgb image significantly processed terms color tone manipulation <eos> makes srgb jpeg image undesirable many computer vision tasks assume direct relationship between pixel values incoming light <eos> such applications raw image format preferred raw represents minimally processed sensor specific rgb image higher dynamic range linear respect scene radiance <eos> drawback raw image however they require large amounts storage well supported many imaging applications <eos> address issue present method encode necessary metadata within srgb image reconstruct high quality raw image <eos> approach requires no calibration camera reconstruct original raw within <eos> error only kb overhead additional data <eos> more importantly output fully self contained complainant srgb jpeg file used affecting any existing image workflow raw image extracted when needed ignored otherwise <eos> detail approach show its effectiveness against competing strategies <eos> <eop> group mad competition new methodology compare objective image quality models <eos> objective image quality assessment iqa models aim automatically predict human visual perception image quality fundamental importance field image processing computer vision <eos> increasing number iqa models proposed how fairly compare their performance becomes major challenge due enormous size image space limited resource subjective testing <eos> standard approach literature compute several correlation metrics between subjective mean opinion scores moss objective model predictions several well known subject rated databases contain distorted image generated few dozens source image provide extremely limited representation real world image <eos> moreover most iqa models were developed after databases became publicly available often involve machine learning manual parameter tuning steps boost their performance databases thus their generalization capabilities questionable <eos> here propose substantially different methodology compare iqa models <eos> first build database contains source natural image together distorted image created them <eos> then propose novel mechanism namely group maximum differentiation gmad competition helps automatically select subsets image pairs database provide strongest test let iqa models compete each other <eos> subjective testing selected subsets reveals relative performance iqa models provides useful insights potential ways improve them <eos> report gmad competition result between well known iqa models but framework extendable allowing future iqa models added into competition <eos> <eop> non local image dehazing <eos> haze limits visibility reduces image contrast outdoor image <eos> degradation different every pixel depends distance scene point camera <eos> dependency expressed transmission coefficients control scene attenuation amount haze every pixel <eos> previous method solve single image dehazing problem using various patch based priors <eos> other hand propose algorithm based new non local prior <eos> algorithm relies assumption colors haze free image well approximated few hundred distinct colors form tight clusters rgb space <eos> key observation pixels given cluster often non local <eos> they spread over entire image plane located different distances camera <eos> presence haze varying distances translate different transmission coefficients <eos> therefore each color cluster clear image becomes line rgb space term haze line <eos> using haze lines algorithm recovers both distance map haze free image <eos> algorithm linear size image deterministic requires no training <eos> performs well wide variety image competitive other state art method <eos> <eop> holistic approach cross channel image noise modeling its application image denoising <eos> modelling analyzing noise image fundamental task many computer vision systems <eos> traditionally noise modelled per color channel assuming color channels independent <eos> although color channels considered mutually independent camera raw image signals different color channels get mixed during imaging process inside camera due gamut mapping tone mapping compression <eos> show influence camera imaging pipeline noise propose new noise model three dimensional rgb space accounts color channel mix ups <eos> data driven approach determining parameters new noise model introduced well its application image denoising <eos> experiments show noise model represents noise regular jpeg image more accurately compared previous models advantageous image denoising <eos> <eop> multispectral image denoising intrinsic tensor sparsity regularization <eos> multispectral image msi help deliver more faithful representation real scenes than traditional image system enhance performance many computer vision tasks <eos> real cases however msi always corrupted various noises <eos> paper propose new tensor based denoising approach fully considering two intrinsic characteristics underlying msi <eos> global correlation along spectrum gcs nonlocal self similarity across space nss <eos> specific construct new tensor sparsity measure called intrinsic tensor sparsity its measure encodes both sparsity insights delivered most typical tucker candecomp parafac cp low rank decomposition general tensor <eos> then build new msi denoising model applying proposed its measure tensors formed non local similar patches within msi <eos> intrinsic gcs nss knowledge then efficiently explored under regularization tensor sparsity measure finely rectify recovery msi its corruption <eos> series experiments simulated real msi denoising problems show method outperforms all state arts under comprehensive quantitative performance measures <eos> <eop> comparative study single image blind deblurring <eos> numerous single image blind deblurring algorithms proposed restore latent sharp image under camera motion <eos> however algorithms mainly evaluated using either synthetic datasets few selected real blurred image <eos> thus unclear how algorithms would perform image acquired wild how could gauge progress field <eos> paper aim bridge gap <eos> present first comprehensive perceptual study analysis single image blind deblurring using real world blurred image <eos> first collect dataset real blurred image dataset synthetically blurred image <eos> using datasets conduct large scale user study quantify performance several representative state art blind deblurring algorithms <eos> second systematically analyze subject preferences including level agreement significance tests score differences rationales preferring one method over another <eos> third study correlation between human subjective scores several full reference no reference image quality metrics <eos> evaluation analysis indicate performance gap between synthetically blurred image real blurred image sheds light future research single image blind deblurring <eos> <eop> spatiotemporal bundle adjustment dynamic three dimensional reconstruction <eos> bundle adjustment jointly optimizes camera intrinsics extrinsics three dimensional point triangulation reconstruct static scene <eos> triangulation constraint however invalid moving point captured multiple unsynchronized video bundle adjustment purposed estimate temporal alignment between cameras <eos> paper present spatiotemporal bundle adjustment approach jointly optimizes four coupled sub problems estimating camera intrinsics extrinsics triangulating three dimensional static point well subframe temporal alignment between cameras estimating three dimensional trajectories dynamic point <eos> key joint optimization careful integration physics based motion priors within reconstruction pipeline validated large motion capture corpus <eos> present end end pipeline takes multiple uncalibrated unsynchronized video streams produces dynamic reconstruction event <eos> because video aligned sub frame precision reconstruct three dimensional trajectories unconstrained outdoor activities much higher temporal resolution than input video <eos> <eop> inextensible non rigid shape motion second order cone programming <eos> present global convex formulation template less three dimensional reconstruction deforming object perspective camera <eos> show first time how construct second order cone programming socp problem non rigid shape motion nrsfm using maximum depth heuristic mdh <eos> regard deviate strongly general trend using affine cameras factorization based method solve nrsfm <eos> mdh point depths maximized so distance between neighbouring point camera space upper bounded geodesic distance <eos> nrsfm both geodesic camera space distances unknown <eos> show nonetheless given point correspondences camera intrinsics whole problem convex solvable socp <eos> show extensive experiments method accurately reconstructs quasi isometric surfaces partial views under articulated strong deformations <eos> naturally handles missing correspondences non smooth object very simple implement compared previous method only one free parameter neighbourhood size <eos> <eop> optimal relative pose unknown correspondences <eos> previous work estimating epipolar geometry two views relies being able reliably match feature point based appearance <eos> paper go one step further show feasible compute both epipolar geometry correspondences same time based geometry only <eos> globally optimal manner <eos> approach based efficient branch bound technique combination bipartite matching solve correspondence problem <eos> rely several recent works obtain good bounding functions battle combinatorial explosion possible matchings <eos> experimentally demonstrated more difficult cases handled more inlier correspondences obtained being less restrictive matching phase <eos> <eop> homography estimation common self polar triangle separate ellipses <eos> how avoid ambiguity challenging problem conic based homography estimation <eos> paper address problem homography estimation two separate ellipses <eos> find any two ellipses unique common self polar triangle provide three line correspondences <eos> furthermore investigating location feature common self polar triangle show one vertex triangle lies outside both ellipses while other two vertices lies inside ellipses separately <eos> accordingly one more line correspondence obtained intersections conics common self polar triangle <eos> therefore four line correspondences obtained based common self polar triangle provide enough constraints homography estimation <eos> main contributions paper include new discovery location feature common self polar triangle separate ellipses <eos> novel approach homography estimation <eos> simulate experiments real experiments conducted demonstrate feasibility accuracy approach <eos> <eop> heterogeneous light fields <eos> contrast traditional binocular multi view stereo approaches adequately sampled space observations light field imaging allows obtain dense high quality depth maps <eos> also extends capabilities beyond traditional method <eos> previously constant intensity assumed estimating disparity orientation most approaches analyze epipolar plane image epis <eos> here introduce modified structure tensor approach improves depth estimation <eos> extension also includes model non constant intensity epi manifolds <eos> derive approach estimate high quality depth maps luminance gradient light fields well color filtered light fields <eos> color filtered light fields pose particular challenges due fact structures change significantly appearance wavelength completely vanish some wavelength <eos> demonstrate solutions challenge obtain dense srgb image reconstruction addition dense depth maps <eos> <eop> consensus based framework distributed bundle adjustment <eos> paper study large scale optimization problems multi view geometry particular bundle adjustment problem <eos> its conventional formulation complexity existing solvers scale poorly problem size hence component structure motion pipeline quickly become bottle neck <eos> here present novel formulation solving bundle adjustment truly distributed manner using consensus based optimization method <eos> algorithm presented concise derivation based proximal splitting along theoretical proof convergence brief discussions complexity implementation <eos> experiments number real image datasets convincingly demonstrates potential proposed method outperforming conventional bundle adjustment formulation orders magnitude <eos> <eop> globally optimal manhattan frame estimation real time <eos> given set surface normals pose manhattan frame mf estimation problem consensus set maximization maximizes number inliers over rotation search space <eos> solve problem through branch bound framework mathematically guarantees globally optimal solution <eos> however computational time conventional branch bound algorithms intractable real time performance <eos> paper propose novel bound computation method within efficient measurement domain mf estimation <eos> extended gaussian image egi <eos> relaxing original problem compute bounds real time while preserving global optimality <eos> furthermore quantitatively qualitatively demonstrate performance proposed method synthetic real world data <eos> also show versatility approach through two applications extension multiple mf estimation video stabilization <eos> <eop> mirror surface reconstruction under uncalibrated camera <eos> paper addresses problem mirror surface reconstruction solution based observing reflections moving reference plane mirror surface proposed <eos> unlike previous approaches require tedious work calibrate camera method recover both camera intrinsics extrinsics together mirror surface reflections reference plane under least three unknown distinct poses <eos> previous work demonstrated three dimensional poses reference plane registered common coordinate system using reflection correspondences established across image <eos> leads bunch registered three dimensional lines formed reflection correspondences <eos> given lines first derive analytical solution recover camera projection matrix through estimating line projection matrix <eos> then optimize camera projection matrix minimizing reprojection errors computed based cross ratio formulation <eos> mirror surface finally reconstructed based optimized cross ratio constraint <eos> experimental result both synthetic real data presented demonstrate feasibility accuracy method <eos> <eop> hole filling approach based background reconstruction view synthesis three dimensional video <eos> depth image based rendering dibr plays key role three dimensional video synthesis other virtual views generated video its depth map <eos> however synthesis process background occluded foreground object might exposed new view resulting some holes synthetized video <eos> paper hole filling approach based background reconstruction proposed temporal correlation information both video its corresponding depth map exploited construct background video <eos> construct clean background video foreground object detected removed <eos> also motion compensation applied make background reconstruction model suitable moving camera scenario <eos> each frame projected current plane modified gaussian mixture model performed <eos> constructed background video used eliminate holes synthetized video <eos> experimental result indicated proposed approach better quality synthetized three dimensional video compared other method <eos> <eop> direct least squares solution pnp problem unknown focal length <eos> work propose direct least squares solution perspective point pose estimation problem partially calibrated camera whose intrinsic parameters except focal length known <eos> basic idea construct proper objective function respect target variables extract all its stationary point so find global minimum <eos> advantages proposed solution over existing ones objective function directly built upon imaging equation such all three dimensional correspondences treated balance ii proposed solution noniterative sense stationary point retrieved means standard eigenvalue factorization common iterative refinement step needed <eos> addition proposed solution complexity used handle both planar nonplanar three dimensional point <eos> experimental result shown proposed solution much more accurate than existing state art solutions even comparable maximum likelihood estimation minimizing reprojection error <eos> <eop> efficient intersection three quadrics applications computer vision <eos> paper present new algorithm finding all intersections three quadrics <eos> proposed method algebraic nature considerably more efficient than groebner basis resultant based solutions previously used computer vision applications <eos> identify several computer vision problems formulated solved systems three quadratic equations algorithm readily delivers considerably faster result <eos> also propose new formulations three important vision problems absolute camera pose unknown focal length generalized pose scale hand eye calibration known translation <eos> new formulations allow algorithm significantly outperform state art speed <eos> <eop> using spatial order boost elimination incorrect feature matches <eos> correctly matching feature point pair image important preprocessing step many computer vision applications <eos> paper propose efficient method estimating number correct matches without explicitly computing them <eos> addition method estimates region overlap between image <eos> end propose analyze set matches using spatial order feature projected axis image <eos> set feature each image thus represented sequence <eos> reduces analysis matching problem analysis permutation between sequences <eos> using kendall distance metric between permutations natural assumptions distribution correct incorrect matches show how estimate above mentioned values <eos> demonstrate usefulness method two applications new halting condition ransac based epipolar geometry estimation method considerably reduce running time ii discarding spatially unrelated image pairs structure motion pipeline <eos> furthermore analysis allows compute probability given match correct based estimated number correct matches rank feature within sequences <eos> experiments large number synthetic real data demonstrate effectiveness method <eos> example running time image matching stage structure motion pipeline may reduced about while preserving about correctly matched feature point <eos> <eop> probabilistic framework color based point set registration <eos> recent years sensors capable measuring both color depth information become increasingly popular <eos> despite abundance colored point set data state art probabilistic registration techniques ignore available color information <eos> paper propose probabilistic point set registration framework exploits available color information associated point <eos> method based model joint distribution three dimensional point observations their color information <eos> proposed model captures discriminative color information while being computationally efficient <eos> derive em algorithm jointly estimating model parameters relative transformations <eos> comprehensive experiments performed stanford lounge dataset captured rgb camera two point set captured lidar sensor <eos> result demonstrate significant gain robustness accuracy when incorporating color information <eos> stanford lounge dataset approach achieves relative reduction failure rate compared baseline <eos> furthermore proposed model outperforms standard strategies combining color three dimensional point information leading state art result <eos> <eop> blind image deconvolution automatic gradient activation <eos> blind image deconvolution ill posed inverse problem often addressed through application appropriate prior <eos> although some priors informative general many image strictly conform leading degraded performance kernel estimation <eos> more critically real image may contaminated nonuniform noise such saturation outliers <eos> method removing specific image areas based some priors proposed but they operate either manually defining fixed criteria <eos> show here subset image gradients adequate estimate blur kernel robustly no matter gradient image sparse <eos> thus introduce gradient activation method automatically select subset gradients latent image cutting plane based optimization scheme kernel estimation <eos> no extra assumption used model greatly improves accuracy flexibility <eos> more importantly proposed method affords great convenience handling noise outliers <eos> experiments both synthetic data real world image demonstrate effectiveness robustness proposed method comparison state art method <eos> <eop> psyco manifold span reduction super resolution <eos> main challenge super resolution sr discover mapping between low high resolution manifolds image patches complex ill posed problem recently addressed through piecewise linear regression promising result <eos> paper present novel regression based sr algorithm benefits extended knowledge structure both manifolds <eos> propose transform collapses variations induced dihedral group transforms <eos> rotations vertical horizontal reflections antipodality <eos> diametrically opposed point unitary sphere into single primitive <eos> key idea transform study different dihedral elements group symmetries within high dimensional manifold <eos> obtain respective set mirror symmetry axes means frequency analysis dihedral elements use them collapse redundant variability through modified symmetry distance <eos> experimental validation algorithm shows effectiveness approach obtains competitive quality dictionary little atoms reducing other method dictionaries least factor further pushing state art atoms dictionary <eos> <eop> parametric object motion blur <eos> motion blur adversely affect number vision tasks hence generally considered nuisance <eos> instead treat motion blur useful signal allows compute motion object single image <eos> drawing success joint segmentation parametric motion models context optical flow estimation propose parametric object motion model combined segmentation mask exploit localized non uniform motion blur <eos> parametric image formation model differentiable <eos> motion parameters enables generalize marginal likelihood techniques uniform blind deblurring localized non uniform blur <eos> two stage pipeline first derivative space then image space allows estimate both parametric object motion well motion segmentation single image alone <eos> experiments demonstrate its ability cope very challenging cases object motion blur <eos> <eop> image deblurring using smartphone inertial sensors <eos> removing image blur caused camera shake ill posed problem both latent image point spread function psf unknown <eos> recent approach address problem record camera motion through inertial sensors <eos> gyroscopes accelerometers then reconstruct spatially variant psfs readings <eos> while approach effective high quality inertial sensors infeasible inertial sensors smartphones relatively low quality present number challenging issues including varying sensor parameters high sensor noise calibration error <eos> paper identify issues plague smartphone inertial sensors propose solution successfully utilizes sensor readings image deblurring <eos> both sensor data image itself proposed method able accurately estimate sensor parameters online also spatially variant psfs enhanced deblurring performance <eos> effectiveness technique demonstrated experiments popular mobile phone <eos> approach quality image deblurring appreciably raised most common imaging devices <eos> <eop> seven ways improve example based single image super resolution <eos> paper present seven techniques everybody should know improve example based single image super resolution sr augmentation data use large dictionaries efficient search structures cascading image self similarities back projection refinement enhanced prediction consistency check context reasoning <eos> validate seven techniques standard sr benchmarks <eos> set set method <eos> srcnn anr zeyde yang achieve substantial improvements <eos> techniques widely applicable require no changes only minor adjustments sr method <eos> moreover improved ia method set new state art result outperforming up <eos> db average psnr whilst maintaining low time complexity <eos> <eop> real time single image video super resolution using efficient sub pixel convolutional neural network <eos> recently several models based deep neural network achieved great success terms both reconstruction accuracy computational performance single image super resolution <eos> method low resolution lr input image upscaled high resolution hr space using single filter commonly bicubic interpolation before reconstruction <eos> means super resolution sr operation performed hr space <eos> demonstrate sub optimal adds computational complexity <eos> paper present first convolutional neural network cnn capable real time sr video single gpu <eos> achieve propose novel cnn architecture feature maps extracted lr space <eos> addition introduce efficient sub pixel convolution layer learns array upscaling filters upscale final lr feature maps into hr output <eos> doing so effectively replace handcrafted bicubic filter sr pipeline more complex upscaling filters specifically trained each feature map whilst also reducing computational complexity overall sr operation <eos> evaluate proposed approach using image video publicly available datasets show performs significantly better <eos> db video order magnitude faster than previous cnn based method <eos> <eop> they equally reliable semantic event search using differentiated concept classifiers <eos> complex event detection unconstrained internet video seen much progress recent years <eos> however state art performance degrades dramatically when number positive training exemplars falls short <eos> since label acquisition costly laborious time consuming there real need consider much more challenging semantic event search problem no example video given <eos> paper present state art event search system without any example video <eos> relying key observation events <eos> dog show usually compositions multiple mid level concepts <eos> dog theater dog jumping first train skip gram model measure relevance each concept event interest <eos> relevant concept classifiers then cast votes test video but their reliability due lack labeled training video largely unaddressed <eos> propose combine concept classifiers based principled estimate their accuracy unlabeled test video <eos> novel warping technique proposed improve performance efficient highly scalable algorithm provided quickly solve resulting optimization <eos> conduct extensive experiments latest trecvid medtest medtest ccv datasets achieve state art performances <eos> <eop> going deeper into first person activity recognition <eos> bring together ideas recent work feature design egocentric action recognition under one framework exploring use deep convolutional neural network cnn <eos> recent work shown feature such hand appearance object attributes local hand motion camera ego motion important characterizing first person actions <eos> integrate ideas under one framework propose twin stream network architecture one stream analyzes appearance information other stream analyzes motion information <eos> appearance stream encodes prior knowledge egocentric paradigm explicitly training network segment hands localize object <eos> visualizing certain neuron activation network show proposed architecture naturally learns feature capture object attributes hand object configurations <eos> extensive experiments benchmark egocentric action datasets show deep architecture enables recognition rates significantly outperform state art techniques average <eos> increase accuracy over all datasets <eos> furthermore learning recognize object actions activities jointly performance individual recognition tasks also increase actions object <eos> also include result extensive ablative analysis highlight importance network design decisions <eos> <eop> cascaded interactional targeting network egocentric video analysis <eos> knowing how hands move object being manipulated two key sub tasks analyzing first person egocentric action <eos> however lack fully annotated hand data well imprecise foreground segmentation make either sub task challenging <eos> work aims explicitly address two issues via introducing cascaded interactional targeting <eos> infer both hand active object region deep neural network <eos> firstly novel em like learning framework proposed train pixel level deep convolutional neural network dcnn seamlessly integrating weakly supervised data <eos> massive bounding box annotations small set strongly supervised data <eos> fully annotated hand segmentation maps achieve state art hand segmentation performance <eos> secondly resulting high quality hand segmentation maps further paired corresponding motion maps object feature maps order explore contextual information among object motion hand generate interactional foreground region operated object <eos> resulting interactional target maps hand active object cascaded dcnn further utilized form discriminative action representation <eos> experiments show framework achieved state art egocentric action recognition performance benchmark dataset activities daily living adl <eos> <eop> fast temporal activity proposals efficient detection human actions untrimmed video <eos> many large scale video analysis scenarios one interested localizing recognizing human activities occur short temporal intervals within long untrimmed video <eos> current approaches activity detection still struggle handle large scale video collections task remains relatively unexplored <eos> part due computational complexity current action recognition approaches lack method proposes fewer intervals video activity processing focused <eos> paper introduce proposal method aims recover temporal segments containing actions untrimmed video <eos> building techniques learning sparse dictionaries introduce learning framework represent retrieve activity proposals <eos> demonstrate capabilities method only producing high quality proposals but also its efficiency <eos> finally show positive impact method recognition performance when used action detection while running fps <eos> <eop> discriminative hierarchical rank pooling activity recognition <eos> present hierarchical rank pooling video sequence encoding method activity recognition <eos> consists network rank pooling functions captures dynamics rich convolutional neural network feature within video sequence <eos> stacking non linear feature functions rank pooling over one another obtain high capacity dynamic encoding mechanism used action recognition <eos> present method jointly learning video representation activity classifier parameters <eos> method obtains state art result three important activity recognition benchmarks <eos> <eop> convolutional two stream network fusion video action recognition <eos> recent applications convolutional neural network convnets human action recognition video proposed different solutions incorporating appearance motion information <eos> study number ways fusing convnet towers both spatially temporally order best take advantage spatio temporal information <eos> make following findings rather than fusing softmax layer spatial temporal network fused convolution layer without loss performance but substantial saving parameters ii better fuse such network spatially last convolutional layer than earlier additionally fusing class prediction layer boost accuracy finally iii pooling abstract convolutional feature over spatiotemporal neighbourhoods further boosts performance <eos> based studies propose new convnet architecture spatiotemporal fusion video snippets evaluate its performance standard benchmarks architecture achieves state art result <eos> <eop> learning activity progression lstms activity detection early detection <eos> work improve training temporal deep models better learn activity progression activity detection early detection <eos> conventionally when training recurrent neural network specifically long short term memory lstm model training loss only considers classification error <eos> however argue detection score correct activity category detection score margin between correct incorrect categories should monotonically non decreasing model observes more activity <eos> design novel ranking losses directly penalize model violation such monotonicities used together classification loss training lstm models <eos> evaluation activitynet shows significant benefits proposed ranking losses both activity detection early detection tasks <eos> <eop> vlad encoding dynamics deep feature action recognition <eos> previous approaches action recognition deep feature tend process video frames only within small temporal region model long range dynamic information explicitly <eos> however such information important accurate recognition actions especially discrimination complex activities share sub actions when dealing untrimmed video <eos> here propose representation vlad deep dynamics vlad accounts different levels video dynamics <eos> captures short term dynamics deep convolutional neural network feature relying linear dynamic systems lds model medium range dynamics <eos> account long range inhomogeneous dynamics vlad descriptor derived lds pooled over whole video arrive final vlad representation <eos> extensive evaluation was performed olympic sports ucf thumos use vlad representation leads state art result <eos> <eop> multi stream bi directional recurrent neural network fine grained action detection <eos> present multi stream bi directional recurrent neural network fine grained action detection <eos> recently two stream convolutional neural network cnn trained stacked optical flow image frames successful action recognition video <eos> system uses tracking algorithm locate bounding box around person provides frame reference appearance motion also suppresses background noise within bounding box <eos> train two additional streams motion appearance cropped tracked bounding box along full frame streams <eos> motion streams use pixel trajectories frame raw feature displacement values corresponding moving scene point same spatial position across several frames <eos> model long term temporal dynamics within between actions multi stream cnn followed bi directional long short term memory lstm layer <eos> show bi directional lstm network utilizes about seconds video sequence predict action label <eos> test two action detection datasets mpii cooking dataset new merl shopping dataset introduce make available community paper <eos> result demonstrate method significantly outperforms state art action detection method both datasets <eos> <eop> hierarchical deep temporal model group activity recognition <eos> group activity recognition temporal dynamics whole activity inferred based dynamics individual people representing activity <eos> build deep model capture dynamics based lstm long short term memory models <eos> make use observations present stage deep temporal model group activity recognition problem <eos> model lstm model designed represent action dynamics individual people sequence another lstm model designed aggregate person level information whole activity understanding <eos> evaluate model over two datasets collective activity dataset new volleyball dataset <eos> experimental result demonstrate proposed model improves group activity recognition performance compared baseline method <eos> <eop> hierarchical pose based approach complex action understanding using dictionaries actionlets motion poselets <eos> paper introduce new hierarchical model human action recognition able categorize complex actions performed video <eos> model also able perform spatio temporal annotation atomic actions compose overall complex action <eos> each atomic action model generates temporal atomic action annotations inferring starting ending times atomic action well spatial annotations inferring human body parts involved each atomic action <eos> model three key properties trained no spatial supervision able automatically discover relevant body parts temporal action annotations only ii its jointly learned poselet actionlet representation encodes visual variability actions good generalization power iii its mechanism handling noisy body pose estimates make robust common pose estimation errors <eos> experimentally evaluate performance method multiple action recognition benchmarks <eos> model consistently outperform baselines state art action recognition method <eos> <eop> key volume mining deep framework action recognition <eos> recently deep learning approaches demonstrated remarkable progresses action recognition video <eos> most existing deep frameworks equally treat every volume <eos> spatial temporal video clip directly assign video label all volumes sampled <eos> however within video discriminative actions may occur sparsely few key volumes most other volumes irrelevant labeled action category <eos> training large proportion irrelevant volumes will hurt performance <eos> address issue propose key volume mining deep framework identify key volumes conduct classification simultaneously <eos> specifically framework trained end end em like loop <eos> forward pass network mines key volumes each action class <eos> backward pass updates network parameters help mined key volumes <eos> addition propose stochastic out handle key volumes multi modalities effective yet simple unsupervised key volume proposal method high quality volume sampling <eos> experiments show action recognition performance significantly improved mining key volumes method achieve state art performance ucf <eos> <eop> improved hamming distance search using variable length substrings <eos> paper addresses problem ultra large scale search hamming spaces <eos> there considerable research generating compact binary codes vision example visual search tasks <eos> however issue efficient searching through huge set binary codes remains largely unsolved <eos> end propose novel unsupervised approach thresholded search hamming space supporting long codes <eos> bits wide range hamming distance radii <eos> method capable working efficiently billions codes delivering between one three orders magnitude acceleration compared prior art <eos> achieved relaxing equal size constraint multi index hashing approach leading multiple hash tables variable length hash keys <eos> based theoretical analysis retrieval probabilities multiple hash tables propose novel search algorithm obtaining suitable set hash key lengths <eos> resulting retrieval mechanism shown empirically improve efficiency over state art across range datasets bit depths retrieval thresholds <eos> <eop> shortlist selection residual aware distance estimator nearest neighbor search <eos> paper introduce novel shortlist computation algorithm approximate high dimensional nearest neighbor search <eos> method relies novel distance estimator residual aware distance estimator accounts residual distances data point their respective quantized centroids uses accurate shortlist computation <eos> furthermore perform residual aware distance estimation little additional memory computational cost through simple pre computation method inverted index multi index schemes <eos> because modifies initial shortlist collection phase new algorithm applicable most inverted indexing method use vector quantization <eos> tested proposed method inverted index multi index diverse set benchmarks including up one billion data point varying dimensions found method robustly improves accuracy shortlists up relatively higher over state art techniques comparable even faster computational cost <eos> <eop> supervised quantization similarity search <eos> paper address problem searching semantically similar image large database <eos> present compact coding approach supervised quantization <eos> approach simultaneously learns feature selection linearly transforms database point into low dimensional discriminative subspace quantizes data point transformed space <eos> optimization criterion quantized point only approximate transformed point accurately but also semantically separable point belonging class lie cluster overlapped other clusters corresponding other classes formulated classification problem <eos> experiments several standard datasets show superiority approach over state art supervised hashing unsupervised quantization algorithms <eos> <eop> efficient large scale approximate nearest neighbor search gpu <eos> present new approach efficient approximate nearest neighbor ann search high dimensional spaces extending idea product quantization <eos> propose two level product vector quantization tree reduces number vector comparisons required during tree traversal <eos> approach also includes novel highly parallelizable re ranking method candidate vectors efficiently reusing already computed intermediate values <eos> due its small memory footprint during traversal method lends itself efficient parallel gpu implementation <eos> product quantization tree approach significantly outperforms recent state art method high dimensional nearest neighbor queries standard reference datasets <eos> ours first work demonstrates gpu performance superior cpu performance high dimensional large scale ann problems time critical real world applications like loop closing video <eos> <eop> collaborative quantization cross modal similarity search <eos> cross modal similarity search problem about designing search system supporting querying across content modalities <eos> using image search texts using text search image <eos> paper presents compact coding solution efficient search focus quantization approach already shown superior performance over hashing solutions single modal similarity search <eos> propose cross modal quantization approach among early attempts introduce quantization into cross modal search <eos> major contribution lies jointly learning quantizers both modalities through aligning quantized representations each pair image text belonging document <eos> addition approach simultaneously learns common space both modalities quantization conducted enable efficient effective search using euclidean distance computed common space fast distance table lookup <eos> experimental result compared several competitive algorithms over three benchmark datasets demonstrate proposed approach achieves state art performance <eos> <eop> aggregating image text quantized correlated components <eos> cross modal tasks occur naturally multimedia content described along two more modalities like visual content text <eos> such tasks require translate information one modality another <eos> method like kernelized canonical correlation analysis kcca attempt solve such tasks finding aligned subspaces description spaces different modalities <eos> since they favor correlations against modality specific information method shown some success both cross modal bi modal tasks <eos> however show direct use subspace alignment obtained kcca only leads coarse translation abilities <eos> address problem first put forward here new representation method aggregates information provided projections both modalities their aligned subspaces <eos> further suggest method relying neighborhoods subspaces complete uni modal information <eos> proposal exhibits state art result bi modal classification pascal voc cross modal retrieval flickr flickr <eos> <eop> efficient indexing billion scale datasets deep descriptors <eos> existing billion scale nearest neighbor search systems mostly compared single dataset billion sift vectors systems based inverted multi index imi performing very well achieving state art recall several milliseconds <eos> sift like descriptors however quickly being replaced descriptors based deep neural network dnn provide better performance many computer vision tasks <eos> paper introduce new dataset one billion descriptors based dnns reveal relative inefficiency imi based indexing such descriptors compared sift data <eos> then introduce two new indexing structures non orthogonal inverted multi index no imi generalized non orthogonal inverted multi index gno imi <eos> show due additional flexibility new structures able adapt dnn descriptor distribution better way <eos> particular extensive experiments new dataset demonstrate data structures provide considerably better trade off between speed retrieval recall given similar amount memory compared standard inverted multi index <eos> <eop> deep supervised hashing fast image retrieval <eos> paper present new hashing method learn compact binary codes highly efficient image retrieval large scale datasets <eos> while complex image appearance variations still pose great challenge reliable retrieval light recent progress convolutional neural network cnn learning robust image representation various vision tasks paper proposes novel deep supervised hashing dsh method learn compact similarity preserving binary code huge body image data <eos> specifically devise cnn architecture takes pairs image similar dissimilar training inputs encourages output each image approximate discrete values <eos> end loss function elaborately designed maximize discriminability output space encoding supervised information input image pairs simultaneously imposing regularization real valued outputs approximate desired discrete values <eos> image retrieval new coming query image easily encoded propagating through network then quantizing network outputs binary codes representation <eos> extensive experiments two large scale datasets cifar nus wide show promising performance method compared state arts <eos> <eop> efficient large scale similarity search using matrix factorization <eos> consider image retrieval problem finding image dataset most similar query image <eos> goal reduce number vector operations memory performing search without sacrificing accuracy returned image <eos> adopt group testing formulation design decoding architecture using either dictionary learning eigendecomposition <eos> latter plausible option small medium sized problems high dimensional global image descriptors whereas dictionary learning applicable large scale scenarios <eos> evaluate approach global descriptors obtained both sift cnn feature <eos> experiments standard image search benchmarks including yahoo dataset comprising million image show method gives comparable sometimes superior accuracy compared exhaustive search while requiring only vector operations memory <eos> moreover same search complexity method gives significantly better accuracy compared approaches based dimensionality reduction locality sensitive hashing <eos> <eop> incremental object discovery time varying image collections <eos> abstract paper address problem object discovery time varying large scale image collections <eos> core part approach novel limited horizon minimum spanning tree lh mst structure closely approximates minimum spanning tree small fraction latter computational cost <eos> proposed tree structure created local neighborhood matching graph during image retrieval efficiently updated whenever image database extended <eos> show how lh mst used within both single link hierarchical agglomer ative clustering iconoid shift framework object discovery image collections resulting significant efficiency gains making both approaches capable incremental clustering online updates <eos> evaluate approach dataset image city paris compare its result batch version both clustering algorithms <eos> <eop> detecting migrating birds night <eos> bird migration critical indicator environmental health biodiversity climate change <eos> existing techniques monitoring bird migration either expensive <eos> satellite tracking labor intensive <eos> moon watching indirect thus less accurate <eos> weather radar intrusive <eos> attaching geolocators captured birds <eos> paper present vision based system detecting migrating birds flight night <eos> system takes stereo video night sky inputs detects multiple flying birds estimates their orientations speeds altitudes <eos> main challenge lies detecting flying birds unknown trajectories under high noise level due low light environment <eos> address problem incorporating stereo constraints rejecting physically implausible configurations gathering evidence two more views <eos> specifically develop robust stereo based three dimensional line fitting algorithm geometric verification deformable part response accumulation strategy trajectory verification <eos> demonstrate effectiveness proposed approach through quantitative evaluation real video birds migrating night collected near infrared cameras <eos> <eop> when naive bayes nearest neighbors meet convolutional neural network <eos> since convolutional neural network cnn become leading learning paradigm visual recognition naive bayes nearest neighbor nbnn based classifiers lost momentum community <eos> because such algorithms cannot use cnn activations input feature they cannot used final layer cnn architectures end end training they generally scalable hence cannot handle big data <eos> paper proposes framework addresses all issues thus bringing back nbnns map <eos> solve first extracting cnn activations local patches multiple scale levels similarly <eos> address simultaneously second third proposing scalable version naive bayes non linear learning nbnl <eos> result obtained using pre trained cnn standard scene domain adaptation databases show strength approach opening new season nbnns <eos> <eop> traffic sign detection classification wild <eos> although promising result achieved areas traffic sign detection classification few works provided simultaneous solutions two tasks realistic real world image <eos> make two contributions problem <eos> firstly created large traffic sign benchmark tencent street view panoramas going beyond previous benchmarks <eos> provides image containing traffic sign instances <eos> image cover large variations illuminance weather conditions <eos> each traffic sign benchmark annotated class label its bounding box pixel mask <eos> call benchmark tsinghua tencent <eos> secondly demonstrate how robust end end convolutional neural network cnn simultaneously detect classify traffic signs <eos> most previous cnn image processing solutions target object occupy large proportion image such network work well target object occupying only small fraction image like traffic signs here <eos> experimental result show robustness network its superiority alternatives <eos> benchmark source code cnn model introduced paper publicly available <eos> <eop> large scale semi supervised object detection using visual semantic knowledge transfer <eos> deep cnn based object detection systems achieved remarkable success several large scale object detection benchmarks <eos> however training such detectors requires large number labeled bounding boxes more difficult obtain than image level annotations <eos> previous work addresses issue transforming image level classifiers into object detectors <eos> done modeling differences between two categories both image level bounding box annotations transferring information convert classifiers detectors categories without bounding box annotations <eos> improve previous work incorporating knowledge about object similarities visual semantic domains during transfer process <eos> intuition behind proposed method visually semantically similar categories should exhibit more common transferable properties than dissimilar categories <eos> better detector would result transforming differences between dog classifier dog detector onto cat class than would transforming violin class <eos> experimental result challenging ilsvrc detection dataset demonstrate each proposed object similarity based knowledge transfer method outperforms baseline method <eos> found strong evidence visual similarity semantic relatedness complementary task when combined notably improve detection achieving state art detection performance semi supervised setting <eos> <eop> exploit all layer fast accurate cnn object detector scale dependent pooling cascaded rejection classifiers <eos> paper investigate two new strategies detect object accurately efficiently using deep convolutional neural network scale dependent pooling layer wise cascaded rejection classifiers <eos> scale dependent pooling sdp improves detection accuracy exploiting appropriate convolutional feature depending scale candidate object proposals <eos> cascaded rejection classifiers crc effectively utilize convolutional feature eliminate negative object proposals cascaded manner greatly speeds up detection while maintaining high accuracy <eos> combination two method achieves significantly better accuracy compared other state arts three challenging datasets pascal object detection challenge kitti object detection benchmark newly collected inner city dataset while being more efficient <eos> <eop> dictionary pair classifier driven convolutional neural network object detection <eos> feature representation object category classification two key components most object detection method <eos> while significant improvements achieved deep feature representation learning traditional svm softmax classifiers remain dominant method final object category classification <eos> however svm softmax classifiers lack capacity explicitly exploiting complex structure deep feature they purely discriminative method <eos> recently proposed discriminative dictionary pair learning dpl model involves fidelity term minimize reconstruction loss discrimination term enhance discriminative capability learned dictionary pair thus appropriate balancing representation discrimination boost object detection performance <eos> paper propose novel object detection system unifying dpl convolutional feature learning <eos> specifically incorporate dpl dictionary pair classifier layer dpcl into deep architecture develop end end learning algorithm optimizing dictionary pairs neural network simultaneously <eos> moreover design multi task loss guiding model accomplish three correlated tasks objectness estimation categoryness computation bounding box regression <eos> extensive experiments pascal voc benchmarks approach demonstrates effectiveness substantially improve performances over popular existing object detection frameworks <eos> cnn frcn achieves new state arts <eos> <eop> monocular three dimensional object detection autonomous driving <eos> goal paper perform three dimensional object detection single monocular image domain autonomous driving <eos> method first aims generate set candidate class specific object proposals then run through standard cnn pipeline obtain high quality object detections <eos> focus paper proposal generation <eos> particular propose probabilistic model places object candidates three dimensional using prior ground plane <eos> then score each candidate box projected image plane via several intuitive potentials such semantic segmentation contextual information size location priors typical object shape <eos> weights model trained svm <eos> experiments show object proposal generation approach significantly outperforms all monocular baselines achieves best detection performance challenging kitti benchmark among published monocular competitors <eos> <eop> how hard estimating difficulty visual search image <eos> address problem estimating image difficulty defined human response time solving visual search task <eos> collect human annotations image difficulty pascal voc data set through crowd sourcing platform <eos> then analyze human interpretable image properties impact visual search difficulty how accurate properties predicting difficulty <eos> next build regression model based deep feature learned state art convolutional neural network show better result predicting ground truth visual search difficulty scores produced human annotators <eos> model able correctly rank about image pairs according their difficulty score <eos> also show difficulty predictor generalizes well new classes seen during training <eos> finally demonstrate predicted difficulty scores useful weakly supervised object localization improvement semi supervised object classification improvement <eos> <eop> deep relative distance learning tell difference between similar vehicles <eos> growing explosion use surveillance cameras public security highlights importance vehicle search large scale image video database <eos> however compared person re identification face recognition vehicle search problem long neglected researchers vision community <eos> paper focuses interesting but challenging problem vehicle re identification <eos> precise vehicle search <eos> propose deep relative distance learning drdl method exploits two branch deep convolutional network project raw vehicle image into euclidean space distance directly used measure similarity arbitrary two vehicles <eos> further facilitate future research problem also present carefully organized large scale image database vehicleid includes multiple image same vehicle captured different real world cameras city <eos> evaluate drdl method vehicleid dataset another recently released vehicle model classification dataset compcars three set experiments vehicle re identification vehicle model verification vehicle retrieval <eos> experimental result show method achieve promising result outperforms several state art approaches <eos> <eop> eye tracking everyone <eos> scientific research commercial applications eye tracking important tool across many domains <eos> despite its range applications eye tracking yet become pervasive technology <eos> believe put power eye tracking everyone palm building eye tracking software works commodity hardware such mobile phones tablets without need additional sensors devices <eos> tackle problem introducing gazecapture first large scale dataset eye tracking containing data over people consisting almost frames <eos> using gazecapture train itracker convolutional neural network eye tracking achieves significant reduction error over previous approaches while running real time fps modern mobile device <eos> model achieves prediction error <eos> cm without calibration mobile phones tablets respectively <eos> further demonstrate feature learned itracker generalize well other datasets achieving state art result <eos> code data models available gazecapture <eos> <eop> efficient globally optimal three dimensional deformable shape matching <eos> propose first algorithm non rigid three dimensional shape matching input query shape well three dimensional target shape output continuous matching curve represented closed contour three dimensional shape <eos> cast problem finding shortest circular path product manifold two shapes <eos> prove optimal matching computed polynomial time worst case complexity log denote number vertices three dimensional shape respectively <eos> quantitative evaluation confirms method provides excellent result sketch based deformable three dimensional shape retrieval <eos> <eop> ambiguity helps classification disagreements crowdsourced annotations <eos> imagine show image person ask her him decide whether scene image warm warm whether easy spot squirrel image <eos> exactly same image answers questions likely differ person person <eos> because task inherently ambiguous <eos> such ambiguous therefore challenging task pushing boundary computer vision showing learned visual data <eos> crowdsourcing invaluable collecting annotations <eos> particularly so task goes beyond clear cut dichotomy multiple human judgments per image needed reach consensus <eos> paper makes conceptual technical contributions <eos> conceptual side define disagreements among annotators privileged information about data instance <eos> technical side propose framework incorporate annotation disagreements into classifiers <eos> proposed framework simple relatively fast outperforms classifiers take into account disagreements especially if tested high confidence annotations <eos> <eop> task oriented approach cost sensitive recognition <eos> recent progress visual recognition already started see surge vision related real world applications <eos> applications unlike general scene understanding task oriented require specific information visual data <eos> considering current growth new sensory devices feature designs feature learning method algorithms search space feature models becomes combinatorial <eos> paper propose novel cost sensitive task oriented recognition method based combination linguistic semantics visual cues <eos> task oriented framework able generalize unseen tasks there no training data outperforms state art cost based recognition baselines new task based dataset <eos> <eop> refining architectures deep convolutional neural network <eos> deep convolutional neural network cnn recently evinced immense success various image recognition tasks <eos> however question paramount importance somewhat unanswered deep learning research selected cnn optimal dataset terms accuracy model size paper intend answer question introduce novel strategy alters architecture given cnn specified dataset potentially enhance original accuracy while possibly reducing model size <eos> use two operations architecture refinement viz <eos> stretching symmetrical splitting <eos> stretching increases number hidden units nodes given cnn layer while symmetrical split say between two layer separates input output channels into equal groups connects only corresponding input output channel groups <eos> procedure starts pre trained cnn given dataset optimally decides stretch split factors across network refine architecture <eos> empirically demonstrate necessity two operations <eos> evaluate approach two natural scenes attributes datasets sun attributes camit nsad architectures googlenet vgg quite contrasting their construction <eos> justify choice datasets show they interestingly distinct each other together pose challenge architectural refinement algorithm <eos> result substantiate usefulness proposed method <eos> <eop> ilab large scale controlled object dataset investigate deep learning <eos> tolerance image variations <eos> translation scale pose illumination background important desired property any object recognition system human machine <eos> moving towards increasingly bigger datasets trending computer vision especially emergence highly popular deep learning models <eos> while being very useful learning invariance object inter intra class shape variability large scale wild datasets very useful learning invariance other parameters urging researchers resort other tricks training model <eos> work introduce large scale synthetic dataset freely publicly available use answer several fundamental questions regarding selectivity invariance properties convolutional neural network <eos> dataset contains two parts object shot turntable categories rotation angles cameras semi circular arch lighting conditions focus levels variety backgrounds <eos> per instance generating image per instance about million image total scenes robotic arm takes pictures object scale scene <eos> study invariance selectivity different cnn layer knowledge transfer one object category another systematic random sampling image build train set domain adaptation synthetic natural scenes order knowledge delivery cnn <eos> also discuss how analyses lead field develop more efficient deep learning method <eos> <eop> recursive recurrent nets attention modeling ocr wild <eos> present recursive recurrent neural network attention modeling am lexicon free optical character recognition natural scene image <eos> primary advantages proposed method use recursive convolutional neural network cnn allow parametrically efficient effective image feature extraction implicitly learned character level language model embodied recurrent neural network avoids need use grams use soft attention mechanism allowing model selectively exploit image feature coordinated way allowing end end training within standard backpropagation framework <eos> validate method state art performance challenging benchmark datasets street view text iiit icdar synth <eos> <eop> deep decision network multi class image classification <eos> paper present novel deep decision network ddn provides alternative approach towards building efficient deep learning network <eos> during learning phase starting root network node ddn automatically builds network splits data into disjoint clusters classes would handled subsequent expert network <eos> result tree like structured network driven data <eos> proposed method provides insight into data identifying group classes hard classify require more attention when compared others <eos> ddn also ability make early decisions thus making suitable time sensitive applications <eos> validate ddn two publicly available benchmark datasets cifar cifar yields state art classification performance both datasets <eos> proposed algorithm no limitations applied any generic classification problems <eos> <eop> less more zero shot learning online textual documents noise suppression <eos> classifying visual concept merely its associated online textual source such wikipedia article attractive research topic zero shot learning because alleviates burden manually collecting semantic attributes <eos> several recent works pursued approach exploring various ways connecting visual text domains <eos> paper revisits idea stepping further consider one important factor textual representation usually too noisy zero shot learning application <eos> consideration motivates design simple but effective zero shot learning method capable suppressing noise text <eos> more specifically propose norm based objective function simultaneously suppress noisy signal text learn function match text document visual feature <eos> also develop optimization algorithm efficiently solve resulting problem <eos> conducting experiments two large datasets demonstrate proposed method significantly outperforms competing method rely online information sources but without explicit noise suppression <eos> further make depth analysis proposed method provide insight kind information documents useful zero shot learning <eos> <eop> fast algorithms linear kernel svm <eos> svm approach shown excellent performance visual recognition tasks exploiting privileged information training data <eos> paper propose two efficient algorithms solving linear kernel svm respectively <eos> linear svm absorb bias term into weight vector formulate new optimization problem simpler constraints dual form <eos> then develop efficient dual coordinate descent algorithm solve new optimization problem <eos> kernel svm further apply loss leads simpler optimization problem dual form only half dual variables when compared dual form original svm method <eos> more interestingly show new dual problem efficiently solved using smo algorithm one class svm problem <eos> comprehensive experiments three datasets clearly demonstrate proposed algorithms achieve significant speed up than state art solvers linear kernel svm <eos> <eop> hierarchically gated deep network semantic segmentation <eos> semantic segmentation aims parse scene structure image annotating labels each pixel so image segmented into different region <eos> while image structures usually various scales difficult use single scale model spatial contexts all individual pixels <eos> multi scale convolutional neural network cnn their variants made striking success modeling global scene structure image <eos> however they limited labeling fine grained local structures like pixels patches since spatial contexts might blindly mixed up without appropriately customizing their scales <eos> address challenge develop novel paradigm multi scale deep network model spatial contexts surrounding different pixels various scales <eos> builds multiple layer memory cells learning feature representations individual pixels their customized scales hierarchically absorbing relevant spatial contexts via memory gates between layer <eos> such hierarchically gated deep network hgdns customize suitable scale each pixel thereby delivering better performance labeling scene structures various scales <eos> conduct experiments two datasets show competitive result compared other multi scale deep network semantic segmentation task <eos> <eop> deep structured scene parsing learning image descriptions <eos> paper addresses problem structured scene parsing <eos> parsing input scene into configuration including hierarchical semantic object their interaction relations <eos> propose deep architecture consisting two network convolutional neural network cnn extracting image representation pixelwise object labeling ii recursive neural network rnn discovering hierarchical object structure inter object relations <eos> rather than relying elaborative annotations <eos> manually labeled semantic maps relations train deep model weakly supervised manner leveraging descriptive sentences training image <eos> specifically decompose each sentence into semantic tree consisting nouns verb phrases facilitate trees discovering configurations training image <eos> once scene configurations determined then parameters both cnn rnn updated accordingly back propagation <eos> entire model training accomplished through expectation maximization method <eos> extensive experiments suggest model capable producing meaningful structured scene configurations achieving more favorable scene labeling performance pascal voc over other state art weakly supervised method <eos> <eop> cnn rnn unified framework multi label image classification <eos> while deep convolutional neural network cnn shown great success single label image classification important note most real world image contain multiple labels could correspond different object scenes actions attributes image <eos> traditional approaches multi label image classification learn independent classifiers each category employ ranking thresholding classification result <eos> techniques although working well fail explicitly exploit label dependencies image <eos> paper utilize recurrent neural network rnns address problem <eos> combined cnn proposed cnn rnn framework learns joint image label embedding characterize semantic label dependency well image label relevance trained end end scratch integrate both information unified framework <eos> experimental result public benchmark datasets demonstrate proposed architecture achieves better performance than state art multi label classification models <eos> <eop> walk learn facial attribute representation learning egocentric video contextual data <eos> way people look terms facial attributes ethnicity hair color facial hair etc <eos> clothes accessories they wear sunglasses hat hoodies etc <eos> highly dependent geo location weather condition respectively <eos> work explores first time use contextual information people wearable cameras walk across different neighborhoods city order learn rich feature representation facial attribute classification without costly manual annotation required previous method <eos> tracking faces casual walkers more than hours egocentric video able cover tens thousands different identities automatically extract nearly million pairs image connected different face tracks along their weather location context under pose lighting variations <eos> image pairs then fed into deep network preserves similarity image connected same track order capture identity related attribute feature optimizes location weather prediction capture additional facial attribute feature <eos> finally network fine tuned manually annotated sample <eos> perform extensive experimental analysis wearable data two standard benchmark datasets based web image lfwa celeba <eos> method outperforms large margin network trained scratch <eos> moreover even without using manually annotated identity labels pre training previous method approach achieves result better than state art <eos> <eop> cnn gram handwriting word recognition <eos> given image handwritten word cnn employed estimate its gram frequency profile set grams contained word <eos> frequencies unigrams bigrams trigrams estimated entire word parts <eos> canonical correlation analysis then used match estimated profile true profiles all words large dictionary <eos> cnn used employs several novelties such use multiple fully connected branches <eos> applied all commonly used handwriting recognition benchmarks method outperforms very large margin all existing method <eos> <eop> synthetic data text localisation natural image <eos> paper introduce new method text detection natural image <eos> method comprises two contributions first fast scalable engine generate synthetic image text clutter <eos> engine overlays synthetic text existing background image natural way accounting local three dimensional scene geometry <eos> second use synthetic image train fully convolutional regression network fcrn efficiently performs text detection bounding box regression all locations multiple scales image <eos> discuss relation fcrn recently introduced yolo detector well other end end object detection systems based deep learning <eos> resulting detection network significantly out performs current method text detection natural image achieving measure <eos> standard icdar benchmark <eos> furthermore process image per second gpu <eos> <eop> end end people detection crowded scenes <eos> current people detectors operate either scanning image sliding window fashion classifying discrete set proposals <eos> propose model based decoding image into set people detections <eos> system takes image input directly outputs set distinct detection hypotheses <eos> because generate predictions jointly common post processing steps such non maximum suppression unnecessary <eos> use recurrent lstm layer sequence generation train model end end new loss function operates set detections <eos> demonstrate effectiveness approach challenging task detecting people crowded scenes <eop> real time salient object detection minimum spanning tree <eos> paper present real time salient object detection system based minimum spanning tree <eos> due fact background region typically connected image boundaries salient object extracted computing distances boundaries <eos> however measuring image boundary connectivity efficiently challenging problem <eos> existing method either rely superpixel representation reduce processing units approximate distance transform <eos> instead propose exact iteration free solution minimum spanning tree <eos> minimum spanning tree representation image inherently reveals object geometry information scene <eos> meanwhile largely reduces search space shortest paths resulting efficient high quality distance transform algorithm <eos> further introduce boundary dissimilarity measure compliment shortage distance transform salient object detection <eos> extensive evaluations show proposed algorithm achieves leading performance compared state art method terms efficiency accuracy <eos> <eop> local background enclosure rgb salient object detection <eos> recent work salient object detection considered incorporation depth cues rgb image <eos> most cases depth contrast used main feature <eos> however areas high contrast background region cause false positives such method background frequently contains region highly variable depth <eos> here propose novel rgb saliency feature <eos> local background enclosure lbe captures spread angular directions background respect candidate region object part <eos> show feature improves over state art rgb saliency approaches well rgb method rgbd njuds datasets <eos> <eop> adaptive object detection using adjacency zoom prediction <eos> state art object detection systems rely accurate set region proposals <eos> several recent method use neural network architecture hypothesize promising object locations <eos> while approaches computationally efficient they rely fixed image region anchors predictions <eos> paper propose use search strategy adaptively directs computational resources sub region likely contain object <eos> compared method based fixed anchor locations approach naturally adapts cases object instances sparse small <eos> approach comparable terms accuracy state art faster cnn approach while using two orders magnitude fewer anchors average <eos> code publicly available <eos> <eop> semantic channels fast pedestrian detection <eos> pedestrian detection semantic segmentation high potential tasks many real time applications <eos> however most top performing approaches provide state art result high computational costs <eos> work propose fast solution achieving state art result both pedestrian detection semantic segmentation <eos> baseline pedestrian detection use sliding windows over cost efficient multiresolution filtered luv hog channels <eos> use same channels classifying pixels into eight semantic classes <eos> using short range long range multiresolution channel feature achieve more robust segmentation result compared traditional codebook based approaches much lower computational costs <eos> resulting segmentations used additional semantic channels order achieve more powerful pedestrian detector <eos> also achieve fast pedestrian detection employ multiscale detection scheme based single flexible pedestrian model single image scale <eos> proposed solution provides competitive result both pedestrian detection semantic segmentation benchmarks fps cpu fps gpu being fastest top performing approach <eos> <eop> cnn iterative grid based object detector <eos> introduce cnn object detection technique based cnn works without proposal algorithms <eos> cnn starts multi scale grid fixed bounding boxes <eos> train regressor move scale elements grid towards object iteratively <eos> cnn models problem object detection finding path fixed grid boxes tightly surrounding object <eos> cnn around boxes multi scale grid performs comparably fast cnn uses around bounding boxes generated proposal technique <eos> strategy makes detection faster removing object proposal stage well reducing number boxes processed <eos> <eop> recurrent face aging <eos> modeling aging process human face important cross age face verification recognition <eos> paper introduce recurrent face aging rfa framework based recurrent neural network identify ages people <eos> due lack labeled face data same person captured long range ages traditional face aging models usually split ages into discrete groups learn one step face feature transformation each pair adjacent age groups <eos> however method neglect between evolving states between adjacent age groups synthesized faces often suffer severe ghosting artifacts <eos> since human face aging smooth progression more appropriate age face going through smooth transition states <eos> way ghosting artifacts effectively eliminated intermediate aged faces between two discrete age groups also obtained <eos> towards target employ two layer gated recurrent unit basic recurrent module whose bottom layer encodes young face latent representation top layer decodes representation corresponding older face <eos> experimental result demonstrate proposed rfa provides better aging faces over other state art age progression method <eos> <eop> face face real time face capture reenactment rgb video <eos> present novel approach real time facial reenactment monocular target video sequence <eos> source sequence also monocular video stream captured live commodity webcam <eos> goal animate facial expressions target video source actor re render manipulated output video photo realistic fashion <eos> end first address under constrained problem facial identity recovery monocular video non rigid model based bundling <eos> run time track facial expressions both source target video using dense photometric consistency measure <eos> reenactment then achieved fast efficient deformation transfer between source target <eos> mouth interior best matches re targeted expression retrieved target sequence warped produce accurate fit <eos> finally convincingly re render synthesized target face top corresponding video stream such seamlessly blends real world illumination <eos> demonstrate method live setup youtube video reenacted real time <eos> <eop> self adaptive matrix completion heart rate estimation face video under realistic conditions <eos> recent studies computer vision shown while practically invisible human observer skin color changes due blood flow captured face video surprisingly used estimate heart rate hr <eos> while considerable progress made last few years still many issues remain open <eos> particular state art approaches robust enough operate natural conditions <eos> case spontaneous movements facial expressions illumination changes <eos> opposite previous approaches estimate hr processing all skin pixels inside fixed region interest introduce strategy dynamically select face region useful robust hr estimation <eos> approach inspired recent advances matrix completion theory allows predict hr while simultaneously discover best region face used estimation <eos> thorough experimental evaluation conducted public benchmarks suggests proposed approach significantly outperforms state art hr estimation method naturalistic conditions <eos> <eop> visually indicated sounds <eos> object make distinctive sounds when they hit scratched <eos> sounds reveal aspects object material properties well actions produced them <eos> paper propose task predicting sound object makes when struck way studying physical interactions within visual scene <eos> present algorithm synthesizes sound silent video people hitting scratching object drumstick <eos> algorithm uses recurrent neural network predict sound feature video then produces waveform feature example based synthesis procedure <eos> show sounds predicted model realistic enough fool participants real fake psychophysical experiment they convey significant information about material properties physical interactions <eos> <eop> image style transfer using convolutional neural network <eos> rendering semantic content image different styles difficult image processing task <eos> arguably major limiting factor previous approaches lack image representations explicitly represent semantic information thus allow separate image content style <eos> here use image representations derived convolutional neural network optimised object recognition make high level image information explicit <eos> introduce neural algorithm artistic style separate recombine image content style natural image <eos> algorithm allows produce new image high perceptual quality combine content arbitrary photograph appearance numerous well known artworks <eos> result provide new insights into deep image representations learned convolutional neural network demonstrate their potential high level image synthesis manipulation <eos> <eop> patch based convolutional neural network whole slide tissue image classification <eos> convolutional neural network cnn state art models many image classification tasks <eos> however recognize cancer subtypes automatically training cnn gigapixel resolution whole slide tissue image wsi currently computationally impossible <eos> differentiation cancer subtypes based cellular level visual feature observed image patch scale <eos> therefore argue situation training patch level classifier image patches will perform better than similar image level classifier <eos> challenge becomes how intelligently combine patch level classification result model fact all patches will discriminative <eos> propose train decision fusion model aggregate patch level predictions given patch level cnn best knowledge shown before <eos> furthermore formulate novel expectation maximization em based method automatically locates discriminative patches robustly utilizing spatial relationships patches <eos> apply method classification glioma non small cell lung carcinoma cases into subtypes <eos> classification accuracy method similar inter observer agreement between pathologists <eos> although impossible train cnn wsis experimentally demonstrate using comparable non cancer dataset smaller image patch based cnn outperform image based cnn <eos> <eop> hedgehog shape priors multi object segmentation <eos> star convexity prior popular interactive single object segmentation due its simplicity amenability binary graph cut optimization <eos> propose more general multi object segmentation approach <eos> moreover each object constrained more descriptive shape prior hedgehog <eos> each hedgehog shape its surface normals locally constrained arbitrary given vector field <eos> gradient user scribble distance transform <eos> contrast star convexity tightness normal constraint changed giving better control over allowed shapes <eos> example looser constraints <eos> wider cones allowed normals give more relaxed hedgehog shapes <eos> other hand tightest constraint enforces skeleton consistency scribbles <eos> general hedgehog shapes more descriptive than star only special case corresponding radial vector field weakest tightness <eos> approach significantly more applications than standard single star convex segmentation <eos> medical data separate multiple non star organs similar appearances weak edges <eos> optimization done modified expansion moves shown submodular multi hedgehog shapes <eos> <eop> latent variable graphical model selection using harmonic analysis applications human connectome project hcp <eos> major goal imaging studies such ongoing human connectome project hcp characterize structural network map human brain identify its associations covariates such genotype risk factors so correspond individual <eos> but set image derived measures set covariates both large so must first estimate parsimonious set relations between measurements <eos> instance gaussian graphical model will show conditional independences between random variables then used setup specific hypothesis based analyses downstream <eos> but most such data involve large list latent variables remain unobserved yet affect observed variables sustantially <eos> accounting such latent variables falls outside scope standard inverse covariance matrix estimation tackled via highly specialized optimization method <eos> paper offers unique harmonic analysis view problem <eos> casting estimation precision matrix terms composition low frequency latent variables high frequency sparse terms show how problem formulated using new wavelet type expansion non euclidean spaces <eos> formalization poses estimation problem entirely frequency space shows how solved simple sub gradient scheme involving single variable <eos> provide compelling set scientific result scans recently released hcp data algorithm recovers highly interpretable sparse conditional dependencies between brain connectivity pathways well known covariates <eos> <eop> simultaneous estimation near ir brdf fine scale surface geometry <eos> near infrared nir image most materials exhibit less texture albedo variations making them beneficial vision tasks such intrinsic image decomposition structured light depth estimation <eos> understanding reflectance properties brdf materials nir wavelength range further useful many photometric method including shape shading inverse rendering <eos> however even less albedo variation many materials <eos> fabrics leaves etc <eos> exhibit complex fine scale surface detail making hard accurately estimate brdf <eos> paper present approach simultaneously estimate nir brdf fine scale surface details imaging materials under different ir lighting viewing directions <eos> achieved iterative scheme alternately estimates surface detail nir brdf materials <eos> setup require complicated gantries calibration present first nir dataset materials including variety fabrics knits weaves cotton satin leather organic skin leaves jute trunk fur inorganic materials plastic concrete carpet <eos> nir brdfs measured material sample used shape shading algorithm demonstrate fine scale reconstruction object single nir image <eos> <eop> yourself hyperspectral imaging everyday digital cameras <eos> capturing hyperspectral image requires expensive specialized hardware readily accessible most users <eos> digital cameras other hand significantly cheaper comparison easily purchased used <eos> paper present framework reconstructing hyperspectral image using multiple consumer level digital cameras <eos> approach works exploiting different spectral sensitivities different camera sensors <eos> particular due differences spectral sensitivities cameras different cameras yield different rgb measurements same spectral signal <eos> introduce algorithm able combine convert different rgb measurements into single hyperspectral image both indoor outdoor scenes <eos> camera based approach allows hyperspectral imaging fraction cost most existing hyperspectral hardware <eos> validate accuracy reconstruction against ground truth hyperspectral image using both synthetic real cases show its usage relighting applications <eos> <eop> automatic content aware color tone stylization <eos> introduce new technique automatically generates diverse visually compelling stylizations photograph unsupervised manner <eos> achieve learning style ranking given input using large photo collection selecting diverse subset matching styles final style transfer <eos> also propose improved technique transfers global color tone chosen exemplars input photograph while avoiding common visual artifacts produced existing style transfer method <eos> together style selection transfer techniques produce compelling artifact free result wide range input photographs user study shows result preferred over other techniques <eos> <eop> combining markov random fields convolutional neural network image synthesis <eos> paper studies combination generative markov random field mrf models discriminatively trained deep convolutional neural network dcnns synthesizing image <eos> generative mrf acts higher levels dcnn feature pyramid controling image layout abstract level <eos> apply method both photographic non photo realistic artwork synthesis tasks <eos> mrf regularizer prevents over excitation artifacts reduces implausible feature mixtures common previous dcnn inversion approaches permitting synthezing photographic content increased visual plausibility <eos> unlike standard mrf based texture synthesis combined system both match adapt local feature considerable variability yielding result far out reach classic generative mrf method <eos> <eop> dcan deep contour aware network accurate gland segmentation <eos> morphology glands used routinely pathologists assess malignancy degree adenocarcinomas <eos> accurate segmentation glands histology image crucial step obtain reliable morphological statistics quantitative diagnosis <eos> paper proposed efficient deep contour aware network dcan solve challenging problem under unified multi task learning framework <eos> proposed network multi level contextual feature hierarchical architecture explored auxiliary supervision accurate gland segmentation <eos> when incorporated multi task regularization during training discriminative capability intermediate feature further improved <eos> moreover network only output accurate probability maps glands but also depict clear contours simultaneously separating clustered object further boosts gland segmentation performance <eos> unified framework efficient when applied large scale histopathological data without resorting additional steps generate contours based low level cues post separating <eos> method won miccai gland segmentation challenge out competitive teams surpassing all other method significant margin <eos> <eop> learning read chest rays recurrent neural cascade model automated image annotation <eos> despite recent advances automatically describing image contents their applications mostly limited image caption datasets containing natural image <eos> paper present deep learning model efficiently detect disease image annotate its contexts <eos> location severity affected organs <eos> employ publicly available radiology dataset chest rays their reports use its image annotations mine disease names train convolutional neural network cnn <eos> doing so adopt various regularization techniques circumvent large normal vs diseased cases bias <eos> recurrent neural network rnns then trained describe contexts detected disease based deep cnn feature <eos> moreover introduce novel approach use weights already trained pair cnn rnn domain specific image text dataset infer joint image text contexts composite image labeling <eos> significantly improved image annotation result demonstrated using recurrent neural cascade model taking joint image text contexts into account <eos> <eop> conformal surface alignment optimal mobius search <eos> deformations surfaces same intrinsic shape often described accurately conformal model <eos> major focus computational conformal geometry estimation conformal mapping aligns given pair object surfaces <eos> uniformization theorem en ables task acccomplished canonical main wherein surfaces aligned using obius transformation <eos> current algorithms estimating obius transformations however often cannot provide satisfactory alignment computationally too costly <eos> paper troduces novel globally optimal algorithm estimating mobius transformations align surfaces topologi cal discs <eos> unlike previous method proposed algorithm deterministically calculates best transformation out requiring good initializations <eos> further algorithm also much faster than previous techniques practice <eos> demonstrate efficacy algorithm data commonly used computational conformal geometry <eos> <eop> coupled harmonic bases longitudinal characterization brain network <eos> there great deal interest using large scale brain imaging studies understand how brain connectivity evolves over time individual how varies over different levels quantiles cognitive function <eos> so one typically performs so called tractography procedures diffusion mr brain image derives measures brain connectivity expressed graphs <eos> nodes correspond distinct brain region edges encode strength connection <eos> scientific interest characterizing evolution graphs over time healthy individuals diseased <eos> pose important question terms laplacian connectivity graphs derived various longitudinal disease time point quantifying its progression then expressed terms coupling harmonic bases full set laplacians <eos> derive coupled system generalized eigenvalue problems corresponding numerical optimization schemes whose solution helps characterize full life cycle brain connectivity evolution given dataset <eos> finally show set result diffusion mr imaging dataset middle aged people risk alzheimer disease ad who cognitively healthy <eos> such asymptomatic adults find framework characterizing brain connectivity evolution provides ability predict cognitive scores individual subjects estimating progression participant brain connectivity into future <eos> <eop> automating carotid intima media thickness video interpretation convolutional neural network <eos> cardiovascular disease cvd leading cause mortality yet largely preventable but key prevention identify risk individuals before adverse events <eos> predicting individual cvd risk carotid intima media thickness cimt noninvasive ultrasound method proven valuable offering several advantages over ct coronary artery calcium score <eos> however each cimt examination includes several ultrasound video interpreting each cimt video involves three operations select three end diastolic ultrasound frames euf video localize region interest roi each selected frame trace lumen intima interface media adventitia interface each roi measure cimt <eos> operations tedious laborious time consuming serious limitation hinders widespread utilization cimt clinical practice <eos> overcome limitation paper presents new system automate cimt video interpretation <eos> extensive experiments demonstrate suggested system significantly outperforms state art method <eos> superior performance attributable unified framework based convolutional neural network cnn coupled informative image representation effective post processing cnn outputs uniquely designed each above three operations <eos> <eop> context encoders feature learning inpainting <eos> present unsupervised visual feature learning algorithm driven context based pixel prediction <eos> analogy auto encoders propose context encoders convolutional neural network trained generate contents arbitrary image region conditioned its surroundings <eos> order succeed task context encoders need both understand content entire image well produce plausible hypothesis missing part <eos> when training context encoders experimented both standard pixel wise reconstruction loss well reconstruction plus adversarial loss <eos> latter produces much sharper result because better handle multiple modes output <eos> found context encoder learns representation captures just appearance but also semantics visual structures <eos> quantitatively demonstrate effectiveness learned feature cnn pre training classification detection segmentation tasks <eos> furthermore context encoders used semantic inpainting tasks either stand alone initialization non parametric method <eos> <eop> comparative deep learning hybrid representations image recommendations <eos> many image related tasks learning expressive discriminative representations image essential deep learning studied automating learning such representations <eos> some user centric tasks such image recommendations call effective representations only image but also preferences intents users over image <eos> such representations termed hybrid addressed via deep learning approach paper <eos> design dual net deep network two sub network map input image preferences users into same latent semantic space then distances between image users latent space calculated make decisions <eos> further propose comparative deep learning cdl method train deep network using pair image compared against one user learn pattern their relative distances <eos> cdl embraces much more training data than naive deep learning thus achieves superior performance than latter no cost increasing network complexity <eos> experimental result real world data set image recommendations shown proposed dual net network cdl greatly outperform other state art image recommendation solutions <eos> <eop> fast convnets using group wise brain damage <eos> revisit idea brain damage <eos> pruning coefficients neural network suggest how brain damage modified used speedup convolutional layer convnets <eos> approach uses fact many efficient implementations reduce generalized convolutions matrix multiplications <eos> suggested brain damage process prunes convolutional kernel tensor group wise fashion <eos> after such pruning convolutions reduced multiplications thinned dense matrices leads speedup <eos> investigate different ways add group wise prunning learning process show several fold speedups convolutional layer attained using group sparsity regularizers <eos> approach adjust shapes receptive fields convolutional layer even prune excessive feature maps convnets all data driven way <eos> <eop> learning co generate object proposals deep structured network <eos> generating object proposals become key component modern object detection pipelines <eos> however most existing method generate object candidates independently each other <eos> paper present approach co generating object proposals multiple image thus leveraging collective power multiple object candidates <eos> particular introduce deep structured network jointly predicts objectness scores bounding box locations multiple object candidates <eos> deep structured network consists fully connected conditional random field built top set deep convolutional neural network learn feature model both individual object candidate similarity between multiple candidates <eos> train deep structured network develop end end learning algorithm unrolling crf inference procedure lets backpropagate loss gradient throughout entire structured network <eos> demonstrate effectiveness approach two benchmark datasets showing significant improvement over state art object proposal algorithms <eos> <eop> deepfool simple accurate method fool deep neural network <eos> state art deep neural network achieved impressive result many image classification tasks <eos> however same architectures shown unstable small well sought perturbations image <eos> despite importance phenomenon no effective method proposed accurately compute robustness state art deep classifiers such perturbations large scale datasets <eos> paper fill gap propose deepfool algorithm efficiently compute perturbations fool deep network thus reliably quantify robustness classifiers <eos> extensive experimental result show approach outperforms recent method task computing adversarial perturbations making classifiers more robust <eos> <eop> blockout dynamic model selection hierarchical deep network <eos> most deep architectures image classification even trained classify large number diverse categories learn shared image representations single model <eos> intuitively however categories more similar should share more information than very different <eos> while hierarchical deep network address problem learning separate feature subsets related categories current implementations require simplified models using fixed architectures specified via heuristic clustering method <eos> instead propose blockout method regularization model selection simultaneously learns both model architecture parameters <eos> generalization dropout approach gives novel parametrization hierarchical architectures allows structure learning via back propagation <eos> demonstrate its utility evaluate blockout cifar imagenet datasets demonstrating improved classification accuracy better regularization performance faster training clear emergence hierarchical network structures <eos> <eop> firecaffe near linear acceleration deep neural network training compute clusters <eos> long training times high accuracy deep neural network dnns impede research into new dnn architectures slow development high accuracy dnns <eos> paper present firecaffe successfully scales deep neural network training across cluster gpus <eos> also present number best practices aid comparing advancements method scaling accelerating training deep neural network <eos> speed scalability distributed algorithms almost always limited overhead communicating between servers dnn training exception rule <eos> therefore key consideration here reduce communication overhead wherever possible while degrading accuracy dnn models train <eos> approach three key pillars <eos> first select network hardware achieves high bandwidth between gpu servers infiniband cray interconnects ideal <eos> second consider number communication algorithms find reduction trees more efficient scalable than traditional parameter server approach <eos> third optionally increase batch size reduce total quantity communication during dnn training identify hyperparameters allow reproduce small batch accuracy while training large batch sizes <eos> when training googlenet network network imagenet achieve speedup respectively when training cluster gpus <eos> <eop> mdl cw multimodal deep learning framework cross weights <eos> deep learning received much attention most powerful approaches multimodal representation learning recent years <eos> ideal model multimodal data reason about missing modalities using available ones usually provides more information when multiple modalities being considered <eos> all previous deep models contain separate modality specific network find shared representation top network <eos> therefore they only consider high level interactions between modalities find joint representation them <eos> paper propose multimodal deep learning framework mdl cw exploits cross weights between representation modalities try gradually learn interactions modalities deep network manner low high level interactions <eos> moreover theoretically show considering interactions provide more intra modality information introduce multi stage pre training method based properties multi modal data <eos> proposed framework opposed existing deep method multi modal data try reconstruct representation each modality given level representation other modalities previous layer <eos> extensive experimental result show proposed model outperforms state art information retrieval method both image text queries pascal sentence sun attribute databases <eos> <eop> structured receptive fields cnn <eos> learning powerful feature representations cnn hard when training data limited <eos> pre training one way overcome but requires large datasets sufficiently similar target domain <eos> another option design priors into model range tuned hyperparameters fully engineered representations like scattering network <eos> combine ideas into structured receptive field network model fixed filter basis yet retains flexibility cnn <eos> flexibility achieved expressing receptive fields cnn weighted sum over fixed basis similar spirit scattering network <eos> key difference learn arbitrary effective filter set basis rather than modeling filters <eos> approach explicitly connects classical multiscale image analysis general cnn <eos> structured receptive field network improve considerably over unstructured cnn small medium dataset scenarios well over scattering large datasets <eos> validate findings ilsvrc cifar cifar mnist <eos> realistic small dataset example show state art classification result popular three dimensional mri brain disease datasets pre training difficult due lack large public datasets similar domain <eos> <eop> first person action recognition using deep learned descriptors <eos> focus problem wearer action recognition first person <eos> problem more challenging than third person activity recognition due unavailability wearer pose sharp movements video caused natural head motion wearer <eos> carefully crafted feature based hands object cues problem shown successful limited targeted datasets <eos> propose convolutional neural network cnn end end learning classification wearer actions <eos> proposed network makes use egocentric cues capturing hand pose head motion saliency map <eos> also trained relatively small number labeled egocentric video available <eos> show proposed network generalize give state art performance various disparate egocentric action datasets <eos> <eop> recognizing micro actions reactions paired egocentric video <eos> aim understand dynamics social interactions between two people recognizing their actions reactions using head mounted camera <eos> work will impact several first person vision tasks need detailed understanding social interactions such automatic video summarization group events assistive systems <eos> recognize micro level actions reactions such slight shifts attention subtle nodding small hand actions only subtle body motion apparent propose use paired egocentric video recorded two interacting people <eos> show first person second person point view feature two people enabled paired egocentric video complementary essential reliably recognizing micro actions reactions <eos> also build new dataset dyadic two persons interactions comprises more than pairs egocentric video enable systematic evaluations task micro action reaction recognition <eos> <eop> mining three dimensional key pose motifs action recognition <eos> recognizing action sequence three dimensional skeletal poses challenging task <eos> first different actors may perform same action various performing styles <eos> second estimated poses sometimes inaccurate due sensory noises <eos> challenges cause large variations between instances same class <eos> third datasets usually small only few actors performing few repetitions each action <eos> hence training complex classifiers risks over fitting data <eos> address task mining set key pose motifs each action class <eos> key pose motif contains set ordered poses action units short sequence poses required close but necessarily adjacent action sequences <eos> representation robust style variations outlier poses <eos> key pose motifs represented terms dictionary using soft quantization probabilities deal inaccuracies caused quantization <eos> propose efficient algorithm mine key pose motifs taking into account probabilities <eos> classify sequence matching motifs each class select class maximizes matching score <eos> simple classifier obtains state art performance two benchmark datasets outperforms deep network approach <eos> <eop> predicting actors actions through online action localization <eos> paper proposes novel approach tackle challenging problem online action localization entails predicting actions their locations they happen video <eos> typically action localization recognition performed offline manner all frames video processed together action labels predicted future <eos> dis allows timely localization actions important consideration surveillance tasks <eos> approach given batch frames immediate past video estimate pose over segment current frame into superpixels <eos> next discriminatively train actor foreground model superpixels using pose bounding boxes <eos> conditional random field superpixels nodes edges connecting spatio temporal neighbors used obtain action segments <eos> action confidence predicted using dynamic programming svm scores obtained short segments video thereby capturing sequential information actions <eos> issue visual drift handled updating appearance model pose refinement online manner <eos> lastly introduce new measure quantify performance action prediction <eos> online action localization analyzes how prediction accuracy varies function observed portion video <eos> experiments suggest despite using only few frames localize actions each time instant able predict action obtain competitive result state art offline method <eos> <eop> actions transformations <eos> defines action like kicking ball argue true meaning action lies change transformation action brings environment <eos> paper propose novel representation actions modeling action transformation changes state environment before action happens precondition state after action effect <eos> motivated recent advancements video representation using deep learning design siamese network models action transformation high level feature space <eos> show model gives improvements standard action recognition datasets including ucf hmdb <eos> more importantly approach able generalize beyond learned action categories shows significant performance improvement cross category generalization new act dataset <eos> <eop> visual path prediction complex scenes crowded moving object <eos> paper proposes novel path prediction algorithm progressing one step further than existing works focusing single target path prediction <eos> paper consider moving dynamics co occurring object path prediction scene includes crowded moving object <eos> solve problem first suggest two layered probabilistic model find major movement patterns their co occurrence tendency <eos> utilizing unsupervised learning result model present algorithm find future location any target object <eos> through extensive qualitative quantitative experiments show algorithm find plausible future path complex scenes large number moving object <eos> <eop> end end learning action detection frame glimpses video <eos> work introduce fully end end approach action detection video learns directly predict temporal bounds actions <eos> intuition process detecting actions naturally one observation refinement observing moments video refining hypotheses about when action occurring <eos> based insight formulate model recurrent neural network based agent interacts video over time <eos> agent observes video frames decides both look next whether emit prediction <eos> since backpropagation adequate non differentiable setting use reinforce learn agent task specific decision policy <eos> model achieves state art result thumos activitynet datasets while observing only fraction less video frames <eos> <eop> action recognition video using sparse coding relative feature <eos> work presents approach category based action recognition video using sparse coding techniques <eos> proposed approach includes two main contributions new method handle intra class variations decomposing each video into reduced set representative atomic action acts key sequences ii new video descriptor itra inter temporal relational act descriptor exploits power comparative reasoning capture relative similarity relations among key sequences <eos> terms method obtain key sequences introduce loss function each video leads identification sparse set representative key frames capturing both relevant particularities arising input video well relevant generalities arising complete class collection <eos> terms method obtain itra descriptor introduce novel scheme quantify relative intra inter class similarities among local temporal patterns arising video <eos> resulting itra descriptor demonstrates highly effective discriminate among action categories <eos> result proposed approach reaches remarkable action recognition performance several popular benchmark datasets outperforming alternative state art techniques large margin <eos> <eop> improving human action recognition non action classification <eos> paper consider task recognizing human actions realistic video human actions dominated irrelevant factors <eos> first study benefits removing non action video segments ones portray any human action <eos> then learn non action classifier use down weight irrelevant video segments <eos> non action classifier trained using actionthread dataset shot level annotation occurrence absence human action <eos> non action classifier used identify non action shots high precision subsequently used improve performance action recognition systems <eos> <eop> actionness estimation using hybrid fully convolutional network <eos> actionness was introduced quantify likelihood containing generic action instance specific location <eos> accurate efficient estimation actionness important video analysis may benefit other relevant tasks such action recognition action detection <eos> paper presents new deep architecture actionness estimation called hybrid fully convolutional network fcn composed appearance fcn fcn motion fcn fcn <eos> two fcns leverage strong capacity deep models estimate actionness maps perspectives static appearance dynamic motion respectively <eos> addition fully convolutional nature fcn allows efficiently process video arbitrary sizes <eos> experiments conducted challenging datasets stanford ucf sports jhmdb verify effectiveness fcn actionness estimation demonstrate method achieves superior performance previous ones <eos> moreover apply estimated actionness maps action proposal generation action detection <eos> actionness maps advance current state art performance tasks substantially <eos> <eop> real time action recognition enhanced motion vector cnn <eos> deep two stream architecture exhibited excellent performance video based action recognition <eos> most computationally expensive step approach comes calculation optical flow prevents real time <eos> paper accelerates architecture replacing optical flow motion vector obtained directly compressed video without extra calculation <eos> however motion vector lacks fine structures contains noisy inaccurate motion patterns leading evident degradation recognition performance <eos> key insight relieving problem optical flow motion vector inherent correlated <eos> transferring knowledge learned optical flow cnn motion vector cnn significantly boost performance latter <eos> specifically introduce three strategies initialization transfer supervision transfer their combination <eos> experimental result show method achieves comparable recognition performance state art while method process <eos> frames per second times faster than original two stream method <eos> <eop> laplacian patch based image synthesis <eos> patch based image synthesis enriched global optimization image pyramid <eos> successively gradient based synthesis improved structural coherence details <eos> however gradient operator directional inconsistent requires computing multiple operators <eos> also introduces significantly heavy computational burden solve poisson equation often accompanies artifacts non integrable gradient fields <eos> paper propose patch based synthesis using laplacian pyramid improve searching correspondence enhanced awareness edge structures <eos> contrary gradient operators laplacian pyramid advantage being isotropic detecting changes provide more consistent performance decomposing base structure detailed localization <eos> furthermore require heavy computation employs approximation differences gaussians <eos> examine potentials laplacian pyramid enhanced edge aware correspondence search <eos> demonstrate effectiveness laplacian based approach over state art patch based image synthesis method <eos> <eop> rain streak removal using layer priors <eos> paper addresses problem rain streak removal single image <eos> rain streaks impair visibility image introduce undesirable interference severely affect performance computer vision algorithms <eos> rain streak removal formulated layer decomposition problem rain streak layer superimposed background layer containing true scene content <eos> existing decomposition method address problem employ either dictionary learning method impose low rank structure appearance rain streaks <eos> while method improve overall visibility they tend leave too many rain streaks background image over smooth background image <eos> paper propose effective method uses simple patch based priors both background rain layer <eos> priors based gaussian mixture models accommodate multiple orientations scales rain streaks <eos> simple approach removes rain streaks better than existing method qualitatively quantitatively <eos> overview method demonstrate its effectiveness over prior work number examples <eos> <eop> gradient domain image reconstruction framework intensity range base structure constraints <eos> paper presents novel unified gradient domain image reconstruction framework intensity range constraint base structure constraint <eos> existing method manipulating base structures detailed textures classifiable into two major approaches gradient domain ii layer decomposition <eos> generate detail preserving artifact free output image combine benefits two approaches into proposed framework introducing intensity range constraint base structure constraint <eos> preserve details input image proposed method takes advantage reconstructing output image gradient domain while output intensity guaranteed lie within specified intensity range <eos> intensity range constraint <eos> addition reconstructed image lies close base structure base structure constraint effective restraining artifacts <eos> experimental result show proposed framework effective various applications such tone mapping seamless image cloning detail enhancement image restoration <eos> <eop> removing clouds recovering ground observations satellite image sequences via temporally contiguous robust matrix completion <eos> consider problem removing replacing clouds satellite image sequences wide range applications remote sensing <eos> approach first detects removes cloud contaminated part image sequences then recovers missing scenes clean parts proposed tecromac temporally contiguous robust matrix completion objective <eos> objective function balances temporal smoothness low rank solution while staying close original observations <eos> matrix rows pixels columns days image low rank because pixels reflect land types such vegetation roads lakes there relatively few <eos> provide efficient optimization algorithms tecromac so run image containing millions pixels <eos> empirical result real satellite image sequences well simulated data demonstrate approach able recover underlying image heavily cloud contaminated observations <eos> <eop> deep dual domain based fast restoration jpeg compressed image <eos> paper design deep dual domain based fast restoration model remove artifacts jpeg compressed image <eos> leverages large learning capacity deep network well problem specific expertise was hardly incorporated past design deep architectures <eos> latter take into consideration both prior knowledge jpeg compression scheme successful practice sparsity based dual domain approach <eos> further design one step sparse inference si module efficient light weighted feed forward approximation sparse coding <eos> extensive experiments verify superiority proposed model over several state art method <eos> specifically best model capable outperforming latest deep model around db psnr times faster <eos> <eop> bows arrows rolling shutter rectification urban scenes <eos> rule perspectivity straight lines must remain straight easily inflected cmos cameras distortions introduced motion <eos> lines rendered curves due row wise exposure mechanism known rolling shutter rs <eos> solve problem correcting distortions arising handheld cameras due rs effect single image free motion blur special relevance urban scenes <eos> develop procedure extract prominent curves rs image since essential deciphering varying row wise motion <eos> pose optimization problem line desirability costs based straightness angle length resolve geometric ambiguities while estimating camera motion based rotation only model assuming known camera intrinsic matrix <eos> finally rectify rs image based estimated camera trajectory using inverse mapping <eos> show rectification result rs image captured using mobile phone cameras <eos> also compare single image method against existing video nonblind rs rectification method typically require multiple image <eos> <eop> weighted variational model simultaneous reflectance illumination estimation <eos> propose weighted variational model estimate both reflectance illumination observed image <eos> show though widely adopted ease modeling log transformed image task ideal <eos> based previous investigation logarithmic transformation new weighted variational model proposed better prior representation imposed regularization terms <eos> different conventional variational models proposed model preserve estimated reflectance more details <eos> moreover proposed model suppress noise some extent <eos> alternating minimization scheme adopted solve proposed model <eos> experimental result demonstrate effectiveness proposed model its algorithm <eos> compared other variational method proposed method yields comparable better result both subjective objective assessments <eos> <eop> visualizing understanding deep texture representations <eos> number recent approaches used deep convolutional neural network cnn build texture representations <eos> nevertheless still unclear how mod els represent texture invariances categorical variations <eos> work conducts systematic evaluation recent cnn based texture descriptors recognition attempts understand nature invariances captured representations <eos> first show recently proposed bilinear cnn model excellent generalpurpose texture descriptor compares favorably other cnn based descriptors various texture scene recognition benchmarks <eos> model translationally invariant obtains better accuracy imagenet dataset without requiring spatial jittering data compared corresponding models trained spatial jittering <eos> based recent work propose technique visualize pre image providing means understanding categorical properties captured representations <eos> finally show preliminary result how unified parametric model texture analysis synthesis used attribute based image manipulation <eos> make image more swirly honeycombed knitted <eos> source code additional visualizations available vis www <eos> <eop> robust kernel estimation outliers handling image deblurring <eos> estimating blur kernels real world image challenging problem linear image formation assumption hold when significant outliers such saturated pixels non gaussian noise present <eos> while some existing non blind deblurring algorithms deal outliers certain extent few blind deblurring method developed well estimate blur kernels blurred image outliers <eos> paper present algorithm address problem exploiting reliable edges removing outliers intermediate latent image thereby estimating blur kernels robustly <eos> analyze effects outliers kernel estimation show most state art blind deblurring method may recover delta kernels when blurred image contain significant outliers <eos> propose robust energy function describes properties outliers final latent image restoration <eos> furthermore show proposed algorithm applied improve existing method deblur image outliers <eos> extensive experiments different kinds challenging blurry image significant amount outliers demonstrate proposed algorithm performs favorably against state art method <eos> <eop> online collaborative learning open vocabulary visual classifiers <eos> focus learning open vocabulary visual classifiers scale up large portion natural language vocabulary <eos> over tens thousands classes <eos> particular training data large scale weakly labeled web image since difficult acquire sufficient well labeled data category scale <eos> paper propose novel online learning paradigm towards challenging task <eos> different traditional way independent classifiers generally fail handle extremely sparse inter related labels classifiers learn continuous label embeddings discovered collaboratively decomposing sparse image label matrix <eos> leveraging structure proposed collaborative learning formulation develop efficient online algorithm jointly learn label embeddings visual classifiers <eos> algorithm learn over classes training image within second standard gpu <eos> extensively experimental result four benchmarks demonstrate effectiveness method <eos> <eop> rethinking inception architecture computer vision <eos> convolutional network core most state art computer vision solutions wide variety tasks <eos> since very deep convolutional network started become mainstream yielding substantial gains various benchmarks <eos> although increased model size computational cost tend translate immediate quality gains most tasks long enough labeled data provided training computational efficiency low parameter count still enabling factors various use cases such mobile vision big data scenarios <eos> here exploring ways scale up network ways aim utilizing added computation efficiently possible <eos> benchmark method ilsvrc classification challenge validation set demonstrate substantial gains over state art via carefully factorized convolutions aggressive regularization <eos> top error single frame evaluation using network computational cost billion multiply adds per inference using less than million parameters <eos> <eop> cross modal distillation supervision transfer <eos> work propose technique transfers supervision between image different modalities <eos> use learned representations large labeled modality supervisory signal training representations new unlabeled paired modality <eos> method enables learning rich representations unlabeled modalities used pre training procedure new modalities limited labeled data <eos> transfer supervision labeled rgb image unlabeled depth optical flow image demonstrate large improvements both cross modal supervision transfers <eos> <eop> efficient point process inference large scale object detection <eos> tackle problem large scale object detection image number object arbitrarily large exhibit significant overlap occlusion <eos> successful approach modelling large scale nature problem via point process density functions jointly encode object qualities spatial interactions <eos> but corresponding optimisation problem typically difficult intractable many best current method rely monte carlo markov chain mcmc simulation converges slowly large solution space <eos> propose efficient point process inference large scale object detection using discrete energy minimization <eos> particular approximate solution space finite set object proposals cast point process density function corresponding energy function binary variables whose values indicate object proposals accepted <eos> resort local submodular approximation lsa based trust region optimisation find optimal solution <eos> furthermore analyse error lsa approximation show how adjust point process energy dramatically speed up convergence without harms optimality <eos> demonstrate superior efficiency accuracy method using variety large scale object detection applications such crowd human detection birds cells counting localization <eos> <eop> weakly supervised deep detection network <eos> weakly supervised learning object detection important problem image understanding still satisfactory solution <eos> paper address problem exploiting power deep convolutional neural network pre trained large scale image level classification tasks <eos> propose weakly supervised deep detection architecture modifies one such network operate level image region performing simultaneously region selection classification <eos> trained image classifier architecture implicitly learns object detectors better than alternative weakly supervised detection systems pascal voc data <eos> model simple elegant end end architecture outperforms standard data augmentation fine tuning techniques task image level classification well <eos> <eop> border oriented rectangles approach texture less object recognition <eos> paper presents algorithm coined border bounding oriented rectangle descriptors enclosed region texture less object recognition <eos> fusing regional object encompassment concept descriptor based pipelines extend local patches into scalable object sized oriented rectangles optimal object information encapsulation minimal outliers <eos> correspondingly introduce modified line segment detection technique termed linelets stabilize keypoint repeatability homogenous conditions <eos> addition unique sampling technique facilitates incorporation robust angle primitives produce discriminative rotation invariant descriptors <eos> border high competence object recognition particularly excels homogenous conditions obtaining superior detection rates presence high clutter occlusion scale rotation changes when compared modern state art texture less object detectors such bold line public texture less object databases <eos> <eop> active image segmentation propagation <eos> propose semi automatic method obtain foreground object masks large set related image <eos> develop stagewise active approach propagation each stage actively determine image appear most valuable human annotation then revise foreground estimates all unlabeled image accordingly <eos> order identify image once annotated will propagate well other examples introduce active selection procedure operates joint segmentation graph over all image <eos> prioritizes human intervention image uncertain influential graph while also mutually diverse <eos> apply method obtain foreground masks over million image <eos> method yields state art accuracy imagenet mit object discovery datasets focuses human attention more effectively than existing propagation strategies <eos> <eop> inside outside net detecting object context skip pooling recurrent neural network <eos> well known contextual multi scale representations important accurate visual recognition <eos> paper present inside outside net ion object detector exploits information both inside outside region interest <eos> contextual information outside region interest integrated using spatial recurrent neural network <eos> inside use skip pooling extract information multiple scales levels abstraction <eos> through extensive experiments evaluate design space provide readers overview tricks trade important <eos> ion improves state art pascal voc object detection <eos> new more challenging ms coco dataset improve state art <eos> ms coco detection challenge ion model won best student entry finished rd place overall <eos> intuition suggests detection result provide strong evidence context multi scale representations improve small object detection <eos> <eop> rifd cnn rotation invariant fisher discriminative convolutional neural network object detection <eos> thanks powerful feature representations obtained through deep convolutional neural network cnn performance object detection recently substantially boosted <eos> despite remarkable success problems object rotation within class variability between class similarity remain several major challenges <eos> address problems paper proposes novel effective method learn rotation invariant fisher discriminative cnn rifd cnn model <eos> achieved introducing learning rotation invariant layer fisher discriminative layer respectively basis existing high capacity cnn architectures <eos> specifically rotation invariant layer trained imposing explicit regularization constraint objective function enforces invariance cnn feature before after rotating <eos> fisher discriminative layer trained imposing fisher discrimination criterion cnn feature so they small within class scatter but large between class separation <eos> experiments comprehensively evaluate proposed method object detection task public available aerial image dataset pascal voc dataset <eos> state art result achieved compared existing baseline method <eos> <eop> reinforcement learning visual object detection <eos> one most widely used strategies visual object detection based exhaustive spatial hypothesis search <eos> while method like sliding windows successful effective many years they still brute force independent image content visual category being searched <eos> paper present formally rigorous sequential models accumulate evidence collected small set image locations order detect visual object effectively <eos> formulating sequential search reinforcement learning search policy including stopping condition fully trainable model explicitly balance each class specifically conflicting goals exploration sampling more image region better accuracy exploitation stopping search efficiently when sufficiently confident target location <eos> methodology general applicable any detector response function <eos> report encouraging result pascal voc object detection test set showing proposed methodology achieves almost two orders magnitude speed up over sliding window method <eos> <eop> detecting repeating object using patch correlation analysis <eos> paper describe new method detecting counting repeating object image <eos> while method relies fairly sophisticated deformable part model unlike existing techniques estimates model parameters unsupervised fashion thus alleviating need user annotated training data avoiding associated specificity <eos> automatic fitting process carried out exploiting recurrence small image patches associated repeating object analyzing their spatial correlation <eos> analysis allows reject outlier patches recover visual shape parameters part model detect object instances efficiently <eos> order achieve practical system able cope diverse image describe simple intuitive active learning procedure updates object classification querying user very few carefully chosen marginal classifications <eos> evaluation new method against state art techniques demonstrates its ability achieve higher accuracy through better user experience <eos> <eop> analyzing classifiers fisher vectors deep neural network <eos> fisher vector fv classifiers deep neural network dnns popular successful algorithms solving image classification problems <eos> however both generally considered black box predictors non linear transformations involved so far prevented transparent interpretable reasoning <eos> recently principled technique layer wise relevance propagation lrp developed order better comprehend inherent structured reasoning complex nonlinear classification models such bag feature models dnns <eos> paper extend lrp framework also fisher vector classifiers then use analysis tool quantify importance context classification qualitatively compare dnns against fv classifiers terms important image region detect potential flaws biases data <eos> all experiments performed pascal voc ilsvrc data set <eos> <eop> learning deep feature discriminative localization <eos> work revisit global average pooling layer proposed shed light how explicitly enables convolutional neural network cnn remarkable localization ability despite being trained image level labels <eos> while technique was previously proposed means regularizing training find actually builds generic localizable deep representation exposes implicit attention cnn image <eos> despite apparent simplicity global average pooling able achieve <eos> top error object localization ilsvrc without training any bounding box annotation <eos> demonstrate network able localize discriminative image region variety tasks despite being trained them <eos> <eop> seeing through human reporting bias visual classifiers noisy human centric labels <eos> when human annotators given choice about label image they apply their own subjective judgments ignore mention <eos> refer noisy human centric annotations exhibiting human reporting bias <eos> examples such annotations include image tags keywords found photo sharing sites datasets containing image captions <eos> paper use noisy annotations learning visually correct image classifiers <eos> such annotations use consistent vocabulary miss significant amount information present image however demonstrate noise annotations exhibits structure modeled <eos> propose algorithm decouple human reporting bias correct visually grounded labels <eos> result highly interpretable reporting image versus worth saying <eos> demonstrate algorithm efficacy along variety metrics datasets including ms coco yahoo flickr <eos> show significant improvements over traditional algorithms both image classification image captioning doubling performance existing method some cases <eos> <eop> learning aligned cross modal representations weakly aligned data <eos> people recognize scenes across many different modalities beyond natural image <eos> paper investigate how learn cross modal scene representations transfer across modalities <eos> study problem introduce new cross modal scene dataset <eos> while convolutional neural network categorize cross modal scenes well they also learn intermediate representation aligned across modalities undesirable cross modal transfer applications <eos> present method regularize cross modal convolutional neural network so they shared representation agnostic modality <eos> experiments suggest scene representation help transfer representations across modalities retrieval <eos> moreover visualizations suggest units emerge shared representation tend activate consistent concepts independently modality <eos> <eop> probabilistic collaborative representation based approach pattern classification <eos> conventional representation based classifiers ranging classical nearest neighbor classifier nearest subspace classifier recently developed sparse representation based classifier src collaborative representation based classifier crc essentially distance based classifiers <eos> though src crc shown interesting classification result their intrinsic classification mechanism remains unclear <eos> paper propose probabilistic collaborative representation framework probability test sample belongs collaborative subspace all classes well defined computed <eos> consequently present probabilistic collaborative representation based classifier procrc jointly maximizes likelihood test sample belongs each multiple classes <eos> final classification performed checking class maximum likelihood <eos> proposed procrc clear probabilistic interpretation shows superior performance many popular classifiers including src crc svm <eos> coupled cnn feature also leads state art classification result variety challenging visual datasets <eos> <eop> learning structured inference neural network label relations <eos> image scenes various object well abundant attributes diverse levels visual categorization possible <eos> natural image could assigned fine grained labels describe major components coarse grained labels depict high level abstraction set labels reveal attributes <eos> such categorization different concept layer modeled label graphs encoding label information <eos> paper exploit rich information state art deep learning framework propose generic structured model leverages diverse label relations improve image classification performance <eos> approach employs novel stacked label prediction neural network capturing both inter level intra level label semantics <eos> evaluate method benchmark image datasets empirical result illustrate efficacy model <eos> <eop> discriminative multi modal feature fusion rgbd indoor scene recognition <eos> rgbd scene recognition attracted increasingly attention due rapid development depth sensors their wide application scenarios <eos> while many research conducted most work used hand crafted feature difficult capture high level semantic structures <eos> recently feature extracted deep convolutional neural network produced state art result various computer vision tasks inspire researchers explore incorporating cnn learned feature rgbd scene understanding <eos> other hand most existing work combines rgb depth feature without adequately exploiting consistency complementary information between them <eos> inspired some recent work rgbd object recognition using multi modal feature fusion introduce novel discriminative multi modal fusion framework rgbd scene recognition first time simultaneously considers inter intra modality correlation all sample meanwhile regularizing learned feature discriminative compact <eos> result multimodal layer back propagated lower cnn layer hence parameters cnn layer multimodal layer updated iteratively until convergence <eos> experiments recently proposed large scale sun rgb datasets show method achieved state art without any image segmentation <eos> <eop> conditional graphical lasso multi label image classification <eos> multi label image classification aims predict multiple labels single image contains diverse content <eos> utilizing label correlations various techniques developed improve classification performance <eos> however current existing method either neglect image feature when exploiting label correlations lack ability learn image dependent conditional label structures <eos> paper develop conditional graphical lasso cgl handle challenges <eos> cgl provides unified bayesian framework structure parameter learning conditioned image feature <eos> formulate multi label prediction cgl inference problem solved mean field variational approach <eos> meanwhile cgl learning efficient due tailored proximal gradient procedure applying maximum posterior map methodology <eos> cgl performs competitively multi label image classification benchmark datasets mulan scene pascal voc pascal voc compared state art multi label classification algorithms <eos> <eop> region ranking svm image classification <eos> success image classification algorithm largely depends how incorporates local information global decision <eos> popular approaches such average pooling max pooling suboptimal many situations <eos> paper propose region ranking svm rrsvm novel method pooling local information multiple region <eos> rrsvm exploits correlation local region image jointly learns region evaluation function scheme integrating multiple region <eos> experiments pascal voc voc ilsvrc datasets show rrsvm outperforms method use same feature type extract feature same set local region <eos> irsvm achieves similar better than state art performance all datasets <eos> <eop> predicting motivations actions leveraging text <eos> understanding human actions key problem computer vision <eos> however recognizing actions only first step understanding person doing <eos> paper introduce problem predicting why person performed action image <eos> problem many applications human activity understanding such anticipating explaining action <eos> study problem introduce new dataset people performing actions annotated likely motivations <eos> however information image alone may sufficient automatically solve task <eos> since humans rely their lifetime experiences infer motivation propose give computer vision systems access some experiences using recently developed natural language models mine knowledge stored massive amounts text <eos> while still far away fully understanding motivation result suggest transferring knowledge language into vision help machines understand why people image might performing action <eos> <eop> boxcars three dimensional boxes cnn input improved fine grained vehicle recognition <eos> dealing problem fine grained vehicle make model recognition verification <eos> contribution showing extracting additional data video stream besides vehicle image itself feeding into deep convolutional neural network boosts recognition performance considerably <eos> additional information includes three dimensional vehicle bounding box used unpacking vehicle image its rasterized low resolution shape information about three dimensional vehicle orientation <eos> experiments show adding such information decreases classification error accuracy improved <eos> boosts verification average precision <eos> compared baseline pure cnn without any input modifications <eos> also pure baseline cnn outperforms recent state art solution <eos> provide annotated set boxcars surveillance vehicle image augmented various automatically extracted auxiliary information <eos> approach dataset considerably improve performance traffic surveillance systems <eos> <eop> highway vehicle counting compressed domain <eos> paper presents highway vehicle counting method compressed domain aiming achieving acceptable estimation performance approaching pixel domain method <eos> such task essentially challenging because available information <eos> motion vector describe vehicles video quite limited inaccurate vehicle count realistic traffic scenes always varies greatly <eos> tackle issue first develop batch low level feature extracted encoding metadata video mitigate informational insufficiency compressed video <eos> then propose hierarchical classification based regression hcr model estimate vehicle count feature <eos> hcr hierarchically divides traffic scenes into different cases according vehicle density such broad variation characteristics traffic scenes better approximated <eos> finally evaluated proposed method real highway surveillance video <eos> result show method very competitive pixel domain method reach similar performance along its lower complexity <eos> <eop> camera calibration periodic motion pedestrian <eos> camera calibration directly image sequences pedestrian without using any calibration object really challenging task should well solved computer vision especially visual surveillance <eos> paper propose novel camera calibration method based recovering three orthogonal vanishing point tovps just using image sequence pedestrian walking straight line without any assumption scenes motions <eos> control point known three dimensional coordinates parallel perpendicular lines non natural pre designed special human motions often necessary previous method <eos> traces shoes pedestrian carry more rich easily detectable metric information than all other body parts periodic motion pedestrian but such information usually overlooked previous work <eos> paper employ image toes shoes ground plane determine vanishing point corresponding walking direction then utilize harmonic conjugate properties projective geometry recover vanishing point corresponding perpendicular direction walking direction horizontal plane vanishing point corresponding vertical direction <eos> after recovering all tovps intrinsic extrinsic parameters camera determined <eos> experiments various scenes viewing angles prove feasibility accuracy proposed method <eos> <eop> dynamic image network action recognition <eos> introduce concept dynamic image novel compact representation video useful video analysis especially when convolutional neural network cnn used <eos> dynamic image based rank pooling concept obtained through parameters ranking machine encodes temporal evolution frames video <eos> dynamic image obtained directly applying rank pooling raw image pixels video producing single rgb image per video <eos> idea simple but powerful enables use existing cnn models directly video data fine tuning <eos> present efficient effective approximate rank pooling operator speeding up orders magnitude compared rank pooling <eos> new approximate rank pooling cnn layer allows generalize dynamic image dynamic feature maps demonstrate power new representations standard benchmarks action recognition achieving state art performance <eos> <eop> detecting events key actors multi person video <eos> multi person event recognition challenging task often many people active scene but only small subset contributing actual event <eos> paper propose model learns detect events such video while automatically attending people responsible event <eos> model use explicit annotations regarding who people during training testing <eos> particular track people video use recurrent neural network rnn represent track feature <eos> learn time varying attention weights combine feature each time instant <eos> attended feature then processed using another rnn event detection classification <eos> since most video datasets multiple people restricted small number video also collected new basketball dataset comprising basketball games event annotations corresponding event classes <eos> model outperforms state art method both event classification detection new dataset <eos> additionally show attention mechanism able consistently localize relevant players <eos> <eop> regularizing long short term memory three dimensional human skeleton sequences action recognition <eos> paper argues large scale action recognition video greatly improved providing additional modality training data namely three dimensional human skeleton sequences aimed complementing poorly represented missing feature human actions training video <eos> recognition use long short term memory lstm grounded via deep convolutional neural network cnn onto video <eos> training lstm regularized using output another encoder lstm elstm grounded three dimensional human skeleton training data <eos> such regularized training lstm modify standard backpropagation through time bptt order address well known issues gradient descent constraint optimization <eos> evaluation three benchmark datasets sports hmdb ucf shows accuracy improvements <eos> relative state art <eos> <eop> personalizing human video pose estimation <eos> propose personalized convnet pose estimator automatically adapts itself uniqueness person appearance improve pose estimation long video <eos> make following contributions show given few high precision pose annotations <eos> generic convnet pose estimator additional annotations generated throughout video using combination image based matching temporally distant frames dense optical flow temporally local frames ii develop occlusion aware self evaluation model able automatically select high quality reject erroneous additional annotations iii demonstrate high quality annotations used fine tune convnet pose estimator thereby personalize lock key discriminative feature person appearance <eos> outcome substantial improvement pose estimates target video using personalized convnet compared original generic convnet <eos> method outperforms state art including top convnet method large margin three standard benchmarks well new challenging youtube video dataset <eos> furthermore show training automatically generated annotations used improve performance generic convnet other benchmarks <eos> <eop> end end learning deformable mixture parts deep convolutional neural network human pose estimation <eos> recently deep convolutional neural network dcnns applied task human pose estimation shown its potential learning better feature representations capturing contextual relationships <eos> however difficult incorporate domain prior knowledge such geometric relationships among body parts into dcnns <eos> addition training dcnn based body part detectors without consideration global body joint consistency introduces ambiguities increases complexity training <eos> paper propose novel end end framework human pose estimation combines dcnns expressive deformable mixture parts <eos> explicitly incorporate domain prior knowledge into framework greatly regularizes learning process enables flexibility framework loopy models tree structured models <eos> effectiveness jointly learning dcnn deformable mixture parts model evaluated through intensive experiments several widely used benchmarks <eos> proposed approach significantly improves performance compared state art approaches especially benchmarks challenging articulations <eos> <eop> actor action semantic segmentation grouping process models <eos> actor action semantic segmentation made important step toward advanced video understanding action happening who performing action action happening space time <eos> current method based layered crfs problem local unable capture long ranging interactions video parts <eos> propose new model combines labeling crf supervoxel hierarchy supervoxels various scales provide cues possible groupings nodes crf encourage adaptive long ranging interactions <eos> new model defines dynamic continuous process information exchange crf influences supervoxels hierarchy active active supervoxels turn affect connectivities crf hence call grouping process model <eos> further incorporating video level recognition proposed method achieves large margin relative improvement over state art recent large scale video labeling dataset demonstrates effectiveness modeling <eos> <eop> temporal action localization pyramid score distribution feature <eos> investigate feature design classification architectures temporal action localization <eos> application focuses detecting labeling actions untrimmed video brings more challenge than classifying pre segmented video <eos> major difficulty action localization uncertainty action occurrence utilization information different scales <eos> two innovations proposed address issue <eos> first propose pyramid score distribution feature psdf capture motion information multiple resolutions centered each detection window <eos> novel feature mitigates influence unknown action position duration shows significant performance gain over previous detection approaches <eos> second inter frame consistency further explored incorporating psdf into state art recurrent neural network gives additional performance gain detecting actions temporally untrimmed video <eos> tested action localization framework thumos mpii cooking activities dataset both show large performance improvement over previous attempts <eos> <eop> recognizing activities daily living wrist mounted camera <eos> present novel dataset novel algorithm recognizing activities daily living adl first person wearable camera <eos> handled object crucially important egocentric adl recognition <eos> specific examination object related users actions separately other object environment many previous works addressed detection handled object image captured head mounted chest mounted cameras <eos> nevertheless detecting handled object always easy because they tend appear small image <eos> they occluded user body <eos> described herein mount camera user wrist <eos> wrist mounted camera capture handled object large scale thus enables skip object detection process <eos> compare wrist mounted camera head mounted camera also developed novel publicly available dataset includes video annotations daily activities captured simultaneously both cameras <eos> additionally propose discriminative video representation retains spatial temporal information after encoding frame descriptors extracted convolutional neural network <eos> <eop> harnessing object scene semantics large scale video understanding <eos> large scale action recognition video categorization important problems computer vision <eos> address problems propose novel object scene based semantic fusion network representation <eos> semantic fusion network combines three streams information using three layer neural network frame based low level cnn feature ii object feature state art large scale cnn object detector trained recognize classes iii scene feature state art cnn scene detector trained recognize scenes <eos> trained network achieves improvements supervised activity video categorization two complex large scale datasets activitynet fcvid respectively <eos> further examining back propagating information through fusion network semantic relationships correlations between video classes object scenes discovered <eos> video class object video class scene relationships turn used semantic representation video classes themselves <eos> illustrate effectiveness semantic representation through experiments zero shot action video classification clustering <eos> <eop> video story composition via plot analysis <eos> address problem composing story out multiple short video clips taken person during activity experience <eos> inspired plot analysis written stories method generates sequence video clips ordered such way reflects plot dynamics content coherency <eos> given set multiple video clips method composes video call video story <eos> define metrics scene dynamics coherency dense optical flow feature patch matching algorithm <eos> using metrics define objective function video story <eos> efficiently search best video story introduce novel branch bound algorithm guarantees global optimum <eos> collect dataset consisting video set web resulting total individual video clips <eos> acquired dataset perform extensive user studies involving human subjects effectiveness approach quantitatively qualitatively verified <eos> <eop> temporal action detection using statistical language model <eos> while current approaches action recognition pre segmented video clips already achieve high accuracies temporal action detection still far comparably good result <eos> automatically locating classifying relevant action segments video varying lengths proves challenging task <eos> propose novel method temporal action detection including statistical length language modeling represent temporal contextual structure <eos> approach aims globally optimizing joint probability three components length language model discriminative action model without making intermediate decisions <eos> problem finding most likely action sequence corresponding segment boundaries exponentially large search space addressed dynamic programming <eos> provide extensive evaluation each model component thumos large action detection dataset report state art result three datasets <eos> <eop> multi scale patch aggregation mpa simultaneous detection segmentation <eos> aiming simultaneous detection segmentation sds propose proposal free framework detect segment object instances via mid level patches <eos> design unified trainable network patches followed fast effective patch aggregation algorithm infer object instances <eos> method benefits end end training <eos> without object proposal generation computation time also reduced <eos> experiments method yields result <eos> terms mapr voc segmentation val voc sds val state art time submission <eos> also report result microsoft coco test std test dev dataset paper <eos> <eop> instance aware semantic segmentation via multi task network cascades <eos> semantic segmentation research recently witnessed rapid progress but many leading method unable identify object instances <eos> paper present multi task network cascades instance aware semantic segmentation <eos> model consists three network respectively differentiating instances estimating masks categorizing object <eos> network form cascaded structure designed share their convolutional feature <eos> develop algorithm nontrivial end end training causal cascaded structure <eos> solution clean single step training framework generalized cascades more stages <eos> demonstrate state art instance aware semantic segmentation accuracy pascal voc <eos> meanwhile method takes only ms testing image using vgg two orders magnitude faster than previous systems challenging problem <eos> product method also achieves compelling object detection result surpass competitive fast faster cnn systems <eos> method described paper foundation submissions ms coco segmentation competition won st place <eos> <eop> scribblesup scribble supervised convolutional network semantic segmentation <eos> large scale data crucial importance learning semantic segmentation models but annotating per pixel masks tedious inefficient procedure <eos> note topic interactive image segmentation scribbles very widely used academic research commercial software recognized one most user friendly ways interacting <eos> paper propose use scribbles annotate image develop algorithm train convolutional network semantic segmentation supervised scribbles <eos> algorithm based graphical model jointly propagates information scribbles unmarked pixels learns network parameters <eos> present competitive object semantic segmentation result pascal voc dataset using scribbles annotations <eos> scribbles also favored annotating stuff <eos> water sky grass no well defined shape method shows excellent result pascal context dataset thanks extra inexpensive scribble annotations <eos> <eop> feature space optimization semantic video segmentation <eos> present approach long range spatio temporal regularization semantic video segmentation <eos> temporal regularization video challenging because both camera scene may motion <eos> thus euclidean distance space time volume good proxy correspondence <eos> optimize mapping pixels euclidean feature space so minimize distances between corresponding point <eos> structured prediction performed dense crf operates optimized feature <eos> experimental result demonstrate presented approach increases accuracy temporal consistency semantic video segmentation <eos> <eop> large scale semantic three dimensional reconstruction adaptive multi resolution model multi class volumetric labeling <eos> propose adaptive multi resolution formulation semantic three dimensional reconstruction <eos> given set image scene semantic three dimensional reconstruction aims densely reconstruct both three dimensional shape scene segmentation into semantic object classes <eos> jointly reasoning about shape class allows one take into account class specific shape priors <eos> building walls should smooth vertical vice versa smooth vertical surfaces likely building walls leading improved reconstruction result <eos> so far semantic three dimensional reconstruction method limited small scenes low resolution because their large memory footprint computational cost <eos> scale them up large scenes propose hierarchical scheme refines reconstruction only region likely contain surface exploiting fact both high spatial resolution high numerical precision only required region <eos> scheme amounts solving sequence convex optimizations while progressively removing constraints such way energy each iteration tightest possible approximation underlying energy full resolution <eos> experiments method saves up memory computation time without any loss accuracy <eos> <eop> semantic object parsing local global long short term memory <eos> semantic object parsing fundamental task understanding object detail computer vision community incorporating multi level contextual information critical achieving such fine grained pixel level recognition <eos> prior method often leverage contextual information through post processing predicted confidence maps <eos> work propose novel deep local global long short term memory lg lstm architecture seamlessly incorporate short distance long distance spatial dependencies into feature learning over all pixel positions <eos> each lg lstm layer local guidance neighboring positions global guidance whole image imposed each position better exploit complex local global contextual information <eos> individual lstms distinct spatial dimensions also utilized intrinsically capture various spatial layouts semantic parts image yielding distinct hidden memory cells each position each dimension <eos> parsing approach several lg lstm layer stacked appended intermediate convolutional layer directly enhance visual feature allowing network parameters learned end end way <eos> long chains sequential computation stacked lg lstm layer also enable each pixel sense much larger region inference benefiting memorization previous dependencies all positions along all dimensions <eos> comprehensive evaluations three public datasets well demonstrate significant superiority lg lstm over other state art method object parsing <eos> <eop> efficient piecewise training deep structured models semantic segmentation <eos> recent advances semantic image segmentation mostly achieved training deep convolutional neural network cnn <eos> show how improve semantic segmentation through use contextual information specifically explore patch patch context between image region patch background context <eos> learning patch patch context formulate conditional random fields crfs cnn based pairwise potential functions capture semantic correlations between neighboring patches <eos> efficient piecewise training proposed deep structured model then applied avoid repeated expensive crf inference back propagation <eos> capturing patch background context show network design traditional multi scale image input sliding pyramid pooling effective improving performance <eos> experimental result set new state art performance number popular semantic segmentation datasets including nyudv pascal voc pascal context sift flow <eos> particular achieve intersection over union score <eos> challenging pascal voc dataset <eos> <eop> learning transferrable knowledge semantic segmentation deep convolutional neural network <eos> propose novel weakly supervised semantic segmentation algorithm based deep convolutional neural net work dcnn <eos> contrary existing weakly supervised approaches algorithm exploits auxiliary segmentation notations available different categories guide segmentations image only image level class labels <eos> make segmentation knowledge transferrable across categories design decoupled encoder decoder architecture attention model <eos> architecture model generates spatial highlights each category presented image using attention model subsequently per forms binary segmentation each highlighted region using decoder <eos> combining attention model decoder trained segmentation annotations different categories boosts accuracy weakly supervised semantic segmentation <eos> proposed algorithm demonstrates substantially improved performance compared state art weakly supervised techniques pascal voc dataset when model trained annotations exclusive categories microsoft coco dataset <eos> <eop> cityscapes dataset semantic urban scene understanding <eos> visual understanding complex urban street scenes enabling factor wide range applications <eos> object detection benefited enormously large scale datasets especially context deep learning <eos> semantic urban scene understanding however no current dataset adequately captures complexity real world urban scenes <eos> address introduce cityscapes benchmark suite large scale dataset train test approaches pixel level instance level semantic labeling <eos> cityscapes comprised large diverse set stereo video sequences recorded streets different cities <eos> image high quality pixel level annotations additional image coarse annotations enable method leverage large volumes weakly labeled data <eos> crucially effort exceeds previous attempts terms dataset size annotation richness scene variability complexity <eos> accompanying empirical study provides depth analysis dataset characteristics well performance evaluation several state art approaches based benchmark <eos> <eop> gaussian conditional random field network semantic segmentation <eos> contrast existing approaches use discrete conditional random field crf models propose use gaussian crf model task semantic segmentation <eos> propose novel deep network refer gaussian mean field gmf network whose layer perform mean field inference over gaussian crf <eos> proposed gmf network desired property each its layer produces output closer maximum posteriori solution gaussian crf compared its input <eos> combining proposed gmf network deep convolutional neural network cnn propose new end end trainable gaussian conditional random field network <eos> proposed gaussian crf network composed three sub network cnn based unary network generating unary potentials ii cnn based pairwise network generating pairwise potentials iii gmf network performing gaussian crf inference <eos> when trained end end discriminative fashion evaluated challenging pascalvoc segmentation dataset proposed gaussian crf network outperforms various recent semantic segmentation approaches combine cnn discrete crf models <eos> <eop> synthia dataset large collection synthetic image semantic segmentation urban scenes <eos> vision based semantic segmentation urban scenarios key functionality autonomous driving <eos> recent revolutionary result deep convolutional neural network dcnns foreshadow advent reliable classifiers perform such visual tasks <eos> however dcnns require learning many parameters raw image thus having sufficient amount diverse image class annotations needed <eos> annotations obtained via cumbersome human labour particularly challenging semantic segmentation since pixel level annotations required <eos> paper propose use virtual world automatically generate realistic synthetic image pixel level annotations <eos> then address question how useful such data semantic segmentation particular when using dcnn paradigm <eos> order answer question generated synthetic collection diverse urban image named synthia automatically generated class annotations <eos> use synthia combination publicly available real world urban image manually provided annotations <eos> then conduct experiments dcnns show how inclusion synthia training stage significantly improves performance semantic segmentation task <eos> <eop> progressive prioritized multi view stereo <eos> work proposes progressive patch based multi view stereo algorithm able deliver dense point cloud any time <eos> enables immediate feedback reconstruction process user centric scenario <eos> increasing processing time model improved terms resolution accuracy <eos> algorithm explicitly handles input image varying effective scale creates visually pleasing point clouds <eos> priority scheme assures limited computational power invested scene parts user most interested overall error reduced most <eos> architecture proposed pipeline allows fast processing times large scenes using pure open source cpu implementation <eos> show performance algorithm challenging standard datasets well real world scenes compare baseline <eos> <eop> warpnet weakly supervised matching single view reconstruction <eos> present approach matching image object fine grained datasets without using part annotations application challenging problem weakly supervised single view reconstruction <eos> contrast prior works require part annotations since matching object across class pose variations challenging appearance feature alone <eos> overcome challenge through novel deep learning architecture warpnet aligns object one image different object another <eos> exploit structure fine grained dataset create artificial data training network unsupervised discriminative learning approach <eos> output network acts spatial prior allows generalization test time match real image across variations appearance viewpoint articulation <eos> cub dataset bird categories improve ap over appearance only network <eos> further demonstrate warpnet matches together structure fine grained datasets allow single view reconstructions quality comparable using annotated point correspondences <eos> <eop> sparse light field coding reveals about scene structure <eos> paper propose novel method depth estimation light fields employs specifically designed sparse decomposition leverage depth orientation relationship its epipolar plane image <eos> proposed method learns structure central view uses information construct light field dictionary groups atoms correspond unique disparities <eos> dictionary then used code sparse representation light field <eos> analysing coefficients representation respect disparities their corresponding atoms yields accurate robust estimate depth <eos> addition if light field multiple depth layer such reflective transparent surfaces statistical analysis coefficients employed infer respective depth superimposed layer <eos> <eop> online reconstruction indoor scenes rgb streams <eos> system capable performing robust online volumetric reconstruction indoor scenes based input handheld rgb camera presented <eos> system powered two pass reconstruction scheme <eos> first pass tracks camera poses video rate simultaneously constructs pose graph fly <eos> tracker operates real time allows reconstruction result visualized during scanning process <eos> live visual feedbacks makes scanning operation fast intuitive <eos> upon termination scanning second pass takes place handle loop closures reconstruct final model using globally refined camera trajectories <eos> system online low delay returns dense model sufficient accuracy <eos> beauty system lies its speed accuracy simplicity ease implementation when compared previous method <eos> demonstrate performance system several real world scenes quantitatively assess modeling accuracy respect ground truth models obtained lidar scanner <eos> <eop> patches planes probabilities non local prior volumetric three dimensional reconstruction <eos> paper propose non local structured prior volumetric multi view three dimensional reconstruction <eos> towards goal present novel markov random field model based ray potentials assumptions about large three dimensional surface patches such planarity manhattan world constraints efficiently encoded probabilistic priors <eos> further derive inference algorithm reasons jointly about voxels pixels image segments estimates marginal distributions appearance occupancy depth normals planarity <eos> key tractable inference novel hybrid representation spans both voxel pixel space integrates non local information image segmentations principled way <eos> compare non local prior commonly employed local smoothness assumptions variety state art volumetric reconstruction baselines challenging outdoor scenes textureless reflective surfaces <eos> experiments indicate regularizing over larger distances potential resolve ambiguities local regularizers fail <eos> <eop> single image camera calibration lenticular arrays augmented reality <eos> consider problem camera pose estimation scenario camera may continuous unknown changes its focal length <eos> understanding frame frame changes camera focal length vital accurately estimating camera pose vital accurately render virtual object scene correct perspective <eos> however most approaches camera calibration require geometric constraints many frames observation three dimensional calibration object both may feasible augmented reality settings <eos> paper introduces calibration object based flat lenticular array creates color coded light field whose observed color changes depending angle viewed <eos> derive approach estimate focal length camera relative pose object single image <eos> characterize performance camera calibration across various focal lengths camera models demonstrate advantages focal length estimation rendering virtual object video constant zooming <eos> <eop> augmented blendshapes real time simultaneous three dimensional head modeling facial motion capture <eos> propose method build real time animated three dimensional head models using consumer grade rgb camera <eos> framework first one provide simultaneously comprehensive facial motion tracking detailed three dimensional model user head <eos> anyone head instantly reconstructed his facial motion captured without requiring any training pre scanning <eos> user starts facing camera neutral expression first frame but free move talk change his face expression he wills otherwise <eos> facial motion tracked using blendshape representation while fine geometric details captured using bump image mapped over template mesh <eos> propose efficient algorithm grow refine three dimensional model head fly real time <eos> demonstrate robust high fidelity simultaneous facial motion tracking three dimensional head modeling result wide range subjects various head poses facial expressions <eos> proposed method offers interesting possibilities animation production three dimensional video telecommunications <eos> <eop> learned binary spectral shape descriptor three dimensional shape correspondence <eos> dense three dimensional shape correspondence important problem computer vision computer graphics <eos> recently local shape descriptor based three dimensional shape correspondence approaches widely studied local shape descriptor real valued vector characterize geometrical structure shape <eos> different real valued local shape descriptors paper propose learn novel binary spectral shape descriptor deep neural network three dimensional shape correspondence <eos> binary spectral shape descriptor require less storage space enable fast matching <eos> first based eigenvectors laplace beltrami operator construct neural network form nonlinear spectral representation characterize shape <eos> then defined positive negative point shapes train constructed neural network minimizing errors between outputs their corresponding binary descriptors minimizing variations outputs positive point maximizing variations outputs negative point simultaneously <eos> finally binarize output neural network form binary spectral shape descriptor shape correspondence <eos> proposed binary spectral shape descriptor evaluated scape tosca three dimensional shape datasets shape correspondence <eos> experimental result demonstrate effectiveness proposed binary shape descriptor shape correspondence task <eos> <eop> multiple model fitting set coverage problem <eos> paper deals extraction multiple models noisy outlier contaminated data <eos> cast multi model fitting problem terms set covering deriving simple effective method generalizes ransac multiple models deals intersecting structures outliers straightforward principled manner while avoiding typical shortcomings sequential approaches clustering <eos> method compares favourably against state art simulated publicly available real datasets <eos> <eop> piecewise planar three dimensional approximation wide baseline stereo <eos> paper approximates three dimensional geometry scene small number three dimensional planes <eos> method especially suited man made scenes only requires two calibrated wide baseline views inputs <eos> relies computation dense but noisy three dimensional point cloud example obtained matching daisy descriptors between views <eos> then segments one two reference image adopts multi model fitting process assign three dimensional plane each region when region detected occluded <eos> pool three dimensional plane hypotheses first derived three dimensional point cloud include planes reasonably approximate part three dimensional point cloud observed each reference view between randomly selected triplets three dimensional point <eos> hypothesis region assignment problem then formulated energy minimization problem simultaneously optimizes original data fidelity term assignment smoothness over neighboring region number assigned planar proxies <eos> synthesis intermediate viewpoints demonstrates effectiveness three dimensional reconstruction thereby relevance proposed data fidelity metric <eos> <eop> sparse dense three dimensional reconstruction rolling shutter image <eos> well known rolling shutter effect image captured moving rolling shutter camera causes inaccuracies three dimensional reconstructions <eos> problem further aggravated weak visual connectivity wide baseline image captured fast moving camera <eos> paper propose implement pipeline sparse dense three dimensional construction wide baseline image captured fast moving rolling shutter camera <eos> pecifically propose cost function bundle adjustment ba models rolling shutter effect incorporates gps ins readings enforces pairwise smoothness between neighboring poses <eos> optimize over three dimensional structures camera poses velocities <eos> also introduce novel interpolation scheme rolling shutter plane sweep stereo algorithm allows achieve speed up depth map computations dense reconstruction without losing accuracy <eos> evaluate proposed pipeline over <eos> km image sequence captured rolling shutter camera mounted moving car <eos> <eop> consistency silhouettes their duals <eos> silhouettes provide rich information three dimensional shape since intersection associated visual cones generates visual hull encloses approximates original shape <eos> however all silhouettes actually projections same object space simple observation implications object recognition multi view segmentation often implicitly used basis camera calibration <eos> paper investigate conditions multiple silhouettes more generally arbitrary closed image set geometrically consistent <eos> present notion natural generalization traditional multi view geometry deals consistency point <eos> after discussing some general result present dual formulation consistency gives conditions family planar set sections same object <eos> finally introduce more general notion silhouette compatibility under partial knowledge camera projections point out some possible directions future research <eos> <eop> rolling shutter absolute pose problem known vertical direction <eos> present solution rolling shutter rs absolute camera pose problem known vertical direction <eos> new solver pup extension general minimal solution uses double linearized rs camera model initialized standard perspective <eos> here thanks using known vertical directions avoid double linearization get camera absolute pose directly rs model without initialization standard <eos> moreover need only five three dimensional matches while needed six such matches <eos> demonstrate simulated real experiments new pup robust fast very practical method absolute camera pose computation modern cameras mobile devices <eos> compare pup state art rs perspective method demonstrate outperforms them when vertical direction known range accuracy available modern mobile devices <eos> also demonstrate when using pup solver structure motion sfm pipelines better transform already reconstructed scenes into standard position rather than using hard constraints verticality up vectors <eos> <eop> uncertainty driven pose estimation object scenes single rgb image <eos> recent years task estimating pose object instances complete scenes <eos> camera localization single input image received considerable attention <eos> consumer rgb cameras made feasible even difficult texture less object scenes <eos> work show single rgb image sufficient achieve visually convincing result <eos> key concept model exploit uncertainty system all stages processing pipeline <eos> uncertainty comes form continuous distributions over three dimensional object coordinates discrete distributions over object labels <eos> give three technical contributions <eos> firstly develop regularized auto context regression framework iteratively reduces uncertainty object coordinate object label predictions <eos> secondly introduce efficient way marginalize object coordinate distributions over depth <eos> necessary deal missing depth information <eos> thirdly utilize distributions over object labels detect multiple object simultaneously fixed budget ransac hypotheses <eos> tested system object pose estimation camera localization commonly used data set <eos> see major improvement over competing systems <eos> <eop> multicamera calibration visible mirrored epipoles <eos> multicamera rigs used large number three dimensional vision applications such three dimensional modeling motion capture telepresence robust calibration utmost importance order achieve high accuracy result <eos> many practical configurations cameras rig arranged such way they observe each other other words number epipoles correspond real image point <eos> paper propose solution automatic recovery external calibration multicamera system enforcing only simple geometrical constraints arising epipole visibility without using any calibration object such checkerboards laser pointers similar <eos> additionally introduce extension method handles case epipoles being visible reflection planar mirror makes algorithm suitable calibration any multicamera system irrespective number cameras their actual mutual visibility furthermore remark requires only one few image per camera therefore feature high speed usability <eos> produce evidence algorithm effectiveness presenting wide set tests performed synthetic well real datasets compare result obtained using traditional led based algorithm <eos> real datasets captured using multicamera virtual reality vr rig spherical dome configuration three dimensional reconstruction <eos> <eop> joint unsupervised deformable spatio temporal alignment sequences <eos> typically problems spatial temporal alignment sequences considered disjoint <eos> order align two sequences methodology non rigidly aligns image first applied followed temporal alignment obtained aligned image <eos> paper propose first best knowledge methodology jointly spatio temporally align two sequences display highly deformable texture varying object <eos> show treating problems deformable spatial temporal alignment jointly achieve better result than considering problems independent <eos> furthermore show deformable spatio temporal alignment faces performed unsupervised manner <eos> without employing face trackers building person specific deformable models <eos> <eop> deep region multi label learning facial action unit detection <eos> region learning rl multi label learning ml recently attracted increasing attentions field facial action unit au detection <eos> knowing aus active sparse facial region rl aims identify region better specificity <eos> other hand strong statistical evidence au correlations suggests ml natural way model detection task <eos> paper propose deep region multi label learning drml unified deep network simultaneously addresses two problems <eos> one crucial aspect drml novel region layer uses feed forward functions induce important facial region forcing learned weights capture structural information face <eos> region layer serves alternative design between locally connected layer <eos> confined kernels individual pixels conventional convolution layer <eos> shared kernels across entire image <eos> unlike previous studies solve rl ml alternately drml construction addresses both problems allowing two seemingly irrelevant problems interact more directly <eos> complete network end end trainable automatically learns representations robust variations inherent within local region <eos> experiments bp disfa benchmarks show drml performs highest average score auc within across datasets comparison alternative method <eos> <eop> constrained joint cascade regression framework simultaneous facial action unit recognition facial landmark detection <eos> cascade regression framework shown effective facial landmark detection <eos> starts initial face shape gradually predicts face shape update local appearance feature generate facial landmark locations next iteration until convergence <eos> paper improve upon cascade regression framework propose constrained joint cascade regression framework cjcrf simultaneous facial action unit recognition facial landmark detection two related face analysis tasks but seldomly exploited together <eos> particular first learn relationships among facial action units face shapes constraint <eos> then proposed constrained joint cascade regression framework help constraint iteratively update facial landmark locations action unit activation probabilities until convergence <eos> experimental result demonstrate intertwined relationships facial action units face shapes boost performances both facial action unit recognition facial landmark detection <eos> experimental result also demonstrate effectiveness proposed method comparing state art works <eos> <eop> unconstrained face alignment via cascaded compositional learning <eos> present practical approach address problem unconstrained face alignment single image <eos> unconstrained problem need deal large shape appearance variations under extreme head poses rich shape deformation <eos> equip cascaded regressors capability handle global shape variation irregular appearance shape relation unconstrained scenario partition optimisation space into multiple domains homogeneous descent predict shape composition estimations multiple domain specific regressors <eos> specially formulated learning objective novel tree splitting function approach capable estimating robust meaningful composition <eos> addition achieving state art accuracy over existing approaches framework also efficient solution fps thanks fly domain exclusion mechanism capability leveraging fast pixel feature <eos> <eop> automated three dimensional face reconstruction multiple image using quality measures <eos> automated three dimensional reconstruction faces image challenging if image material difficult terms pose lighting occlusions facial expressions if initial feature positions inaccurate unreliable <eos> propose method reconstructs individual three dimensional shapes multiple single image one person judges their quality then combines best all result <eos> done separately different region face <eos> core element algorithm focus paper quality measure judges reconstruction without information about true shape <eos> evaluate different quality measures develop method combining result present complete processing pipeline automated reconstruction <eos> <eop> occlusion free face alignment deep regression network coupled de corrupt autoencoders <eos> face alignment facial landmark detection plays important role many computer vision applications <eos> face recognition facial expression recognition face animation etc <eos> however performance face alignment system degenerates severely when occlusions occur <eos> work propose novel face alignment method cascades several deep regression network coupled de corrupt autoencoders denoted drda explicitly handle partial occlusion problem <eos> different previous works only detect occlusions discard occluded parts proposed de corrupt autoencoder network automatically recover genuine appearance occluded parts recovered parts leveraged together non occluded parts more accurate alignment <eos> coupling de corrupt autoencoders deep regression network deep alignment model robust partial occlusions achieved <eos> besides method localize occluded region rather than merely predict whether landmarks occluded <eos> experiments two challenging occluded face datasets demonstrate method significantly outperforms state art method <eos> <eop> multimodal spontaneous emotion corpus human behavior analysis <eos> emotion expressed multiple modalities yet most research considered most one two <eos> stems part lack large diverse well annotated multimodal databases develop test algorithms <eos> present well annotated multimodal multidimensional spontaneous emotion corpus participants <eos> emotion inductions were highly varied <eos> data were acquired variety sensors face included high resolution three dimensional dynamic imaging high resolution video thermal infrared sensing contact physiological sensors included electrical conductivity skin respiration blood pressure heart rate <eos> facial expression was annotated both occurrence intensity facial action units video experts facial action coding system facs <eos> corpus further includes derived feature three dimensional ir infrared sensors baseline result facial expression action unit detection <eos> entire corpus will made available research community <eos> <eop> learning reconstruction based remote gaze estimation <eos> challenging problem accurately estimate gazes low resolution eye image provide fine detailed feature eyes <eos> existing method attempt establish mapping between visual appearance space gaze space <eos> different direct regression approach reconstruction based approach represents appearance gaze via local linear reconstruction their own spaces <eos> common treatment use same local reconstruction two spaces <eos> reconstruction weights appearance space transferred gaze space gaze reconstruction <eos> however questionable treatment taken granted but never justified leading significant errors gaze estimation <eos> paper focused study fundamental issue <eos> shows distance metric appearance space needs adjusted before same reconstruction used <eos> novel method proposed learn metric such affinity structure appearance space under new metric close possible affinity structure gaze space under normal euclidean metric <eos> furthermore local affinity structure invariance utilized further regularize solution reconstruction weights so obtain more robust accurate solution <eos> effectiveness proposed method validated demonstrated through extensive experiments different subjects <eos> <eop> joint training cascaded cnn face detection <eos> cascade widely used face detection classifier low computation cost firstly used shrink most background while keeping recall <eos> cascade detection popularized seminal viola jones framework then widely used other pipelines such dpm cnn <eos> however best knowledge most previous detection method use cascade greedy manner previous stages cascade fixed when training new stage <eos> so optimizations different cnn isolated <eos> paper propose joint training achieve end end optimization cnn cascade <eos> show back propagation algorithm used training cnn naturally used training cnn cascade <eos> present how jointly training conducted naive cnn cascade more sophisticated region proposal network rpn fast cnn <eos> experiments face detection benchmarks verify advantages joint training <eos> <eop> facial expression intensity estimation using ordinal information <eos> previous studies facial expression analysis focused recognizing basic expression categories <eos> there limited amount work continuous expression intensity estimation important detecting tracking emotion change <eos> part reason lack labeled data annotated expression intensity since expression intensity annotation requires expertise time consuming <eos> work treat expression intensity estimation regression problem <eos> taking advantage natural onset apex offset evolution pattern facial expression proposed method handle different amounts annotations perform frame level expression intensity estimation <eos> fully supervised case all frames provided intensity annotations <eos> weakly supervised case only annotations selected key frames used <eos> while unsupervised case expression intensity estimated without any annotations <eos> efficient optimization algorithm based alternating direction method multipliers admm developed solving optimization problem associated parameter learning <eos> demonstrate effectiveness proposed method comparing against both fully supervised unsupervised approaches benchmark facial expression datasets <eos> <eop> proposal flow <eos> finding image correspondences remains challenging problem presence intra class variations large changes scene layout <eos> semantic flow method designed handle image depicting different instances same object scene category <eos> introduce novel approach semantic flow dubbed proposal flow establishes reliable correspondences using object proposals <eos> unlike prevailing semantic flow approaches operate pixels regularly sampled local region proposal flow benefits characteristics modern object proposals exhibit high repeatability multiple scales take advantage both local geometric consistency constraints among proposals <eos> also show proposal flow effectively transformed into conventional dense flow field <eos> introduce new dataset used evaluate both general semantic flow techniques region based approaches such proposal flow <eos> use benchmark compare different matching algorithms object proposals region feature within proposal flow state art semantic flow <eos> comparison along experiments standard datasets demonstrates proposal flow significantly outperforms existing semantic flow method various settings <eos> <eop> pronet learning propose object specific boxes cascaded neural network <eos> paper aims classify locate object accurately efficiently without using bounding box annotations <eos> challenging object wild could appear arbitrary locations different scales <eos> paper propose novel classification architecture pronet based convolutional neural network <eos> uses computationally efficient neural network propose image region likely contain object applies more powerful but slower network proposed region <eos> basic building block multi scale fully convolutional network assigns object confidence scores boxes different locations scales <eos> show such network trained effectively using image level annotations connected into cascades trees efficient object classification <eos> pronet outperforms previous state art significantly pascal voc ms coco datasets object classification point based localization <eos> <eop> seeing behind camera identifying authorship photograph <eos> introduce novel problem identifying photographer behind photograph <eos> explore feasibility current computer vision techniques address problem created new dataset over image taken well known photographers <eos> using dataset examined effectiveness variety feature low high level including cnn feature identifying photographer <eos> also trained new deep convolutional neural network task <eos> result show high level feature greatly outperform low level feature <eos> provide qualitative result using learned models give insight into method ability distinguish between photographers allow draw interesting conclusions about specific photographers shoot <eos> also demonstrate two applications method <eos> <eop> material classification using raw time flight measurements <eos> propose material classification method using raw time flight tof measurements <eos> tof cameras capture correlation between reference signal temporal response material incident illumination <eos> such measurements encode unique signatures material <eos> degree subsurface scattering inside volume <eos> subsequently offers orthogonal domain feature representation compared conventional spatial angular reflectance based approaches <eos> demonstrate effectiveness robustness efficiency method through experiments comparisons real world materials <eos> <eop> weakly supervised object localization progressive domain adaptation <eos> address problem weakly supervised object localization only image level annotations available training <eos> many existing approaches tackle problem through object proposal mining <eos> however substantial amount noise object proposals causes ambiguities learning discriminative object models <eos> such approaches sensitive model initialization often converge undesirable local minimum <eos> paper address problem progressive domain adaptation two main steps classification adaptation detection adaptation <eos> classification adaptation transfer pre trained network multi label classification task recognizing presence certain object image <eos> detection adaptation first use mask out strategy collect class specific object proposals apply multiple instance learning mine confident candidates <eos> then use selected object proposals fine tune all layer resulting fully adapted detection network <eos> extensively evaluate localization performance pascal voc ilsvrc datasets demonstrate significant performance improvement over state art method <eos> <eop> newtonian scene understanding unfolding dynamics object static image <eos> paper study challenging problem predicting dynamics object static image <eos> given query object image goal provide physical understanding object terms forces acting upon its long term motion response forces <eos> direct explicit estimation forces motion object single image extremely challenging <eos> define intermediate physical abstractions called newtonian scenarios introduce newtonian neural network learns map single image state newtonian scenario <eos> experimental evaluations show method reliably predict dynamics query object single image <eos> addition approach provide physical reasoning supports predicted dynamics terms velocity force vectors <eos> spur research direction compiled visual newtonian dynamics vind dataset includes more than video aligned newtonian scenarios represented using game engines more than still image their ground truth dynamics <eos> <eop> identifying good training data self supervised free space estimation <eos> paper proposes novel technique extract training data free space scene using stereo camera <eos> proposed technique exploits projection planes disparity image paired bayesian linear regression reliably identify training image pixels belonging free space scene <eos> unlike other method literature algorithm require any prior training only one free parameter shown provide consistent result over variety terrains without need any manual tuning <eos> proposed method compared two other data extraction method literature <eos> result support vector classifiers using training data extracted proposed technique superior terms quality consistency free space estimation <eos> furthermore computation time required proposed technique shown smaller more consistent than other training data extraction method <eos> <eop> learning match aerial image deep attentive architectures <eos> image matching fundamental problem computer vision <eos> context feature based matching sift its variants long excelled wide array applications <eos> however ultra wide baselines case aerial image captured under large camera rotations appearance variation goes beyond reach sift ransac <eos> paper propose data driven deep learning based approach sidesteps local correspondence framing problem classification task <eos> furthermore demonstrate local correspondences still useful <eos> so incorporate attention mechanism produce set probable matches allows further increase performance <eos> train models dataset urban aerial imagery consisting same different pairs collected purpose characterize problem via human study annotations amazon mechanical turk <eos> demonstrate models outperform state art ultra wide baseline matching approach human accuracy <eos> <eop> track transfer watching video simulate strong human supervision weakly supervised object detection <eos> status quo approach training object detectors requires expensive bounding box annotations <eos> framework takes markedly different direction transfer tracked object boxes weakly labeled video weakly labeled image automatically generate pseudo ground truth boxes replace manually annotated bounding boxes <eos> first mine discriminative region weakly labeled image collection frequently rarely appear positive negative image <eos> then match region video retrieve corresponding tracked object boxes <eos> finally design hough transform algorithm vote best box serve pseudo gt each image use them train object detector <eos> together lead state art weakly supervised detection result pascal datasets <eos> <eop> deepcamp deep convolutional action attribute mid level patterns <eos> recognition human actions determination human attributes two tasks call fine grained classification <eos> indeed often rather small inconspicuous object feature detected tell their classes apart <eos> order deal challenge propose novel convolutional neural network mines mid level image patches sufficiently dedicated resolve corresponding subtleties <eos> particular train newly designed cnn deeppattern learns discriminative patch groups <eos> there two innovative aspects <eos> one hand pay attention contextual information original fashion <eos> other hand let iteration feature learning patch clustering purify set dedicated patches use <eos> validate method action classification two challenging datasets pascal voc action stanford actions attribute recognition use berkeley attributes people dataset <eos> discriminative mid level mining cnn obtains state art result datasets without need annotations about parts poses <eos> <eop> canny text detector fast robust scene text localization algorithm <eos> paper presents novel scene text detection algorithm canny text detector takes advantage similarity between image edge text effective text localization improved recall rate <eos> closely related edge pixels construct structural information object observe cohesive characters compose meaningful word sentence sharing similar properties such spatial location size color stroke width regardless language <eos> however prevalent scene text detection approaches fully utilized such similarity but mostly rely characters classified high confidence leading low recall rate <eos> exploiting similarity approach quickly robustly localize variety texts <eos> inspired original canny edge detector algorithm makes use double threshold hysteresis tracking detect texts low confidence <eos> experimental result public datasets demonstrate algorithm outperforms state art scene text detection method terms detection rate <eos> <eop> temporal multimodal learning audiovisual speech recognition <eos> view advantages deep network producing useful representation generated feature different modality data such image audio jointly learned using multimodal restricted boltzmann machines mrbm <eos> recently audiovisual speech recognition based mrbm attracted much attention mrbm shows its effectiveness learning joint representation across audiovisual modalities <eos> however built network weakness modeling multimodal sequence natural property speech signal <eos> paper will introduce novel temporal multimodal deep learning architecture named recurrent temporal multimodal rbm rtmrbm models multimodal sequences transforming sequence connected mrbms into probabilistic series model <eos> compared existing multimodal network simple efficient learning temporal joint representation <eos> evaluate model audiovisual speech datasets two public avletters avletters one self build <eos> experimental result demonstrate approach obviously improve accuracy recognition compared standard mrbm temporal model based conditional rbm <eos> addition rtmrbm still outperforms non temporal multimodal deep network presence weakness long term dependencies <eos> <eop> recovering object pose predicting next best view crowd <eos> object detection pose estimation crowd scenes multiple object instances severe foreground occlusions background distractors become important problem many rapidly evolving technological areas such robotics augmented reality <eos> single shot based pose estimators manually designed feature still unable tackle above challenges motivating research towards unsupervised feature learning next best view estimation <eos> work present complete framework both single shot based object pose estimation next best view prediction based hough forests state art object pose estimator performs classification regression jointly <eos> rather than using manually designed feature propose unsupervised feature learnt depth invariant patches using sparse autoencoder offer extensive evaluation various state art feature <eos> furthermore taking advantage clustering performed leaf nodes hough forests learn estimate reduction uncertainty other views formulating problem selecting next best view <eos> further improve pose estimation propose improved joint registration hypotheses verification module final refinement step reject false detections <eos> provide two additional challenging datasets inspired realistic scenarios extensively evaluate state art framework <eos> one related domestic environments other depicts bin picking scenario mostly found industrial settings <eos> show framework significantly outperforms state art both public datasets <eos> <eop> robust three dimensional hand pose estimation single depth image single view cnn multi view cnn <eos> articulated hand pose estimation plays important role human computer interaction <eos> despite recent progress accuracy existing method still satisfactory partially due difficulty embedded high dimensional non linear regression problem <eos> different existing discriminative method regress hand pose single depth image propose first project query depth image onto three orthogonal planes utilize multi view projections regress heat maps estimate joint positions each plane <eos> multi view heat maps then fused produce final three dimensional hand pose estimation learned pose priors <eos> experiments show proposed method largely outperforms state arts challenging dataset <eos> moreover cross dataset experiment also demonstrates good generalization ability proposed method <eos> <eop> semantic segmentation boundary neural fields <eos> state art semantic segmentation currently represented fully convolutional network fcns <eos> however fcns use large receptive fields many pooling layer both cause blurring low spatial resolution deep layer <eos> result fcns tend produce segmentations poorly localized around object boundaries <eos> prior work attempted address issue post processing steps example using color based crf top fcn predictions <eos> however approaches require additional parameters low level feature difficult tune integrate into original network architecture <eos> additionally most crfs use color based pixel affinities well suited semantic segmentation lead spatially disjoint predictions <eos> overcome problems introduce boundary neural field bnf global energy model integrating fcn predictions boundary cues <eos> boundary information used enhance semantic segment coherence improve object localization <eos> specifically first show convolutional filters semantic fcns provide good feature boundary detection <eos> then employ predicted boundaries define pairwise potentials energy <eos> finally show energy decomposes semantic segmentation into multiple binary problems relaxed efficient global optimization <eos> report extensive experiments demonstrating minimization global boundary based energy yields result superior prior globalization method both quantitatively well qualitatively <eos> <eop> hd maps fine grained road segmentation parsing ground aerial image <eos> paper present approach enhance existing maps fine grained segmentation categories such parking spots sidewalk well number location road lanes <eos> towards goal propose efficient approach able estimate fine grained categories doing joint inference over both monocular aerial imagery well ground image taken stereo camera pair mounted top car <eos> important reasoning about alignment between two types imagery even when measurements taken sophisticated gps imu systems alignment sufficiently accurate <eos> demonstrate effectiveness approach new dataset enhances kitti aerial image taken camera mounted airplane flying around city karlsruhe germany <eos> <eop> dag recurrent neural network scene labeling <eos> image labeling local representations image units pixels patches superpixels usually generated their surrounding image patches thus long range contextual information effectively encoded <eos> paper introduce recurrent neural network rnns address issue <eos> specifically directed acyclic graph rnns dag rnns proposed process dag structured image enables network model long range semantic dependencies among image units <eos> dag rnns capable tremendously enhancing discriminative power local representations significantly benefits local classification <eos> meanwhile propose novel class weighting function attends rare classes phenomenally boosts recognition accuracy non frequent classes <eos> integrating convolution deconvolution layer dag rnns achieve new state art result challenging siftflow camvid barcelona benchmarks <eos> <eop> saliency guided dictionary learning weakly supervised image parsing <eos> paper propose novel method perform weakly supervised image parsing based dictionary learning framework <eos> deal challenges caused label ambiguities design saliency guided weight assignment scheme boost discriminative dictionary learning <eos> more specifically collection tagged image proposed method first conducts saliency detection automatically infers confidence each semantic class foreground background <eos> clues then incorporated learn dictionaries weights well sparse representation coefficients meanwhile <eos> once obtained coefficients superpixel use sparse representation classifier determine its semantic label <eos> approach validated msrc pascal voc voc datasets <eos> experimental result demonstrate encouraging performance approach comparison some state arts <eos> <eop> attention scale scale aware semantic image segmentation <eos> incorporating multi scale feature fully convolutional neural network fcns key element achieving state art performance semantic image segmentation <eos> one common way extract multi scale feature feed multiple resized input image shared deep network then merge resulting feature pixel wise classification <eos> work propose attention mechanism learns softly weight multi scale feature each pixel location <eos> adapt state art semantic image segmentation model jointly train multi scale input image attention model <eos> proposed attention model only outperforms average max pooling but allows diagnostically visualize importance feature different positions scales <eos> moreover show adding extra supervision output each scale essential achieving excellent performance when merging multi scale feature <eos> demonstrate effectiveness model extensive experiments three challenging datasets including pascal person part pascal voc subset ms coco <eos> <eop> scene labeling using sparse precision matrix <eos> scene labeling task segment image into meaningful region categorize them into classes object comprised image <eos> commonly used method typically find local feature each segment label them using classifiers <eos> afterwards labeling smoothed order make sure neighboring region receive similar labels <eos> however method ignore expressive connections between labels non local dependencies among region <eos> paper propose use sparse estimation precision matrix also called concentration matrix inverse covariance matrix data obtained graphical lasso find interaction between labels region <eos> formulate problem energy minimization over graph whose structure captured applying sparse constraint elements precision matrix <eos> graph encodes represents only significant interactions avoids fully connected graph typically used reflect long distance associations <eos> use local global information achieve better labeling <eos> assess approach three datasets obtained promising result <eos> <eop> iterative instance segmentation <eos> existing method pixel wise labelling tasks generally disregard underlying structure labellings often leading predictions visually implausible <eos> while incorporating structure into model should improve prediction quality doing so challenging manually specifying form structural constraints may impractical inference often becomes intractable even if structural constraints given <eos> sidestep problem reducing structured prediction sequence unconstrained prediction problems demonstrate approach capable automatically discovering priors shape contiguity region predictions smoothness region contours data without any priori specification <eos> instance segmentation task method outperforms state art achieving mean ap <eos> <eop> recurrent attentional network saliency detection <eos> convolutional deconvolution network adopted perform end end saliency detection <eos> but they work well object multiple scales <eos> overcome such limitation work propose recurrent attentional convolutional deconvolution network racdnn <eos> using spatial transformer recurrent network units racdnn able iteratively attend selected image sub region perform saliency refinement progressively <eos> besides tackling scale problem racdnn also learn context aware feature past iterations enhance saliency refinement future iterations <eos> experiments several challenging saliency detection datasets validate effectiveness racdnn show racdnn outperforms state art saliency detection method <eos> <eop> instance level video segmentation object tracks <eos> address problem segmenting multiple object instances complex video <eos> method require manual pixel level annotation training relies instead readily available object detectors visual object tracking only <eos> given object bounding boxes input cast video segmentation weakly supervised learning problem <eos> proposed objective combines discriminative clustering term background segmentation spectral clustering one grouping pixels same object instances linear constraints enabling instance level segmentation <eos> propose convex relaxation problem solve efficiently using frank wolfe algorithm <eos> report result compare method several baselines new video dataset multi instance person segmentation <eos> <eop> semantic instance annotation street scenes three dimensional label transfer <eos> supplementary material provides additional illustrations visualizations experiments <eos> start showing color coding label mapping used semantic instance label result paper <eos> then provide more details about three dimensional fold curb detection parameter settings used paper <eos> next provide additional quantitative qualitative semi dense inference result both semantic instance segmentation <eos> finally show ability method annotate three dimensional point clouds semantic instance labels byproduct approach <eos> <eop> amplitude modulated video camera light separation dynamic scenes <eos> controlled light conditions improve considerably performance most computer vision algorithms <eos> dynamic light conditions create varying spatial changes color intensity across scene <eos> condition caused moving shadow example force developers create algorithms robust such variations <eos> suggest computational camera produces image influenced environmental variations light conditions <eos> key insight many years ago similar difficulties were already solved radio communication result each channel immune interference other radio channels <eos> amplitude modulated am video camera separates influence modulated light other unknown light sources scene causing am video camera frame appear same independent light conditions was taken <eos> built prototype am video camera using off shelf hardware tested <eos> am video camera was used demonstrate color constancy shadow removal contrast enhancement real time <eos> show theoretically empirically <eos> proposed system produce image similar noise levels standard camera <eos> image created such camera almost completely immune temporal spatial spectral changes background light <eos> <eop> benchmark dataset evaluation non lambertian uncalibrated photometric stereo <eos> recent progress photometric stereo extends technique deal general materials unknown illumination conditions <eos> however due lack suitable benchmark data ground truth shapes normals quantitative comparison evaluation difficult achieve <eos> paper first survey categorize existing method using photometric stereo taxonomy emphasizing non lambertian uncalibrated method <eos> then introduce diligent photometric stereo image dataset calibrated directional lightings object general reflectance ground truth shapes normals <eos> based dataset quantitatively evaluate state art photometric stereo method general non lambertian materials unknown lightings analyze their strengths limitations <eos> <eop> depth semi calibrated stereo defocus <eos> work propose multi camera system combine main high quality camera two low res auxiliary cameras <eos> auxiliary cameras well calibrated act passive depth sensor generating disparity maps <eos> main camera interchangeable lens produce good quality image high resolution <eos> goal given low res depth map auxiliary cameras generate depth map viewpoint main camera <eos> advantage system compared other systems such light field cameras rgbd sensors ability generate high resolution color image complete depth map without sacrificing resolution minimal auxiliary hardware <eos> since main camera interchangeable lens cannot calibrated beforehand directly applying stereo matching either auxiliary cameras often leads unsatisfactory result <eos> utilizing both calibrated cameras once propose novel approach better estimate disparity map main camera <eos> then combining defocus cue main camera disparity map further improved <eos> demonstrate performance algorithm various scenes <eos> <eop> exploiting spectral spatial correlation coded hyperspectral image restoration <eos> conventional scanning multiplexing techniques hyperspectral imaging suffer limited temporal spatial resolution <eos> resolve issue coding techniques becoming increasingly popular developing snapshot systems high resolution hyperspectral imaging <eos> such systems critical task accurately restore three dimensional hyperspectral image its corresponding coded image <eos> paper propose effective method coded hyperspectral image restoration exploits extensive structure sparsity hyperspectral image <eos> specifically simultaneously explore spectral spatial correlation via low rank regularizations formulate restoration problem into variational optimization model solved via iterative numerical algorithm <eos> experimental result using both synthetic data real image show proposed method significantly outperform state art method several popular coding based hyperspectral imaging systems <eos> <eop> variable aperture light field photography overcoming diffraction limited spatio angular resolution tradeoff <eos> light fields many applications machine vision consumer photography robotics microscopy <eos> however prevalent resolution limits existing light field imaging systems hinder widespread adoption <eos> paper analyze fundamental resolution limits light field cameras diffraction limit <eos> propose sequential coded aperture style acquisition scheme optimizes resolution light field reconstructed multiple photographs captured different perspectives number settings <eos> also show proposed acquisition scheme facilitates high dynamic range light field imaging demonstrate proof concept prototype system <eos> work hope advance understanding resolution limits light field photography develop practical computational imaging systems overcome them <eos> <eop> convolutional network shape light field <eos> convolutional neural network cnn recently successfully applied various computer vision cv applications <eos> paper utilize cnn predict depth information given light field lf data <eos> proposed method learns end end mapping between light field representation corresponding depth field terms hyperplane orientations <eos> obtained prediction then further refined post processing step applying higher order regularization <eos> existing lf datasets sufficient purpose training scheme tackled paper <eos> mainly due fact ground truth depth existing datasets inaccurate datasets limited small number lfs <eos> made necessary generate new synthetic lf dataset based raytracing software pov ray <eos> new dataset provides floating point accurate ground truth depth fields due random scene generator dataset scaled required <eos> <eop> panoramic stereo video single camera <eos> present practical solution generating degree stereo panoramic video using single camera <eos> current approaches either use moving camera captures multiple image scene then stitched together form final panorama use multiple cameras synchronized <eos> moving camera limits solution static scenes while multi camera solutions require dedicated calibrated setups <eos> approach improves upon existing solutions two significant ways solves problem using single camera thus minimizing calibration problem providing ability convert any digital camera into panoramic stereo capture device <eos> captures all light rays required stereo panoramas single frame using compact custom designed mirror thus making design practical manufacture easier use <eos> analyze several properties design well present panoramic stereo depth estimation result <eos> <eop> next best underwater view <eos> image high resolution large occlusion prone scenes camera must move above around <eos> degradation visibility due geometric occlusions distances exacerbated scattering when scene participating medium <eos> moreover underwater other media artificial lighting needed <eos> overall data quality depends observed surface medium time varying poses camera light source <eos> work proposes optimize camera light poses they move so surface scanned efficiently descattered recovery highest quality <eos> work generalizes next best view concept robot vision scattering media cooperative movable lighting <eos> also extends descattering platforms move optimally <eos> optimization criterion information gain taken information theory <eos> exploit existence prior rough three dimensional model since underwater such model routinely obtained using sonar <eos> demonstrate principle scaled down setup <eos> <eop> reconstructing shapes appearances thin film object using rgb image <eos> reconstruction shapes appearances thin film object applied many fields such industrial inspection biological analysis archeology research <eos> however comes many challenging issues because appearances thin film change dramatically depending view light directions <eos> appearance deeply dependent only shapes but also optical parameters thin film <eos> paper propose novel method estimate shapes film thickness <eos> first narrow down candidates zenith angle degree polarization determine intensity thin film increases monotonically along zenith angle <eos> second determine azimuth angle occluding boundaries <eos> finally estimate film thickness comparing look up table color along thickness zenith angle captured image <eos> experimentally evaluated accuracy estimated shapes appearances found proposed method effective <eos> <eop> noisy label recovery shadow detection unfamiliar domains <eos> recent shadow detection algorithms shown initial success small datasets image specific domains <eos> however shadow detection broader image domains still challenging due lack annotated training data <eos> due intense manual labor annotating shadow data <eos> paper propose lazy annotation efficient annotation method annotator only needs mark important shadow areas some non shadow areas <eos> yields data noisy labels yet useful training shadow detector <eos> address problem label noise jointly learning shadow region classifier recovering labels training set <eos> consider training labels unknowns formulate label recovery problem minimization sum squared leave one out errors least squares svm efficiently optimized <eos> experimental result show classifier trained recovered labels achieves comparable performance classifier trained properly annotated data <eos> result suggest feasible approach address task detecting shadows unfamiliar domain collecting lazily annotating some image new domain training <eos> will demonstrated approach outperforms method rely precisely annotated but less relevant datasets <eos> initial result suggest more general applicability <eos> <eop> deep hand how train cnn million hand image when your data continuous weakly labelled <eos> work presents new approach learning frame based classifier weakly labelled sequence data embedding cnn within iterative em algorithm <eos> allows cnn trained vast number example image when only loose sequence level information available source video <eos> although demonstrate context hand shape recognition approach wider application any video recognition task frame level labelling available <eos> iterative em algorithm leverages discriminative ability cnn iteratively refine frame level annotation subsequent training cnn <eos> embedding classifier within em framework cnn easily trained million hand image <eos> demonstrate final classifier generalises over both individuals data set <eos> algorithm evaluated over manually labelled hand shape image different classes will released community <eos> furthermore demonstrate its use continuous sign language recognition two publicly available large sign language data set outperforms current state art large margin <eos> knowledge no previous work explored expectation maximization without gaussian mixture models exploit weak sequence labels sign language recognition <eos> <eop> recognizing car fluents video <eos> physical fluents term originally used newton refers time varying object states dynamic scenes <eos> paper interested inferring fluents vehicles video <eos> example door hood trunk open closed through various actions light blinking turn <eos> recognizing fluents broad applications yet received scant attention computer vision literature <eos> car fluent recognition entails unified framework car detection car part localization part status recognition made difficult large structural appearance variations low resolutions occlusions <eos> paper learns spatial temporal hierarchical model represent car fluents <eos> learning model formulated under latent structural svm framework <eos> since there no publicly related dataset collect annotate car fluent dataset consisting car video diverse fluents <eos> experiments proposed method outperforms several highly related baseline method terms car fluent recognition car part localization <eos> <eop> pairwise decomposition image sequences active multi view recognition <eos> multi view image sequence provides much richer capacity object recognition than single image <eos> however most existing solutions multi view recognition typically adopt hand crafted model based geometric method readily embrace recent trends deep learning <eos> propose bring convolutional neural network generic multi view recognition decomposing image sequence into set image pairs classifying each pair independently then learning object classifier weighting contribution each pair <eos> allows recognition over arbitrary camera trajectories without requiring explicit training over potentially infinite number camera paths lengths <eos> building pairwise relationships then naturally extends next best view problem active recognition framework <eos> achieve train second convolutional neural network map directly observed image next viewpoint <eos> finally incorporate into trajectory optimisation task whereby best recognition confidence sought given trajectory length <eos> present state art result both guided unguided multi view recognition modelnet dataset show how method used depth image greyscale image both <eos> <eop> inferring forces learning human utilities video <eos> propose notion affordance takes into account physical quantities generated when human body interacts real world object introduce learning framework incorporates concept human utilities opinion provides deeper finer grained account only object affordance but also people interaction object <eos> rather than defining affordance terms geometric compatibility between body poses three dimensional object devise algorithms employ physics based simulation infer relevant forces pressures acting body parts <eos> observing choices people make video particularly selecting chair sit system learns comfort intervals forces exerted body parts while sitting <eos> account people preferences terms human utilities transcend comfort intervals account also meaningful tasks within scenes spatiotemporal constraints motion planning such purposes robot task planning <eos> <eop> force motion decoding physical sensation first person video <eos> first person video generate powerful physical sensations action observer <eos> paper focus problem force motion decoding sensation passive forces such gravity physical scale motion speed space active forces exerted observer such pedaling bike banking ski turn <eos> sensation gravity observed natural image <eos> learn image cue predicting gravity direction image integrate prediction across image estimate three dimensional gravity direction using structure motion <eos> sense physical scale revealed when body dynamically balanced state <eos> compute unknown physical scale three dimensional reconstructed camera motion leveraging torque equilibrium banked turn relates centripetal force gravity body leaning angle <eos> active force torque governs three dimensional egomotion through physics rigid body dynamics <eos> using inverse dynamics optimization directly minimize reprojection error video respect three dimensional world structure active forces additional passive forces such air drag friction force <eos> use structure motion physical scale gravity direction initialization bundle adjustment force estimation <eos> method shows quantitatively equivalent reconstruction comparing imu measurements terms gravity scale recovery outperforms method based optical flow active action recognition task <eos> apply method first person video mountain biking urban bike racing skiing speedflying parachute wingsuit flying inertial measurements accessible <eos> <eop> robust multi body feature tracker segmentation free approach <eos> feature tracking fundamental problem computer vision applications various tasks including three dimensional reconstruction visual slam <eos> while many method devoted making tasks robust noise outliers less attention attracted improving feature tracking itself <eos> paper introduces novel multi body feature tracker takes advantage multi body rigidity assumption improve tracking robustness <eos> conventional approach addressing problem would consist alternating between solving two subtasks motion segmentation feature tracking under rigidity constraints each segment <eos> approach however requires knowing number motions well assigning point motion groups typically sensitive motion estimates <eos> contrast here introduce segmentation free solution multi body feature tracking bypasses motion assignment step reduces solving series subproblems closed form solutions <eos> experiments demonstrate benefits approach terms tracking accuracy robustness noise <eos> <eop> slow steady feature analysis higher order temporal coherence video <eos> how unlabeled video augment visual learning existing method perform slow feature analysis encouraging temporal coherence image representations temporally close frames exhibit only small differences <eos> while standard approach captures fact high level visual signals change slowly over time fails capture how visual content changes <eos> propose generalize slow feature analysis steady feature analysis <eos> key idea impose prior higher order derivatives learned feature space must small <eos> end train convolutional neural network regularizer minimizes contrastive loss tuples sequential frames unlabeled video <eos> focusing case triplets frames proposed method encourages feature changes over time should smooth <eos> similar most recent changes <eos> using five diverse image video datasets including unlabeled youtube kitti video demonstrate method impact object recognition scene classification action recognition tasks <eos> further show feature learned unlabeled video even surpass standard heavily supervised pretraining approach <eos> <eop> volumetric three dimensional tracking detection <eos> paper propose new framework three dimensional tracking detection based fully volumetric representations <eos> one hand three dimensional tracking detection shown robust use context interaction kinect surface tracking <eos> other hand volumetric representations recently proven efficient both building three dimensional feature addressing three dimensional tracking problem <eos> leverage benefits unifying both families approaches into single fully volumetric tracking detection framework <eos> use centroidal voronoi tessellation cvt representation compactly tessellate shapes optimal discretization construct feature space perform tracking according correspondences provided trained random forests <eos> result show improved tracking training computational efficiency improved memory performance <eos> turn enables use larger training databases than state art approaches leverage proposing cross tracking subject training scheme benefit all subject sequences all tracking situations thus yielding better detection less overfitting <eos> <eop> solution path algorithm identity aware multi object tracking <eos> propose identity aware multi object tracker based solution path algorithm <eos> tracker only produces identity coherent trajectories based cues such face recognition but also ability pinpoint potential tracking errors <eos> tracker formulated quadratic optimization problem norm constraints propose solve solution path algorithm <eos> algorithm successively solves same optimization problem but under different lp norm constraints gradually decreases <eos> inspired success solution path algorithm various machine learning tasks strategy expected converge better local minimum than directly minimizing hardly solvable norm roughly approximated norm constraints <eos> furthermore acquired solution path complies decision making process tracker provides more insight locating potential tracking errors <eos> experiments show only proposed tracker effective but also solution path enables automatic pinpointing potential tracking failures readily utilized active learning framework improve identity aware multi object tracking <eos> <eop> defense sparse tracking circulant sparse tracker <eos> sparse representation introduced visual tracking finding best target candidate minimal reconstruction error within particle filter framework <eos> however most sparse representation based trackers high computational cost less than promising tracking performance limited feature representation <eos> deal above issues propose novel circulant sparse tracker cst exploits circulant target templates <eos> because circulant structure property cst following advantages refine reduce particles using circular shifts target templates <eos> optimization efficiently solved entirely fourier domain <eos> high dimensional feature embedded into cst significantly improve tracking performance without sacrificing much computation time <eos> both qualitative quantitative evaluations challenging benchmark sequences demonstrate cst performs better than all other sparse trackers favorably against state art method <eos> <eop> optical flow semantic segmentation localized layer <eos> existing optical flow method make generic spatially homogeneous assumptions about spatial structure flow <eos> reality optical flow varies across image depending object class <eos> simply put different object move differently <eos> here exploit recent advances static semantic scene segmentation segment image into object different types <eos> define different models image motion region depending type object <eos> example road motion homographies vegetation spatially smooth flow independently moving object like cars planes affine deviations <eos> then pose flow estimation problem using novel formulation localized layer addresses limitations traditional layered models dealing complex scene motion <eos> semantic flow method achieves lowest error any published method kitti flow benchmark produces qualitatively better flow segmentation than recent top method wide range natural video <eos> <eop> video segmentation via object flow <eos> video object segmentation challenging due fast moving object deforming shapes cluttered backgrounds <eos> optical flow used propagate object segmentation over time but unfortunately flow often inaccurate particularly around object boundaries <eos> such boundaries precisely want segmentation accurate <eos> obtain accurate segmentation across time propose efficient algorithm considers video segmentation optical flow estimation simultaneously <eos> video segmentation formulate principled multi scale spatio temporal objective function uses optical flow propagate information between frames <eos> optical flow estimation particularly object boundaries compute flow independently segmented region recompose result <eos> call process object flow demonstrate effectiveness jointly optimizing optical flow video segmentation using iterative scheme <eos> experiments segtrack youtube object datasets show proposed algorithm performs favorably against other state art method <eos> <eop> closed form training mahalanobis distance supervised clustering <eos> clustering task grouping set object so object same cluster more similar each other than other clusters <eos> crucial step most clustering algorithms find appropriate similarity metric both challenging problem dependent <eos> supervised clustering approaches exploit labeled clustered training data share common metric test set thus proposed <eos> unfortunately current metric learning approaches supervised clustering scale large even medium sized datasets <eos> paper propose new structured mahalanobis distance metric learning method supervised clustering <eos> formulate problem instance large margin structured prediction prove solved very efficiently closed form <eos> complexity method most cases linear size training dataset <eos> further reveal striking similarity between approach multivariate linear regression <eos> experiments both synthetic real datasets confirm several orders magnitude speedup while still achieving state art performance <eos> <eop> scalable sparse subspace clustering orthogonal matching pursuit <eos> subspace clustering method based ell nuclear norm regularization become very popular due their simplicity theoretical guarantees empirical success <eos> however choice regularizer greatly impact both theory practice <eos> instance ell regularization guaranteed give subspace preserving affinity <eos> there no connections between point different subspaces under broad conditions <eos> arbitrary subspaces corrupted data <eos> however requires solving large scale convex optimization problem <eos> other hand nuclear norm regularization provide efficient closed form solutions but require very strong assumptions guarantee subspace preserving affinity <eos> independent subspaces uncorrupted data <eos> paper study subspace clustering method based orthogonal matching pursuit <eos> show method both computationally efficient guaranteed give subspace preserving affinity under broad conditions <eos> experiments synthetic data verify theoretical analysis applications handwritten digit face clustering show approach achieves best trade off between accuracy efficiency <eos> moreover approach first one handle data point <eos> <eop> oracle based active set algorithm scalable elastic net subspace clustering <eos> state art subspace clustering method based expressing each data point linear combination other data point while regularizing matrix coefficients nuclear norms <eos> regularization guaranteed give subspace preserving affinity <eos> there no connections between point different subspaces under broad theoretical conditions but clusters may connected <eos> nuclear norm regularization often improve connectivity but give subspace preserving affinity only independent subspaces <eos> mixed nuclear norm regularizations offer balance between subspace preserving connectedness properties but comes cost increased computational complexity <eos> paper studies geometry elastic net regularizer mixture norms uses derive provably correct scalable active set method finding optimal coefficients <eos> geometric analysis also provides theoretical justification geometric interpretation balance between connectedness due regularization subspace preserving due regularization properties elastic net subspace clustering <eos> experiments show proposed active set method only achieves state art clustering performance but also efficiently handles large scale datasets <eos> <eop> sparse coding dictionary learning linear dynamical systems <eos> linear dynamical systems ldss fundamental tools encoding spatio temporal data various disciplines <eos> enhance performance ldss paper address challenging issue performing sparse coding space ldss both data dictionary atoms ldss <eos> rather than approximate extended observability finite order matrix represent space ldss infinite grassmannian consisting orthonormalized extended observability subspaces <eos> via homeomorphic mapping such grassmannian embedded into space symmetric matrices tractable objective function derived sparse coding <eos> then propose efficient method learn system parameters dictionary atoms explicitly imposing symmetric constraint transition matrices data dictionary systems <eos> moreover combine state covariance into algorithm formulation thus further promoting performance models symmetric transition matrices <eos> comparative experimental evaluations reveal superior performance proposed method various tasks including video classification tactile recognition <eos> <eop> sublabel accurate relaxation nonconvex energies <eos> propose novel spatially continuous framework convex relaxations based functional lifting <eos> method interpreted sublabel accurate solution multilabel problems <eos> show previously proposed functional lifting method optimize energy linear between two labels hence require often infinitely many labels faithful approximation <eos> contrast proposed formulation based piecewise convex approximation therefore needs far fewer labels see fig <eos> comparison recent mrf based approaches method formulated spatially continuous setting shows less grid bias <eos> moreover local sense formulation tightest possible convex relaxation <eos> easy implement allows efficient primal dual optimization gpus <eos> show effectiveness approach several computer vision problems <eos> <eop> multiverse loss robust transfer learning <eos> deep learning techniques renowned supporting effective transfer learning <eos> however demonstrate transferred representations support only few modes separation much its dimensionality unutilized <eos> work suggest learn source domain multiple orthogonal classifiers <eos> prove leads reduced rank representation however supports more discriminative directions <eos> interestingly softmax probabilities produced multiple classifiers likely identical <eos> extensive experimental result further demonstrate effectiveness method <eos> <eop> learning mistakes others matching errors cross dataset learning <eos> learn about object classes image looking collection relevant three dimensional models if want learn about human inter actions image benefit video abstract illustrations show actions common aspect settings availability additional privileged data exploited training time will available interest test time <eos> seek generalize learning privileged information lupi framework requires additional information defined per image setting additional information data collection about task interest <eos> framework minimises distribution mismatch between errors made image privileged data <eos> proposed method tested four publicly available datasets image clipart image dobject image video <eos> experimental result reveal new lupi paradigm naturally addresses cross dataset learning <eos> <eop> efficient exact pga algorithm constant curvature manifolds <eos> manifold valued datasets widely encountered many computer vision tasks <eos> non linear analog pca algorithm called principal geodesic analysis pga algorithm suited data lying riemannian manifolds was reported literature decade ago <eos> since objective function pga algorithm highly non linear hard solve efficiently general researchers proposed linear approximation <eos> though linear approximation easy compute lacks accuracy especially when data exhibits large variance <eos> recently alternative called exact pga was proposed tries solve optimization without any linearization <eos> general riemannian manifolds though yields better accuracy than original linearized pga data exhibit large variance optimization computationally efficient <eos> paper propose efficient exact pga algorithm constant curvature riemannian manifolds ccm epga <eos> ccm epga algorithm differs significantly existing pga algorithms two aspects distance between given manifold valued data point principal submanifold computed analytically thus no optimization required existing method <eos> ii unlike existing pga algorithms descent into codimension submanifolds require any optimization but accomplished through use rimeannian inverse exponential map parallel transport operations <eos> present theoretical experimental result constant curvature riemannian manifolds depicting favorable performance ccm epga algorithm compared existing pga algorithms <eos> also present data reconstruction principal components reported literature setting <eos> <eop> online learning bayesian classification trees <eos> randomized classification trees among most popular machine learning tools found successful applications many areas <eos> although classifier was originally designed offline learning algorithm there increased interest last years provide online variant <eos> paper propose online learning algorithm classification trees adheres bayesian principles <eos> contrast state art approaches produce large forests complex trees aim constructing small ensembles consisting shallow trees high generalization capabilities <eos> experiments benchmark machine learning body part recognition datasets show superior performance over state art approaches <eos> <eop> cross stitch network multi task learning <eos> multi task learning convolutional network displayed remarkable success field recognition <eos> success largely attributed learning shared representations multiple supervisory tasks <eos> however existing multi task approaches rely enumerating multiple network architectures specific tasks hand generalize <eos> paper propose principled approach learn shared representations convnets using multi task learning <eos> specifically propose new sharing unit cross stitch unit <eos> units combine activations multiple network trained end end <eos> network cross stitch units learn optimal combination shared task specific representations <eos> proposed method generalizes across multiple tasks shows dramatically improved performance over baseline method categories few training examples <eos> <eop> deep metric learning via lifted structured feature embedding <eos> learning distance metric between pairs examples great importance learning visual recognition <eos> remarkable success state art convolutional neural network recent works shown promising result discriminatively training network learn semantic feature embeddings similar examples mapped close each other dissimilar examples mapped farther apart <eos> paper describe algorithm taking full advantage training batches neural network training lifting vector pairwise distances within batch matrix pairwise distances <eos> step enables algorithm learn state art feature embedding optimizing novel structured prediction objective active hard negative mining lifted problem <eos> additionally collected online products dataset image classes online products metric learning <eos> experiments cub cars online products datasets demonstrate significant improvement over existing deep feature embedding method all experimented embedding sizes googlenet network <eos> source code dataset available github <eos> com rksltnl deep metric learning cvpr <eop> fast algorithms convolutional neural network <eos> deep convolutional neural network take gpu days computation train large data set <eos> pedestrian detection self driving cars requires very low latency <eos> image recognition mobile phones constrained limited processing resources <eos> success convolutional neural network situations limited how fast compute them <eos> conventional fft based convolution fast large filters but state art convolutional neural network use small filters <eos> introduce new class fast algorithms convolutional neural network using winograd minimal filtering algorithms <eos> algorithms compute minimal complexity convolution over small tiles makes them fast small filters small batch sizes <eos> benchmark gpu implementation algorithm vgg network show state art throughput batch sizes <eos> <eop> coordinating multiple disparity proposals stereo computation <eos> while great progress made stereo computation over last decades large textureless region remain challenging <eos> segment based method tackle problem properly but their performances sensitive segmentation result <eos> paper alleviate sensitivity generating multiple proposals absolute relative disparities multi segmentations <eos> proposals supply rich descriptions surface structures <eos> especially relative disparity between distant pixels encode large structure critical handle large texture less region <eos> proposals coordinated point wise competition pairwise collaboration within mrf model <eos> during inference dynamic programming performed different directions various step sizes so long range connections better preserved <eos> experiments carefully analyzed effectiveness major components <eos> result middlebury kitti stereo benchmark show method comparable state art <eos> <eop> joint multiview segmentation localization rgb image using depth induced silhouette consistency <eos> paper propose rgb camera localization approach takes effective geometry constraint <eos> silhouette consistency into consideration <eos> unlike existing approaches usually assume silhouettes provided consider more practical scenarios generate silhouettes multiple views fly <eos> obtain set accurate silhouettes precise camera poses required propagate segmentation cues across views <eos> perform better localization accurate silhouettes needed constrain camera poses <eos> therefore two problems intertwined each other require joint treatment <eos> facilitated available depth introduce simple but effective silhouette consistency energy term binds traditional appearance based multiview segmentation cost rgb frame frame matching cost together <eos> binary segmentation masks camera poses naturally fits graph cut minimization framework gauss newton non linear least squares method respectively <eos> experiments show proposed approach achieves state arts performance both tasks image segmentation camera localization <eos> <eop> large dataset train convolutional network disparity optical flow scene flow estimation <eos> recent work shown optical flow estimation formulated supervised learning task successfully solved convolutional network <eos> training so called flownet was enabled large synthetically generated dataset <eos> present paper extends concept optical flow estimation via convolutional network disparity scene flow estimation <eos> end propose three synthetic stereo video datasets sufficient realism variation size successfully train large network <eos> datasets first large scale datasets enable training evaluation scene flow method <eos> besides datasets present convolutional network real time disparity estimation provides state art result <eos> combining flow disparity estimation network training jointly demonstrate first scene flow estimation convolutional network <eos> <eop> dynamic camera relocalization single reference image <eos> dynamic relocalization camera pose single reference image costly challenging task requires delicate hand eye calibration precision positioning platform three dimensional mechanical rotation translation <eos> paper show high quality camera relocalization achieved much less expensive way <eos> based inexpensive platform unreliable absolute repositioning accuracy ara propose hand eye calibration free strategy actively relocate camera into same pose produces input reference image sequentially correcting three dimensional relative rotation translation <eos> theoretically prove strategy both rotational translational relative pose effectively reduced zero bounded unknown hand eye pose displacement <eos> conquer three dimensional rotation translation ambiguity theoretical strategy further revised practical relocalization algorithm faster convergence rate more reliability jointly adjusting three dimensional relative rotation translation <eos> extensive experiments validate effectiveness superior accuracy proposed approach laboratory tests challenging real world applications <eos> <eop> dense monocular depth estimation complex dynamic scenes <eos> present approach dense depth estimation single monocular camera moving through dynamic scene <eos> approach produces dense depth map two consecutive frames <eos> moving object reconstructed along surrounding environment <eos> provide novel motion segmentation algorithm segments optical flow field into set motion models each its own epipolar geometry <eos> then show scene reconstructed based motion models optimizing convex program <eos> optimization jointly reasons about scales different object assembles scene common coordinate frame determined up global scale <eos> experimental result demonstrate presented approach outperforms prior method monocular depth estimation dynamic scenes <eos> <eop> using self contradiction learn confidence measures stereo vision <eos> learned confidence measures gain increasing importance outlier removal quality improvement stereo vision <eos> however acquiring necessary training data typically tedious time consuming task involves manual interaction active sensing devices synthetic scenes <eos> overcome problem propose new flexible scalable way generating training data only requires set stereo image input <eos> key idea approach use different view point reasoning about contradictions consistencies between multiple depth maps generated same stereo algorithm <eos> enables generate huge amount training data fully automated manner <eos> among other experiments demonstrate potential approach boosting performance three learned confidence measures kitti dataset simply training them vast amount automatically generated training data rather than limited amount laser ground truth data <eos> <eop> understanding real world indoor scenes synthetic data <eos> scene understanding prerequisite many high level tasks any automated intelligent machine operating real world environments <eos> recent attempts supervised learning shown promise direction but also highlighted need enormous quantity supervised data performance increases proportion amount data used <eos> however quickly becomes prohibitive when considering manual labour needed collect such data <eos> work focus attention depth based semantic per pixel labelling scene understanding problem show potential computer graphics generate virtually unlimited labelled data synthetic three dimensional scenes <eos> carefully synthesizing training data appropriate noise models show comparable performance state art rgbd systems nyuv dataset despite using only depth data input set benchmark depth based segmentation sun rgb dataset <eos> <eop> stereo matching color monochrome cameras low light conditions <eos> consumer devices stereo cameras become popular because their low cost depth sensing capability <eos> however systems usually suffer low imaging quality inaccurate depth acquisition under low light conditions <eos> address problem present new stereo matching method color monochrome camera pair <eos> focus fundamental trade off monochrome cameras much better light efficiency than color filtered cameras <eos> key ideas involve compensating radiometric difference between two cross spectral image taking full advantage complementary data <eos> consequently method produces both accurate depth map high quality image applicable various depth aware image processing <eos> method evaluated using various datasets performance depth estimation consistently outperforms state art method <eos> <eop> camera calibration dynamic silhouettes using motion barcodes <eos> computing epipolar geometry between cameras very different viewpoints often problematic matching point hard find <eos> cases proposed use information dynamic object scene suggesting point line correspondences <eos> propose speed up about two orders magnitude well increase robustness accuracy method computing epipolar geometry dynamic silhouettes based new temporal signature motion barcode lines <eos> binary temporal sequence lines indicating each frame existence least one foreground pixel line <eos> motion barcodes two corresponding epipolar lines very similar so search corresponding epipolar lines limited lines having similar barcodes leading increased speed accuracy robustness computing epipolar geometry <eos> <eop> structure motion revisited <eos> incremental structure motion prevalent strategy three dimensional reconstruction unordered image collections <eos> while incremental reconstruction systems tremendously advanced all regards robustness accuracy completeness scalability remain key problems towards building truly general purpose pipeline <eos> propose new sfm technique improves upon state art make further step towards ultimate goal <eos> full reconstruction pipeline released public open source implementation <eos> <eop> constructing canonical region fast effective view selection <eos> view selection little work done optimizing search process views must densely distributed checked individually <eos> thus evaluating poor views wastes much time poor view may even misidentified best one <eos> paper propose search strategy identifying region very likely contain best views referred canonical region <eos> decomposing model under investigation into meaningful parts using canonical views parts generate canonical region <eos> applying existing view selection method canonical region only accelerate search process but also guarantee quality obtained views <eos> result when canonical region used searching best views during comprehensive model analysis attain greater search speed reduce number views required <eos> experimental result show effectiveness method <eos> <eop> prior less compressible structure motion <eos> many non rigid three dimensional structures modelled well through low rank subspace assumption <eos> problematic when comes their reconstruction through structure motion sfm <eos> argue paper more expressive general assumption made around compressible three dimensional structures <eos> vision community however hitherto struggled formulate effective strategies recovering such structures after projection without aid additional priors <eos> temporal ordering rigid substructures etc <eos> paper present prior less approach solve compressible sfm <eos> specifically demonstrate how problem sfm assuming compressible three dimensional structures theoretically characterized block sparse dictionary learning problem <eos> validate approach experimentally demonstrating reconstructions three dimensional structures intractable using current state art low rank sfm approaches <eos> <eop> rolling shutter camera relative pose generalized epipolar geometry <eos> vast majority modern consumer grade cameras employ rolling shutter mechanism <eos> dynamic geometric computer vision applications such visual slam so called rolling shutter effect therefore needs properly taken into account <eos> dedicated relative pose solver appears first problem solve eminent importance bootstrap any derivation multi view geometry <eos> however despite its significance received inadequate attention date <eos> paper presents detailed investigation geometry rolling shutter relative pose problem <eos> introduce rolling shutter essential matrix establish its link existing models such push broom cameras summarized clean hierarchy multi perspective cameras <eos> generalization well established concepts epipolar geometry completed definition sampson distance rolling shutter case <eos> work concluded careful investigation introduced epipolar geometry rolling shutter cameras several dedicated benchmarks <eos> <eop> structure motion object <eos> paper shows first time possible reconstruct position rigid object jointly recover affine camera calibration solely set object detections video sequence <eos> practice work considered extension tomasi kanade factorization method using object <eos> instead using point form rank constrained measurement matrix form matrix similar rank properties using object detection proposals <eos> detail first fit ellipse onto image plane each bounding box given object detector <eos> collection all ellipses dual space used create measurement matrix gives specific rank constraint <eos> matrix factorised metrically upgraded order provide affine camera matrices three dimensional position object ellipsoid <eos> moreover recover full three dimensional quadric thus giving additional information about object occupancy three dimensional pose <eos> finally also show point measurements seamlessly included framework reduce number object required <eos> last aspect unifies classical point based tomasi kanade approach object unique framework <eos> experiments synthetic real data show feasibility approach affine camera case <eos> <eop> deephand robust hand pose estimation completing matrix imputed deep feature <eos> propose deephand estimate three dimensional pose hand using depth data commercial three dimensional sensors <eos> discriminatively train convolutional neural network output low dimensional activation feature given depth map <eos> activation feature vector representative global local joint angle parameters hand pose <eos> efficiently identify spatial nearest neighbors activation feature database feature corresponding synthetic depth maps store some temporal neighbors previous frames <eos> matrix completion algorithm uses spatio temporal activation feature corresponding known pose parameter values estimate unknown pose parameters input feature vector <eos> database activation feature supplements large viewpoint coverage hierarchical estimation pose parameters robust occlusions <eos> show approach compares favorably state art method while achieving real time performance fps standard computer <eos> <eop> multi oriented text detection fully convolutional network <eos> paper propose unconventional approach text detection natural image <eos> both global local cues taken into account localizing text lines coarse fine procedure <eos> first fully convolutional network fcn model trained predicting salient map text region holistic manner <eos> then set hypotheses text lines estimated combining salient map mser components <eos> finally another fcn classifier used predicting centroid each character order remove false hypotheses <eos> framework general handling texts multiple orientations languages fonts <eos> proposed method consistently achieves state art performance three text detection benchmarks msra td icdar icdar <eos> <eop> robust scene text recognition automatic rectification <eos> recognizing text natural image challenging task many unsolved problems <eos> different documents words natural image often possess irregular shapes caused perspective distortion curved character placement etc <eos> propose rare robust text recognizer automatic rectification recognition model robust irregular text <eos> rare specially designed deep neural network consists spatial transformer network stn sequence recognition network srn <eos> testing image firstly rectified via predicted thin plate spline tps transformation into more readable image following srn recognizes text through sequence recognition approach <eos> show model able recognize several types irregular text including perspective text curved text <eos> rare end end trainable requiring only image associated text labels making convenient train deploy model practical systems <eos> state art highly competitive performance achieved several benchmarks well demonstrates effectiveness proposed model <eos> <eop> mnemonic descent method recurrent process applied end end face alignment <eos> cascaded regression recently become method choice solving non linear least squares problems such deformable image alignment <eos> given sizeable training set cascaded regression learns set generic rules sequentially applied minimise least squares problem <eos> despite success cascaded regression problems such face alignment head pose estimation there several shortcomings arising strategies proposed thus far <eos> specifically regressors learnt independently descent directions may cancel one another out handcrafted feature <eos> hogs sift etc <eos> mainly used drive cascade may sub optimal task hand <eos> paper propose combined jointly trained convolutional recurrent neural network architecture allows training end end system attempts alleviate aforementioned drawbacks <eos> recurrent module facilitates joint optimisation regressors assuming cascades form nonlinear dynamical system effect fully utilising information between all cascade levels introducing memory unit shares information across all levels <eos> convolutional module allows network extract feature specialised task hand experimentally shown outperform hand crafted feature <eos> show application proposed architecture problem face alignment result strong improvement over current state art <eos> <eop> large pose face alignment via cnn based dense three dimensional model fitting <eos> large pose face alignment very challenging problem computer vision used prerequisite many important vision tasks <eos> face recognition three dimensional face reconstruction <eos> recently there few attempts solve problem but still more research needed achieve highly accurate result <eos> paper propose face alignment method large pose face image combining powerful cascaded cnn regressor method dmm <eos> formulate face alignment dmm fitting problem camera projection matrix three dimensional shape parameters estimated cascade cnn based regressors <eos> dense three dimensional shape allows design pose invariant appearance feature effective cnn learning <eos> extensive experiments conducted challenging databases aflw afw comparison state art <eos> <eop> adaptive three dimensional face reconstruction unconstrained photo collections <eos> given collection wild face image captured under variety unknown pose expression illumination conditions paper presents method reconstructing three dimensional face surface model individual along albedo information <eos> motivated success recent face reconstruction techniques large photo collections extend prior work adapt low quality photo collections fewer image <eos> achieve fitting three dimensional morphable model form personalized template developing novel photometric stereo formulation under coarse fine scheme <eos> superior experimental result reported synthetic real world photo collections <eos> <eop> online detection classification dynamic hand gestures recurrent three dimensional convolutional neural network <eos> automatic detection classification dynamic hand gestures real world systems intended human computer interaction challenging there large diversity how people perform gestures making detection classification difficult system must work online order avoid noticeable lag between performing gesture its classification fact negative lag classification before gesture finished desirable feedback user then truly instantaneous <eos> paper address challenges recurrent three dimensional convolutional neural network performs simultaneous detection classification dynamic hand gestures multi modal data <eos> employ connectionist temporal classification train network predict class labels progress gestures unsegmented input streams <eos> order validate method introduce new challenging multi modal dynamic hand gesture dataset captured depth color stereo ir sensors <eos> challenging dataset gesture recognition system achieves accuracy <eos> outperforms competing state art algorithms approaches human accuracy <eos> moreover method achieves state art performance skig chalearn benchmarks <eos> <eop> kinematic structure correspondences via hypergraph matching <eos> paper present novel framework finding kinematic structure correspondence between two object video via hypergraph matching <eos> contrast prior appearance graph alignment based matching method applied among two similar static image proposed method finds correspondences between two dynamic kinematic structures heterogeneous object video <eos> main contributions summarised follows casting kinematic structure correspondence problem into hypergraph matching problem incorporating multi order similarities normalising weights ii structural topology similarity measure new topology constrained subgraph isomorphism aggregation iii kinematic correlation measure between pairwise nodes iv combinatorial local motion similarity measure using geodesic distance riemannian manifold <eos> demonstrate robustness accuracy method through number experiments complex articulated synthetic real data <eos> <eop> cp mtml coupled projection multi task metric learning large scale face retrieval <eos> propose novel coupled projection multi task met ric learning cp mtml method large scale face re trieval <eos> contrast previous works were limited low dimensional feature small datasets proposed method scales large datasets high dimensional face descriptors <eos> utilises pairwise dis similarity constraints supervision hence require exhaustive class annotation every training image <eos> while traditionally multi task learning method validated same dataset but different tasks work more chal lenging setting heterogeneous datasets different tasks <eos> show empirical validation multiple face im age datasets different facial traits <eos> identity age expression <eos> use classic local binary pattern lbp de scriptors along recent deep convolutional neural network cnn feature <eos> experiments clearly demon strate scalability improved performance pro posed method tasks identity age based face image retrieval compared competitive existing method standard datasets presence million distractor face image <eos> <eop> patchbatch batch augmented loss optical flow <eos> propose new pipeline optical flow computation based deep learning techniques <eos> suggest using siamese cnn independently parallel compute descriptors both image <eos> learned descriptors then compared efficiently using norm require network processing patch pairs <eos> success method based innovative loss function computes higher moments loss distributions each training batch <eos> combined approximate nearest neighbor patch matching method flow interpolation technique state art performance obtained most challenging competitive optical flow benchmarks <eos> <eop> joint recovery dense correspondence cosegmentation two image <eos> propose new technique jointly recover cosegmentation dense per pixel correspondence two image <eos> method parameterizes correspondence field using piecewise similarity transformations recovers mapping between estimated common foreground region two image allowing them precisely aligned <eos> formulation based hierarchical markov random field model segmentation transformation labels <eos> hierarchical structure uses nested image region constrain inference across multiple scales <eos> unlike prior hierarchical method assume structure given proposed iterative technique dynamically recovers structure variable along labeling <eos> joint inference performed energy minimization framework using iterated graph cuts <eos> evaluate method new dataset image pairs manually obtained ground truth outperforms state art method designed specifically either cosegmentation correspondence estimation <eos> <eop> multi view people tracking via hierarchical trajectory composition <eos> paper presents hierarchical composition approach multi view object tracking <eos> key idea adaptively exploit multiple cues both three dimensional <eos> ground occupancy consistency appearance similarity motion coherence etc <eos> mutually complementary while tracking humans interests over time <eos> while feature online selection extensively studied past literature remains unclear how effectively schedule cues tracking purpose especially when encountering various challenges <eos> occlusions conjunctions appearance variations <eos> so propose hierarchical composition model re formulate multi view multi object tracking problem compositional structure optimization <eos> setup set composition criteria each corresponds one particular cue <eos> hierarchical composition process pursued exploiting different criteria impose constraints between graph node its offsprings hierarchy <eos> learn composition criteria using mle annotated data efficiently construct hierarchical graph iterative greedy pursuit algorithm <eos> experiments demonstrate superior performance approach three public datasets one newly created test various challenges multi view multi object tracking <eos> <eop> object tracking via dual linear structured svm explicit feature map <eos> structured support vector machine ssvm based method demonstrated encouraging performance recent object tracking benchmarks <eos> however complex expensive optimization limits their deployment real world applications <eos> paper present simple yet efficient dual linear ssvm dlssvm algorithm enable fast learning execution during tracking <eos> analyzing dual variables propose primal classifier update formula learning step size computed closed form <eos> online learning method significantly improves robustness proposed linear ssvm low computational cost <eos> second approximate intersection kernel feature representations explicit feature map further improve tracking performance <eos> finally extend proposed dlssvm tracker multiscale manner address drift problem <eos> experimental result large benchmark datasets video sequences show proposed dlssvm tracking algorithm achieves state art performance <eos> <eop> robust real time three dimensional tracking multiple object similar appearances <eos> paper proposes novel method tracking multiple moving object recovering their three dimensional three dimensional models separately using multiple calibrated cameras <eos> robustly tracking object similar appearances proposed method uses geometric information regarding three dimensional scene structure rather than appearance <eos> major limitation previous techniques foreground confusion shapes object ghosting artifacts ignored hence appropriately specified foreground region <eos> overcome limitation method classifies foreground voxels into targets object artifacts each frame using novel probabilistic two stage framework <eos> accomplished step wise application track graph describing how targets interact maximum posteriori expectation maximization algorithm estimation target parameters <eos> introduce mixture models semiparametric component distributions regarding three dimensional target shapes <eos> order confuse artifacts object interest automatically detect track artifacts based closed world assumption <eos> experimental result show method outperforms state art trackers seven public sequences while achieving real time performance <eos> <eop> egocentric look video photographer identity <eos> egocentric cameras being worn increasing number users among them many security forces worldwide <eos> gopro cameras already penetrated mass market reporting substantial increase sales every year <eos> head worn cameras capture photographer may seem anonymity photographer preserved even when video publicly distributed <eos> show camera motion computed egocentric video provides unique identity information <eos> photographer reliably recognized few seconds video captured when walking <eos> proposed method achieves more than recognition accuracy cases random success rate only <eos> applications include theft prevention locking camera when worn its lawful owner <eos> searching video sharing services <eos> youtube egocentric video shot specific photographer may also become possible <eos> important message paper photographers should aware sharing egocentric video will compromise their anonymity even when their face visible <eos> <eop> learning multi domain convolutional neural network visual tracking <eos> propose novel visual tracking algorithm based representations discriminatively trained convolutional neural network cnn <eos> algorithm pretrains cnn using large set video tracking ground truths obtain generic target representation <eos> network composed shared layer multiple branches domain specific layer domains correspond individual training sequences each branch responsible binary classification identify target each domain <eos> train each domain network iteratively obtain generic target representations shared layer <eos> when tracking target new sequence construct new network combining shared layer pretrained cnn new binary classification layer updated online <eos> online tracking performed evaluating candidate windows randomly sampled around previous target state <eos> proposed algorithm illustrates outstanding performance existing tracking benchmarks <eos> <eop> hedged deep tracking <eos> recent years several method developed utilize hierarchical feature learned deep convolutional neural network cnn visual tracking <eos> however feature certain cnn layer characterize object interest only one aspect one level performance such trackers trained feature one layer usually last second layer further improved <eos> paper propose novel cnn based tracking framework takes full advantage feature different cnn layer uses adaptive hedge method hedge several cnn trackers into stronger one <eos> extensive experiments benchmark dataset challenging image sequences demonstrate effectiveness proposed algorithm compared several state art trackers <eos> <eop> structural correlation filter robust visual tracking <eos> paper propose novel structural correlation filter scf model robust visual tracking <eos> proposed scf model takes part based tracking strategies into account correlation filter tracker exploits circular shifts all parts their motion modeling preserve target object structure <eos> compared existing correlation filter trackers proposed tracker several advantages due part strategy learned structural correlation filters less sensitive partial occlusion computational efficiency robustness <eos> learned filters able only distinguish parts background traditional correlation filters but also exploit intrinsic relationship among local parts via spatial constraints preserve object structure <eos> learned correlation filters only make most parts share similar motion but also tolerate outlier parts different motion <eos> both qualitative quantitative evaluations challenging benchmark image sequences demonstrate proposed scf tracking algorithm performs favorably against several state art method <eos> <eop> visual tracking using attention modulated disintegration integration <eos> paper present novel attention modulated visual tracking algorithm decomposes object into multiple cognitive units trains multiple elementary trackers order modulate distribution attention according various feature kernel types <eos> integration stage recombines units memorize recognize target object effectively <eos> respect elementary trackers present novel attentional feature based correlation filter atcf focuses distinctive attentional feature <eos> effectiveness proposed algorithm validated through experimental comparison state art method widely used tracking benchmark datasets <eos> <eop> continuous occlusion model road scene understanding <eos> present physically interpretable continuous three dimensional model handling occlusions applications road scene understanding <eos> probabilistically assign each point space object theoretical modeling reflection transmission probabilities corresponding camera ray <eos> modeling unified handling occlusions across variety scenarios such associating structure motion point tracks potentially occluded object modeling object detection scores applications such three dimensional localization <eos> point track association model uniformly handles static dynamic object advantage over motion segmentation approaches traditionally used multibody sfm <eos> detailed experiments kitti dataset show superiority proposed method over both state art motion segmentation baseline heuristically uses detection bounding boxes resolving occlusions <eos> also demonstrate how continuous occlusion model may applied task three dimensional localization road scenes <eos> <eop> virtual worlds proxy multi object tracking analysis <eos> modern computer vision algorithms typically require expensive data acquisition accurate manual labeling <eos> work instead leverage recent progress computer graphics generate fully labeled dynamic photo realistic proxy virtual worlds <eos> propose efficient real virtual world cloning method validate approach building publicly releasing new video dataset called virtual kitti automatically labeled accurate ground truth object detection tracking scene instance segmentation depth optical flow <eos> provide quantitative experimental evidence suggesting modern deep learning algorithms pre trained real data behave similarly real virtual worlds ii pre training virtual data improves performance <eos> gap between real virtual worlds small virtual worlds enable measuring impact various weather imaging conditions recognition performance all other things being equal <eos> show factors may affect drastically otherwise high performing deep models tracking <eos> <eop> uncalibrated photometric stereo stepwise optimization using principal components isotropic brdfs <eos> uncalibrated photometric stereo problem non lambertian surfaces challenging because large number unknowns its ill posed nature stemming unknown reflectance functions <eos> propose model represents various isotropic reflectance functions using principal components items dataset formulate uncalibrated photometric stereo regression problem <eos> then solve stepwise optimization utilizing principal components order their importance <eos> also developed two techniques lead convergence highly accurate reconstruction namely coarse fine approach normal grouping randomized multipoint search <eos> experimental result synthetic data showed method significantly outperformed previous method <eos> also evaluated algorithm terms real image data gave good reconstruction result <eos> <eop> unbiased photometric stereo colored surfaces variational approach <eos> three dimensional shape recovery using photometric stereo ps gained increasing attention computer vision community last three decades due its ability recover thinnest geometric structures <eos> yet reliabiliy ps color image difficult guarantee because existing method usually formulated sequential estimation colored albedos normals depth <eos> hence overall reliability depends each subtask <eos> work propose new formulation color photometric stereo based image ratios makes technique independent albedos <eos> allows unbiased three dimensional reconstruction colored surfaces single step solving system linear pdes using variational approach <eos> <eop> reconstruction transparent object position normal consistency <eos> estimating shape transparent refractive object one few open problems three dimensional reconstruction <eos> under assumption rays refract only twice when traveling through object present first approach simultaneously reconstructing three dimensional positions normals object surface both refraction locations <eos> acquisition setup requires only two cameras one monitor serves light source <eos> after acquiring ray ray correspondences between each camera monitor solve optimization function enforces new position normal consistency constraint <eos> three dimensional positions surface point shall agree normals required refract rays under snell law <eos> experimental result using both synthetic real data demonstrate robustness accuracy proposed approach <eos> <eop> real time depth refinement specular object <eos> introduction consumer rgb scanners set off major boost three dimensional computer vision research <eos> yet precision existing depth scanners accurate enough recover fine details scanned object <eos> while modern shading based depth refinement method proven work well lambertian object they break down presence specularities <eos> present novel shape shading framework addresses issue enhances both diffuse specular object depth profiles <eos> take advantage built monochromatic ir projector ir image rgb scanners present lighting model accounts specular region input image <eos> using model reconstruct depth map real time <eos> both quantitative tests visual evaluations prove proposed method produces state art depth reconstruction result <eos> <eop> recovering transparent shape time flight distortion <eos> paper presents method recovering shape normal transparent object single viewpoint using time flight tof camera <eos> method built upon fact speed light varies refractive index medium therefore depth measurement transparent object tof camera may distorted <eos> show tof distortion refractive light path uniquely determined estimating single parameter <eos> estimate parameter introducing surface normal consistency between one determined light path candidate other computed corresponding shape <eos> proposed method evaluated both simulation real world experiments shows faithful transparent shape recovery <eos> <eop> robust light field depth estimation noisy scene occlusion <eos> light field depth estimation essential part many light field applications <eos> numerous algorithms developed using various light field characteristics <eos> however conventional method fail when handling noisy scene occlusion <eos> remedy problem present light field depth estimation method more robust occlusion less sensitive noise <eos> novel data costs using angular entropy metric adaptive defocus response introduced <eos> integration both data costs improves occlusion noise invariant capability significantly <eos> cost volume filtering graph cut optimization utilized improve accuracy depth map <eos> experimental result confirm proposed method robust achieves high quality depth maps various scenes <eos> proposed method outperforms state art light field depth estimation method qualitative quantitative evaluation <eos> <eop> rotational crossed slit light field <eos> light fields lfs image based representation records radiance along all rays along every direction through every point space <eos> traditionally lfs acquired using grid evenly spaced pinhole cameras translating pinhole camera along grid using robot arm <eos> paper present novel lf sampling scheme exploiting special non centric camera called crossed slit xslit camera <eos> xslit camera acquires rays simultaneously pass through two oblique slits <eos> show instead translating camera pinhole case effectively sample lf rotating individual both slits while keeping camera fixed <eos> leads fixed location lf acquisition scheme <eos> further show through theoretical analysis experiments resulting xslit lfs provide several advantages they provide more dense spatial angular sampling amenable multi view stereo matching volumetric reconstruction synthesize unique refocusing effects <eos> <eop> single image object modeling based brdf surfaces learning <eos> methodology three dimensional surface modeling single image proposed <eos> principal novelty concave specular surface modeling without any externally imposed prior <eos> main idea method use brdfs generated rendered surfaces transfer normal field computed generated sample unknown surface <eos> transferred information adequate blow sculpt segmented image mask bas relief object <eos> object surface further refined basing photo consistency formulation relates error minimization original image modeled object <eos> <eop> nonlinear regression technique manifold valued data applications medical image analysis <eos> regression essential tool statistical analysis data many applications computer vision machine learning medical imaging various disciplines science engineering <eos> linear nonlinear regression vector space setting well studied literature <eos> however generalizations manifold valued data only recently gaining popularity <eos> exception few most existing method regression manifold valued data limited geodesic regression generalization linear regression vector spaces <eos> paper present novel nonlinear kernel based regression method applicable manifold valued data <eos> method applicable cases when independent dependent variables regression model both manifold valued one manifold valued other vector scalar valued <eos> further unlike most method method require any imposed ordering manifold valued data <eos> performance model tested large number real data set acquired alzhiemers movement disorder parkinsons essential tremor patients <eos> present extensive set result along statistical validation comparisons <eos> <eop> raid robust estimation approximate infinite dimensional gaussian application material recognition <eos> infinite dimensional covariance descriptors provide richer more discriminative information than their low dimensional counterparts <eos> paper propose novel image descriptor namely robust approximate infinite dimensional gaussian raid <eos> challenges raid mainly lie two aspects description infinite dimensional gaussian difficult due its non linear riemannian geometric structure infinite dimensional setting hence effective approximation necessary traditional maximum likelihood estimation mle robust high even infinite dimensional covariance matrix gaussian setting <eos> address challenges explicit feature mapping efm first introduced effective approximation infinite dimensional gaussian induced additive kernel function then new regularized mle method based von neumann divergence proposed robust estimation covariance matrix <eos> efm proposed regularized mle allow closed form raid very efficient effective high dimensional feature <eos> extend raid using outputs deep convolutional neural network original feature apply material recognition <eos> approach evaluated five material benchmarks one fine grained benchmark <eos> accuracy uiuc material database much higher than state arts <eos> <eop> empirical evaluation current convolutional architectures ability manage nuisance location scale variability <eos> conduct empirical study test ability convolutional neural network cnn reduce effects nuisance transformations input data such location scale aspect ratio <eos> isolate factors adopting common convolutional architecture either deployed globally image compute class posterior distributions restricted locally compute class conditional distributions given location scale aspect ratios bounding boxes determined proposal heuristics <eos> theory averaging latter should yield inferior performance compared proper marginalization <eos> yet empirical evidence suggests converse leading conclude current level complexity convolutional architectures scale data set used train them cnn very effective marginalizing nuisance variability <eos> also quantify effects context overall classification task its impact performance cnn propose improved sampling techniques heuristic proposal schemes improve end end performance state art levels <eos> test hypothesis classification task using imagenet challenge benchmark wide baseline matching task using oxford fischer datasets <eos> <eop> learning sparse high dimensional filters image filtering dense crfs bilateral neural network <eos> bilateral filters wide spread use due their edge preserving properties <eos> common use case manually choose parametric filter type usually gaussian filter <eos> paper will generalize parametrization particular derive gradient descent algorithm so filter parameters learned data <eos> derivation allows learn high dimensional linear filters operate sparsely populated feature spaces <eos> build permutohedral lattice construction efficient filtering <eos> ability learn more general forms high dimensional filters used several diverse applications <eos> first demonstrate use applications single filter applications desired runtime reasons <eos> further show how algorithm used learn pairwise potentials densely connected conditional random fields apply different image segmentation tasks <eos> finally introduce layer bilateral filters cnn propose bilateral neural network use high dimensional sparse data <eos> view provides new ways encode model structure into network architectures <eos> diverse set experiments empirically validates usage general forms filters <eos> <eop> mixture bilateral projection two dimensional probabilistic principal component analysis <eos> probabilistic principal component analysis ppca built upon global linear mapping insufficient model complex data variation <eos> paper proposes mixture bilateral projection probabilistic principal component analysis model mixb dppca data <eos> multi components mixture model seen soft cluster algorithm capability modeling data complex structures <eos> bayesian inference scheme proposed based variational em expectation maximization approach learning model parameters <eos> experiments some publicly available databases show performance mixb dppca largely improved resulting more accurate reconstruction errors recognition rates than existing pca based algorithms <eos> <eop> rolling rotations recognizing human actions three dimensional skeletal data <eos> recently skeleton based human action recognition receiving significant attention various research communities due availability depth sensors real time depth based three dimensional skeleton estimation algorithms <eos> work use rolling maps recognizing human actions three dimensional skeletal data <eos> rolling map well defined mathematical concept explored much vision community <eos> first represent each skeleton using relative three dimensional rotations between various body parts <eos> since three dimensional rotations members special orthogonal group so skeletal representation becomes point lie group so <eos> so also riemannian manifold <eos> then using representation model human actions curves lie group <eos> since classification curves non euclidean space difficult task unwrap action curves onto lie algebra vector space combining logarithm map rolling maps perform classification lie algebra <eos> experimental result three action datasets show proposed approach performs equally well better when compared state art <eos> <eop> improving robustness deep neural network via stability training <eos> paper address issue output instability deep neural network small perturbations visual input significantly distort feature embeddings output neural network <eos> such instability affects many deep architectures state art performance wide range computer vision tasks <eos> present general stability training method stabilize deep network against small input distortions result various types common image processing such compression rescaling cropping <eos> validate method stabilizing state art inception architecture against types distortions <eos> addition demonstrate stabilized model gives robust state art performance large scale near duplicate detection similar image ranking classification noisy datasets <eos> <eop> logistic boosting regression label distribution learning <eos> label distribution learning ldl general learning framework includes both single label multi label learning its special cases <eos> one main assumptions made traditional ldl algorithms derivation parametric model maximum entropy model <eos> while reasonable assumption without additional information there no particular evidence supporting problem ldl <eos> alternatively using general ldl model family approximate parametric model avoid potential influence specific model <eos> order learn general model family paper uses method called logistic boosting regression logitboost seen additive weighted function regression statistical viewpoint <eos> each step fit individual weighted regression function base learner realize optimization gradually <eos> base learners chosen weighted regression tree vector tree constitute two algorithms named ldlogitboost aoso ldlogitboost paper <eos> experiments facial expression recognition crowd opinion prediction movies apparent age estimation show ldlogitboost aoso ldlogitboost achieve better performance than traditional ldl algorithms well other logitboost algorithms <eos> <eop> efficient temporal sequence comparison classification using gram matrix embeddings riemannian manifold <eos> paper propose new framework compare classify temporal sequences <eos> proposed approach captures underlying dynamics data while avoiding expensive estimation procedures making suitable process large numbers sequences <eos> main idea first embed sequences into riemannian manifold using positive definite regularized gram matrices their hankelets <eos> advantages approach allows using non euclidean similarity functions positive definite matrix manifold capture better underlying geometry than directly comparing sequences their hankel matrices gram matrices inherit desirable properties underlying hankel matrices their rank measure complexity underlying dynamics rank coefficients associated regressive models invariant affine transformations varying initial conditions <eos> benefits approach illustrated extensive experiments three dimensional action recognition using three dimensional joints sequences <eos> spite its simplicity performance approach competitive better than using state art approaches problem <eos> further result hold across variety metrics supporting idea improvement stems embedding itself rather than using one metrics <eos> <eop> deep reflectance maps <eos> undoing image formation process therefore decomposing appearance into its intrinsic properties challenging task due under constraint nature inverse problem <eos> while significant progress made inferring shape materials illumination image only progress unconstrained setting still limited <eos> propose fully convolutional neural architecture estimate reflectance maps specular materials natural lighting conditions <eos> achieve end end learning formulation directly predicts reflectance map image itself <eos> show how improve estimates facilitating additional supervision indirect scheme first predicts surface orientation afterwards predicts reflectance map learning based sparse data interpolation <eos> order analyze performance difficult task propose new challenge specular materials shapes complex illumination smashing using both synthetic real image <eos> furthermore show application method range image based editing tasks real image <eos> <eop> semantic filtering <eos> edge preserving image operations aim smoothing image without blurring edges <eos> many excellent edge preserving filtering techniques proposed recently reduce computational complexity separate different scale structures <eos> they normally adopt user selected scale measurement control detail texture smoothing <eos> however natural photos contain object different sizes cannot described single scale measurement <eos> other hand edge contour detection analysis closely related edge preserving filtering achieved significant progress recently <eos> nevertheless most state art filtering techniques ignore success area <eos> inspired fact learning based edge detectors classifiers significantly outperform traditional manually designed detectors paper proposes learning based edge preserving filtering technique <eos> synergistically combines efficiency recursive filter effectiveness recent edge detector scale aware edge preserving filtering <eos> unlike previous filtering method propose filter efficiently extract subjectively meaningful structures natural scenes containing multiple scale object <eos> <eop> uav sensor fusion latent dynamic conditional random fields coronal plane estimation <eos> present real time body orientation estimation micro unmanned air vehicle video stream <eos> work part fully autonomous uav system maneuver face single individual challenging outdoor environments <eos> body orientation estimation consists following steps obtaining set visual appearance models each body orientation each model tagged set scene information obtained sensors exploiting mutual information board sensors using latent dynamic conditional random fields ldcrf characterizing each visual appearance model most discriminative sensor information fast estimation body orientation during test flights given ldcrf parameters corresponding sensor readings <eos> key aspects approach add sparsity sensor readings latent variables followed long range dependency analysis <eos> experimental result obtained over real time video streams demonstrate significant improvement both speed fps accuracy compared state art techniques only rely visual data <eos> video demonstration autonomous flights both ground view aerial view included supplementary material <eos> <eop> robust visual place recognition graph kernels <eos> novel method visual place recognition introduced evaluated demonstrating robustness perceptual aliasing observation noise <eos> achieved increasing discrimination through more structured representation visual observations <eos> estimation observation likelihoods based graph kernel formulations utilizing both structural visual information encoded covisibility graphs <eos> proposed probabilistic model able circumvent typically difficult expensive posterior normalization procedure exploiting information available visual observations <eos> furthermore place recognition complexity independent size map <eos> result show improvements over state art diverse set both public datasets novel experiments highlighting benefit approach <eos> <eop> semantic image segmentation task specific edge detection using cnn discriminatively trained domain transform <eos> deep convolutional neural network cnn backbone state art semantic image segmentation systems <eos> recent work shown complementing cnn fully connected conditional random fields crfs significantly enhance their object localization accuracy yet dense crf inference computationally expensive <eos> propose replacing fully connected crf domain transform dt modern edge preserving filtering method amount smoothing controlled reference edge map <eos> domain transform filtering several times faster than dense crf inference show yields comparable semantic segmentation result accurately capturing object boundaries <eos> importantly formulation allows learning reference edge map intermediate cnn feature instead using image gradient magnitude standard dt filtering <eos> produces task specific edges end end trainable system optimizing target semantic segmentation quality <eos> <eop> natural language object retrieval <eos> paper address task natural language object retrieval localize target object within given image based natural language query object <eos> natural language object retrieval differs text based image retrieval task involves spatial information about object within scene global scene context <eos> address issue propose novel spatial context recurrent convnet scrc model scoring function candidate boxes object retrieval integrating spatial configurations global scene level contextual information into network <eos> model processes query text local image descriptors spatial configurations global context feature through recurrent network outputs probability query text conditioned each candidate box score box transfer visual linguistic knowledge image captioning domain task <eos> experimental result demonstrate method effectively utilizes both local global information outperforming previous baseline method significantly different datasets scenarios exploit large scale vision language datasets knowledge transfer <eos> <eop> densecap fully convolutional localization network dense captioning <eos> introduce dense captioning task requires computer vision system both localize describe salient region image natural language <eos> dense captioning task generalizes object detection when descriptions consist single word image captioning when one predicted region covers full image <eos> address localization description task jointly propose fully convolutional localization network fcln architecture processes image single efficient forward pass requires no external region proposals trained end end single round optimization <eos> architecture composed convolutional network novel dense localization layer recurrent neural network language model generates label sequences <eos> evaluate network visual genome dataset comprises image region grounded captions <eos> observe both speed accuracy improvements over baselines based current state art approaches both generation retrieval settings <eos> <eop> unsupervised learning narrated instruction video <eos> address problem automatically learning main steps complete certain task such changing car tire set narrated instruction video <eos> contributions paper three fold <eos> first develop new unsupervised learning approach takes advantage complementary nature input video associated narration <eos> method solves two clustering problems one text one video applied one after each other linked joint constraints obtain single coherent sequence steps both modalities <eos> second collect annotate new challenging dataset real world instruction video internet <eos> dataset contains about frames five different tasks include complex interactions between people object captured variety indoor outdoor settings <eos> third experimentally demonstrate proposed method automatically discover unsupervised manner main steps achieve task locate steps input video <eos> <eop> video paragraph captioning using hierarchical recurrent neural network <eos> present approach exploits hierarchical recurrent neural network rnns tackle video captioning problem <eos> generating one multiple sentences describe realistic video <eos> hierarchical framework contains sentence generator paragraph generator <eos> sentence generator produces one simple short sentence describes specific short video interval <eos> exploits both temporal spatial attention mechanisms selectively focus visual elements during generation <eos> paragraph generator captures inter sentence dependency taking input sentential embedding produced sentence generator combining paragraph history outputting new initial state sentence generator <eos> evaluate approach two large scale benchmark datasets youtubeclips tacos multilevel <eos> experiments demonstrate approach significantly outperforms current state art method bleu scores <eos> <eop> jointly modeling embedding translation bridge video language <eos> automatically describing video content natural language fundamental challenge computer vision <eos> recurrent neural network rnns models sequence dynamics attracted increasing attention visual interpretation <eos> however most existing approaches generate word locally given previous words visual content while relationship between sentence semantics visual content holistically exploited <eos> result generated sentences may contextually correct but semantics <eos> subjects verbs object true <eos> paper presents novel unified framework named long short term memory visual semantic embedding lstm simultaneously explore learning lstm visual semantic embedding <eos> former aims locally maximize probability generating next word given previous words visual content while latter create visual semantic embedding space enforcing relationship between semantics entire sentence visual content <eos> experiments youtube text dataset show proposed lstm achieves date best published performance generating natural sentences <eos> terms bleu meteor respectively <eos> superior performances also reported two movie description datasets vad mpii md <eos> addition demonstrate lstm outperforms several state art techniques predicting subject verb object svo triplets <eos> <eop> humor beings understanding predicting visual humor <eos> humor integral part human lives <eos> despite being tremendously impactful perhaps surprising detailed understanding humor yet <eos> interactions between humans ai systems increase imperative systems taught understand subtleties human expressions such humor <eos> work interested question content scene causes funny first step towards understanding visual humor analyze humor manifested abstract scenes design computational models them <eos> collect two datasets abstract scenes facilitate study humor both scene level object level <eos> analyze funny scenes explore different types humor depicted them via human studies <eos> model two tasks believe demonstrate understanding some aspects visual humor <eos> tasks involve predicting funniness scene altering funniness scene <eos> show models perform well quantitatively qualitatively through human studies <eos> datasets publicly available <eos> <eop> look focus region visual question answering <eos> present method learns answer visual questions selecting image region relevant text based query <eos> method maps textual queries visual feature various region into shared space they compared relevance inner product <eos> method exhibits significant improvements answering questions such color necessary evaluate specific location room selectively identifies informative image region <eos> model tested recently released vqa dataset feature free form human annotated questions answers <eos> <eop> ask me anything free form visual question answering based knowledge external sources <eos> propose method visual question answering combines internal representation content image information extracted general knowledge base answer broad range image based questions <eos> allows more complex questions answered using predominant neural network based approach than previously possible <eos> particularly allows questions asked about contents image even when image itself contain whole answer <eos> method constructs textual representation semantic content image merges textual information sourced knowledge base develop deeper understanding scene viewed <eos> priming recurrent neural network combined information submitted question leads very flexible visual question answering approach <eos> specifically able answer questions posed natural language refer information contained image <eos> demonstrate effectiveness model two publicly available datasets toronto coco qa vqa show produces best reported result both cases <eos> <eop> movieqa understanding stories movies through question answering <eos> introduce movieqa dataset aims evaluate automatic story comprehension both video text <eos> dataset consists questions about movies high semantic diversity <eos> questions range simpler who did whom why how certain events occurred <eos> each question comes set five possible answers correct one four deceiving answers provided human annotators <eos> dataset unique contains multiple sources information video clips plots subtitles scripts dvs <eos> analyze data through various statistics method <eos> further extend existing qa techniques show question answering such open ended semantics hard <eos> make data set public along evaluation benchmark encourage inspiring work challenging domain <eos> <eop> tgif new dataset benchmark animated gif description <eos> recent popularity animated gifs social media there need ways index them rich metadata <eos> advance research animated gif understanding collected new dataset tumblr gif tgif animated gifs tumblr natural language descriptions obtained via crowdsourcing <eos> motivation work develop testbed image sequence description systems task generate natural language descriptions animated gifs video clips <eos> ensure high quality dataset developed series novel quality controls validate free form text input crowdworkers <eos> show there unambiguous association between visual content natural language descriptions dataset making ideal benchmark visual content captioning task <eos> perform extensive statistical analyses compare dataset existing image video description datasets <eos> next provide baseline result animated gif description task using three representative techniques nearest neighbor statistical machine translation recurrent neural network <eos> finally show models fine tuned animated gif description dataset helpful automatic movie description <eos> <eop> image captioning semantic attention <eos> automatically generating natural language description image attracted interests recently both because its importance practical applications because connects two major artificial intelligence fields computer vision natural language processing <eos> existing approaches either top down start gist image convert into words bottom up come up words describing various aspects image then combine them <eos> paper propose new algorithm combines both approaches through model semantic attention <eos> algorithm learns selectively attend semantic concept proposals fuse them into hidden states outputs recurrent neural network <eos> selection fusion form feedback connecting top down bottom up computation <eos> evaluate algorithm two public benchmarks microsoft coco flickr <eos> experimental result show algorithm significantly outperforms state art approaches consistently across different evaluation metrics <eos> <eop> temporally coherent reconstruction complex dynamic scenes <eos> paper presents approach reconstruction temporally coherent models complex dynamic scenes <eos> no prior knowledge required scene structure camera calibration allowing reconstruction multiple moving cameras <eos> sparse dense temporal correspondence integrated joint multi view segmentation reconstruction obtain complete representation static dynamic object <eos> temporal coherence exploited overcome visual ambiguities resulting improved reconstruction complex scenes <eos> robust joint segmentation reconstruction dynamic object achieved introducing geodesic star convexity constraint <eos> comparative evaluation performed variety unstructured indoor outdoor dynamic scenes hand held cameras multiple people <eos> demonstrates reconstruction complete temporally coherent scene models improved non rigid object segmentation shape reconstruction <eos> <eop> consensus non rigid reconstructions <eos> recently there many progresses problem non rigid structure reconstruction based trajectories but still challenging deal complex deformations restricted view ranges <eos> promising alternatives piecewise reconstruction approaches divide trajectories into several local parts stitch their individual reconstructions produce entire three dimensional structure <eos> method show state art performance however most them specialized relatively smooth surfaces some quite complicated <eos> meanwhile reported numerously field pattern recognition obtaining consensus many weak hypotheses give strong powerful result <eos> inspired reports paper push concept part based reconstruction limit instead considering parts explicitly divided local patches draw large number small random trajectory set <eos> their individual reconstructions pull out statistic each three dimensional point retrieve strong reconstruction procedure expressed sparse norm minimization problem <eos> order resolve reflection ambiguity between weak possibly bad reconstructions propose novel optimization framework only involves single eigenvalue decomposition <eos> proposed method applied any type data outperforms existing method benchmark sequences even though composed few simple steps <eos> furthermore easily parallelizable another advantage <eos> <eop> isometric non rigid shape motion linear time <eos> study isometric non rigid shape motion iso nrsfm given multiple intrinsically calibrated monocular image want reconstruct time varying three dimensional shape object undergoing isometric deformations <eos> show iso nrsfm solvable warps inter image geometric transformations <eos> propose new theoretical framework based riemmanian manifolds represent unknown three dimensional surfaces embeddings camera retinal planes <eos> allows use manifolds metric tensor christoffel symbol fields prove related across image simple rules depending only warps <eos> forms set important theoretical result <eos> using infinitesimal planarity formulation then allows derive system two quartics two variables each image pair <eos> sum squares polynomials independent number image solved globally forming well posed problem image whose solution directly leads surface normal field <eos> proposed method outperforms existing work terms accuracy computation cost synthetic real datasets <eos> <eop> learning online smooth predictors realtime camera planning using recurrent decision trees <eos> study problem online prediction realtime camera planning goal predict smooth trajectories correctly track frame object interest <eos> players basketball game <eos> conventional approach training predictors directly consider temporal consistency often produces undesirable jitter <eos> although post hoc smoothing <eos> via kalman filter mitigate issue some degree ideal due overly stringent modeling assumptions <eos> propose recurrent decision tree framework directly incorporate temporal consistency into data driven predictor well learning algorithm efficiently learn such temporally smooth models <eos> approach require any post processing making online smooth predictions much easier generate when noise model unknown <eos> apply approach sports broadcasting given noisy player detections learn camera should look based human demonstrations <eos> experiments exhibit significant improvements over conventional baselines showcase practicality approach <eos> <eop> egocentric future localization <eos> presents method future localization predict plausible future trajectories ego motion egocentric stereo image <eos> paths avoid obstacles move between object even turn around corner into space behind object <eos> byproduct predicted trajectories discover empty space occluded foreground object <eos> one key innovation creation egoretinal map akin illustrated tourist map rearranges pixels taking into accounts depth information ground plane body motion direction so allows motion planning perception object one image space <eos> learn plan trajectories directly egoretinal map using first person experience walking around variety scenes <eos> testing phase given novel scene find multiple hypotheses future trajectories learned experience <eos> refine them minimizing cost function describes compatibility between obstacles egoretinal map trajectories <eos> quantitatively evaluate method show predictive validity apply various real world daily activities including walking shopping social interactions <eos> <eop> full flow optical flow estimation global optimization over regular grids <eos> present global optimization approach optical flow estimation <eos> approach optimizes classical optical flow objective over full space mappings between discrete grids <eos> no descriptor matching used <eos> highly regular structure space mappings enables optimizations reduce computational complexity algorithm inner loop quadratic linear support efficient matching tens thousands nodes tens thousands displacements <eos> show one shot global optimization classical horn schunck type objective over regular grids single resolution sufficient initialize continuous interpolation achieve state art performance challenging modern benchmarks <eos> <eop> structured feature learning pose estimation <eos> paper propose structured feature learning framework reason correlation among body joints feature level human pose estimation <eos> different existing approaches modeling structures score maps predicted labels feature maps preserve substantially richer descriptions body joints <eos> relationships between feature maps joints captured introduced geometrical transform kernels easily implemented convolution layer <eos> feature their relationships jointly learned end end learning system <eos> bi directional tree structured model proposed so feature channels body joint well receive information other joints <eos> proposed framework improves feature learning substantially <eos> very simple post processing reaches best mean pcp lsp flic datasets <eos> compared baseline learning feature each joint separately convnet mean pcp improved flic <eos> code released public <eos> <eop> convolutional pose machines <eos> pose machines provide sequential prediction framework learning rich implicit spatial models <eos> work show systematic design how convolutional network incorporated into pose machine framework learning image feature image dependent spatial models task pose estimation <eos> contribution paper implicitly model long range dependencies between variables structured prediction tasks such articulated pose estimation <eos> achieve designing sequential architecture composed convolutional network directly operate belief maps previous stages producing increasingly refined estimates part locations without need explicit graphical model style inference <eos> approach addresses characteristic difficulty vanishing gradients during training providing natural learning objective function enforces intermediate supervision thereby replenishing back propagated gradients conditioning learning procedure <eos> demonstrate state art performance outperform competing method standard benchmarks including mpii lsp flic datasets <eos> <eop> human pose estimation iterative error feedback <eos> hierarchical feature extractors such convolutional network convnets achieved impressive performance variety classification tasks using purely feedforward processing <eos> feedforward architectures learn rich representations input space but explicitly model dependencies output spaces quite structured tasks such articulated human pose estimation object segmentation <eos> here propose framework expands expressive power hierarchical feature extractors encompass both input output spaces introducing top down feedback <eos> instead directly predicting outputs one go use self correcting model progressively changes initial solution feeding back error predictions process call iterative error feedback ief <eos> ief shows excellent performance task articulated pose estimation challenging mpii lsp benchmarks matching state art without requiring ground truth scale annotation <eos> <eop> weldon weakly supervised learning deep convolutional neural network <eos> paper introduce novel framework weakly supervised learning deep convolutional neural network weldon <eos> method dedicated automatically selecting relevant image region weak annotations <eos> global image labels encompasses following contributions <eos> firstly weldon leverages recent improvements multiple instance learning paradigm <eos> negative evidence scoring top instance selection <eos> secondly deep cnn trained optimize average precision fine tuned target dataset efficient computations due convolutional feature sharing <eos> thorough experimental validation shows weldon outperforms state art result six different datasets <eos> <eop> disturblabel regularizing cnn loss layer <eos> during long period time combating over fitting cnn training process model regularization including weight decay model averaging data augmentation etc <eos> paper present disturblabel extremely simple algorithm randomly replaces part labels incorrect values each iteration <eos> although seems weird intentionally generate incorrect training labels show disturblabel prevents network training over fitting implicitly averaging over exponentially many network trained different label set <eos> best knowledge disturblabel serves first work adds noises loss layer <eos> meanwhile disturblabel cooperates well dropout provide complementary regularization functions <eos> experiments demonstrate competitive recognition result several popular image recognition datasets <eos> <eop> gradual dropin layer train very deep neural network <eos> introduce concept dynamically growing neural network during training <eos> particular untrainable deep network starts trainable shallow network newly added layer slowly organically added during training thereby increasing network depth <eos> accomplished new layer call dropin <eos> dropin layer starts passing output previous layer effectively skipping over newly added layer then increasingly including units new layer both feedforward backpropagation <eos> show deep network untrainable conventional method will converge dropin layer interspersed architecture <eos> addition demonstrate dropin provides regularization during training analogous way dropout <eos> experiments described mnist dataset various expanded lenet architectures cifar dataset its architecture expanded layer imagenet dataset alexnet architecture expanded layer vgg layer architecture <eos> <eop> structure inference machines recurrent neural network analyzing relations group activity recognition <eos> rich semantic relations important variety visual recognition problems <eos> concrete example group activity recognition involves interactions relative spatial relations set people scene <eos> state art recognition method center deep learning approaches training highly effective complex classifiers interpreting image <eos> however bridging relatively low level concepts output method interpret higher level compositional scenes remains challenge <eos> graphical models standard tool task <eos> paper propose method integrate graphical models deep neural network into joint framework <eos> instead using traditional inference method use sequential inference modeled recurrent neural network <eos> beyond appropriate structure inference learned imposing gates edges between nodes <eos> empirical result group activity recognition demonstrate potential model handle highly structured learning tasks <eos> <eop> deep simnets <eos> present deep layered architecture generalizes convolutional neural network convnets <eos> architecture called simnets driven two operators similarity function generalizes inner product ii log mean exp function called mex generalizes maximum average <eos> two operators applied succession give rise standard neuron but feature space <eos> feature spaces realized simnets depend choice similarity operator <eos> simplest setting corresponds convolution realizes feature space exponential kernel while other settings realize feature spaces more powerful kernels generalized gaussian includes special cases rbf laplacian even dynamically learned feature spaces generalized multiple kernel learning <eos> result simnet contains higher abstraction level compared traditional convnet <eos> argue enhanced expressiveness important when network small due run time constraints such imposed mobile applications <eos> empirical evaluation validates superior expressiveness simnets showing significant gain accuracy over convnets when computational resources run time limited <eos> also show large scale settings computational complexity less concern additional capacity simnets controlled proper regularization yielding accuracies comparable state art convnets <eos> <eop> studying very low resolution recognition using deep network <eos> visual recognition research often assumes sufficient resolution region interest roi <eos> usually violated practice inspiring explore very low resolution recognition vlrr problem <eos> typically roi vlrr problem smaller than pixels challenging recognized even human experts <eos> attempt solve vlrr problem using deep learning method <eos> taking advantage techniques primarily super resolution domain adaptation robust regression formulate dedicated deep learning method demonstrate how techniques incorporated step step <eos> any extra complexity when introduced fully justified both analysis simulation result <eos> resulting robust partially coupled network achieves feature enhancement recognition simultaneously <eos> allows both flexibility combat lr hr domain mismatch robustness outliers <eos> finally effectiveness proposed models evaluated three different vlrr tasks including face identification digit recognition font recognition all obtain very impressive performances <eos> <eop> deep gaussian conditional random field network model based deep network discriminative denoising <eos> propose novel end end trainable deep network architecture image denoising based gaussian conditional random field gcrf model <eos> contrast existing discriminative denoising method train separate model each individual noise level proposed deep network explicitly models input noise variance hence capable handling range noise levels <eos> deep network refer deep gcrf network consists two sub network parameter generation network generates pairwise potential parameters based noisy input image ii inference network whose layer perform computations involved iterative gcrf inference procedure <eos> train two deep gcrf network each network operates over range noise levels one low input noise levels one high input noise levels discriminatively maximizing peak signal noise ratio measure <eos> experiments berkeley segmentation pascalvoc datasets show proposed approach produces result par state art without training separate network each individual noise level <eos> <eop> event specific image importance <eos> when creating photo album event people typically select few important image keep share <eos> there some consistency process choosing important image discarding unimportant ones <eos> modeling selection process will assist automatic photo selection album summarization <eos> paper show selection important image consistent among different viewers selection process related event type album <eos> introduce concept event specific image importance <eos> collected new event album dataset human annotation relative image importance each event album <eos> also propose convolutional neural network cnn based method predict image importance score given event album using novel rank loss function progressive training scheme <eos> result demonstrate method significantly outperforms various baseline method <eos> <eop> quantized convolutional neural network mobile devices <eos> recently convolutional neural network cnn demonstrated impressive performance various computer vision tasks <eos> however high performance hardware typically indispensable application cnn models due high computation complexity prohibits their further extensions <eos> paper propose efficient framework namely quantized cnn simultaneously speed up computation reduce storage memory overhead cnn models <eos> both filter kernels convolutional layer weighting matrices fully connected layer quantized aiming minimizing estimation error each layer response <eos> extensive experiments ilsvrc benchmark demonstrate speed up compression merely one percentage loss classification accuracy <eos> quantized cnn model even mobile devices accurately classify image within one second <eos> <eop> inverting visual representations convolutional network <eos> feature representations both hand designed learned ones often hard analyze interpret even when they extracted visual data <eos> propose new approach study image representations inverting them up convolutional neural network <eos> apply method shallow representations hog sift lbp well deep network <eos> shallow representations approach provides significantly better reconstructions than existing method revealing there surprisingly rich information contained feature <eos> inverting deep network trained imagenet provides several insights into properties feature representation learned network <eos> most strikingly colors rough contours image reconstructed activations higher network layer even predicted class probabilities <eos> <eop> pose aware face recognition wild <eos> propose method push frontiers unconstrained face recognition wild focusing problem extreme pose variations <eos> opposed current techniques either expect single model learn pose invariance through massive amounts training data normalize image single frontal pose method explicitly tackles pose variation using multiple pose specific models rendered face image <eos> leverage deep convolutional neural network cnn learn discriminative representations call pose aware models pams using image casia webface dataset <eos> present comparative evaluation new iarpa janus benchmark ijb pipa datasets <eos> datasets pams achieve remarkably better performance than commercial products surprisingly also outperform method specifically fine tuned target dataset <eos> <eop> multi view deep network cross view classification <eos> cross view recognition intends classify sample between different views important problem computer vision <eos> large discrepancy between different even heterogenous views make problem quite challenging <eos> eliminate complex maybe even highly nonlinear view discrepancy favorable cross view recognition propose multi view deep network mvdn seeks non linear discriminant view invariant representation shared between multiple views <eos> specifically proposed mvdn network consists two sub network view specific sub network attempting remove view specific variations following common sub network attempting obtain common representation shared all views <eos> objective mvdn network fisher loss <eos> rayleigh quotient objective calculated sample all views so guide learning whole network <eos> result representation topmost layer mvdn network robust view discrepancy also discriminative <eos> experiments face recognition across pose face recognition across feature type three datasets views respectively demonstrate superiority proposed method especially compared typical linear ones <eos> <eop> sparsifying neural network connections face recognition <eos> paper proposes learn high performance deep convnets sparse neural connections referred sparse convnets face recognition <eos> sparse convnets learned iterative way each time one additional layer sparsified entire model re trained given initial weights learned previous iterations <eos> one important finding directly training sparse convnet scratch failed find good solutions face recognition while using previously learned denser model properly initialize sparser model critical continue learning effective feature face recognition <eos> paper also proposes new neural correlation based weight selection criterion empirically verifies its effectiveness selecting informative connections previously learned models each iteration <eos> when taking moderately sparse structure weights dense model proposed sparse convnet model significantly improves face recognition performance previous state art deepid models given same training data while keeps performance baseline model only original parameters <eos> <eop> pairwise linear regression classification image set retrieval <eos> paper proposes pairwise linear regression classification plrc image set retrieval <eos> plrc first define new concept unrelated subspace introduce two strategies constitute unrelated subspace <eos> order increase information maximizing query set unrelated image set introduce combination metric two new classifiers based two constitution strategies unrelated subspace <eos> extensive experiments six well known databases prove performance plrc better than dlrc several state art classifiers different vision recognition tasks cluster based face recognition video based face recognition object recognition action recognition <eos> <eop> megaface benchmark million faces recognition scale <eos> recent face recognition experiments major benchmark lfw show stunning performance number algorithms achieve near perfect score surpassing human recognition rates <eos> paper advocate evaluations million scale lfw includes only photos people <eos> end assembled megaface dataset created first megaface challenge <eos> dataset includes one million photos capture more than different individuals <eos> challenge evaluates performance algorithms increasing numbers distractors going gallery set <eos> present both identification verification performance evaluate performance respect pose person age compare function training data size photos people <eos> report result state art baseline algorithms <eos> megaface dataset baseline code evaluation scripts all publicly released further experimentations megaface <eos> <eop> learnt quasi transitive similarity retrieval large collections faces <eos> interested identity based retrieval face set large unlabelled collections acquired uncontrolled environments <eos> given baseline algorithm measuring similarity two face set meta algorithm introduced paper seeks leverage structure data corpus make best use available baseline <eos> particular show how partial transitivity inter personal similarity exploited improve retrieval particularly challenging set poorly match query under baseline measure <eos> describe use proxy set means computing similarity between two set ii introduce transitivity meta feature based similarity salient modes appearance variation between set iii show how quasi transitivity learnt such feature without any labelling manual intervention iv demonstrate effectiveness proposed methodology through experiments notoriously challenging youtube database <eos> <eop> latent factor guided convolutional neural network age invariant face recognition <eos> while considerable progresses made face recognition age invariant face recognition aifr still remains major challenge real world applications face recognition systems <eos> major difficulty aifr arises fact facial appearance subject significant intra personal changes caused aging process over time <eos> order address problem propose novel deep face recognition framework learn age invariant deep face feature through carefully designed cnn model <eos> best knowledge first attempt show effectiveness deep cnn advancing state art aifr <eos> extensive experiments conducted several public domain face aging datasets morph album fgnet cacd vs demonstrate effectiveness proposed model over state art <eos> also verify excellent generalization new model famous lfw dataset <eos> <eop> copula ordinal regression joint estimation facial action unit intensity <eos> joint modeling intensity facial action units aus face image challenging due large number aus their intensity levels <eos> part due lack suitable models efficiently handle such large number outputs classes simultaneously but also due lack target data <eos> reason majority method proposed resort independent classifiers au intensity <eos> suboptimal least two reasons facial appearance some aus changes depending intensity other aus some aus co occur more often than others <eos> encoding expected improve estimation target au intensities especially case noisy image feature head pose variations imbalanced training data <eos> end introduce novel modeling framework copula ordinal regression cor leverages power copula functions crfs detangle probabilistic modeling au dependencies marginal modeling au intensity <eos> consequently cor model achieves joint learning inference intensities multiple aus while being computationally tractable <eos> show two challenging datasets naturalistic facial expressions proposed approach consistently outperforms independent modeling au intensities ii state art approach target task <eos> <eop> robust multilinear model learning framework three dimensional faces <eos> multilinear models widely used represent statistical variations three dimensional human faces they decouple shape changes due identity expression <eos> existing method learn multilinear face model degrade if every person captured every expression if face scans noisy partially occluded if expressions erroneously labeled if vertex correspondence inaccurate <eos> limitations impose requirements training data disqualify large amounts available three dimensional face data being usable learn multilinear model <eos> overcome introduce first framework robustly learn multilinear model three dimensional face databases missing data corrupt data wrong semantic correspondence inaccurate vertex correspondence <eos> achieve robustness erroneous training data framework jointly learns multilinear model fixes data <eos> evaluate framework two publicly available three dimensional face databases show framework achieves data completion accuracy comparable state art tensor completion method <eos> method reconstructs corrupt data more accurately than state art method improves quality learned model significantly erroneously labeled expressions <eos> <eop> ordinal regression multiple output cnn age estimation <eos> address non stationary property aging patterns age estimation cast ordinal regression problem <eos> however processes extracting feature learning regression model often separated optimized independently previous work <eos> paper propose end end learning approach address ordinal regression problems using deep convolutional neural network could simultaneously conduct feature learning regression modeling <eos> particular ordinal regression problem transformed into series binary classification sub problems <eos> propose multiple output cnn learning algorithm collectively solve classification sub problems so correlation between tasks could explored <eos> addition publish asian face age dataset afad containing more than facial image precise age ground truths largest public age dataset date <eos> best knowledge first work address ordinal regression problems using cnn achieves state art performance both morph afad datasets <eos> <eop> deepcut joint subset partition labeling multi person pose estimation <eos> paper considers task articulated human pose estimation multiple people real world image <eos> propose approach jointly solves tasks detection pose estimation infers number persons scene identifies occluded body parts disambiguates body parts between people close proximity each other <eos> joint formulation contrast previous strategies address problem first detecting people subsequently estimating their body pose <eos> propose partitioning labeling formulation set body part hypotheses generated cnn based part detectors <eos> formulation instance integer linear program implicitly performs non maximum suppression set part candidates groups them form configurations body parts respecting geometric appearance constraints <eos> experiments four different datasets demonstrate state art result both single person multi person pose estimation <eos> <eop> thin slicing pose learning understand pose without explicit pose estimation <eos> address problem learning pose aware compact embedding projects image similar human poses placed close embedding space <eos> embedding function built deep convolutional network trained triplet based rank constraints real image data <eos> architecture allows learn robust representation captures differences human poses effectively factoring out variations clothing background imaging conditions wild <eos> variety pose related tasks proposed pose embedding provides cost efficient natural alternative explicit pose estimation circumventing challenges localizing body joints <eos> demonstrate efficacy embedding pose based image retrieval action recognition problems <eos> <eop> dual source approach three dimensional pose estimation single image <eos> one major challenge three dimensional pose estimation single rgb image acquisition sufficient training data <eos> particular collecting large amounts training data contain unconstrained image annotated accurate three dimensional poses infeasible <eos> therefore propose use two independent training sources <eos> first source consists image annotated poses second source consists accurate three dimensional motion capture data <eos> integrate both sources propose dual source approach combines pose estimation efficient robust three dimensional pose retrieval <eos> experiments show approach achieves state art result even competitive when skeleton structure two sources differ substantially <eos> <eop> efficiently creating three dimensional training data fine hand pose estimation <eos> while many recent hand pose estimation method critically rely training set labelled frames creation such dataset challenging task overlooked so far <eos> result existing datasets limited few sequences individuals limited accuracy prevents method delivering their full potential <eos> propose semi automated method efficiently accurately labeling each frame hand depth video corresponding three dimensional locations joints user asked provide only estimate reprojections visible joints some reference frames automatically selected minimize labeling work efficiently optimizing sub modular loss function <eos> then exploit spatial temporal appearance constraints retrieve full three dimensional poses hand over complete sequence <eos> show data used train recent state art hand pose estimation method leading increased accuracy <eos> <eop> sparseness meets deepness three dimensional human pose estimation monocular video <eos> paper addresses challenge three dimensional full body human pose estimation monocular image sequence <eos> here two cases considered image locations human joints provided ii image locations joints unknown <eos> former case novel approach introduced integrates sparsity driven three dimensional geometric prior temporal smoothness <eos> latter case former case extended treating image locations joints latent variables order take into account considerable uncertainties joint locations <eos> deep fully convolutional network trained predict uncertainty maps joint locations <eos> three dimensional pose estimates realized via expectation maximization algorithm over entire sequence shown joint location uncertainties conveniently marginalized out during inference <eos> empirical evaluation human <eos> dataset shows proposed approaches achieve greater three dimensional pose estimation accuracy over state art baselines <eos> further proposed approach outperforms publicly available pose estimation baseline challenging pennaction dataset <eos> <eop> answer type prediction visual question answering <eos> recently algorithms object recognition related tasks become sufficiently proficient new vision tasks now pursued <eos> paper build system capable answering open ended text based questions about image known visual question answering vqa <eos> approach key insight predict form answer question <eos> formulate solution bayesian framework <eos> when approach combined discriminative model combined model achieves state art result four benchmark datasets open ended vqa daquar coco qa vqa dataset visual <eos> <eop> visual word vec vis learning visually grounded word embeddings using abstract scenes <eos> propose model learn visually grounded word embeddings vis capture visual notions semantic relatedness <eos> while word embeddings trained using text extremely successful they cannot uncover notions semantic relatedness implicit visual world <eos> instance although eats stares seem unrelated text they share semantics visually <eos> when people eating something they also tend stare food <eos> grounding diverse relations like eats stares into vision remains challenging despite recent progress vision <eos> note visual grounding words depends semantics literal pixels <eos> thus use abstract scenes created clipart provide visual grounding <eos> find embeddings learn capture fine grained visually grounded notions semantic relatedness <eos> show improvements over text only word embeddings word vec three tasks common sense assertion classification visual paraphrasing text based image retrieval <eos> code datasets available online <eos> <eop> visual grounded question answering image <eos> seen great progress basic perceptual tasks such object recognition detection <eos> however ai models still fail match humans high level vision tasks due lack capacities deeper reasoning <eos> recently new task visual question answering qa proposed evaluate model capacity deep image understanding <eos> previous works established loose global association between qa sentences image <eos> however many questions answers practice relate local region image <eos> establish semantic link between textual descriptions image region object level grounding <eos> enables new type qa visual answers addition textual answers used previous work <eos> study visual qa tasks grounded setting large collection multiple choice qa pairs <eos> furthermore evaluate human performance several baseline models qa tasks <eos> finally propose novel lstm model spatial attention tackle qa tasks <eos> <eop> learning deep structure preserving image text embeddings <eos> paper proposes method learning joint embeddings image text using two branch neural network multiple layer linear projections followed nonlinearities <eos> network trained using large margin objective combines cross view ranking constraints within view neighborhood structure preservation constraints inspired metric learning literature <eos> extensive experiments show approach gains significant improvements accuracy image text text image retrieval <eos> method achieves new state art result flickr mscoco image sentence datasets shows promise new task phrase localization flickr entities dataset <eos> <eop> yin yang balancing answering binary visual questions <eos> complex compositional structure language makes problems intersection vision language challenging <eos> but language also provides strong prior result good superficial performance without underlying models truly understanding visual content <eos> hinder progress pushing state art computer vision aspects multi modal ai <eos> paper address binary visual question answering vqa abstract scenes <eos> formulate problem visual verification concepts inquired questions <eos> specifically convert question tuple concisely summarizes visual concept detected image <eos> if concept found image answer question yes otherwise no <eos> abstract scenes play two roles they allow focus high level semantics vqa task opposed low level recognition problems perhaps more importantly they provide modality balance dataset such language priors controlled role vision essential <eos> particular collect fine grained pairs scenes every question such answer question yes one scene no other exact same question <eos> indeed language priors alone perform better than chance balanced dataset <eos> moreover proposed approach matches performance state art vqa approach unbalanced dataset outperforms balanced dataset <eos> <eop> gift real time scalable three dimensional shape search engine <eos> projective analysis important solution three dimensional shape retrieval since human visual perceptions three dimensional shapes rely various observations different view point <eos> although multiple informative discriminative views utilized most projection based retrieval systems suffer heavy computational cost thus cannot satisfy basic requirement scalability search engines <eos> paper present real time three dimensional shape search engine based projective image three dimensional shapes <eos> real time property search engine result following aspects efficient projection view feature extraction using gpu acceleration first inverted file referred if utilized speed up procedure multi view matching second inverted file if captures local distribution three dimensional shapes feature manifold adopted efficient context based reranking <eos> result each query retrieval task finished within one second despite necessary cost io overhead <eos> name proposed three dimensional shape search engine combines gpu acceleration inverted file twice gift <eos> besides its high efficiency gift also outperforms state art method significantly retrieval accuracy various shape benchmarks competitions <eop> functional faces groupwise dense correspondence using functional maps <eos> paper present method computing dense correspondence between set three dimensional face meshes using functional maps <eos> functional maps paradigm brings number advantages face correspondence <eos> first allows combine various notions correspondence <eos> so proposing number face specific functions suited either within between subject correspondence <eos> second propose groupwise variant method allowing compute cycle consistent functional maps between all faces training set <eos> since functional maps much lower dimension than point point correspondences feasible even when input meshes very high resolution <eos> finally show how functional map provides geometric constraint used filter feature matches between non rigidly deforming surfaces <eos> <eop> similarity metric curved shapes euclidean space <eos> paper introduce similarity metric curved shapes described distinctively ordered point <eos> proposed method represents given curve point deformation space direct product rigid transformation matrices such successive action matrices fixed starting point reconstructs full curve <eos> general both open closed curves represented deformation space modulo shape orientation orientation preserving diffeomorphisms <eos> use direct product lie groups represent curved shapes led explicit formula geodesic curves formulation similarity metric between shapes norm lie algebra <eos> additionally invariance reparametrization estimation point correspondence between shapes performed intermediate step computing geodesics <eos> furthermore since there no computation differential quantities curves representation more robust local perturbations needs no pre smoothing <eos> compare method elastic shape metric defined through square root velocity srv mapping other shape matching approaches <eos> <eop> shape analysis hyperbolic wasserstein distance <eos> shape space active research field computer vision study <eos> shape distance defined shape space may provide simple refined index represent unique shape <eos> wasserstein distance defines riemannian metric wasserstein space <eos> intrinsically measures similarities between shapes robust image noise <eos> thus potential three dimensional shape indexing classification research <eos> while algorithms computing wasserstein distance extensively studied most them only work genus surfaces <eos> paper proposes novel framework compute wasserstein distance between general topological surfaces hyperbolic metric <eos> computational algorithms based ricci flow hyperbolic harmonic map hyperbolic power voronoi diagram method general robust <eos> apply method study human facial expression longitudinal brain cortical morphometry normal aging cortical shape classification alzheimer disease ad <eos> experimental result demonstrate method may used effective shape index outperforms some other standard shape measures ad versus healthy control classification study <eos> <eop> tensor power iteration multi graph matching <eos> due its wide range applications matching between two graphs extensively studied remains active topic <eos> contrast still under exploited how jointly match multiple graphs partly due its intrinsic computational intractability <eos> work address challenging problem principled way under rank tensor approximation framework <eos> particular formulate multi graph matching combinational optimization problem two main ingredients unary matching over graph vertices structure matching over graph edges both across multiple graphs <eos> then propose efficient power iteration solution resulted np hard optimization problem <eos> proposed algorithm several advantages intrinsic matching consistency across multiple graphs based high order tensor optimization free employment powerful high order node affinity flexible integration between various types node affinities edge hyper edge affinities <eos> experiments diverse challenging datasets validate effectiveness proposed approach comparison state arts <eos> <eop> multivariate regression grassmannian predicting novel domains <eos> study problem predicting how recognise visual object novel domains neither labelled nor unlabelled training data <eos> domain adaptation now established research area due its value ameliorating issue domain shift between train test data <eos> however conventionally assumed domains discrete entities least unlabelled data provided testing domains <eos> paper consider case domains parametrised vector continuous values <eos> time lighting view angle <eos> aim use such domain metadata predict novel domains recognition <eos> allows recognition model pre calibrated new domain advance <eos> future time view angle without waiting data collection re training <eos> achieve posing problem one multivariate regression grassmannian regress domain subspace point grassmannian against independent vector domain parameters <eos> derive two novel methodologies achieve challenging task direct kernel regression indirect method better extrapolation properties <eos> evaluate method two cross domain visual recognition benchmarks they perform close upper bound full data domain adaptation <eos> demonstrates data necessary domain adaptation if domain parametrically described <eos> <eop> learning cross domain landmarks heterogeneous domain adaptation <eos> while domain adaptation da aims associate learning tasks across data domains heterogeneous domain adaptation hda particularly deals learning cross domain data different types feature <eos> other words hda data source target domains observed separate feature spaces thus exhibit distinct distributions <eos> paper propose novel learning algorithm cross domain landmark selection cdls solving above task <eos> goal deriving domain invariant feature subspace hda cdls able identify representative cross domain data including unlabeled ones target domain performing adaptation <eos> addition adaptation capabilities such cross domain landmarks determined accordingly <eos> reason why cdls able achieve promising hda performance when comparing state art hda method <eos> conduct classification experiments using data across different feature domains modalities <eos> effectiveness proposed method successfully verified <eos> <eop> geospatial correspondences multimodal registration <eos> growing availability very high resolution pixel satellite aerial image opened up unprecedented opportunities monitor analyze evolution land cover land use across world <eos> so image same geographical areas acquired different times potentially different sensors must efficiently parsed update maps detect land cover changes <eos> however naive transfer ground truth labels one location source image corresponding location target image generally feasible image often only loosely registered up non uniform errors <eos> furthermore land cover changes area over time must taken into account accurate ground truth transfer <eos> tackle challenges propose mid level sensor invariant representation encodes image region terms spatial distribution their spectral neighbors <eos> incorporate representation markov random field simultaneously account nonlinear mis registrations enforce locality priors find matches between multi sensor image <eos> show how approach used assist several multimodal land cover update change detection problems <eos> <eop> constrained deep transfer feature learning its applications <eos> feature learning deep models achieved impressive result both data representation classification various vision tasks <eos> deep feature learning however typically requires large amount training data may feasible some application domains <eos> transfer learning one approaches alleviate problem transferring data data rich source domain data scarce target domain <eos> existing transfer learning method typically perform one shot transfer learning often ignore specific properties transferred data must satisfy <eos> address issues introduce constrained deep transfer feature learning method perform simultaneous transfer learning feature learning performing transfer learning progressively improving feature space iteratively order better narrow gap between target domain source domain effective transfer data source domain target domain <eos> furthermore propose exploit target domain knowledge incorporate such prior knowledge constraint during transfer learning ensure transferred data satisfies certain properties target domain <eos> demonstrate effectiveness proposed constrained deep transfer feature learning method apply thermal feature learning eye detection transferring visible domain <eos> also applied proposed method cross view facial expression recognition second application <eos> experimental result demonstrate effectiveness proposed method both applications <eos> <eop> deep canonical time warping <eos> machine learning algorithms analysis time series often depend assumption utilised data temporally aligned <eos> any temporal discrepancies arising data certain lead ill generalisable models turn fail correctly capture properties task hand <eos> temporal alignment time series thus crucial challenge manifesting multitude applications <eos> nevertheless vast majority algorithms oriented towards temporal alignment time series applied directly observation space utilise simple linear projections <eos> thus they fail capture complex hierarchical non linear representations may prove beneficial towards task temporal alignment particularly when dealing multi modal data <eos> aligning visual acoustic information <eos> end present deep canonical time warping dctw method automatically learns complex non linear representations multiple time series generated such they highly correlated ii temporally alignment <eos> means experiments four real datasets show representations learnt via proposed dctw significantly outperform state art method temporal alignment elegantly handling scenarios highly heterogeneous feature such temporal alignment acoustic visual feature <eos> <eop> multilinear hyperplane hashing <eos> hashing become increasingly popular technique fast nearest neighbor search large databases <eos> despite its successful progress classic point point search there few studies regarding point hyperplane search strong practical capabilities scaling up many applications like active learning svms <eos> existing hyperplane hashing method enable fast search based randomly generated hash codes but still suffer low collision probability thus usually require long codes satisfying performance <eos> overcome problem paper proposes multilinear hyperplane hashing generates hash bit using multiple linear projections <eos> theoretical analysis shows product even number random linear projections multilinear hash function possesses increasing power locality sensitivity hyperplane queries <eos> leverage its sensitivity angle distance further introduce angular quantization based learning framework compact multilinear hashing considerably boosts search performance less hash bits <eos> experiments applications large scale up one million active learning two datasets demonstrate overall superiority proposed approach <eos> <eop> large scale hard sample mining monte carlo tree search <eos> investigate efficient strategy collect false positives very large training set context object detection <eos> approach scales up standard bootstrapping procedure using hierarchical decomposition image collection reflects statistical regularity detector responses <eos> based decomposition procedure uses monte carlo tree search prioritize sampling toward sub families image observed rich false positives while maintaining fraction sampling toward unexplored sub families image <eos> resulting procedure increases substantially proportion false positive sample among visited ones compared naive uniform sampling <eos> apply experimentally new procedure face detection collection background image pedestrian detection image <eos> show two standard detectors proposed strategy cuts number image visit half obtain same amount false positives same final performance <eos> <eop> multi label ranking positive unlabeled data <eos> paper specifically examine training multi label classifier data incompletely assigned labels <eos> problem fundamentally important many multi label applications because almost impossible human annotators assign complete set labels although their judgments reliable <eos> other words multi label dataset usually properties assigned labels definitely positive some labels absent but still considered positive <eos> such setting studied positive unlabeled pu classification problem binary setting <eos> treat incomplete label assignment problems multi label pu ranking extension classical binary pu problems well studied rank based multi label classification <eos> derive conditions should satisfied cancel negative effects label incompleteness <eos> experimentally obtained result demonstrate effectiveness conditions <eos> <eop> joint unsupervised learning deep representations image clusters <eos> paper propose recurrent framework joint unsupervised learning deep representations image clusters <eos> framework successive operations clustering algorithm expressed steps recurrent process stacked top representations output convolutional neural network cnn <eos> during training image clusters representations updated jointly image clustering conducted forward pass while representation learning backward pass <eos> key idea behind framework good representations beneficial image clustering clustering result provide supervisory signals representation learning <eos> integrating two processes into single model unified weighted triplet loss function optimizing end end obtain only more powerful representations but also more precise image clusters <eos> extensive experiments show method outperforms state art image clustering across variety image datasets <eos> moreover learned representations generalize well when transferred other tasks <eos> <eop> kernel sparse subspace clustering symmetric positive definite manifolds <eos> sparse subspace clustering ssc one most successful subspace clustering method achieved notable clustering accuracy computer vision tasks <eos> however ssc applies only vector data euclidean space <eos> such there still no satisfactory approach solve subspace clustering self expressive principle symmetric positive definite spd matrices very useful computer vision <eos> paper embedding spd matrices into reproducing kernel hilbert space rkhs kernel subspace clustering method constructed spd manifold through appropriate log euclidean kernel termed kernel sparse subspace clustering spd riemannian manifold ksscr <eos> exploiting intrinsic riemannian geometry within data ksscr effectively characterize geodesic distance between spd matrices uncover underlying subspace structure <eos> experimental result several famous database demonstrate proposed method achieves better clustering result than state art approaches <eos> <eop> symmetry recaptcha <eos> reaction poor performance symmetry detection algorithms real world image benchmarked since cvpr <eos> systematic study reveals significant difference between human labeled reflection rotation symmetries photos output computer vision algorithms same photo set <eos> exploit human machine symmetry perception gap proposing novel symmetry based turing test <eos> leveraging comprehensive user interface collected more than symmetry labels amazon mechanical turk raters nearly photos microsoft coco dataset <eos> using set ground truth symmetries automatically generated noisy human labels effectiveness work evidenced separate test over success rate achieved <eos> demonstrate statistically significant outcomes using symmetry perception powerful alternative image based recaptcha <eos> <eop> unsupervised learning discriminative attributes visual representations <eos> attributes offer useful mid level feature interpret visual data <eos> while most attribute learning method supervised costly human generated labels introduce simple yet powerful unsupervised approach learn predict visual attributes directly data <eos> given large unlabeled image collection input train deep convolutional neural network cnn output set discriminative binary attributes often semantic meanings <eos> specifically first train cnn coupled unsupervised discriminative clustering then use cluster membership soft supervision discover shared attributes clusters while maximizing their separability <eos> learned attributes shown capable encoding rich imagery properties both natural image contour patches <eos> visual representations learned way also transferrable other tasks such object detection <eos> show other convincing result related tasks image retrieval classification contour detection <eos> <eop> when vlad met hilbert <eos> many challenging visual recognition tasks training data limited vectors locally aggregated descriptors vlad emerged powerful image video representations compete outperform state art approaches <eos> paper address two fundamental limitations vlad its requirement local descriptors vector form its restriction linear classifiers due its high dimensionality <eos> end introduce kernelized version vlad <eos> only lets inherently exploit more sophisticated classification schemes but also enables efficiently aggregate non vector descriptors <eos> manifold valued data vlad framework <eos> furthermore propose approximate formulation allows accelerate coding process while still benefiting properties kernel vlad <eos> experiments demonstrate effectiveness approach handling manifold valued data such covariance descriptors several classification tasks <eos> result also evidence benefits nonlinear vlad descriptors against linear ones euclidean space using several standard benchmark datasets <eos> <eop> approximate log hilbert schmidt distances between covariance operators image classification <eos> paper presents novel framework visual object recognition using infinite dimensional covariance operators input feature paradigm kernel method infinite dimensional riemannian manifolds <eos> formulation provides rich representation image feature exploiting their non linear correlations using power kernel method riemannian geometry <eos> theoretically provide approximate formulation log hilbert schmidt distance between covariance operators efficient compute scalable large datasets <eos> empirically apply framework task image classification eight different challenging datasets <eos> almost all cases result obtained outperform other state art method demonstrating competitiveness potential framework <eos> <eop> subspace clustering priors via sparse quadratically constrained quadratic programming <eos> paper considers problem recovering subspace arrangement noisy sample potentially corrupted outliers <eos> main result shows problem formulated convex semi definite optimization problem subject additional rank constrain involves only very small number variables <eos> established first reducing problem generically non convex quadratically constrained quadratic problem then using its special sparse structure find conditions guaranteeing suitably built convex relaxation indeed exact <eos> when combined commonly used nuclear norm relaxation rank result above lead computationally efficient algorithms optimality guarantees <eos> salient feature proposed approach its ability incorporate existing priori information about noise co ocurrences percentage outliers <eos> result illustrated several examples proposed algorithm shown outperform existing approaches <eos> <eop> robust tensor factorization unknown noise <eos> because limitations matrix factorization such losing spatial structure information concept tensor factorization applied recovery low dimensional subspace high dimensional visual data <eos> generally recovery achieved minimizing loss function between observed data factorization representation <eos> under different assumptions noise distribution loss functions various forms like norms <eos> however real data often corrupted noise unknown distribution <eos> then any specific form loss function one specific kind noise often fails tackle such real data unknown noise <eos> paper propose tensor factorization algorithm model noise mixture gaussians mog <eos> mog ability universally approximating any hybrids continuous distributions algorithm effectively recover low dimensional subspace various forms noisy observations <eos> parameters mog estimated under em framework through new developed algorithm weighted low rank tensor factorization wlrtf <eos> effectiveness algorithm substantiated extensive experiments both synthetic data real image data <eos> <eop> kernel approximation via empirical orthogonal decomposition unsupervised feature learning <eos> kernel approximation method important tools various machine learning problems <eos> there two major method used approximate kernel function nystrom method random feature method <eos> however nystrom method requires relatively high complexity post processing calculate solution random feature method provide sufficient generalization performance <eos> paper propose method good generalization performance without high complexity postprocessing via empirical orthogonal decomposition using probability distribution estimated training data <eos> provide bound approximation error proposed method <eos> experiments show proposed method better than random feature method comparable nystrom method terms approximation error classification accuracy <eos> also show hierarchical feature extraction using kernel approximation demonstrates better performance than existing method <eos> <eop> active learning delineation curvilinear structures <eos> many recent delineation techniques owe much their increased effectiveness path classification algorithms make possible distinguish promising paths others <eos> downside development they require annotated training data tedious produce <eos> paper propose active learning approach considerably speeds up annotation process <eos> unlike standard ones takes advantage specificities delineation problem <eos> operates graph reduce training set size up without compromising reconstruction quality <eos> will show approach outperforms conventional ones various biomedical natural image datasets thus showing broadly applicable <eos> <eop> recognizing emotions abstract paintings using non linear matrix completion <eos> advanced computer vision machine learning techniques tried automatically categorize emotions elicited abstract paintings limited success <eos> since annotation emotional content highly resource consuming datasets abstract paintings either constrained size partially annotated <eos> consequently natural address targeted task within transductive framework <eos> intuitively use multi label classification techniques desirable so synergically exploit relations between multiple latent variables such emotional content technique author etc <eos> very popular approach transductive multi label recognition under linear classification settings matrix completion <eos> study introduce non linear matrix completion nlmc thus extending classical linear matrix completion techniques non linear case <eos> together theory grounding model propose efficient optimization solver <eos> shown extensive experimental validation two publicly available datasets nlmc outperforms state art method when recognizing emotions abstract paintings <eos> <eop> tensor robust principal component analysis exact recovery corrupted low rank tensors via convex optimization <eos> paper studies tensor robust principal component trpca problem extends known robust pca tensor case <eos> model based new tensor singular value decomposition svd its induced tensor tubal rank tensor nuclear norm <eos> consider way tensor such low tubal rank sparse <eos> possible recover both components work prove under certain suitable assumptions recover both low rank sparse components exactly simply solving convex program whose objective weighted combination tensor nuclear norm norm <eos> interestingly trpca involves rpca special case when thus simple elegant tensor extension rpca <eos> also numerical experiments verify theory application image denoising demonstrates effectiveness method <eos> <eop> sliced wasserstein kernels probability distributions <eos> optimal transport distances otherwise known wasserstein distances recently drawn ample attention computer vision machine learning powerful discrepancy measures probability distributions <eos> recent developments alternative formulations optimal transport allowed faster solutions problem revamped their practical applications machine learning <eos> paper exploit widely used kernel method provide family provably positive definite kernels based sliced wasserstein distance demonstrate benefits kernels variety learning tasks <eos> work provides new perspective application optimal transport flavored distances through kernel method machine learning tasks <eos> <eop> trace quotient meets sparsity method learning low dimensional image representations <eos> paper presents algorithm allows learn low dimensional representations image unsupervised manner <eos> core idea combine two criteria play important roles unsupervised representation learning namely sparsity trace quotient <eos> former known convenient tool identify underlying factors latter known disentanglement underlying discriminative factors <eos> work develop generic cost function learning jointly sparsifying dictionary dimensionality reduction transformation <eos> leads several counterparts classic low dimensional representation method such principal component analysis local linear embedding laplacian eigenmap <eos> proposed optimisation algorithm leverages efficiency geometric optimisation riemannian manifolds closed form solution elastic net problem <eos> <eop> backtracking scspm image classifier weakly supervised top down saliency <eos> top down saliency models produce probability map peaks target locations specified task goal such object detection <eos> they usually trained supervised setting involving annotations object <eos> propose weakly supervised top down saliency framework using only binary labels indicate presence absence object image <eos> first probabilistic contribution each image patch confidence scspm based classifier produces reverse scspm scspm saliency map <eos> neighborhood information then incorporated through contextual saliency map estimated using logistic regression learnt patches having high scspm saliency <eos> both saliency maps combined obtain final saliency map <eos> evaluate performance proposed weakly supervised top down saliency achieves comparable performance fully supervised approaches <eos> experiments carried out challenging datasets across different applications <eos> <eop> msr vtt large video description dataset bridging video language <eos> while there increasing interest task describing video natural language current computer vision algorithms still severely limited terms variability complexity video their associated language they recognize <eos> part due simplicity current benchmarks mostly focus specific fine grained domains limited video simple descriptions <eos> while researchers provided several benchmark datasets image captioning aware any large scale video description dataset comprehensive categories yet diverse video content <eos> paper present msr vtt standing abc video text new large scale video benchmark video understanding especially emerging task translating video text <eos> achieved collecting popular queries commercial video search engine video each query <eos> its current version msr vtt provides web video clips <eos> hours clip sentence pairs total covering most comprehensive categories diverse visual content representing largest dataset terms sentence vocabulary <eos> each clip annotated about natural sentences amt workers <eos> present detailed analysis msr vtt comparison complete set existing datasets together summarization different state art video text approaches <eos> also provide extensive evaluation approaches dataset showing hybrid recurrent neural network based approach combines single frame motion representations soft attention pooling strategy yields best generalization capability msr vtt <eos> <eop> netvlad cnn architecture weakly supervised place recognition <eos> tackle problem large scale visual place recognition task quickly accurately recognize location given query photograph <eos> present following three principal contributions <eos> first develop convolutional neural network cnn architecture trainable end end manner directly place recognition task <eos> main component architecture netvlad new generalized vlad layer inspired vector locally aggregated descriptors image representation commonly used image retrieval <eos> layer readily pluggable into any cnn architecture amenable training via backpropagation <eos> second develop training procedure based new weakly supervised ranking loss learn parameters architecture end end manner image depicting same places over time downloaded google street view time machine <eos> finally show proposed architecture significantly outperforms non learnt image representations off shelf cnn descriptors two challenging place recognition benchmarks improves over current state art compact image representations standard image retrieval benchmarks <eos> <eop> structural rnn deep learning spatio temporal graphs <eos> deep recurrent neural network architectures though remarkably capable modeling sequences lack intuitive high level spatio temporal structure <eos> while many problems computer vision inherently underlying high level structure benefit <eos> spatio temporal graphs popular tool imposing such high level intuitions formulation real world problems <eos> paper propose approach combining power high level spatio temporal graphs sequence learning success recurrent neural network rnns <eos> develop scalable method casting arbitrary spatio temporal graph rich rnn mixture feedforward fully differentiable jointly trainable <eos> proposed method generic principled used transforming any spatio temporal graph through employing certain set well defined steps <eos> evaluations proposed approach diverse set problems ranging modeling human motion object interactions shows improvement over state art large margin <eos> expect method empower new approaches problem formulation through high level spatio temporal graphs recurrent neural network <eos> <eop> learning select pre trained deep representations bayesian evidence framework <eos> propose bayesian evidence framework facilitate transfer learning pre trained deep convolutional neural network cnn <eos> framework formulated top least squares svm ls svm classifier simple fast both training testing achieves competitive performance practice <eos> regularization parameters ls svm estimated automatically without grid search cross validation maximizing evidence useful measure select best performing cnn out multiple candidates transfer learning evidence optimized efficiently employing aitken delta squared process accelerates convergence fixed point update <eos> proposed bayesian evidence framework also provides good solution identify best ensemble heterogeneous cnn through greedy algorithm <eos> bayesian evidence framework transfer learning tested visual recognition datasets illustrates state art performance consistently terms prediction accuracy modeling efficiency <eos> <eop> synthesized classifiers zero shot learning <eos> given semantic descriptions object classes zero shot learning aims accurately recognize object unseen classes no examples available training stage associating them seen classes labeled examples provided <eos> propose tackle problem perspective manifold learning <eos> main idea align semantic space derived external information model space concerns itself recognizing visual feature <eos> end introduce set phantom object classes whose coordinates live both semantic space model space <eos> serving bases dictionary they optimized labeled data such synthesized real object classifiers achieve optimal discriminative performance <eos> demonstrate superior accuracy approach over state art four benchmark datasets zero shot learning including full imagenet fall dataset more than unseen classes <eos> <eop> semi supervised vocabulary informed learning <eos> despite significant progress object categorization recent years number important challenges remain mainly ability learn limited labeled data ability recognize object classes within large potentially open set labels <eos> zero shot learning one way addressing challenges but only shown work limited sized class vocabularies typically requires separation between supervised unsupervised classes allowing former inform latter but vice versa <eos> propose notion semi supervised vocabulary informed learning alleviate above mentioned challenges address problems supervised zero shot open set recognition using unified framework <eos> specifically propose maximum margin framework semantic manifold based recognition incorporates distance constraints both supervised unsupervised vocabulary atoms ensuring labeled sample projected closest their correct prototypes embedding space than others <eos> show resulting model shows improvements supervised zero shot large open set recognition up class vocabulary awa imagenet datasets <eos> <eop> simultaneous clustering model selection tensor affinities <eos> estimating number clusters remains difficult model selection problem <eos> consider problem domain affinity relations involve groups more than two nodes <eos> building previous formulation pairwise affinity case exploit mathematical structures higher order case <eos> express original minimal rank positive semi definite psd constraints form amenable numerical implementation original constraints either intractable even undefined general higher order case <eos> scale large problem sizes also propose alternative formulation so efficiently solved via stochastic optimization online fashion <eos> evaluate algorithm different applications demonstrate its superiority show adapt varying levels unbalancedness clusters <eos> <eop> discriminatively embedded means multi view clustering <eos> real world applications more more data example image video data high dimensional represented multiple views describe different perspectives data <eos> efficiently clustering such data challenge <eos> address problem paper proposes novel multi view clustering method called discriminatively embedded means dekm embeds synchronous learning multiple discriminative subspaces into multi view means clustering construct unified framework adaptively control intercoordinations between subspaces simultaneously <eos> framework firstly design weighted multi view linear discriminant analysis lda then develop unsupervised optimization scheme alternatively learn common clustering indicator multiple discriminative subspaces weights heterogeneous feature convergence <eos> comprehensive evaluations three benchmark datasets comparisons several state art multi view clustering algorithms demonstrate superiority proposed work <eos> <eop> min norm point algorithm higher order mrf map inference <eos> many tasks computer vision machine learning modelled inference problems mrf map formulation reduced minimizing submodular function <eos> using higher order clique potentials model complex dependencies between pixels improves performance but current state art inference algorithms fail scale larger clique sizes <eos> adapt well known min norm point algorithm mathematical optimization literature exploit sum submodular structure found mrf map formulation <eos> unlike some contemporary method make any assumptions other than submodularity type clique potentials <eos> current state art inference algorithms general submodular function takes many hours problems clique size fail scale beyond <eos> other hand algorithm highly efficient perform optimal inference few seconds even clique size order magnitude larger <eos> proposed algorithm even scale clique sizes many hundreds unlocking usage really large size cliques mrf map inference problems computer vision <eos> demonstrate efficacy approach experimenting synthetic well real datasets <eos> <eop> learning deep representation imbalanced classification <eos> data vision domain often exhibit highly skewed class distribution <eos> most data belong few majority classes while minority classes only contain scarce amount instances <eos> mitigate issue contemporary classification method based deep convolutional neural network cnn typically follow classic strategies such class re sampling cost sensitive training <eos> paper conduct extensive systematic experiments validate effectiveness classic schemes representation learning class imbalanced data <eos> further demonstrate more discriminative deep representation learned enforcing deep network maintain both inter cluster inter class margins <eos> tighter constraint effectively reduces class imbalance inherent local data neighborhood <eos> show margins easily deployed standard deep learning framework through quintuplet instance sampling associated triple header hinge loss <eos> representation learned approach when combined simple nearest neighbor knn algorithm shows significant improvements over existing method both high low level vision classification tasks exhibit imbalanced class distribution <eos> <eop> learning local image descriptors deep siamese triplet convolutional network minimising global loss functions <eos> recent innovations training deep convolutional neural network convnet models motivated design new method automatically learn local image descriptors <eos> latest deep convnets proposed task consist siamese network trained penalising misclassification pairs local image patches <eos> current result machine learning show replacing siamese triplet network improve classification accuracy several problems but yet demonstrated local image descriptor learning <eos> moreover current siamese triplet network trained stochastic gradient descent computes gradient individual pairs triplets local image patches make them prone overfitting <eos> paper first propose use triplet network problem local image descriptor learning <eos> furthermore also propose use global loss minimises overall classification error all patches present training set improve generalisation capability model <eos> using ubc benchmark dataset comparing local image descriptors show triplet network produces more accurate embedding than siamese network terms ubc dataset errors <eos> moreover also demonstrate combination triplet global losses produces best embedding field using triplet network <eos> finally also show use central surround siamese network trained global loss produces best result field ubc dataset <eos> <eop> sparse coding third order super symmetric tensor descriptors application texture recognition <eos> super symmetric tensors higher order extension scatter matrices becoming increasingly popular machine learning computer vision modeling data statistics co occurrences even visual descriptors <eos> they were shown recently outperform second order approaches however size tensors exponential data dimensionality significant concern <eos> paper study third order super symmetric tensor descriptors context dictionary learning sparse coding <eos> purpose propose novel non linear third order texture descriptor <eos> goal approximate tensors sparse conic combinations atoms learned dictionary <eos> apart significant benefits tensor compression framework offers experiments demonstrate sparse coefficients produced scheme lead better aggregation high dimensional data showcase superior performance two common computer vision tasks compared state art <eos> <eop> random feature sparse signal classification <eos> random feature approach kernel based inference large datasets <eos> paper derive performance guarantees random feature signals like image enjoy sparse representations show number random feature required achieve desired approximation kernel similarity matrix significantly smaller sparse signals <eos> based propose scheme termed compressive random feature first obtains low dimensional projections dataset subsequently derives random feature low dimensional projections <eos> scheme provides significant improvements signal dimensionality computational time storage costs over traditional random feature while enjoying similar theoretical guarantees achieving inference performance <eos> support claims providing empirical result across many datasets <eos> <eop> high quality depth uncalibrated small motion clip <eos> propose novel approach generates high quality depth map set image captured small viewpoint variation namely small motion clip <eos> opposed prior method recover scene geometry camera motions using pre calibrated cameras introduce self calibrating bundle adjustment tailored small motion <eos> allows dense stereo algorithm produce high quality depth map user without need camera calibration <eos> dense matching distributions intensity profiles analyzed leverage benefit having negligible intensity changes within scene due minuscule variation viewpoint <eos> depth maps obtained proposed framework show accurate extremely fine structures unmatched previous literature under same small motion configuration <eos> <eop> efficient three dimensional room shape recovery single panorama <eos> propose method recover shape three dimensional room full view indoor panorama <eos> algorithm automatically infer three dimensional shape collection partially oriented superpixel facets line segments <eos> core part algorithm constraint graph includes lines superpixels vertices encodes their geometric relations edges <eos> novel approach proposed perform three dimensional reconstruction based constraint graph solving all geometric constraints constrained linear least squares <eos> selected constraints used reconstruction identified using occlusion detection method markov random field <eos> experiments show method recover room shapes addressed previous approaches <eos> method also efficient inference time each panorama less than minute <eos> <eop> structured prediction unobserved voxels single depth image <eos> building complete three dimensional model scene given only single depth image underconstrained <eos> gain full volumetric model one needs either multiple views single view together library unambiguous three dimensional models will fit shape each individual object scene <eos> hypothesize object dissimilar semantic classes often share similar three dimensional shape components enabling limited dataset model shape wide range object hence estimate their hidden geometry <eos> exploring hypothesis propose algorithm complete unobserved geometry tabletop sized object based supervised model trained already available volumetric elements <eos> model maps local observation single depth image estimate surface shape surrounding neighborhood <eos> validate approach both qualitatively quantitatively range indoor object collections challenging real scenes <eos> <eop> hyperdepth learning depth structured light without matching <eos> structured light sensors popular due their robustness untextured scenes multipath <eos> systems triangulate depth solving correspondence problem between each camera projector pixel <eos> often framed local stereo matching task correlating patches pixels observed reference image <eos> however computationally intensive leading reduced depth accuracy framerate <eos> contribute algorithm solving correspondence problem efficiently without compromising depth accuracy <eos> first time problem cast classification regression task solve extremely efficiently using ensemble cascaded random forests <eos> algorithm scales number disparities each pixel processed independently parallel <eos> no matching even access corresponding reference pattern required runtime regressed labels directly mapped depth <eos> gpu based algorithm runs khz <eos> mp input output image disparity error <eos> show prototype high framerate depth camera running hz useful solving tracking related problems <eos> demonstrate algorithmic performance creating high resolution real time depth maps surpass quality current state art depth technologies highlighting quantization free result reduced holes edge fattening other stereo based depth artifacts <eos> <eop> svbrdf invariant shape reflectance estimation light field cameras <eos> light field cameras recently emerged powerful tool one shot passive three dimensional shape capture <eos> however obtaining shape glossy object like metals plastics ceramics remains challenging since standard lambertian cues like photo consistency cannot easily applied <eos> paper derive spatially varying sv brdf invariant theory recovering three dimensional shape reflectance light field cameras <eos> key theoretical insight novel analysis diffuse plus single lobe svbrdfs under light field setup <eos> show although direct shape recovery possible equation relating depths normals still derived <eos> using equation then propose using polynomial quadratic shape prior resolve shape ambiguity <eos> once shape estimated also recover reflectance <eos> present extensive synthetic data entire merl brdf dataset well number real examples validate theory simultaneously recover shape brdfs single image taken lytro illum camera <eos> <eop> semantic three dimensional reconstruction continuous regularization ray potentials using visibility consistency constraint <eos> propose approach dense semantic three dimensional reconstruction uses data term defined potentials over viewing rays combined continuous surface area penalization <eos> formulation convex relaxation augment crucial non convex constraint ensures exact handling visibility <eos> tackle non convex minimization problem propose majorize minimize type strategy converges critical point <eos> demonstrate benefits using non convex constraint experimentally <eos> geometry only case set new state art two datasets commonly used middlebury multi view stereo benchmark <eos> moreover general purpose formulation directly reconstructs thin object usually treated specialized algorithms <eos> qualitative evaluation dense semantic three dimensional reconstruction task shows improve significantly over previous method <eos> <eop> theory practice structure motion using affine correspondences <eos> affine correspondences acs more informative than point correspondences pcs used input mainstream algorithms structure motion sfm <eos> since acs enable estimate models fewer correspondences its use dramatically reduce number combinations during iterative step sample test exists most sfm pipelines <eos> however using acs instead pcs input sfm passes fully understanding relations between acs multi view geometry well establishing practical effective ac based algorithms <eos> article step forward into direction providing clear account about how acs constrain two view geometry proposing new algorithms plane segmentation visual odometry compare favourably respect method relying pcs <eos> <eop> just look image viewpoint specific surface normal prediction improved multi view reconstruction <eos> present multi view reconstruction method combines conventional multi view stereo mvs appearance based normal prediction obtain dense accurate three dimensional surface models <eos> reliable surface normals reconstructed multi view correspondence serve training data convolutional neural network cnn predicts continuous normal vectors raw image patches <eos> training known point same image prediction specifically tailored materials lighting conditions particular scene well precise camera viewpoint <eos> therefore lot easier learn than generic single view normal estimation <eos> estimated normal maps together known depth values mvs integrated dense depth maps turn fused into three dimensional model <eos> experiments dtu dataset show method delivers three dimensional reconstructions same accuracy mvs but significantly higher completeness <eos> <eop> dusk till dawn modeling dark <eos> internet photo collections naturally contain large variety illumination conditions largest difference between day night image <eos> current modeling techniques embrace broad illumination range often leading reconstruction failure severe artifacts <eos> present algorithm leverages appearance variety obtain more complete accurate scene geometry along consistent multi illumination appearance information <eos> proposed method relies automatic scene appearance grouping used obtain separate dense three dimensional models <eos> subsequent model fusion combines separate models into complete accurate reconstruction scene <eos> addition propose method derive appearance information model under different illumination conditions even scene parts observed under one illumination condition <eos> achieve develop cross illumination color transfer technique <eos> evaluate method large variety landmarks across europe reconstructed database <eos> <eop> accelerated generative models three dimensional point cloud data <eos> finding meaningful structured representations three dimensional point cloud data pcd become core task spatial perception applications <eos> paper introduce method constructing compact generative representations pcd multiple levels detail <eos> opposed deterministic structures such voxel grids octrees propose probabilistic subdivisions data through local mixture modeling show how subdivisions provide maximum likelihood segmentation data <eos> final representation hierarchical compact parametric statistically derived facilitating run time occupancy calculations through stochastic sampling <eos> unlike traditional deterministic spatial subdivision method technique enables dynamic creation voxel grids according application best needs <eos> contrast other generative models pcd explicitly enforce sparsity among point mixtures technique call expectation sparsification <eos> leads highly parallel hierarchical expectation maximization em algorithm well suited gpu real time execution <eos> explore trade offs between model fidelity model size various levels detail tests showing favorable performance when compared octree ndt based method <eos> <eop> monocular depth estimation using neural regression forest <eos> paper presents novel deep architecture called neural regression forest nrf depth estimation single image <eos> nrf combines random forests convolutional neural network cnn <eos> scanning windows extracted image represent sample passed down trees nrf predicting their depth <eos> every tree node sample filtered cnn associated node <eos> result convolutional filtering passed left right children nodes <eos> corresponding cnn bernoulli probability until leaves depth estimations made <eos> cnn every node designed fewer parameters than seen recent work but their stacked processing along path tree effectively amounts deeper cnn <eos> nrf allows parallelizable training all shallow cnn efficient enforcing smoothness depth estimation result <eos> evaluation benchmark make nyuv datasets demonstrates nrf outperforms state art gracefully handles gradually decreasing training datasets <eos> <eop> deepstereo learning predict new views world imagery <eos> deep network recently enjoyed enormous success when applied recognition classification problems computer vision but their use graphics problems limited notable recent exceptions <eos> work present novel deep architecture per forms new view synthesis directly pixels trained large number posed image set <eos> contrast tradi tional approaches consist multiple complex stages processing each require careful tuning fail unexpected ways system trained end end <eos> pixels neighboring views scene presented network then directly produces pixels unseen view <eos> benefits approach include gen erality only require posed image set easily apply method different domains high quality result traditionally difficult scenes <eos> believe due end end nature system able plausibly generate pixels according color depth tex ture priors learnt automatically training data <eos> show view interpolation result imagery kitti dataset data well streetview image <eos> knowledge work first apply deep learning problem new view synthesis set real world natural imagery <eos> <eop> wider face face detection benchmark <eos> face detection one most studied topics computer vision community <eos> much progresses made availability face detection benchmark datasets <eos> show there gap between current face detection performance real world requirements <eos> facilitate future face detection research introduce wider face dataset times larger than existing datasets <eos> dataset contains rich annotations including occlusions poses event categories face bounding boxes <eos> faces proposed dataset extremely challenging due large variations scale pose occlusion shown fig <eos> furthermore show wider face dataset effective training source face detection <eos> benchmark several representative detection systems providing overview state art performance propose solution deal large scale variation <eos> finally discuss common failure cases worth further investigated <eos> <eop> situation recognition visual semantic role labeling image understanding <eos> paper introduces situation recognition problem producing concise summary situation image depicts including main activity <eos> clipping participating actors object substances locations <eos> man shears sheep wool field most importantly roles participants play activity <eos> man clipping shears his tool wool being clipped sheep clipping field <eos> use framenet verb role lexicon developed linguists define large space possible situations collect large scale dataset containing over activities roles object image unique situations <eos> also introduce structured prediction baselines show activity centric image situation driven prediction object activities outperforms independent object activity recognition <eos> <eop> three dimensional morphable model learnt faces <eos> present large scale facial model lsfm three dimensional morphable model dmm automatically constructed distinct facial identities <eos> best knowledge lsfm largest scale morphable model ever constructed containing statistical information huge variety human population <eos> build such large model introduce novel fully automated robust morphable model construction pipeline <eos> dataset lsfm trained includes rich demographic information about each subject allowing construction only global dmm but also models tailored specific age gender ethnicity groups <eos> application example utilise proposed model perform age classification three dimensional shape alone <eos> furthermore perform systematic analysis constructed dmms showcases their quality descriptive power <eos> presented extensive qualitative quantitative evaluations reveal proposed dmm achieves state art result outperforming existing models large margin <eos> finally benefit research community make publicly available source code proposed automatic dmm construction pipeline <eos> addition constructed global dmm variety bespoke models tailored age gender ethnicity available application researchers involved medically oriented research <eos> <eop> some like hot visual guidance preference prediction <eos> people first impressions someone determining importance <eos> they hard alter through further information <eos> begs question if computer reach same judgement <eos> earlier research already pointed out age gender average attractiveness estimated reasonable precision <eos> improve state art but also predict based someone known preferences how much particular person attracted novel face <eos> computational pipeline comprises face detector convolutional neural network extraction deep feature standard support vector regression gender age facial beauty main novelties visual regularized collaborative filtering infer inter person preferences well novel regression technique handling visual queries without rating history <eos> validate method using very large dataset dating site well image celebrities <eos> experiments yield convincing result <eos> predict ratings correctly solely based image reveal some sociologically relevant conclusions <eos> also validate collaborative filtering solution standard movielens rating dataset augmented movie posters predict individual movie rating <eos> demonstrate algorithms howhot <eos> io went viral around internet more than million pictures evaluated first month <eos> <eop> emotionet accurate real time algorithm automatic annotation million facial expressions wild <eos> research face perception emotion theory requires very large annotated databases image facial expressions emotion <eos> annotations should include action units aus their intensities well emotion category <eos> goal cannot readily achieved manually <eos> herein present novel computer vision algorithm annotate large database one million image facial expressions emotion wild <eos> face image downloaded internet <eos> first show newly proposed algorithm recognize aus their intensities reliably across databases <eos> knowledge first published algorithm achieve highly accurate result recognition aus their intensities across multiple databases <eos> algorithm also runs real time image second allowing work large numbers image video sequences <eos> second use wordnet download image facial expressions associated emotion keywords internet <eos> image then automatically annotated aus au intensities emotion categories algorithm <eos> result highly useful database readily queried using semantic descriptions applications computer vision affective computing social cognitive psychology neuroscience <eos> show me all image happy faces all image au intensity <eos> <eop> forgetmenot memory aware forensic facial sketch matching <eos> investigate whether possible improve performance automated facial forensic sketch matching learning examples facial forgetting over time <eos> forensic facial sketch recognition key capability law enforcement but remains unsolved problem <eos> extremely challenging because there three distinct contributors domain gap between forensic sketches photos well studied sketch photo modality gap less studied gaps due forgetting process eye witness ii their inability elucidate their memory <eos> paper address memory problem head introducing database forensic sketches created different time delays <eos> based database build model reverse forgetting process <eos> surprisingly show possible systematically un forget facial details <eos> moreover possible apply model dramatically improve forensic sketch recognition practice achieve state art result when matching benchmark forensic sketches against corresponding photos mugshot database <eos> <eop> lomo latent ordinal model facial analysis video <eos> study problem facial analysis video <eos> first contribution novel weakly supervised learning method models video event pain expression etc <eos> sequence automatically mined discriminative sub events eg <eos> neutral face raising brows contracting lips <eos> proposed model inspired recent works multiple instance learning latent svm hcrf extends such frameworks model ordinal temporal aspect video approximately <eos> show consistent improvements over relevant competitive baselines four challenging publicly available video based facial analysis datasets prediction expression clinical pain intent dyadic conversations <eos> combination complimentary feature report state art result datasets <eos> <eop> discriminative invariant kernel feature bells whistles free approach unsupervised face recognition pose estimation <eos> propose explicitly discriminative simple approach generate invariance nuisance transformations modeled unitary <eos> practice approach works well handle non unitary transformations well <eos> theoretical result extend reach recent theory invariance discriminative kernelized feature based unitary kernels <eos> special case single common framework used generate subject specific pose invariant feature face recognition vice versa pose estimation <eos> show main proposed method dikf perform well under very challenging large scale semi synthetic face matching pose estimation protocols unaligned faces using no land marking whatsoever <eos> additionally benchmark cmu mpie outperform previous work almost all cases off angle face matching while par previous state art lfw unsupervised image restricted protocols without any low level image descriptors other than raw pixels <eos> <eop> bottom up top down reasoning hierarchical rectified gaussians <eos> convolutional neural nets cnn demonstrated remarkable performance recent history <eos> such approaches tend work unidirectional bottom up feed forward fashion <eos> however practical experience biological evidence tells feedback plays crucial role particularly detailed spatial understanding tasks <eos> work explores bidirectional architectures also reason top down feedback neural units influenced both lower higher level units <eos> so treating units rectified latent variables quadratic energy function seen hierarchical rectified gaussian model rgs <eos> show rgs optimized quadratic program qp turn optimized recurrent neural network rectified linear units <eos> allows rgs trained gpu optimized gradient descent <eos> theoretical perspective rgs help establish connection between cnn hierarchical probabilistic models <eos> practical perspective rgs well suited detailed spatial tasks benefit top down reasoning <eos> illustrate them challenging task keypoint localization under occlusions local bottom up evidence may misleading <eos> demonstrate state art result challenging benchmarks <eos> <eop> fits like glove rapid reliable hand shape personalization <eos> present fast practical method personalizing hand shape basis individual user detailed hand shape using only small set depth image <eos> achieve minimize energy based sum render compare cost functions called golden energy <eos> however energy only piecewise continuous due pixels crossing occlusion boundaries therefore obviously amenable efficient gradient based optimization <eos> key insight energy combination smooth low frequency function high frequency low amplitude piecewise continuous function <eos> central finite difference approximation suitable step size therefore jump over discontinuities obtain good approximation energy low frequency behavior allowing efficient gradient based optimization <eos> experimental result quantitatively demonstrate first time detailed personalized models improve accuracy hand tracking achieve competitive result both tracking model registration <eos> <eop> slicing convolutional neural network crowd video understanding <eos> learning capturing both appearance dynamic representations pivotal crowd video understanding <eos> convolutional neural network cnn shown its remarkable potential learning appearance representations image <eos> however learning dynamic representation how effectively combined appearance feature video analysis remains open problem <eos> study propose novel spatio temporal cnn named slicing cnn cnn based decomposition three dimensional feature maps into spatio temporal slices representations <eos> decomposition brings unique advantages model capable capturing dynamics different semantic units such groups object learns separated appearance dynamic representations while keeping proper interactions between them exploits selectiveness spatial filters discard irrelevant background clutter crowd understanding <eos> demonstrate effectiveness proposed cnn model www crowd video dataset attribute recognition observe significant performance improvements state art method <eos> <eop> linear shape deformation models local support using graph based structured matrix factorisation <eos> representing three dimensional shape deformations high dimensional linear models many applications computer vision medical imaging <eos> commonly using principal components analysis low dimensional subspace high dimensional shape space determined <eos> however resulting factors most dominant eigenvectors covariance matrix global support <eos> changing coefficient single factor deforms entire shape <eos> based matrix factorisation sparsity graph based regularisation terms present method obtain deformation factors local support <eos> benefits include better flexibility interpretability well possibility interactively deforming shapes locally <eos> demonstrate brain shapes method outperforms state art local support models respect generalisation sparse reconstruction whereas body shapes method gives more realistic deformations <eos> <eop> motion structure mfs searching three dimensional object cluttered point trajectories <eos> object detection long standing problem computer vision state art approaches rely use sophisticated feature classifiers <eos> however learning based approaches heavily depend quality quantity labeled data generalize well extreme poses textureless object <eos> work explore use three dimensional shape models detect object video unsupervised manner <eos> call problem motion structure mfs given set point trajectories three dimensional model object interest find subset trajectories correspond three dimensional model estimate its alignment <eos> compute motion matrix <eos> mfs related structure motion sfm motion segmentation problems unlike sfm structure object known but correspondence between trajectories object unknown unlike motion segmentation mfs problem incorporates three dimensional structure providing robustness tracking mismatches outliers <eos> experiments illustrate how mfs algorithm outperforms alternative approaches both synthetic data real video extracted youtube <eos> <eop> volumetric multi view cnn object classification three dimensional data <eos> three dimensional shape models becoming widely available easier capture making available three dimensional information crucial progress object classification <eos> current state art method rely cnn address problem <eos> recently witness two types cnn being developed cnn based upon volumetric representations versus cnn based upon multi view representations <eos> empirical result two types cnn exhibit large gap indicating existing volumetric cnn architectures approaches unable fully exploit power three dimensional representations <eos> paper aim improve both volumetric cnn multi view cnn according extensive analysis existing approaches <eos> end introduce two distinct network architectures volumetric cnn <eos> addition examine multi view cnn introduce multi resolution filtering three dimensional overall able outperform current state art method both volumetric cnn multi view cnn <eos> provide extensive experiments designed evaluate underlying design choices thus providing better understanding space method available object classification three dimensional data <eos> <eop> detecting vanishing point using global image context non manhattan world <eos> propose novel method detecting horizontal vanishing point zenith vanishing point man made environments <eos> dominant trend existing method first find candidate vanishing point then remove outliers enforcing mutual orthogonality <eos> method reverses process propose set horizon line candidates score each based vanishing point contains <eos> key element approach use global image context extracted deep convolutional network constrain set candidates under consideration <eos> method make manhattan world assumption operate effectively scenes only single horizontal vanishing point <eos> evaluate approach three benchmark datasets achieve state art performance each <eos> addition approach significantly faster than previous best method <eos> <eop> learning weight uncertainty stochastic gradient mcmc shape classification <eos> learning representation shape cues three dimensional object recognition fundamental task computer vision <eos> deep neural network dnns shown promising performance task <eos> due large variability shapes accurate recognition relies good estimates model uncertainty ignored traditional training dnns typically learned via stochastic optimization <eos> paper leverages recent advances stochastic gradient markov chain monte carlo sg mcmc learn weight uncertainty dnns <eos> yields principled bayesian interpretations commonly used dropout dropconnect techniques incorporates them into sg mcmc framework <eos> extensive experiments three dimensional shape datasets various dnn models demonstrate superiority proposed approach over stochastic optimization <eos> approach yields higher recognition accuracy when used conjunction dropout batch normalization <eos> <eop> field model repairing three dimensional shapes <eos> paper proposes field model repairing three dimensional shapes constructed multi view rgb data <eos> specifically represent three dimensional shape markov random field mrf geometric information encoded random binary variables appearance information retrieved set rgb image captured multiple viewpoints <eos> local priors mrf model capture local structures object shapes learnt three dimensional shape templates using convolutional deep belief network <eos> repairing three dimensional shape formulated maximum posteriori map estimation corresponding mrf <eos> variational mean field approximation technique adopted map estimation <eos> proposed method was evaluated both artificial data real data obtained reconstruction practical scenes <eos> experimental result shown robustness efficiency proposed method repairing noisy incomplete three dimensional shapes <eos> <eop> gogma globally optimal gaussian mixture alignment <eos> gaussian mixture alignment family approaches frequently used robustly solving point set registration problem <eos> however since they use local optimisation they susceptible local minima only guarantee local optimality <eos> consequently their accuracy strongly dependent quality initialisation <eos> paper presents first globally optimal solution three dimensional rigid gaussian mixture alignment problem under distance between mixtures <eos> algorithm named gogma employs branch bound approach search space three dimensional rigid motions se guaranteeing global optimality regardless initialisation <eos> geometry se was used find novel upper lower bounds objective function local optimisation was integrated into scheme accelerate convergence without voiding optimality guarantee <eos> evaluation empirically supported optimality proof showed method performed much more robustly two challenging datasets than existing globally optimal registration solution <eos> <eop> efficient deep learning stereo matching <eos> past year convolutional neural network shown perform extremely well stereo estimation <eos> however current architectures rely siamese network exploit concatenation followed further processing layer requiring minute gpu computation per image pair <eos> contrast paper propose matching network able produce very accurate result less than second gpu computation <eos> towards goal exploit product layer simply computes inner product between two representations siamese architecture <eos> train network treating problem multi class classification classes all possible disparities <eos> allows get calibrated scores result much better matching performance when compared existing approaches <eos> <eop> efficient coarse fine patchmatch large displacement optical flow <eos> key component many computer vision systems optical flow estimation especially large displacements remains open problem <eos> paper present simple but powerful matching method works coarse fine scheme optical flow estimation <eos> inspired nearest neighbor field nnf algorithms approach called cpm coarse fine patchmatch blends efficient random search strategy coarse fine scheme optical flow problem <eos> unlike existing nnf techniques efficient but result often too noisy optical flow caused lack global regularization propose propagation step constrained random search radius between adjacent levels hierarchical architecture <eos> resulting correspondences enjoys built smoothing effect more suited optical flow estimation than nnf techniques <eos> furthermore approach also capture tiny structures large motions problem traditional coarse fine optical flow algorithms <eos> interpolated edge preserving interpolation method epicflow method outperforms state art mpi sintel kitti runs much faster than competing method <eos> <eop> fanng fast approximate nearest neighbour graphs <eos> present new method approximate nearest neighbour search large datasets high dimensional feature vectors such sift gist descriptors <eos> approach constructs directed graph efficiently explored nearest neighbour queries <eos> each vertex graph represents feature vector dataset being searched <eos> directed edges computed exploiting fact datasets intrinsic dimensionality local manifold like structure formed elements dataset significantly lower than embedding space <eos> also provide efficient search algorithm uses graph rapidly find nearest neighbour query high probability <eos> show how method adapted give strong guarantee recall query within threshold distance its nearest neighbour <eos> demonstrate method significantly more efficient than existing state art method <eos> particular gpu implementation deliver recall queries data set million sift descriptors rate over <eos> million queries per second titan <eos> finally also demonstrate how method scales datasets entries <eos> <eop> exemplar driven top down saliency detection via deep association <eos> top down saliency detection knowledge driven search task <eos> while some previous method aim learn knowledge category specific data others transfer existing annotations large dataset through appearance matching <eos> contrast propose paper locate exemplar strategy <eos> approach challenging only use few exemplars up appearances among query object exemplars very different <eos> address design two stage deep model learn intra class association between exemplars query object <eos> first stage learning object object association second stage learn background discrimination <eos> extensive experimental evaluations show proposed method outperforms different baselines category specific models <eos> addition explore influence exemplar properties terms exemplar number quality <eos> furthermore show learned model universal model offers great generalization unseen object <eos> <eop> unconstrained salient object detection via proposal subset optimization <eos> aim detecting salient object unconstrained image <eos> unconstrained image number salient object if any varies image image given <eos> present salient object detection system directly outputs compact set detection windows if any input image <eos> system leverages convolutional neural network model generate location proposals salient object <eos> location proposals tend highly overlapping noisy <eos> based maximum posteriori principle propose novel subset optimization framework generate compact set detection windows out noisy proposals <eos> experiments show subset optimization formulation greatly enhances performance system system attains relative improvement average precision compared state art three challenging salient object datasets <eos> <eop> recombinator network learning coarse fine feature aggregation <eos> deep neural network alternating convolutional max pooling decimation layer widely used state art architectures computer vision <eos> max pooling purposefully discards precise spatial information order create feature more robust typically organized lower resolution spatial feature maps <eos> some tasks such whole image classification max pooling derived feature well suited however tasks requiring precise localization such pixel level prediction segmentation max pooling destroys exactly information required perform well <eos> precise localization may preserved shallow convnets without pooling but expense robustness <eos> max pooled multi layered cake eat too several papers proposed summation concatenation based method combining upsampled coarse abstract feature finer feature produce robust pixel level predictions <eos> here introduce another model dubbed recombinator network coarse feature inform finer feature early their formation such finer feature make use several layer computation deciding how use coarse feature <eos> model trained once end end performs better than summation based architectures reducing error previous state art two facial keypoint datasets afw aflw beating current state art without using extra data <eos> improve performance even further adding denoising prediction model based novel convnet formulation <eos> <eop> end end saliency mapping via probability distribution prediction <eos> most saliency estimation method aim explicitly model low level conspicuity cues such edges blobs may additionally incorporate top down cues using face text detection <eos> data driven method training saliency models using eye fixation data increasingly popular particularly introduction large scale datasets deep architectures <eos> however current method latter paradigm use loss functions designed classification regression tasks whereas saliency estimation evaluated topographical maps <eos> work introduce new saliency map model formulates map generalized bernoulli distribution <eos> then train deep architecture predict such maps using novel loss functions pair softmax activation function measures designed compute distances between probability distributions <eos> show extensive experiments effectiveness such loss functions over standard ones four public benchmark datasets demonstrate improved performance over state art saliency method <eos> <eop> paradigm building generalized models human image perception through data fusion <eos> many sub fields researchers collect datasets human ground truth used create new algorithm <eos> example research image perception datasets collected topics such makes image aesthetic memorable <eos> despite high costs human data collection datasets infrequently reused beyond their own fields interest <eos> moreover algorithms built them domain specific predict small set attributes usually unconnected one another <eos> paper present paradigm building generalized expandable models human image perception <eos> first fuse multiple fragmented partially overlapping datasets through data imputation <eos> then create theoretically structured statistical model human image perception fit fused datasets <eos> resulting model many advantages <eos> generalized going beyond content constituent datasets easily expanded fusing additional datasets <eos> provides new ontology usable network expand human data cost effective way <eos> guide design generalized computational algorithm multi dimensional visual perception <eos> indeed experimental result show model based algorithm outperforms state art method predicting visual sentiment visual realism interestingness <eos> paradigm used various visual tasks <eos> <eop> longitudinal face modeling via temporal deep restricted boltzmann machines <eos> modeling face aging process challenging task due large non linear variations present different stages face development <eos> paper presents deep model approach face age progression efficiently capture non linear aging process automatically synthesize series age progressed faces various age ranges <eos> approach first decompose long term age progress into sequence short term changes model face sequence <eos> temporal deep restricted boltzmann machines based age progression model together prototype faces then constructed learn aging transformation between faces sequence <eos> addition enhance wrinkles faces later age ranges wrinkle models further constructed using restricted boltzmann machines capture their variations different facial region <eos> geometry constraints also taken into account last step more consistent age progressed result <eos> proposed approach evaluated using various face aging databases <eos> fg net cross age celebrity dataset cacd morph collected large scale aging database named aging faces wild agfw <eos> addition when ground truth age available input image proposed system able automatically estimate age input face before aging process employed <eos> <eop> saliency unified deep architecture simultaneous eye fixation prediction salient object segmentation <eos> human eye fixations often correlate locations salient object scene <eos> however only handful approaches attempted simultaneously address related aspects eye fixations object saliency <eos> work propose deep convolutional neural network cnn capable predicting eye fixations segmenting salient object unified framework <eos> design initial network layer shared between both tasks such they capture object level semantics global contextual aspects saliency while deeper layer network address task specific aspects <eos> addition network captures saliency multiple scales via inception style convolution blocks <eos> network shows significant improvement over current state art both eye fixation prediction salient object segmentation across number challenging datasets <eos> <eop> estimating correspondences deformable object wild <eos> during past few years witnessed development many methodologies building fitting statistical deformable models sdms <eos> construction accurate sdms requires careful annotation image regards consistent set landmarks <eos> however manual annotation large amount image tedious laborious expensive procedure <eos> furthermore several deformable object <eos> human body difficult define consistent set landmarks thus becomes impossible train humans order accurately annotate collection image <eos> nevertheless majority object possible extract shape object segmentation even shape drawing <eos> paper show first time best knowledge possible construct sdms putting object shapes dense correspondence <eos> such sdms built much less effort large battery object <eos> additionally show sampling dense model part based sdm learned its parts being correspondence <eos> employ framework develop sdms human arms legs used segmentation outline human body well provide better more consistent annotations body joints <eos> <eop> gravitational approach point set registration <eos> paper new astrodynamics inspired rigid point set registration algorithm introduced gravitational approach ga <eos> formulate point set registration modified body problem additional constraints obtain algorithm unique properties fully scalable number processing cores <eos> ga template point set moves viscous medium under gravitational forces induced reference point set <eos> pose updates completed numerically solving differential equations newtonian mechanics <eos> discuss techniques efficient implementation new algorithm evaluate several synthetic real world scenarios <eos> ga compared widely used iterative closest point state art rigid coherent point drift algorithms <eos> experiments evidence new approach robust against noise handle challenging scenarios structured outliers <eos> <eop> context aware gaussian fields non rigid point set registration <eos> point set registration psr fundamental problem computer vision pattern recognition successfully applied many applications <eos> although widely used existing psr method cannot align point set robustly under degradations such deformation noise occlusion outlier rotation multi view changes <eos> paper proposes context aware gaussian fields ca lapgf non rigid psr subject global rigid local non rigid geometric constraints laplacian regularized term added preserve intrinsic geometry transformed set <eos> ca lapgf uses robust objective function quasi newton algorithm estimate likely correspondences non rigid transformation parameters between two point set iteratively <eos> ca lapgf estimate non rigid transformations mapped reproducing kernel hilbert spaces accurately robustly presence degradations <eos> experimental result synthetic real image reveal how ca lapgf outperforms state art algorithms non rigid psr <eos> <eop> trust no one low rank matrix factorization using hierarchical ransac <eos> paper present system performing low rank matrix factorization <eos> low rank matrix factorization essential problem many areas including computer vision applications <eos> affine structure motion photometric stereo non rigid structure motion <eos> specifically target structured data patterns outliers large amounts missing data <eos> using recently developed characterizations minimal solutions matrix factorization problems missing data show how used building blocks hierarchical system performs bootstrapping all levels <eos> gives robust fast system state art performance <eos> <eop> relaxation based preprocessing techniques markov random field inference <eos> markov random fields mrfs widely used graphical model but inference problem np hard <eos> first order mrfs binary labels dead end elimination dee qpbo find optimal labeling some variables much harder case larger label set addressed kovtun related method impose substantial computational overhead <eos> describe efficient algorithm correctly label subset variables arbitrary mrfs particularly good performance binary mrfs <eos> propose sufficient condition check if partial labeling optimal generalization dee purely local test <eos> give hierarchy relaxations provide larger optimal partial labelings cost additional computation <eos> empirical studies were conducted several benchmarks using expansion moves inference <eos> algorithm runs few seconds improves speed mrf inference expansion moves factor <eos> <eop> sparse coding classification via discrimination ensemble <eos> discriminative sparse coding emerged promising technique image analysis recognition couples process classifier training process dictionary learning improving discriminability sparse codes <eos> many existing approaches consider only simple single linear classifier whose discriminative power rather weak <eos> paper proposed discriminative sparse coding method jointly learns dictionary sparse coding ensemble classifier discrimination <eos> ensemble classifier composed set linear predictors constructed via both subsampling data subspace projection sparse codes <eos> advantages proposed method over existing ones multi fold better discriminability sparse codes weaker dependence peculiarities training data more expressibility classifier classification <eos> advantages also justified experiments method outperformed several state art method several recognition tasks <eos> <eop> principled parallel mean field inference discrete random fields <eos> mean field variational inference one most popular approaches inference discrete random fields <eos> standard mean field optimization based coordinate descent many situations impractical <eos> thus practice various parallel techniques used either rely ad hoc smoothing heuristically set parameters put strong constraints type models <eos> paper propose novel proximal gradient based approach optimizing variational objective <eos> naturally parallelizable easy implement <eos> prove its convergence then demonstrate practice yields faster convergence often finds better optima than more traditional mean field optimization techniques <eos> moreover method less sensitive choice parameters <eos> <eop> guaranteed outlier removal mixed integer linear programs <eos> maximum consensus problem fundamentally important robust geometric fitting computer vision <eos> solving problem exactly computationally demanding effort required increases rapidly problem size <eos> although randomized algorithms much more efficient optimality solution guaranteed <eos> towards goal solving maximum consensus exactly present guaranteed outlier removal technique reduce runtime exact algorithms <eos> specifically before conducting global optimization attempt remove data provably true outliers <eos> exist maximum consensus set <eos> propose algorithm based mixed integer linear programming perform removal <eos> result algorithm smaller data instance admits much faster solution subsequent exact algorithm while yielding same globally optimal result original problem <eos> demonstrate overall speedups up achieved common vision problems <eos> <eop> memory efficient max flow multi label submodular mrfs <eos> multi label submodular markov random fields mrfs shown solvable using max flow based encoding labels proposed ishikawa each variable represented nodes number labels arranged column <eos> however method general requires edges each pair neighbouring variables <eos> makes inapplicable realistic problems many variables labels due excessive memory requirement <eos> paper introduce variant max flow algorithm requires much less storage <eos> consequently algorithm makes possible optimally solve multi label submodular problems involving large numbers variables labels standard computer <eos> <eop> proximal riemannian pursuit large scale trace norm minimization <eos> trace norm regularization plays important role many areas such machine learning computer vision <eos> solving trace norm regularized trace norm regularization plays important role many areas such computer vision machine learning <eos> when solving general large scale trace norm regularized problems existing method may computationally expensive due many high dimensional truncated singular value decompositions svds unawareness matrix ranks <eos> paper propose proximal riemannian pursuit prp paradigm addresses sequence trace norm regularized subproblems defined nonlinear matrix varieties <eos> address subproblem extend proximal gradient method vector space nonlinear matrix varieties svds intermediate solutions maintained cheap low rank qr decompositions therefore making proposed method more scalable <eos> empirical studies several tasks such matrix completion low rank representation based subspace clustering demonstrate competitive performance proposed paradigms over existing method <eos> <eop> minimizing maximal rank <eos> computer vision many problems formulated finding low rank approximation given measurement matrix <eos> ideally if all elements measurement matrix available easily solved norm using factorization <eos> however practice rarely case <eos> lately problem addressed using different approaches one replace rank term convex nuclear norm another derive convex envelope rank term plus data term <eos> latter case matrices divided into sub matrices envelope computed each sub block individually <eos> paper new convex envelope derived takes all sub matrices into account simultaneously <eos> leads simpler formulation using only one parameter applications one seeks low rank approximations multiple matrices same rank <eos> show paper how general framework used manifold denoising several image once well just denoising one image <eos> get comparable result other well known method framework also used other applications such linear shape models <eos> <eop> solving temporal puzzles <eos> many physical phenomena within short time windows explained low order differential relations <eos> discrete world relations described using low order difference equations equivalently low order auto regressive ar models <eos> paper based intuition propose algorithm solving time sort temporal puzzles defined scrambled time series need sorted out <eos> frame highly combinatorial problem using mixed integer semi definite programming formulation show how turn into mixed integer linear programming problem using recently introduced atomic norm framework <eos> experiments show effectiveness generality approach different scenarios <eos> <eop> estimating sparse signals smooth support via convex programming block sparsity <eos> conventional algorithms sparse signal recovery sparse representation rely norm regularized variational method <eos> however when applied reconstruction sparse image <eos> image only few pixels non zero simple norm based method ignore poten tial correlations support between adjacent pixels <eos> number applications one interested image only sparse but also support smooth contiguous boundaries <eos> existing algorithms take into account such support structure mostly rely non convex method consequence scale well high dimensional problems converge global optima <eos> paper explore use new block norm regularizers enforce image sparsity while simultaneously promoting smooth support structure <eos> exploiting convexity regularizers develop new computationally efficient recovery algorithms guarantee global optimality <eos> demonstrate efficacy regularizers variety imaging tasks including compressive image recovery image restoration robust pca <eos> <eop> tensr multi dimensional tensor sparse representation <eos> conventional sparse model relies data representation form vectors <eos> represents vector valued vectorized one dimensional version signal highly sparse linear combination basis atoms large dictionary <eos> modeling though simple ignores inherent structure breaks local correlation inside multidimensional md signals <eos> also dramatically increases demand memory well computational resources especially when dealing high dimensional signals <eos> paper propose new sparse model tensr based tensor md data representation along corresponding md sparse coding md dictionary learning algorithms <eos> proposed tensr model able well approximate structure each mode inherent md signals series adaptive separable structure dictionaries via dictionary learning <eos> proposed md sparse coding algorithm proximal method further reduces computational cost significantly <eos> experimental result real world md signals <eos> three dimensional multi spectral image show proposed tensr greatly reduces both computational memory costs competitive performance comparison state art sparse representation method <eos> believe proposed tensr model promising way empower sparse representation especially large scale high order signals <eos> <eop> moral lineage tracing <eos> lineage tracing tracking living cells they move divide central problem biological image analysis <eos> solutions called lineage forests key understanding how structure multicellular organisms emerges <eos> propose integer linear program ilp whose feasible solutions define every image sequence decomposition into cells segmentation across image lineage forest cells tracing <eos> ilp path cut inequalities enforce morality lineages <eos> constraint cells merge <eos> find feasible solutions np hard problem certified bounds global optimum define efficient separation procedures apply part branch cut algorithm <eos> show effectiveness approach analyze feasible solutions real microscopy data terms bounds run time their weighted edit distance lineage forests traced humans <eos> <eop> globally optimal rigid intensity based registration fast fourier domain approach <eos> high computational cost main obstacle adapting globally optimal branch bound algorithms intensity based registration <eos> existing techniques speed up such algorithms use multiresolution pyramid image bounds target function among different resolutions rigidly aligning two image <eos> paper propose dual algorithm optimization done fourier domain multiple resolution levels replaced multiple frequency bands <eos> algorithm starts computing target function lower frequency bands keeps adding higher frequency bands until current subregion either rejected divided into smaller areas branch bound manner <eos> unlike spatial multiresolution approaches compute target function wider frequency area one just needs compute target residual bands <eos> therefore if area discarded performs just enough computations required rejection <eos> property also enables use rather large number frequency bands compared limited number resolution levels used space domain algorithm <eos> experimental result real image demonstrate considerable speed gains over space domain method most cases <eos> <eop> benefits selection diversity via bilevel exclusive sparsity <eos> sparse feature dictionary selection critical various tasks computer vision machine learning pattern recognition avoid overfitting <eos> while extensive research efforts conducted feature selection using sparsity group sparsity note there lack development applications there particular preference diversity <eos> selected feature expected come different groups categories <eos> diversity preference motivated many real world applications such advertisement recommendation privacy image classification design survey <eos> paper proposed general bilevel exclusive sparsity formulation pursue diversity restricting overall sparsity sparsity each group <eos> solve proposed formulation np hard general heuristic procedure proposed <eos> main contributions paper include linear convergence rate established proposed algorithm provided theoretical error bound improves approaches such norm types method only use overall sparsity quantitative benefits using diversity sparsity provided <eos> best knowledge first work show theoretical benefits using diversity sparsity extensive empirical studies provided validate proposed formulation algorithm theory <eos> <eop> fast training triplet based deep binary embedding network <eos> paper aim learn mapping embedding image compact binary space hamming distances correspond ranking measure image retrieval task <eos> make use triplet loss because shown most effective ranking problems <eos> how ever training previous works prohibitively expensive due fact optimization directly performed triplet space number possible triplets training cubic number training examples <eos> address issue propose formulate high order binary codes learning multi label classification problem explicitly separating learning into two interleaved stages <eos> solve first stage design large scale high order binary codes inference algorithm reduce high order objective standard binary quadratic problem such graph cuts used efficiently infer binary codes serve labels each training datum <eos> second stage propose map original image compact binary codes via carefully designed deep convolutional neural network cnn hash ing function fitting solved training binary cnn classifiers <eos> incremental interleaved optimization strategy proffered ensure two steps interactive each other during training better accuracy <eos> conduct experiments several benchmark datasets demonstrate both improved training time much two orders magnitude well producing state art hashing various retrieval tasks <eos> <eop> marr revisited three dimensional alignment via surface normal prediction <eos> introduce approach leverages surface normal predictions along appearance cues retrieve three dimensional models object depicted still image large cad object library <eos> critical success approach ability recover accurate surface normals object depicted scene <eos> introduce skip network model built pre trained oxford vgg convolutional neural network surface normal prediction <eos> model achieves state art accuracy nyuv rgb dataset surface normal prediction recovers fine object detail compared previous method <eos> furthermore develop two stream network over input image predicted surface normals jointly learns pose style cad model retrieval <eos> when using predicted surface normals two stream network matches prior work using surface normals computed rgb image task pose prediction achieves state art when using rgb input <eos> finally two stream network allows retrieve cad models better match style pose depicted object compared baseline approaches <eos> <eop> recovering missing link predicting class attribute associations unsupervised zero shot learning <eos> collecting training image all visual categories only expensive but also impractical <eos> zero shot learning zsl especially using attributes offers pragmatic solution problem <eos> however test time most attribute based method require full description attribute associations each unseen class <eos> providing associations time consuming often requires domain specific knowledge <eos> work aim carry out attribute based zero shot classification unsupervised manner <eos> propose approach learn relations couples class embeddings their corresponding attributes <eos> given only name unseen class learned relationship model used automatically predict class attribute associations <eos> furthermore model facilitates transferring attributes across data set without additional effort <eos> integrating knowledge multiple sources result significant additional improvement performance <eos> evaluate two public data set animals attributes apascal ayahoo <eos> approach outperforms state art method both predicting class attribute associations unsupervised zsl large margin <eos> <eop> fast zero shot image tagging <eos> well known word analogy experiments show recent word vectors capture fine grained linguistic regularities words linear vector offsets but unclear how well simple vector offsets encode visual regularities over words <eos> study particular image word relevance relation paper <eos> result tell given image its relevant tags word vectors rank ahead irrelevant tags along principal direction word vector space <eos> inspired observation propose solve image tagging estimating principal direction image <eos> particularly exploit linear mappings nonlinear deep neural network approximate principal direction input image <eos> arrive quite versatile tagging model <eos> runs fast given test image constant time <eos> training set size <eos> only gives rise superior performance conventional tagging task nus wide dataset but also outperforms competitive baselines annotating image previously unseen tags <eos> end name approach fast zero shot image tagging fast tag recognize possesses advantages both fasttag chen <eos> zero shot learning <eos> <eop> modality component aware feature fusion rgb scene classification <eos> while convolutional neural network cnn excellent object recognition greater spatial variability scene image typically meant standard full image cnn feature suboptimal scene classification <eos> paper investigate framework allowing greater spatial flexibility fisher vector fv encoded distribution local cnn feature obtained multitude region proposals per image considered instead <eos> cnn feature computed augmented pixel wise representation comprising multiple modalities rgb hha surface normals extracted rgb data <eos> more significantly make two postulates component sparsity only small variety region proposals their corresponding fv gmm components contribute scene discriminability modal non sparsity within discriminative components all modalities important contribution <eos> framework implemented through regularization terms applying group lasso gmm components exclusive group lasso across modalities <eos> learning combining regressors both proposal based fv feature global cnn feature were able achieve state art scene classification performance sunrgbd dataset nyu depth dataset <eos> <eop> ppp joint pointwise pairwise image label prediction <eos> pointwise label pairwise label both widely used computer vision tasks <eos> example supervised image classification annotation approaches use pointwise label while attribute based image relative learning often adopts pairwise labels <eos> two types labels often considered independently most existing efforts utilize them separately <eos> however pointwise labels image classification tag annotation inherently related pairwise labels <eos> example image labeled coast annotated beach sea sand sky more likely higher ranking score terms attribute open while men shoes ranked highly attribute formal likely annotated leather lace up than buckle fabric <eos> existence potential relations between pointwise labels pairwise labels motivates fuse them together jointly addressing related vision tasks <eos> particular provide principled way capture relations between class labels tags attributes propose novel framework ppp pointwise pairwise image label prediction based overlapped group structure extracted pointwise pairwise label bipartite graph <eos> experiments benchmark datasets demonstrate proposed framework achieves superior performance three vision tasks compared state art method <eos> <eop> cataloging public object using aerial street level image urban trees <eos> each corner inhabited world imaged multiple viewpoints increasing frequency <eos> online map services like google maps here maps provide direct access huge amounts densely sampled georeferenced image street view aerial perspective <eos> there opportunity design computer vision systems will help search catalog monitor public infrastructure buildings artifacts <eos> explore architecture feasibility such system <eos> main technical challenge combining test time information multiple views each geographic location <eos> aerial street views <eos> implement two modules det geo detects set loca tions object belonging given category geo cat computes fine grained category object given location <eos> introduce solution adapts state art cnn based object detectors classifiers <eos> test method pasadena urban trees new dataset trees geographic species annotations show combining multiple views significantly improves both tree detection tree species classification rivaling human performance <eos> <eop> deep exemplar three dimensional detection adapting real rendered views <eos> paper presents end end convolutional neural network cnn three dimensional exemplar detection <eos> demonstrate ability adapt feature natural image better align cad rendered views critical success technique <eos> show adaptation learned compositing rendered views textured object models natural image <eos> approach naturally incorporated into cnn detection pipeline extends accuracy speed benefits recent advances deep learning three dimensional exemplar detection <eos> applied method two tasks instance detection evaluated ikea dataset object category detection out perform aubry <eos> chair detection subset pascal voc dataset <eos> <eop> zero shot learning via joint latent similarity embedding <eos> zero shot recognition zsr deals problem predicting class labels target domain instances based source domain side information <eos> attributes unseen classes <eos> formulate zsr binary prediction problem <eos> resulting classifier class independent <eos> takes arbitrary pair source target domain instances input predicts whether they come same class <eos> whether there match <eos> model posterior probability match since sufficient statistic propose latent probabilistic model context <eos> develop joint discriminative learning framework based dictionary learning jointly learn parameters model both domains ultimately leads class independent classifier <eos> many existing embedding method viewed special cases probabilistic model <eos> zsr method shows <eos> improvement over state art accuracy averaged across four benchmark datasets <eos> also adapt zsr method zero shot retrieval show <eos> improvement accordingly mean average precision map <eos> <eop> craft object image <eos> object detection fundamental problem image understanding <eos> one popular solution cnn framework its fast versions <eos> they decompose object detection problem into two cascaded easier tasks generating object proposals image classifying proposals into various object categories <eos> despite handling two relatively easier tasks they solved perfectly there still room improvement <eos> paper push divide conquer solution even further dividing each task into two sub tasks <eos> call proposed method craft cascade region proposal network fast rcnn tackles each task carefully designed network cascade <eos> show cascade structure helps both tasks proposal generation provides more compact better localized object proposals object classification reduces false positives mainly between ambiguous categories capturing both inter intra category variances <eos> craft achieves consistent considerable improvement over state art object detection benchmarks like pascal voc ilsvrc <eos> <eop> 