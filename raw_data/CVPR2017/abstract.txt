graph structured representations visual question answering <eos> paper proposes improve visual question answering vqa structured representations both scene contents questions <eos> key challenge vqa require joint reasoning over visual text domains <eos> predominant cnn lstm based approach vqa limited monolithic vector representations largely ignore structure scene question <eos> cnn feature vectors cannot effectively capture situations simple multiple object instances lstms process questions series words reflect true complexity language structure <eos> instead propose build graphs over scene object over question words describe deep neural network exploits structure representations <eos> show approach achieves significant improvements over state art increasing accuracy <eos> accuracy abstract scenes multiple choice benchmark <eos> accuracy over pairs balanced scenes <eos> image fine grained differences opposite yes no answers same question <eos> <eop> physics inspired optimization semantic transfer feature alternative method room layout estimation <eos> paper propose alternative method estimate room layouts cluttered indoor scenes <eos> method enjoys benefits two novel techniques <eos> first one semantic transfer st formulation integrate relationship between scene clutter room layout into convolutional neural network architecture end end trained practical strategy initialize weights very deep network under unbalanced training data distribution <eos> st allows extract highly robust feature under various circumstances order address computation redundance hidden feature develop principled efficient inference scheme named physics inspired optimization pio <eos> pio basic idea formulate some phenomena observed st feature into mechanics concepts <eos> evaluations public datasets lsun hedau show proposed method more accurate than state art method <eos> <eop> local binary convolutional neural network <eos> propose local binary convolution lbc efficient alternative convolutional layer standard convolutional neural network cnn <eos> design principles lbc motivated local binary patterns lbp <eos> lbc layer comprises set fixed sparse pre defined binary convolutional filters updated during training process non linear activation function set learnable linear weights <eos> linear weights combine activated filter responses approximate corresponding activated filter responses standard convolutional layer <eos> lbc layer affords significant parameter savings number learnable parameters compared standard convolutional layer <eos> furthermore sparse binary nature weights also result up savings model size compared standard convolutional layer <eos> demonstrate both theoretically experimentally local binary convolution layer good approximation standard convolutional layer <eos> empirically cnn lbc layer called local binary convolutional neural network lbcnn achieves performance parity regular cnn range visual datasets mnist svhn cifar imagenet while enjoying significant computational savings <eos> <eop> designing effective inter pixel information flow natural image matting <eos> present novel purely affinity based natural image matting algorithm <eos> method relies carefully defined pixel pixel connections enable effective use information available image trimap <eos> control information flow known opacity region into unknown region well within unknown region itself utilizing multiple definitions pixel affinities <eos> way achieve significant improvements matte quality near challenging region foreground object <eos> among other forms information flow introduce color mixture flow builds upon local linear embedding effectively encapsulates relation between different pixel opacities <eos> resulting novel linear system formulation solved closed form robust against several fundamental challenges natural matting such holes remote intricate structures <eos> while method primarily designed standalone natural matting tool show also used regularizing mattes obtained various sampling based method <eos> evaluation using public alpha matting benchmark suggests significant performance improvement over state art <eos> <eop> face normals wild using fully convolutional network <eos> work pursue data driven approach problem estimating surface normals single intensity image focusing particular human faces <eos> introduce new method exploit currently available facial databases dataset construction tailor deep convolutional neural network task estimating facial surface normals wild <eos> train fully convolutional network accurately recover facial normals image including challenging variety expressions facial poses <eos> compare against state art face shape shading three dimensional reconstruction techniques show proposed network recover substantially more accurate realistic normals <eos> furthermore contrast other existing face specific surface recovery method require solving explicit alignment step due fully convolutional nature network <eos> <eop> face morphable models wild <eos> three dimensional morphable models dmms powerful statistical models three dimensional facial shape texture among state art method reconstructing facial shape single image <eos> advent new three dimensional sensors many three dimensional facial datasets collected containing both neutral well expressive faces <eos> however all datasets captured under controlled conditions <eos> thus even though powerful three dimensional facial shape models learnt such data difficult build statistical texture models sufficient reconstruct faces captured unconstrained conditions wild <eos> paper propose first best knowledge wild dmm combining powerful statistical model facial shape describes both identity expression wild texture model <eos> show employment such wild texture model greatly simplifies fitting procedure because there no need optimise regards illumination parameters <eos> furthermore propose new fast algorithm fitting dmm arbitrary image <eos> finally captured first three dimensional facial database relatively unconstrained conditions report quantitative evaluations state art performance <eos> complementary qualitative reconstruction result demonstrated standard wild facial databases <eos> <eop> towards quality metric dense light fields <eos> light fields become popular representation three dimensional scenes there interest their processing resampling compression <eos> operations often result loss quality there need quantify <eos> work collect new dataset dense reference distorted light fields well corresponding quality scores scaled perceptual units <eos> scores were acquired subjective experiment using interactive light field viewing setup <eos> dataset contains typical artifacts occur light field processing chain due light field reconstruction multi view compression limitations automultiscopic displays <eos> test number existing objective quality metrics determine how well they predict quality light fields <eos> find existing image quality metrics provide good measures light field quality but require dense reference light fields optimal performance <eos> more complex tasks comparing two distorted light fields their performance drops significantly reveals need new light field specific metrics <eos> <eop> position tracking virtual reality using commodity wifi <eos> today experiencing virtual reality vr cumbersome experience either requires dedicated infrastructure like infrared cameras track headset hand motion controllers <eos> oculus rift htc vive provides only dof degrees freedom tracking severely limits user experience <eos> truly enable vr everywhere need position tracking available ubiquitous service <eos> paper presents wicapture novel approach leverages commodity wifi infrastructure ubiquitous today tracking purposes <eos> prototype wicapture using off shelf wifi radios show achieves accuracy <eos> cm compared sophisticated infrared based tracking systems like oculus while providing much higher range resistance occlusion ubiquity ease deployment <eos> <eop> material classification using frequency depth dependent time flight distortion <eos> paper presents material classification method using off shelf time flight tof camera <eos> use key observation depth measurement tof camera distorted object certain materials especially translucent materials <eos> show distortion caused variations time domain impulse responses across materials also measurement mechanism existing tof cameras <eos> specifically reveal amount distortion varies according modulation frequency tof camera material object distance between camera object <eos> method uses depth distortion tof measurements feature achieves material classification scene <eos> effectiveness proposed method demonstrated numerical evaluation real world experiments showing its capability even classifying visually similar object <eos> <eop> learning association versatile semi supervised training method neural network <eos> many real world scenarios labeled data specific machine learning task costly obtain <eos> semi supervised training method make use abundantly available unlabeled data smaller number labeled examples <eos> propose new framework semi supervised training deep neural network inspired learning humans <eos> associations made embeddings labeled sample unlabeled ones back <eos> optimization schedule encourages correct association cycles end up same class association was started penalizes wrong associations ending different class <eos> implementation easy use added any existing end end training setup <eos> demonstrate capabilities learning association several data set show improve performance classification tasks tremendously making use additionally available unlabeled data <eos> particular cases few labeled data training scheme outperforms current state art svhn <eos> <eop> non convex variational approach photometric stereo under inaccurate lighting <eos> paper tackles photometric stereo problem presence inaccurate lighting obtained either calibration uncalibrated photometric stereo method <eos> based precise modeling noise outliers robust variational approach introduced <eos> explicitly accounts self shadows enforces robustness cast shadows specularities resorting redescending estimators <eos> resulting non convex model solved means computationally efficient alternating reweighted least squares algorithm <eos> since implicitly enforces integrability new variational approach refine both intensities directions lighting <eos> <eop> learning synthetic humans <eos> estimating human pose shape motion image video fundamental challenges many applications <eos> recent advances human pose estimation use large amounts manually labeled training data learning convolutional neural network cnn <eos> such data time consuming acquire difficult extend <eos> moreover manual labeling three dimensional pose depth motion impractical <eos> work present surreal new large scale dataset synthetically generated but realistic image people rendered three dimensional sequences human motion capture data <eos> generate more than million frames together ground truth pose depth maps segmentation masks <eos> show cnn trained synthetic dataset allow accurate human depth estimation human part segmentation real rgb image <eos> result new datast open up new possibilities advancing person analysis using chap large scale synthetic data <eos> <eop> correlational gaussian processes cross domain visual recognition <eos> present probabilistic model captures higher order co occurrence statistics joint visual recognition collection image across multiple domains <eos> more importantly predict structured output across multiple domains correlating outputs multi classes gaussian process classifiers each individual domain <eos> set correlational tensors adopted model relationship within single domain well across multiple domains <eos> renders possible explore high order relational model instead using just set pairwise relational models <eos> such tensor relations based both positive negative co occurrences different categories visual instances across multi domains <eos> contrast most previous models only pair wise relationships explored <eos> conduct experiments four challenging image collections <eos> experimental result clearly demonstrate efficacy proposed model <eos> <eop> revisiting variable projection method separable nonlinear least squares problems <eos> variable projection varpro framework solve optimization problems efficiently optimally eliminating subset unknowns <eos> particular adapted separable nonlinear least squares snls problems class optimization problems including low rank matrix factorization missing data affine bundle adjustment instances <eos> varpro based method received much attention over last decade due experimentally observed large convergence basin certain problem classes they clear advantage over standard method based joint optimization over all unknowns <eos> yet no clear answers found literature why varpro outperforms others why joint optimization successful solving many computer vision tasks fails type problems <eos> also fact varpro mainly tested small medium sized datasets raised questions about its scalability <eos> paper intends address unsolved puzzles <eos> <eop> learning detect salient object image level supervision <eos> deep neural network dnns substantially improved state art salient object detection <eos> however training dnns requires costly pixel level annotations <eos> paper leverage observation image level tags provide important cues foreground salient object develop weakly supervised learning method saliency detection using image level tags only <eos> foreground inference network fin introduced challenging task <eos> first stage training method fin jointly trained fully convolutional network fcn image level tag prediction <eos> global smooth pooling layer proposed enabling fcn assign object category tags corresponding object region while fin capable capturing all potential foreground region predicted saliency maps <eos> second stage fin fine tuned its predicted saliency maps ground truth <eos> refinement ground truth iterative conditional random field developed enforce spatial label consistency further boost performance <eos> method alleviates annotation efforts allows usage existing large scale training set image level tags <eos> model runs fps outperforms unsupervised ones large margin achieves comparable even superior performance than fully supervised counterparts <eos> <eop> binary coding partial action analysis limited observation ratios <eos> traditional action recognition method aim recognize actions complete observations executions <eos> however often difficult capture fully executed actions due occlusions interruptions etc <eos> meanwhile action prediction recognition advance based partial observations essential preventing situation deteriorating <eos> besides fast spotting human activities using partially observed data critical ingredient retrieval systems <eos> inspired recent success data binarization efficient retrieval recognition propose novel approach named partial reconstructive binary coding prbc action analysis based limited frame glimpses during any period complete execution <eos> specifically learn discriminative compact binary codes partial actions via joint learning framework collaboratively tackles feature reconstruction well binary coding <eos> obtain solution prbc based discrete alternating iteration algorithm <eos> extensive experiments four realistic action datasets terms three tasks <eos> partial action retrieval recognition prediction clearly show superiority prbc over state art method along significantly reduced memory load computational costs during online test <eos> <eop> temporal convolutional network action segmentation detection <eos> ability identify temporally segment fine grained human actions throughout video crucial robotics surveillance education beyond <eos> typical approaches decouple problem first extracting local spatiotemporal feature video frames then feeding them into temporal classifier captures high level temporal patterns <eos> describe class temporal models call temporal convolutional network tcns use hierarchy temporal convolutions perform fine grained action segmentation detection <eos> encoder decoder tcn uses pooling upsampling efficiently capture long range temporal patterns whereas dilated tcn uses dilated convolutions <eos> show tcns capable capturing action compositions segment durations long range dependencies over magnitude faster train than competing lstm based recurrent neural network <eos> apply models three challenging fine grained datasets show large improvements over state art <eos> <eop> deligan generative adversarial network diverse limited data <eos> class recent approaches generating image called generative adversarial network gan used generate impressively realistic image object bedrooms handwritten digits variety other image modalities <eos> however typical gan based approaches require large amounts training data capture diversity across image modality <eos> paper propose deligan novel gan based architecture diverse limited training data scenarios <eos> approach reparameterize latent generative space mixture model learn mixture model parameters along gan <eos> seemingly simple modification gan framework surprisingly effective result models enable diversity generated sample although trained limited data <eos> work show deligan generate image handwritten digits object hand drawn sketches all using limited amounts data <eos> quantitatively characterize intra class diversity generated sample also introduce modified version inception score measure found correlate well human assessment generated sample <eos> <eop> template matching deformable diversity similarity <eos> propose novel measure template matching named deformable diversity similarity based diversity feature matches between target image window template <eos> rely both local appearance geometric information jointly lead powerful approach matching <eos> key contribution similarity measure robust complex deformations significant background clutter occlusions <eos> empirical evaluation most up date benchmark shows method outperforms current state art its detection accuracy while improving computational complexity <eos> <eop> surface motion capture transfer gaussian process regression <eos> address problem transferring motion between captured models <eos> particularly focus human subjects ability automatically augment datasets propagating movements between subjects interest great deal recent vision applications builds human visual corpus <eos> given training set two subjects sparse set corresponding keyposes known method able transfer newly captured motion one subject other <eos> aim generalize transfers input motions possibly very diverse respect training set method contributes new transfer model based non linear pose interpolation <eos> building gaussian process regression model intends capture preserve individual motion properties thereby realism accounting pose inter dependencies during motion transfers <eos> experiments show visually qualitative quantitative improvements over existing pose mapping method confirm generalization capabilities method compared state art <eos> <eop> generating holistic three dimensional scene abstractions text based image retrieval <eos> spatial relationships between object provide important information text based image retrieval <eos> users more likely describe scene real world perspective using three dimensional spatial relationships rather than relationships assume particular viewing direction one main challenges infer three dimensional structure bridges image users text descriptions <eos> however direct inference three dimensional structure image requires learning large scale annotated data <eos> since interactions between object reduced limited set atomic spatial relations three dimensional study possibility inferring three dimensional structure text description rather than image applying physical relation models synthesize holistic three dimensional abstract object layouts satisfying spatial constraints present textual description <eos> present generic framework retrieving image textual description scene matching image generated abstract object layouts <eos> image ranked matching object detection outputs bounding boxes layout candidates also represented bounding boxes obtained projecting three dimensional scenes sampled camera directions <eos> validate approach using public indoor scene datasets show method outperforms baselines built upon object occurrence histograms learned pairwise relations <eos> <eop> unsupervised video summarization adversarial lstm network <eos> paper addresses problem unsupervised video summarization formulated selecting sparse subset video frames optimally represent input video <eos> key idea learn deep summarizer network minimize distance between training video distribution their summarizations unsupervised way <eos> such summarizer then applied new video estimating its optimal summarization <eos> learning specify novel generative adversarial framework consisting summarizer discriminator <eos> summarizer autoencoder long short term memory network lstm aimed first selecting video frames then decoding obtained summarization reconstructing input video <eos> discriminator another lstm aimed distinguishing between original video its reconstruction summarizer <eos> summarizer lstm cast adversary discriminator <eos> trained so maximally confuse discriminator <eos> learning also regularized sparsity <eos> evaluation four benchmark datasets consisting video showing diverse events first third person views demonstrates competitive performance comparison fully supervised state art approaches <eos> <eop> sphereface deep hypersphere embedding face recognition <eos> paper addresses deep face recognition fr problem under open set protocol ideal face feature expected smaller maximal intra class distance than minimal inter class distance under suitably chosen metric space <eos> however few existing algorithms effectively achieve criterion <eos> end propose angular softmax softmax loss enables convolutional neural network cnn learn angularly discriminative feature <eos> geometrically softmax loss viewed imposing discriminative constraints hypersphere manifold intrinsically matches prior faces also lie manifold <eos> moreover size angular margin quantitatively adjusted parameter <eos> further derive specific approximate ideal feature criterion <eos> extensive analysis experiments labeled face wild lfw youtube faces ytf megaface challenge show superiority softmax loss fr tasks <eos> <eop> one shot video object segmentation <eos> paper tackles task semi supervised video object segmentation <eos> separation object background video given mask first frame <eos> present one shot video object segmentation osvos based fully convolutional neural network architecture able successively transfer generic semantic information learned imagenet task foreground segmentation finally learning appearance single annotated object test sequence hence one shot <eos> although all frames processed independently result temporally coherent stable <eos> perform experiments two annotated video segmentation databases show osvos fast improves state art significant margin <eos> <eop> sgm nets semi global matching neural network <eos> paper deals deep neural network predicting accurate dense disparity map semi global matching sgm <eos> sgm widely used regularization method real scenes because its high accuracy fast computation speed <eos> even though sgm obtain accurate result tuning sgm penalty parameters control smoothness discontinuity disparity map uneasy empirical method proposed <eos> propose learning based penalties estimation method call sgm nets consist convolutional neural network <eos> small image patch its position input into sgmnets predict penalties three dimensional object structures <eos> order train network introduce novel loss function able use sparsely annotated disparity maps such captured lidar sensor real environments <eos> moreover propose novel sgm parameterization deploys different penalties depending either positive negative disparity changes order represent object structures more discriminatively <eos> sgm nets outperformed state art accuracy kitti benchmark datasets <eos> <eop> question using visual questions form supervision <eos> collecting fully annotated image datasets challenging expensive <eos> many types weak supervision explored weak manual annotations web search result temporal continuity ambient sound others <eos> focus one particular unexplored mode visual questions asked about image <eos> key observation inspires work question itself provides useful information about image even without answer being available <eos> instance question breed dog informs ai animal scene dog there only one dog present <eos> make three contributions providing extensive qualitative quantitative analysis information contained human visual questions proposing two simple but surprisingly effective modifications standard visual question answering models allow them make use weak supervision form unanswered questions associated image demonstrating simple data augmentation strategy inspired insights result <eos> improvement standard vqa benchmark <eos> <eop> context aware captions context agnostic supervision <eos> introduce inference technique produce discriminative context aware image captions captions describe differences between image visual concepts using only generic context agnostic training data captions describe concept image isolation <eos> example given image captions siamese cat tiger cat generate language describes siamese cat way distinguishes tiger cat <eos> key novelty show how joint inference over language model context agnostic listener distinguishes closely related concepts <eos> first apply technique justification task namely describe why image contains particular fine grained category opposed another closely related category cub dataset <eos> then study discriminative image captioning generate language uniquely refers one two semantically similar image coco dataset <eos> evaluations discriminative ground truth justification human studies discriminative image captioning reveal approach outperforms baseline generative speaker listener approaches discrimination <eos> <eop> polyhedral conic classifiers visual object detection classification <eos> propose family quasi linear discriminants outperform current large margin method sliding window visual object detection open set recognition tasks <eos> tasks classification problems both numerically imbalanced positive object class training test windows much rarer than negative non class ones geometrically asymmetric positive sample typically form compact visually coherent groups while negatives much more diverse including anything all well centred sample target class <eos> difficult cover such negative classes using training sample doubly so open set applications run time negatives may stem classes were seen all during training <eos> so there need discriminants whose decision region focus tightly circumscribing positive class while still taking account negatives zones two classes overlap <eos> paper introduces family quasi linear polyhedral conic discriminants whose positive region distorted balls <eos> method properties run time complexities comparable linear support vector machines svms they trained either binary positive only sample using constrained quadratic programs related svms <eos> experiments show they significantly outperform both linear svms existing one class discriminants wide range object detection open set recognition conventional closed set classification tasks <eos> <eop> unsupervised monocular depth estimation left right consistency <eos> learning based method shown very promising result task depth estimation single image <eos> however most existing approaches treat depth prediction supervised regression problem result require vast quantities corresponding ground truth depth data training <eos> just recording quality depth data range environments challenging problem <eos> paper innovate beyond existing approaches replacing use explicit depth data during training easier obtain binocular stereo footage <eos> propose novel training objective enables convo lutional neural network learn perform single image depth estimation despite absence ground truth depth data <eos> ex ploiting epipolar geometry constraints generate disparity image training network image reconstruction loss <eos> show solving image reconstruction alone re sults poor quality depth image <eos> overcome problem propose novel training loss enforces consistency tween disparities produced relative both left right image leading improved performance robustness com pared existing approaches <eos> method produces state art result monocular depth estimation kitti driving dataset even outperforming supervised method trained ground truth depth <eos> <eop> compact matrix factorization dependent subspaces <eos> traditional matrix factorization method approximate high dimensional data low dimensional subspace <eos> imposes constraints matrix elements allow estimation missing entries <eos> lower rank provides stronger constraints makes estimation missing entries less ambiguous cost measurement fit <eos> paper propose new factorization model further constrains matrix entries <eos> approach seen unification traditional low rank matrix factorization more recent union subspace approach <eos> adaptively finds clusters modeled low dimensional local subspaces simultaneously uses global rank constraint capture overall scene interactions <eos> inference use energy penalizes trade off between data fit degrees freedom resulting factorization <eos> show qualitatively quantitatively regularizing both local global dynamics yields significantly improved missing data estimation <eos> <eop> deep reinforcement learning based image captioning embedding reward <eos> image captioning challenging problem owing complexity understanding image content diverse ways describing natural language <eos> recent advances deep neural network substantially improved performance task <eos> most state art approaches follow encoder decoder framework generates captions using sequential recurrent prediction model <eos> however paper introduce novel decision making framework image captioning <eos> utilize policy network value network collaboratively generate captions <eos> policy network serves local guidance providing confidence predicting next word according current state <eos> additionally value network serves global lookahead guidance evaluating all possible extensions current state <eos> essence adjusts goal predicting correct words towards goal generating captions similar ground truth captions <eos> train both network using actor critic reinforcement learning model novel reward defined visual semantic embedding <eos> extensive experiments analyses microsoft coco dataset show proposed framework outperforms state art approaches across different evaluation metrics <eos> <eop> dual attention network multimodal reasoning matching <eos> propose dual attention network dans jointly leverage visual textual attention mechanisms capture fine grained interplay between vision language <eos> dans attend specific region image words text through multiple steps gather essential information both modalities <eos> based framework introduce two types dans multimodal reasoning matching respectively <eos> reasoning model allows visual textual attentions steer each other during collaborative inference useful tasks such visual question answering vqa <eos> addition matching model exploits two attention mechanisms estimate similarity between image sentences focusing their shared semantics <eos> extensive experiments validate effectiveness dans combining vision language achieving state art performance public benchmarks vqa image text matching <eos> <eop> exploiting floorplan building scale panorama rgbd alignment <eos> paper presents novel algorithm utilizes floorplan align panorama rgbd scans <eos> while effective panorama rgbd alignment techniques exist such system requires extremely dense rgbd image sampling <eos> approach significantly reduce number necessary scans aid floorplan image <eos> formulate novel markov random field inference problem scan placement over floorplan opposed conventional scan scan alignment <eos> technical contributions lie multi modal image correspondence cues between scans schematic floorplan well novel coverage potential avoiding inherent stacking bias <eos> proposed approach evaluated five challenging large indoor spaces <eos> best knowledge present first effective system utilizes floorplan image building scale three dimensional pointcloud alignment <eos> source code data shared community further enhance indoor mapping research <eos> <eop> hierarchical approach generating descriptive image paragraphs <eos> recent progress image captioning made possible generate novel sentences describing image natural language but compressing image into single sentence describe visual content only coarse detail <eos> while one new captioning approach dense captioning potentially describe image finer levels detail captioning many region within image turn unable produce coherent story image <eos> paper overcome limitations generating entire paragraphs describing image tell detailed unified stories <eos> develop model decomposes both image paragraphs into their constituent parts detecting semantic region image using hierarchical recurrent neural network reason about language <eos> linguistic analysis confirms complexity paragraph generation task thorough experiments new dataset image paragraph pairs demonstrate effectiveness approach <eos> <eop> visual dialog <eos> introduce task visual dialog requires ai agent hold meaningful dialog humans natural conversational language about visual content <eos> specifically given image dialog history question about image agent ground question image infer context history answer question accurately <eos> visual dialog disentangled enough specific downstream task so serve general test machine intelligence while being grounded vision enough allow objective evaluation individual responses benchmark progress <eos> develop novel two person chat data collection protocol curate large scale visual dialog dataset visdial <eos> visdial contains dialog question answer pairs image coco dataset total <eos> dialog question answer pairs <eos> introduce family neural encoder decoder models visual dialog encoders late fusion hierarchical recurrent encoder memory network decoders generative discriminative outperform number sophisticated baselines <eos> propose retrieval based evaluation protocol visual dialog ai agent asked sort set candidate answers evaluated metrics such mean reciprocal rank human response <eos> quantify gap between machine human performance visual dialog task via human studies <eos> dataset code trained models will released publicly visualdialog <eos> putting all together demonstrate first visual chatbot <eop> desire distant future prediction dynamic scenes interacting agents <eos> introduce deep stochastic ioc rnn encoder decoder framework desire task future predictions multiple interacting agents dynamic scenes <eos> desire effectively predicts future locations object multiple scenes accounting multi modal nature future prediction <eos> given same context future may vary foreseeing potential future outcomes make strategic prediction based reasoning only past motion history but also scene context well interactions among agents <eos> desire achieves single end end trainable neural network model while being computationally efficient <eos> model first obtains diverse set hypothetical future prediction sample employing conditional variational auto encoder ranked refined following rnn scoring regression module <eos> sample scored accounting accumulated future rewards enables better long term strategic decisions similar ioc frameworks <eos> rnn scene context fusion module jointly captures past motion histories semantic scene context interactions among multiple agents <eos> feedback mechanism iterates over ranking refinement further boost prediction accuracy <eos> evaluate model two publicly available datasets kitti stanford drone dataset <eos> experiments show proposed model significantly improves prediction accuracy compared other baseline method <eos> <eop> mining object parts cnn via active question answering <eos> given convolutional neural network cnn pre trained object classification paper proposes use active question answering semanticize neural patterns conv layer cnn mine part concepts <eos> each part concept mine neural patterns pre trained cnn related target part use patterns construct graph aog represent four layer semantic hierarchy part <eos> interpretable model aog associates different cnn units different explicit object parts <eos> use active human computer communication incrementally grow such aog pre trained cnn follows <eos> allow computer actively identify object whose neural patterns cannot explained current aog <eos> then computer asks human about unexplained object uses answers automatically discover certain cnn patterns corresponding missing knowledge <eos> incrementally grow aog encode new knowledge discovered during active learning process <eos> experiments method exhibits high learning efficiency <eos> method uses about part annotations training but achieves similar better part localization performance than fast rcnn method <eos> <eop> multi way multi level kernel modeling neuroimaging classification <eos> owing prominence diagnostic tool probing neural correlates cognition neuroimaging tensor data focus intense investigation <eos> although many supervised tensor learning approaches proposed they either cannot capture nonlinear relationships tensor data cannot preserve complex multi way structural information <eos> paper propose multi way multi level kernel mmk model extract discriminative nonlinear structural preserving representations tensor data <eos> specifically introduce kernelized cp tensor factorization technique equivalent performing low rank tensor factorization possibly much higher dimensional space implicitly defined kernel function <eos> further employ multi way nonlinear feature mapping derive dual structural preserving kernels used conjunction kernel machines <eos> extensive experiments real world neuroimages demonstrate proposed mmk method effectively boost classification performance diverse brain disorders <eos> alzheimer disease adhd hiv <eos> <eop> low rank bilinear pooling fine grained classification <eos> pooling second order local feature statistics form high dimensional bilinear feature shown achieve state art performance variety fine grained classification tasks <eos> address computational demands high feature dimensionality propose represent covariance feature matrix apply low rank bilinear classifier <eos> resulting classifier evaluated without explicitly computing bilinear feature map allows large reduction compute time well decreasing effective number parameters learned <eos> further compress model propose classifier co decomposition factorizes collection bilinear classifiers into common factor compact per class terms <eos> co decomposition idea deployed through two convolutional layer trained end end architecture <eos> suggest simple yet effective initialization avoids explicitly first training factorizing larger bilinear classifiers <eos> through extensive experiments show model achieves state art performance several public datasets fine grained classification trained only category labels <eos> importantly final model order magnitude smaller than recently proposed compact bilinear model three orders smaller than standard bilinear cnn model <eos> <eop> knowing when look adaptive attention via visual sentinel image captioning <eos> attention based neural encoder decoder frameworks widely adopted image captioning <eos> most method force visual attention active every generated word <eos> however decoder likely requires little no visual information image predict non visual words such <eos> other words may seem visual often predicted reliably just language model <eos> sign after behind red stop phone following talking cell <eos> paper propose novel adaptive attention model visual sentinel <eos> each time step model decides whether attend image if so region visual sentinel <eos> model decides whether attend image order extract meaningful information sequential word generation <eos> test method coco image captioning challenge dataset flickr <eos> approach set new state art significant margin <eos> <eop> learning deep context aware feature over body latent parts person re identification <eos> person re identification reid identify same person across different cameras <eos> challenging task due large variations person pose occlusion background clutter etc <eos> how extract powerful feature fundamental problem reid still open problem today <eos> paper design multi scale context aware network mscan learn powerful feature over full body body parts well capture local context knowledge stacking multi scale convolutions each layer <eos> moreover instead using predefined rigid parts propose learn localize deformable pedestrian parts using spatial transformer network stn novel spatial constraints <eos> learned body parts release some difficulties <eos> pose variations background clutters part based representation <eos> finally integrate representation learning processes full body body parts into unified framework person reid through multi class person identification tasks <eos> extensive evaluations current challenging large scale person reid datasets including image based market cuhk sequence based mars datasets show proposed method achieves state art result <eos> <eop> turning urban scene video into cinemagraph <eos> paper proposes algorithm turns regular video capturing urban scenes into high quality endless animation known cinemagraph <eos> creation cinemagraph usually requires static camera carefully configured scene <eos> task becomes challenging regular video moving camera object <eos> approach first warps input video into viewpoint reference camera <eos> based warped video propose effective temporal analysis algorithms detect region static geometry dynamic appearance geometric modeling reliable visually attractive animations created <eos> lastly algorithm applies sequence video processing techniques produce cinemagraph movie <eos> tested proposed approach numerous challenging real scenes <eos> knowledge work first automatically generate cinemagraph animations regular movies wild <eos> <eop> beyond triplet loss deep quadruplet network person re identification <eos> person re identification reid important task wide area video surveillance focuses identifying people across different cameras <eos> recently deep learning network triplet loss become common framework person reid <eos> however triplet loss pays main attentions obtaining correct orders training set <eos> still suffers weaker generalization capability training set testing set thus resulting inferior performance <eos> paper design quadruplet loss lead model output larger inter class variation smaller intra class variation compared triplet loss <eos> result model better generalization ability achieve higher performance testing set <eos> particular quadruplet deep network using margin based online hard negative mining proposed based quadruplet loss person reid <eos> extensive experiments proposed network outperforms most state art algorithms representative datasets clearly demonstrates effectiveness proposed method <eos> <eop> surveillance video parsing single frame supervision <eos> surveillance video parsing segments video frames into several labels <eos> face pants left leg wide applications <eos> however annotating all frames pixel wisely tedious inefficient <eos> paper develop single frame video parsing svp method requires only one labeled frame per video training stage <eos> parse one particular frame video segment preceding frame jointly considered <eos> svp roughly parses frames within video segment estimates optical flow between frames fuses rough parsing result warped optical flow produce refined parsing result <eos> three components svp namely frame parsing optical flow estimation temporal fusion integrated end end manner <eos> experimental result two surveillance video datasets reveal svp superior than state arts <eos> <eop> semantically coherent co segmentation reconstruction dynamic scenes <eos> paper propose framework spatially temporally coherent semantic co segmentation reconstruction complex dynamic scenes multiple static moving cameras <eos> semantic co segmentation exploits coherence semantic class labels both spatially between views single time instant temporally between widely spaced time instants dynamic object similar shape appearance <eos> demonstrate semantic coherence result improved segmentation reconstruction complex scenes <eos> joint formulation proposed semantically coherent object based co segmentation reconstruction scenes enforcing consistent semantic labelling between views over time <eos> semantic tracklets introduced enforce temporal coherence semantic labelling reconstruction between widely spaced instances dynamic object <eos> tracklets dynamic object enable unsupervised learning appearance shape priors exploited joint segmentation reconstruction <eos> evaluation challenging indoor outdoor sequences hand held moving cameras shows improved accuracy segmentation temporally coherent semantic labelling three dimensional reconstruction dynamic scenes <eos> <eop> transition forests learning discriminative temporal transitions action recognition detection <eos> human action seen transitions between one body poses over time transition depicts temporal relation between two poses <eos> recognizing actions thus involves learning classifier sensitive pose transitions well static poses <eos> paper introduce novel method called transitions forests ensemble decision trees both learn discriminate static poses transitions between pairs two independent frames <eos> during training node splitting driven alternating two criteria standard classification objective maximizes discrimination power individual frames proposed one pairwise frame transitions <eos> growing trees tends group frames similar associated transitions share same action label incorporating temporal information was available otherwise <eos> unlike conventional decision trees best split node determined independently other nodes transition forests try find best split nodes jointly within layer incorporating distant node transitions <eos> when inferring class label new frame passed down trees prediction made based previous frame predictions current one efficient online manner <eos> apply method varied skeleton action recognition online detection datasets showing its suitability over several baselines state art approaches <eos> <eop> pixelwise instance segmentation dynamically instantiated network <eos> semantic segmentation object detection research recently achieved rapid progress <eos> however former task no notion different instances same object latter operates coarse bounding box level <eos> propose instance segmentation system produces segmentation map each pixel assigned object class instance identity label <eos> most approaches adapt object detectors produce segments instead boxes <eos> contrast method based initial semantic segmentation module feeds into instance subnetwork <eos> subnetwork uses initial category level segmentation along cues output object detector within end end crf predict instances <eos> part model dynamically instantiated produce variable number instances per image <eos> end end approach requires no post processing considers image holistically instead processing independent proposals <eos> therefore unlike some related work pixel cannot belong multiple instances <eos> furthermore far more precise segmentations achieved shown state art result particularly high iou thresholds pascal voc cityscapes datasets <eos> <eop> video propagation network <eos> propose technique propagates information forward through video data <eos> method conceptually simple applied tasks require propagation structured information such semantic labels based video content <eos> propose video propagation network processes video frames adaptive manner <eos> model applied online propagates information forward without need access future frames <eos> particular combine two components temporal bilateral network dense video adaptive filtering followed spatial network refine feature increased flexibility <eos> present experiments video object segmentation semantic video segmentation show increased performance comparing best previous task specific method while having favorable runtime <eos> additionally demonstrate approach example regression task color propagation grayscale video <eos> <eop> global hypothesis generation object pose estimation <eos> paper addresses task estimating pose known three dimensional object single rgb image <eos> most modern approaches solve task three steps compute local feature ii generate pool pose hypotheses iii select refine pose pool <eos> work focuses second step <eos> while all existing approaches generate hypotheses pool via local reasoning <eos> ransac hough voting first show global reasoning beneficial stage <eos> particular formulate novel fully connected conditional random field crf outputs very small number pose hypotheses <eos> despite potential functions crf being non gaussian give new efficient two step optimization procedure some guarantees optimality <eos> utilize global hypotheses generation procedure produce result exceed state art challenging occluded object dataset <eos> <eop> dilated residual network <eos> convolutional network image classification progressively reduce resolution until image represented tiny feature maps spatial structure scene no longer discernible <eos> such loss spatial acuity limit image classification accuracy complicate transfer model downstream applications require detailed scene understanding <eos> problems alleviated dilation increases resolution output feature maps without reducing receptive field individual neurons <eos> show dilated residual network drns outperform their non dilated counterparts image classification without increasing model depth complexity <eos> then study gridding artifacts introduced dilation develop approach removing artifacts degridding show further increases performance drns <eos> addition show accuracy advantage drns further magnified downstream applications such object localization semantic segmentation <eos> <eop> robust interpolation correspondences large displacement optical flow <eos> interpolation correspondences epicflow was widely used optical flow estimation most recent works <eos> advantage edge preserving efficiency <eos> however vulnerable input matching noise inevitable modern matching techniques <eos> paper present robust interpolation method correspondences called ricflow overcome weakness <eos> first scene over segmented into superpixels revitalize early idea piecewise flow model <eos> then each model estimated robustly its support neighbors based graph constructed superpixels <eos> propose propagation mechanism among pieces estimation models <eos> propagation models significantly more efficient than independent estimation each model yet retains accuracy <eos> extensive experiments three public datasets demonstrate ricflow more robust than epicflow outperforms state art method <eos> <eop> supervising neural attention models video captioning human gaze data <eos> attention mechanisms deep neural network inspired human attention sequentially focuses most relevant parts information over time generate prediction output <eos> attention parameters models implicitly trained end end manner yet there few trials explicitly incorporate human gaze tracking supervise attention models <eos> paper investigate whether attention models benefit explicit human gaze labels especially task video captioning <eos> collect new dataset called vas consisting movie clips corresponding multiple descriptive sentences along human gaze tracking data <eos> propose video captioning model named gaze encoding attention network gean leverage gaze tracking information provide spatial temporal attention sentence generation <eos> through evaluation language similarity metrics human assessment via amazon mechanical turk demonstrate spatial attentions guided human gaze data indeed improve performance multiple captioning method <eos> moreover show proposed approach achieves state art performance both gaze prediction video captioning only vas dataset but also standard datasets <eos> lsmdc hollywood <eop> modeling temporal dynamics spatial configurations actions using two stream recurrent neural network <eos> recently skeleton based action recognition gains more popularity due cost effective depth sensors coupled real time skeleton estimation algorithms <eos> traditional approaches based handcrafted feature limited represent complexity motion patterns <eos> recent method use recurrent neural network rnn handle raw skeletons only focus contextual dependency temporal domain neglect spatial configurations articulated skeletons <eos> paper propose novel two stream rnn architecture model both temporal dynamics spatial configurations skeleton based action recognition <eos> explore two different structures temporal stream stacked rnn hierarchical rnn <eos> hierarchical rnn designed according human body kinematics <eos> also propose two effective method model spatial structure converting spatial graph into sequence joints <eos> improve generalization model further exploit three dimensional transformation based data augmentation techniques including rotation scaling transformation transform three dimensional coordinates skeletons during training <eos> experiments three dimensional action recognition benchmark datasets show method brings considerable improvement variety actions <eos> generic actions interaction activities gestures <eos> <eop> self learning scene specific pedestrian detectors using progressive latent model <eos> paper self learning approach proposed towards solving scene specific pedestrian detection problem without any human annotation involved <eos> self learning approach deployed progressive steps object discovery object enforcement label propagation <eos> learning procedure object locations each frame treated latent variables solved progressive latent model plm <eos> compared conventional latent models proposed plm incorporates spatial regularization term reduce ambiguities object proposals enforce object localization also graph based label propagation discover harder instances adjacent frames <eos> difference convex dc objective functions plm efficiently optimized concave convex programming thus guaranteeing stability self learning <eos> extensive experiments demonstrate even without annotation proposed self learning approach outperforms weakly supervised learning approaches while achieving comparable performance transfer learning fully supervised approaches <eos> <eop> oriented response network <eos> deep convolution neural network dcnns capable learning unprecedentedly effective image representations <eos> however their ability handling significant local global image rotations remains limited <eos> paper propose active rotating filters arfs actively rotate during convolution produce feature maps location orientation explicitly encoded <eos> arf acts virtual filter bank containing filter itself its multiple unmaterialised rotated versions <eos> during back propagation arf collectively updated using errors all its rotated versions <eos> dcnns using arfs referred oriented response network orns produce within class rotation invariant deep feature while maintaining inter class discrimination classification tasks <eos> oriented response produced orns also used image object orientation estimation tasks <eos> over multiple state art dcnn architectures such vgg resnet stn consistently observe replacing regular filters proposed arfs leads significant reduction number network parameters improvement classification performance <eos> report best result several commonly used benchmarks <eos> <eop> video acceleration magnification <eos> ability amplify reduce subtle image changes over time useful contexts such video editing medical video analysis product quality control sports <eos> contexts there often large motion present severely distorts current video amplification method magnify change linearly <eos> work propose method cope large motions while still magnifying small changes <eos> make following two observations large motions linear temporal scale small changes ii small changes deviate linearity <eos> ignore linear motion propose magnify acceleration <eos> method pure eulerian require any optical flow temporal alignment region annotations <eos> link temporal second order derivative filtering spatial acceleration magnification <eos> apply method moving object show motion magnification color magnification <eos> provide quantitative well qualitative evidence method while comparing state art <eos> <eop> irina iris recognition even inaccurately segmented data <eos> effectiveness current iris recognition systems depends accurate segmentation parameterisation iris boundaries failures point misalign coefficients biometric signatures <eos> paper describes irina algorithm iris recognition robust against inaccurately segmented sample makes good candidate work poor quality data <eos> process based concept corresponding patch between pairs image used estimate posterior probabilities patches regard same biological region even case segmentation errors non linear texture deformations <eos> such information enables infer free form deformation field registration vectors between image whose first second order statistics provide effective biometric discriminating power <eos> extensive experiments were carried out four datasets casia irisv lamp casia irisv lamp casia irisv thousand wvu show irina only achieves state art performance good quality data but also handles effectively severe segmentation errors large differences pupillary dilation constriction <eos> <eop> forecasting human dynamics static image <eos> paper presents first study forecasting human dynamics static image <eos> problem input single rgb image generate sequence upcoming human body poses three dimensional address problem propose three dimensional pose forecasting network three dimensional pfnet <eos> three dimensional pfnet integrates recent advances single image human pose estimation sequence prediction converts predictions into three dimensional space <eos> train three dimensional pfnet using three step training strategy leverage diverse source training data including image video based human pose datasets three dimensional motion capture mocap data <eos> demonstrate competitive performance three dimensional pfnet pose forecasting three dimensional structure recovery through quantitative qualitative result <eos> <eop> discriminative bimodal network visual localization detection natural language queries <eos> associating image region text queries recently explored new way bridge visual linguistic representations <eos> few pioneering approaches proposed based recurrent neural language models trained generatively <eos> generating captions but achieving somewhat limited localization accuracy <eos> better address natural language based visual entity localization propose discriminative approach <eos> formulate discriminative bimodal neural network dbnet trained classifier extensive use negative sample <eos> training objective encourages better localization single image incorporates text phrases broad range properly pairs image region text phrases into positive negative examples <eos> experiments visual genome dataset demonstrate proposed dbnet significantly outperforms previous state art method both localization single image detection multiple image <eos> also establish evaluation protocol natural language visual detection <eos> <eop> linear extrinsic calibration kaleidoscopic imaging system single three dimensional point <eos> paper proposes new extrinsic calibration kaleidoscopic imaging system estimating normals distances mirrors <eos> problem solved paper simultaneous estimation all mirror parameters consistent throughout multiple reflections <eos> unlike conventional method utilizing pair direct mirrored image reference three dimensional object estimate parameters per mirror basis method renders simultaneous estimation problem into solving linear set equations <eos> key contribution paper introduce linear estimation multiple mirror parameters kaleidoscopic projections single three dimensional point unknown geometry <eos> evaluations synthesized real image demonstrate performance proposed algorithm comparison conventional method <eos> <eop> efficient multiple instance metric learning using weakly supervised data <eos> consider learning distance metric weakly supervised setting bags set instances labeled bags labels <eos> general approach formulate problem multiple instance learning mil problem metric learned so distances between instances inferred similar smaller than distances between instances inferred dissimilar <eos> classic approaches alternate optimization over learned metric assignment similar instances <eos> paper propose efficient method jointly learns metric assignment instances <eos> particular model learned solving extension means mil problems instances assigned categories depending annotations provided bag level <eos> learning algorithm much faster than existing metric learning method mil problems obtains state art recognition performance automated image annotation instance classification face identification <eos> <eop> asynchronous temporal fields action recognition <eos> actions more than just movements trajectories cook eat hold cup drink <eos> thorough understanding video requires going beyond appearance modeling necessitates reasoning about sequence activities well higher level constructs such intentions <eos> but how model reason about propose fully connected temporal crf model reasoning over various aspects activities includes object actions intentions potentials predicted deep network <eos> end end training such structured models challenging endeavor inference learning need construct mini batches consisting whole video leading mini batches only few video <eos> causes high correlation between data point leading breakdown backprop algorithm <eos> address challenge present asynchronous variational inference method allows efficient end end training <eos> method achieves classification map <eos> charades benchmark outperforming state art <eos> map offers equal gains task temporal localization <eos> <eop> scene flow action map new representation rgb based action recognition convolutional neural network <eos> scene flow describes motion three dimensional object real world potentially could basis good feature three dimensional action recognition <eos> however its use action recognition especially context convolutional neural network convnets previously studied <eos> paper propose extraction use scene flow action recognition rgb data <eos> previous works considered depth rgb modalities separate channels extract feature later fusion <eos> take different approach consider modalities one entity thus allowing feature extraction action recognition beginning <eos> two key questions about use scene flow action recognition addressed how organize scene flow vectors how represent long term dynamics video based scene flow <eos> order calculate scene flow correctly available datasets propose effective self calibration method align rgb depth data spatially without knowledge camera parameters <eos> based scene flow vectors propose new representation namely scene flow action map sfam describes several long term spatio temporal dynamics action recognition <eos> adopt channel transform kernel transform scene flow vectors optimal color space analogous rgb <eos> transformation takes better advantage trained convnets models over imagenet <eos> experimental result indicate new representation surpass performance state art method two large public datasets <eos> <eop> point set generation network three dimensional object reconstruction single image <eos> generation three dimensional data deep neural network attracting increasing attention research community <eos> majority extant works resort regular representations such volumetric grids collection image however representations obscure natural invariance three dimensional shapes under geometric transformations also suffer number other issues <eos> paper address problem three dimensional reconstruction single image generating straight forward form output point cloud coordinates <eos> along problem arises unique interesting issue groundtruth shape input image may ambiguous <eos> driven unorthordox output form inherent ambiguity groundtruth design architecture loss function learning paradigm novel effective <eos> final solution conditional shape sampler capable predicting multiple plausible three dimensional point clouds input image <eos> experiments only system outperform state art method single image based three dimensional reconstruction benchmarks but also shows strong performance three dimensional shape completion promising ability making multiple plausible predictions <eos> <eop> automatic discovery association estimation learning semantic attributes thousand categories <eos> attribute based recognition models due their impressive performance their ability generalize well novel categories widely adopted many computer vision applications <eos> however usually both attribute vocabulary class attribute associations provided manually domain experts large number annotators <eos> very costly necessarily optimal regarding recognition performance most importantly limits applicability attribute based models large scale data set <eos> tackle problem propose end end unsupervised attribute learning approach <eos> utilize online text corpora automatically discover salient discriminative vocabulary correlates well human concept semantic attributes <eos> moreover propose deep convolutional model optimize class attribute associations linguistic prior accounts noise missing data text <eos> thorough evaluation imagenet demonstrate model able efficiently discover learn semantic attributes large scale <eos> furthermore demonstrate model outperforms state art zero shot learning three data set imagenet animals attributes apascal ayahoo <eos> finally enable attribute based learning imagenet will share attributes associations future research <eos> <eop> deep laplacian pyramid network fast accurate super resolution <eos> convolutional neural network recently demonstrated high quality reconstruction single image super resolution <eos> paper propose laplacian pyramid super resolution network lapsrn progressively reconstruct sub band residuals high resolution image <eos> each pyramid level model takes coarse resolution feature maps input predicts high frequency residuals uses transposed convolutions upsampling finer level <eos> method require bicubic interpolation pre processing step thus dramatically reduces computational complexity <eos> train proposed lapsrn deep supervision using robust charbonnier loss function achieve high quality reconstruction <eos> furthermore network generates multi scale predictions one feed forward pass through progressive reconstruction thereby facilitates resource aware applications <eos> extensive quantitative qualitative evaluations benchmark datasets show proposed algorithm performs favorably against state art method terms speed accuracy <eos> <eop> scene parsing through ade dataset <eos> scene parsing recognizing segmenting object stuff image one key problems computer vision <eos> despite community efforts data collection there still few image datasets covering wide range scenes object categories dense detailed annotations scene parsing <eos> paper introduce analyze ade dataset spanning diverse annotations scenes object parts object some cases even parts parts <eos> scene parsing benchmark built upon ade object stuff classes included <eos> several segmentation baseline models evaluated benchmark <eos> novel network design called cascade segmentation module proposed parse scene into stuff object object parts cascade improve over baselines <eos> further show trained scene parsing network lead applications such image content removal scene synthesis dataset pretrained models available groups <eos> edu vision datasets ade <eos> <eop> wildcat weakly supervised learning deep convnets image classification pointwise localization segmentation <eos> paper introduces wildcat deep learning method jointly aims aligning image region gaining spatial invariance learning strongly localized feature <eos> model trained using only global image labels devoted three main visual recognition tasks image classification weakly supervised object localization semantic segmentation <eos> wildcat extends state art convolutional neural network three main levels use fully convolutional network maintaining spatial resolution explicit design network local feature related different class modalities new way pool feature provide global image prediction required weakly supervised training <eos> extensive experiments show model significantly outperforms state art method <eos> <eop> pointnet deep learning point set three dimensional classification segmentation <eos> point cloud important type geometric data structure <eos> due its irregular format most researchers transform such data regular three dimensional voxel grids collections image <eos> however renders data unnecessarily voluminous causes issues <eos> paper design novel type neural network directly consumes point clouds well respects permutation invariance point input <eos> network named pointnet provides unified architecture applications ranging object classification part segmentation scene semantic parsing <eos> though simple pointnet highly efficient effective <eos> empirically shows strong performance par even better than state art <eos> theoretically provide analysis towards understanding network learnt why network robust respect input perturbation corruption <eos> <eop> net deep learning discriminative patch descriptor euclidean space <eos> research focus designing local patch descriptors gradually shifted handcrafted ones <eos> sift learned ones <eos> paper propose learn high per formance descriptor euclidean space via convolu tional neural network cnn <eos> method distinctive four aspects propose progressive sampling strat egy enables network access billions train ing sample few epochs <eos> ii derived ba sic concept local patch matching problem empha size relative distance between descriptors <eos> iii extra supervision imposed intermediate feature maps <eos> iv compactness descriptor taken into account <eos> proposed network named net since out put descriptor matched euclidean space distance <eos> net achieves state art performance brown datasets oxford dataset new ly proposed hpatches dataset <eos> good generaliza tion ability shown experiments indicates net serve direct substitution existing handcrafted de scriptors <eos> pre trained net publicly available <eos> <eop> video frame interpolation via adaptive convolution <eos> video frame interpolation typically involves two steps motion estimation pixel synthesis <eos> such two step approach heavily depends quality motion estimation <eos> paper presents robust video frame interpolation method combines two steps into single process <eos> specifically method considers pixel synthesis interpolated frame local convolution over two input frames <eos> convolution kernel captures both local motion between input frames coefficients pixel synthesis <eos> method employs deep fully convolutional neural network estimate spatially adaptive convolution kernel each pixel <eos> deep neural network directly trained end end using widely available video data without any difficult obtain ground truth data like optical flow <eos> experiments show formulation video interpolation single convolution process allows method gracefully handle challenges like occlusion blur abrupt brightness change enables high quality video frame interpolation <eos> <eop> crossing nets combining gans vaes shared latent space hand pose estimation <eos> state art method three dimensional hand pose estimation depth image require large amounts annotated training data <eos> propose modelling statistical relationship three dimensional hand poses corresponding depth image using two deep generative models shared latent space <eos> design architecture allows learning unlabeled image data semi supervised manner <eos> assuming one one mapping between pose depth map any given point shared latent space projected into both hand pose into corresponding depth map <eos> regressing hand pose then done learning discriminator estimate posterior latent pose given some depth map <eos> prevent over fitting better exploit unlabeled depth maps generator discriminator trained jointly <eos> each iteration generator updated back propagated gradient discriminator synthesize realistic depth maps articulated hand while discriminator benefits augmented training set synthesized sample unlabeled depth maps <eos> proposed discriminator network architecture highly efficient runs fps cpu accuracies comparable better than state art publicly available benchmarks <eos> <eop> attention aware face hallucination via deep reinforcement learning <eos> face hallucination domain specific super resolution problem goal generate high resolution hr faces low resolution lr input image <eos> contrast existing method often learn single patch patch mapping lr hr image regardless contextual interdependency between patches propose novel attention aware face hallucination attention fh framework resorts deep reinforcement learning sequentially discovering attended patches then performing facial part enhancement fully exploiting global interdependency image <eos> specifically each time step recurrent policy network proposed dynamically specify new attended region incorporating happened past <eos> face hallucination result whole image thus exploited updated local enhancement network selected region <eos> attention fh approach jointly learns recurrent policy network local enhancement network through maximizing long term reward reflects hallucination performance over whole image <eos> therefore proposed attention fh capable adaptively personalizing optimal searching path each face image according its own characteristic <eos> extensive experiments show approach significantly surpasses state arts wild faces large pose illumination variations <eos> face hallucination result whole image thus exploited updated local enhancement network selected region <eos> attention fh approach jointly learns recurrent policy network local enhancement network through maximizing long term reward reflects hallucination performance over whole image <eos> therefore proposed attention fh capable adaptively personalizing optimal searching path each face image according its own characteristic <eos> extensive experiments show approach significantly surpasses state arts wild faces large pose illumination variations <eos> <eop> neural scene de rendering <eos> study problem holistic scene understanding <eos> would like obtain compact expressive interpretable representation scenes encodes information such number object their categories poses positions etc <eos> such representation would allow reason about even reconstruct manipulate elements scene <eos> previous works used encoder decoder based neural architectures learn image representations however representations obtained way typically uninterpretable only explain single object scene <eos> work propose new approach learn interpretable distributed representation scenes <eos> approach employs deterministic rendering function decoder mapping naturally structured disentangled scene description named scene xml image <eos> doing so encoder forced perform inverse rendering operation <eos> de rendering transform input image structured scene xml decoder used produce image <eos> use object proposal based encoder trained minimizing both supervised prediction unsupervised reconstruction errors <eos> experiments demonstrate approach works well scene de rendering two different graphics engines learned representation easily adapted wide range applications like image editing inpainting visual analogy making image captioning <eos> <eop> deep ten texture encoding network <eos> propose deep texture encoding network ten novel encoding layer integrated top convolutional layer ports entire dictionary learning encoding pipeline into single model <eos> current method build distinct components using standard encoders separate off shelf feature such such sift descriptors pre trained cnn feature material recognition <eos> new approach provides end end learning framework inherent visual vocabularies learned directly loss function <eos> feature dictionaries encoding representation classifier all learned simultaneously <eos> representation orderless therefore particularly useful material texture recognition <eos> encoding layer generalizes robust residual encoders such vlad fisher vectors property discarding domain specific information makes learned convolutional feature easier transfer <eos> additionally joint training using multiple datasets varied sizes class labels supported resulting increased recognition performance <eos> experimental result show superior performance compared state art method using gold standard databases such minc flicker material database kth tips new ground terrain multiview database <eos> source code complete system will publicly available upon publication <eos> <eop> polynet pursuit structural diversity very deep network <eos> number studies shown increasing depth width convolutional network rewarding approach improve performance image recognition <eos> study however observed difficulties along both directions <eos> one hand pursuit very deep network met diminishing return increased training difficulty other hand widening network would result quadratic growth both computational cost memory demand <eos> difficulties motivate explore structural diversity designing deep network new dimension beyond just depth width <eos> specifically present new family modules namely polyinception flexibly inserted isolation composition replacements different parts network <eos> choosing polyinception modules guidance architectural efficiency improve expressive power while preserving comparable computational cost <eos> very deep polynet designed following direction demonstrates substantial improvements over state art ilsvrc benchmark <eos> compared inception resnet reduces top validation error single crops <eos> <eop> object detection video tubelet proposal network <eos> object detection video drawn increasing attention recently introduction large scale imagenet vid dataset <eos> different object detection static image temporal information video vital object detection <eos> fully utilize temporal information state art method based spatiotemporal tubelets essentially sequences associated bounding boxes across time <eos> however existing method major limitations generating tubelets terms quality efficiency <eos> motion based method able obtain dense tubelets efficiently but lengths generally only several frames optimal incorporating long term temporal information <eos> appearance based method usually involving generic object tracking could generate long tubelets but usually computationally expensive <eos> work propose framework object detection video consists novel tubelet proposal network efficiently generate spatiotemporal proposals long short term memory lstm network incorporates temporal information tubelet proposals achieving high object detection accuracy video <eos> experiments large scale imagenet vid dataset demonstrate effectiveness proposed framework object detection video <eos> <eop> amvh asymmetric multi valued hashing <eos> most existing hashing method resort binary codes similarity search owing high efficiency computation storage <eos> however binary codes lack enough capability similarity preservation resulting less desirable performance <eos> address issue propose asymmetric multi valued hashing method supported two different non binary embeddings <eos> real valued embedding used representing newly coming query <eos> multi integer embedding employed compressing whole database modeled binary sparse representation fixed sparsity <eos> two non binary embeddings similarities between data point preserved precisely <eos> perform meaningful asymmetric similarity computation efficient semantic search embeddings jointly learnt preserving label based similarity <eos> technically result mixed integer programming problem efficiently solved alternative optimization <eos> extensive experiments three multilabel datasets demonstrate approach only outperforms existing binary hashing method search accuracy but also retains their query storage efficiency <eos> <eop> real time three dimensional model tracking color depth single cpu core <eos> present novel method track three dimensional models color depth data <eos> end introduce approximations accelerate state art region based tracking order magnitude while retaining similar accuracy <eos> furthermore show how method made more robust presence depth data consequently formulate new joint contour icp tracking energy <eos> present better result than state art while being much faster then most other method achieving all above single cpu core <eos> <eop> weakly supervised action learning rnn based fine coarse modeling <eos> present approach weakly supervised learning human actions <eos> given set video ordered list occurring actions goal infer start end frames related action classes within video train respective action classifiers without any need hand labeled frame boundaries <eos> address task propose combination discriminative representation subactions modeled recurrent neural network coarse probabilistic model allow temporal alignment inference over long sequences <eos> while system alone already generates good result show performance further improved approximating number subactions characteristics different action classes <eos> end adapt number subaction classes iterating realignment reestimation during training <eos> proposed system evaluated two benchmark datasets breakfast hollywood extended dataset showing competitive performance various weak learning tasks such temporal action segmentation action alignment <eos> <eop> differential angular imaging material recognition <eos> material recognition real world outdoor surfaces become increasingly important computer vision support its operation wild <eos> computational surface modeling underlies material recognition transitioned reflectance modeling using lab controlled radiometric measurements image based representations based internet mined image materials captured scene <eos> propose take middle ground approach material recognition takes advantage both rich radiometric cues flexible image capture <eos> realize developing framework differential angular imaging small angular variations image capture provide enhanced appearance representation significant recognition improvement <eos> build large scale material database ground terrain outdoor scenes gtos database geared towards real use autonomous agents <eos> database consists over image covering classes outdoor ground terrain under varying weather lighting conditions <eos> develop novel approach material recognition called differential gular imaging network dain fully leverage large dataset <eos> novel network architecture extract characteristics materials encoded angular spatial gradients their appearance <eos> result show dain achieves recognition performance surpasses single view coarsely quantized multiview image <eos> result demonstrate effectiveness differential angular imaging means flexible place material recognition <eos> <eop> forecasting interactive dynamics pedestrians fictitious play <eos> develop predictive models pedestrian dynamics encoding coupled nature multi pedestrian interaction using game theory deep learning based visual analysis estimate person specific behavior parameters <eos> focus predictive models since they important developing interactive autonomous systems <eos> autonomous cars home robots smart homes understand different human behavior pre emptively respond future human actions <eos> building predictive models multi pedestrian interactions however very challenging due two reasons dynamics interaction complex interdependent processes decision one person affect others dynamics variable each person may behave differently <eos> older person may walk slowly while younger person may walk faster <eos> address challenges utilize concepts game theory model intertwined decision making process multiple pedestrians use visual classifiers learn mapping pedestrian appearance behavior parameters <eos> evaluate proposed model several public multiple pedestrian interaction video datasets <eos> result show strategic planning model predicts explains human interactions better when compared state art activity forecasting method <eos> <eop> real time neural style transfer video <eos> recent research endeavors shown potential using feed forward convolutional neural network accomplish fast style transfer image <eos> work take one step further explore possibility exploiting feed forward network perform style transfer video simultaneously maintain temporal consistency among stylized video frames <eos> feed forward network trained enforcing outputs consecutive frames both well stylized temporally consistent <eos> more specifically hybrid loss proposed capitalize content information input frames style information given style image temporal information consecutive frames <eos> calculate temporal loss during training stage novel two frame synergic training mechanism proposed <eos> compared directly applying existing image style transfer method video proposed method employs trained network yield temporally consistent stylized video much more visually pleasant <eos> contrast prior video style transfer method relies time consuming optimization fly method runs real time while generating competitive visual result <eos> <eop> incremental kernel null space discriminant analysis novelty detection <eos> novelty detection aims determine whether given data belongs any category training data considered important challenging problem areas pattern recognition machine learning etc <eos> recently kernel null space method knda was reported state art performance novelty detection <eos> however knda hard scale up because its high computational cost <eos> ever increasing size data accelerating implementing speed knda desired critical <eos> moreover becomes incapable when there exist successively injected data <eos> address issues propose incremental kernel null space based discriminant analysis iknda algorithm <eos> key idea extract new information brought newly added sample integrate existing model efficient updating scheme <eos> experiments conducted two publicly available datasets demonstrate proposed iknda yields comparable performance batch knda yet significantly reduces computational complexity iknda based novelty detection method markedly outperform approaches using deep neural network dnn classifiers <eos> validates superiority iknda against state art novelty detection large scale data <eos> <eop> self calibration based approach critical motion sequences rolling shutter structure motion <eos> paper consider critical motion sequences cmss rolling shutter rs sfm <eos> employing rs camera model linearized pure rotation show rs distortion approximately expressed two internal parameters imaginary camera plus one parameter nonlinear transformation similar lens distortion <eos> then reformulate problem self calibration imaginary camera its skew aspect ratio unknown varying image sequence <eos> formulation derive general representation cmss <eos> also show method explain cms was recently reported literature then present new remedy deal degeneracy <eos> theoretical result agree well experimental result explains degeneracies observed when employ naive bundle adjustment how they resolved method <eos> <eop> recurrent three dimensional pose sequence machines <eos> three dimensional human articulated pose recovery monocular image sequences very challenging due diverse appearances viewpoints occlusions also human three dimensional pose inherently ambiguous monocular imagery <eos> thus critical exploit rich spatial temporal long range dependencies among body joints accurate three dimensional pose sequence prediction <eos> existing approaches usually manually design some elaborate prior terms human body kinematic constraints capturing structures often insufficient exploit all intrinsic structures scalable all scenarios <eos> contrast paper presents recurrent three dimensional pose sequence machine rpsm automatically learn image dependent structural constraint sequence dependent temporal context using multi stage sequential refinement <eos> each stage rpsm composed three modules predict three dimensional pose sequences based previously learned pose representations three dimensional poses pose module extracting image dependent pose representations ii three dimensional pose recurrent module regressing three dimensional poses iii feature adaption module serving bridge between module ii enable representation transformation three dimensional domain <eos> three modules then assembled into sequential prediction framework refine predicted poses multiple recurrent stages <eos> extensive evaluations human <eos> dataset humaneva dataset show rpsm outperforms all state art approaches three dimensional pose estimation <eos> <eop> efficient solvers minimal problems syzygy based reduction <eos> paper study problem automatically generating polynomial solvers minimal problems <eos> main contribution new method finding small elimination templates making use syzygies <eos> polynomial relations exist between original equations <eos> using syzygies essentially parameterize set possible elimination templates <eos> evaluate method wide variety problems geometric computer vision show improvement compared both handcrafted automatically generated solvers <eos> furthermore apply method two previously unsolved relative orientation problems <eos> <eop> conditional similarity network <eos> makes image similar measure similarity between image they typically embedded feature vector space their distance preserve relative dissimilarity <eos> however when learning such similarity embeddings simplifying assumption commonly made image only compared one unique measure similarity <eos> main reason contradicting notions similarities cannot captured single space <eos> address shortcoming propose conditional similarity network csns learn embeddings differentiated into semantically distinct subspaces capture different notions similarities <eos> csns jointly learn disentangled embedding feature different similarities encoded separate dimensions well masks select reweight relevant dimensions induce subspace encodes specific similarity notion <eos> show approach learns interpretable image representations visually relevant semantic subspaces <eos> further when evaluating triplet questions multiple similarity notions model even outperforms accuracy obtained training individual specialized network each notion separately <eos> <eop> learning noisy large scale datasets minimal supervision <eos> present approach effectively use millions image noisy annotations conjunction small subset cleanly annotated image learn powerful image representations <eos> one common approach combine clean noisy data first pre train network using large noisy dataset then fine tune clean dataset <eos> show approach fully leverage information contained clean set <eos> thus demonstrate how use clean annotations reduce noise large dataset before fine tuning network using both clean set full set reduced noise <eos> approach comprises multi task network jointly learns clean noisy annotations accurately classify image <eos> evaluate approach recently released open image dataset containing million image multiple annotations per image over unique classes <eos> small clean set annotations use quarter validation set image <eos> result demonstrate proposed approach clearly outperforms direct fine tuning across all major categories classes open image dataset <eos> further approach particularly effective large number classes wide range noise annotations false positive annotations <eos> <eop> deep variation structured reinforcement learning visual relationship attribute detection <eos> despite progress visual perception tasks such image classification detection computers still struggle understand interdependency object scene whole <eos> relations between object their attributes <eos> existing method often ignore global context cues capturing interactions among different object instances only recognize handful types exhaustively training individual detectors all possible relationships <eos> capture such global interdependency propose deep variation structured reinforcement learning vrl framework sequentially discover object relationships attributes whole image <eos> first directed semantic action graph built using language priors provide rich compact representation semantic correlations between object categories predicates attributes <eos> next use variation structured traversal over action graph construct small adaptive action set each step based current state historical actions <eos> particular ambiguity aware object mining scheme used resolve semantic ambiguity among object categories object detector fails distinguish <eos> then make sequential predictions using deep rl framework incorporating global context cues semantic embeddings previously extracted phrases state vector <eos> experiments visual relationship detection vrd dataset large scale visual genome dataset validate superiority vrl achieve significantly better detection result datasets involving thousands relationship attribute types <eos> also demonstrate vrl able predict unseen types embedded action graph learning correlations shared graph nodes <eos> <eop> convolutional random walk network semantic image segmentation <eos> most current semantic segmentation method rely fully convolutional network fcns <eos> however their use large receptive fields many pooling layer cause low spatial resolution inside deep layer <eos> leads predictions poor localization around boundaries <eos> prior work attempted address issue post processing predictions crfs mrfs <eos> but such models often fail capture semantic relationships between object causes spatially disjoint predictions <eos> overcome problems recent method integrated crfs mrfs into fcn framework <eos> downside new models they much higher complexity than traditional fcns renders training testing more challenging <eos> work introduce simple yet effective convolutional random walk network rwn addresses issues poor boundary localization spatially fragmented predictions very little increase model complexity <eos> proposed rwn jointly optimizes objectives pixelwise affinity semantic segmentation <eos> combines two objectives via novel random walk layer enforces consistent spatial grouping deep layer network <eos> rwn implemented using standard convolution matrix multiplication <eos> allows easy integration into existing fcn frameworks enables end end training whole network via standard back propagation <eos> implementation rwn requires just additional parameters compared traditional fcns yet consistently produces improvement over fcns semantic segmentation scene labeling <eos> <eop> predicting ground level scene layout aerial imagery <eos> introduce novel strategy learning extract semantically meaningful feature aerial imagery <eos> instead manually labeling aerial imagery propose predict noisy semantic feature automatically extracted co located ground imagery <eos> network architecture takes aerial image input extracts feature using convolutional neural network then applies adaptive transformation map feature into ground level perspective <eos> use end end learning approach minimize difference between semantic segmentation extracted directly ground image semantic segmentation predicted solely based aerial image <eos> show model learned using strategy no additional training already capable rough semantic labeling aerial imagery <eos> furthermore demonstrate finetuning model achieve more accurate semantic segmentation than two baseline initialization strategies <eos> use network address task estimating geolocation geo orientation ground image <eos> finally show how feature extracted aerial image used hallucinate plausible ground level panorama <eos> <eop> simple weakly supervised instance semantic segmentation <eos> semantic labelling instance segmentation two tasks require particularly costly annotations <eos> starting weak supervision form bounding box detection annotations propose new approach require modification segmentation training procedure <eos> show when carefully designing input labels given bounding boxes even single round training enough improve over previously reported weakly supervised result <eos> overall weak supervision approach reaches quality fully supervised model both semantic labelling instance segmentation <eos> <eop> fast fourier color constancy <eos> present fast fourier color constancy ffcc color constancy algorithm solves illuminant estimation reducing spatial localization task torus <eos> operating frequency domain ffcc produces lower error rates than previous state art while being times faster <eos> unconventional approach introduces challenges regarding aliasing directional statistics preconditioning address <eos> producing complete posterior distribution over illuminants instead single illuminant estimate ffcc enables better training techniques effective temporal smoothing technique richer method error analysis <eos> implementation ffcc runs frames per second mobile device allowing used accurate real time temporally coherent automatic white balance algorithm <eos> <eop> attend you personalized image captioning context sequence memory network <eos> address personalization issues image captioning discussed yet previous research <eos> query image aim generate descriptive sentence accounting prior knowledge such user active vocabularies previous documents <eos> applications personalized image captioning tackle two post automation tasks hashtag prediction post generation newly collected instagram dataset consisting <eos> propose novel captioning model named context sequence memory network csmn <eos> its unique updates over previous memory network models include exploiting memory repository multiple types context information ii appending previously generated words into memory capture long term information without suffering vanishing gradient problem iii adopting cnn memory structure jointly represent nearby ordered memory slots better context understanding <eos> quantitative evaluation user studies via amazon mechanical turk show effectiveness three novel feature csmn its performance enhancement personalized image captioning over state art captioning models <eos> <eop> scalable surface reconstruction point clouds extreme scale density diversity <eos> paper present scalable approach robustly computing three dimensional surface mesh multi scale multi view stereo point clouds handle extreme jumps point density experiments three orders magnitude <eos> backbone approach combination octree data partitioning local delaunay tetrahedralization graph cut optimization <eos> graph cut optimization used twice once extract surface hypotheses local delaunay tetrahedralizations once merge overlapping surface hypotheses even when local tetrahedralizations share same topology <eos> formulation allows obtain constant memory consumption per sub problem while same time retaining density independent interpolation properties delaunay based optimization <eos> multiple public datasets demonstrate approach highly competitive state art terms accuracy completeness outlier resilience <eos> further demonstrate multi scale potential approach processing newly recorded dataset billion point point density variation more than four orders magnitude requiring less than gb ram per process <eos> <eop> weakly supervised cascaded convolutional network <eos> object detection challenging task visual understanding domain even more so if supervision weak <eos> recently few efforts handle task without expensive human annotations established promising deep neural network <eos> new architecture cascaded network proposed learn convolutional neural network cnn under such conditions <eos> introduce two such architectures either two cascade stages three trained end end pipeline <eos> first stage both architectures extracts best candidate class specific region proposals training fully convolutional network <eos> case three stage architecture middle stage provides object segmentation using output activation maps first stage <eos> final stage both architectures part convolutional neural network performs multiple instance learning proposals extracted previous stage <eos> experiments pascal voc large scale object datasets ilsvrc datasets show improvements areas weakly supervised object detection classification localization <eos> <eop> exclusivity consistency regularized multi view subspace clustering <eos> multi view subspace clustering aims partition set multi source data into their underlying groups <eos> boost performance multi view clustering numerous subspace learning algorithms developed recent years but rare exploitation representation complementarity between different views well indicator consistency among representations let alone considering them simultaneously <eos> paper propose novel multi view subspace clustering model attempts harness complementary information between different representations introducing novel position aware exclusivity term <eos> meanwhile consistency term employed make complementary representations further common indicator <eos> formulate above concerns into unified optimization framework <eos> experimental result several benchmark datasets conducted reveal effectiveness algorithm over other state arts <eos> <eop> look into person self supervised structure sensitive learning new benchmark human parsing <eos> human parsing recently attracted lot research interests due its huge application potentials <eos> however existing datasets limited number image annotations lack variety human appearances coverage challenging cases unconstrained environment <eos> paper introduce new benchmark look into person lip makes significant advance terms scalability diversity difficulty contribution feel crucial future developments human centric analysis <eos> comprehensive dataset contains over elaborately annotated image semantic part labels captured wider range viewpoints occlusions background complexity <eos> given rich annotations perform detailed analysis leading human parsing approaches gaining insights into success failures method <eos> furthermore contrast existing efforts improving feature discriminative capability solve human parsing exploring novel self supervised structure sensitive learning approach imposes human pose structures into parsing result without resorting extra supervision <eos> no need specifically labeling human joints model training <eos> self supervised learning framework injected into any advanced neural network help incorporate rich high level knowledge regarding human joints global perspective improve parsing result <eos> extensive evaluations lip public pascal person part dataset demonstrate superiority method <eos> <eop> semi calibrated near field photometric stereo <eos> three dimensional reconstruction shading information through photometric stereo considered very challenging problem computer vision <eos> although technique potentially provide highly detailed shape recovery its accuracy critically dependent numerous set factors among them reliability light sources emitting constant amount light <eos> work propose novel variational approach solve so called semi calibrated near field photometric stereo problem positions but brightness light sources known <eos> additionally take into account realistic modeling feature such perspective viewing geometry heterogeneous scene composition containing both diffuse specular object <eos> furthermore also relax point light source assumption usually constraints near field formulation explicitly calculating light attenuation maps <eos> synthetic experiments performed quantitative evaluation wide range cases whilst real experiments provide comparisons qualitatively outperforming state art <eos> <eop> finding tiny faces <eos> though tremendous strides made object recognition one remaining open challenges detecting small object <eos> explore three aspects problem context finding small faces role scale invariance image resolution contextual reasoning <eos> while most recognition approaches aim scale invariant cues recognizing px tall face fundamentally different than recognizing px tall face <eos> take different approach train separate detectors different scales <eos> maintain efficiency detectors trained multi task fashion they make use feature extracted multiple layer single deep feature hierarchy <eos> while training detectors large object straightforward crucial challenge remains training detectors small object <eos> show context crucial define templates make use massively large receptive fields template extends beyond object interest <eos> finally explore role scale pre trained deep network providing ways extrapolate network tuned limited scales rather extreme ranges <eos> demonstrate state art result massively benchmarked face datasets fddb wider face <eos> particular when compared prior art wider face result reduce error factor models produce ap while prior art ranges <eos> <eop> visual inertial semantic scene representation three dimensional object detection <eos> describe system detect object three dimensional space using video inertial sensors accelerometer gyrometer ubiquitous modern mobile platforms phones drones <eos> inertials afford ability impose class specific scale priors object provide global orientation reference <eos> minimal sufficient representation posterior semantic identity syntactic pose attributes object space decomposed into geometric term maintained localization mapping filter likelihood function approximated discriminatively trained convolutional neural network resulting system process video stream causally real time provides representation object scene persistent confidence presence object grows evidence object previously seen kept memory even when temporarily occluded their return into view automatically predicted prime re detection <eos> <eop> actionvlad learning spatio temporal aggregation action classification <eos> work introduce new video representation action classification aggregates local convolutional feature across entire spatio temporal extent video <eos> so integrating state art two stream network learnable spatio temporal feature aggregation <eos> resulting architecture end end trainable whole video classification <eos> investigate different strategies pooling across space time combining signals different streams <eos> find important pool jointly across space time but ii appearance motion streams best aggregated into their own separate representations <eos> finally show representation outperforms two stream base architecture large margin relative well outperforms other baselines comparable base architectures hmdb ucf charades video classification benchmarks <eos> <eop> predictive corrective network action detection <eos> while deep feature learning revolutionized techniques static image understanding same quite hold video processing <eos> architectures optimization techniques used video largely based off static image potentially underutilizing rich video information <eos> work rethink both underlying network architecture stochastic learning paradigm temporal data <eos> so draw inspiration classic theory linear dynamic systems modeling time series <eos> extending such models include nonlinear mappings derive series novel recurrent neural network sequentially make top down predictions about future then correct predictions bottom up observations <eos> predictive corrective network number desirable properties they adaptively focus computation surprising frames predictions require large corrections they simplify learning only residual like corrective terms need learned over time they naturally decorrelate input data stream hierarchical fashion producing more reliable signal learning each layer network <eos> provide extensive analysis lightweight interpretable framework demonstrate model competitive two stream network three challenging datasets without need computationally expensive optical flow <eos> <eop> fastmask segment multi scale object candidates one shot <eos> object appear scale differently natural image <eos> fact requires method dealing object centric tasks <eos> object proposal robust performance over variances object scales <eos> paper present novel segment proposal framework namely fastmask takes advantage hierarchical feature deep convolutional neural network segment multi scale object one shot <eos> innovatively adapt segment proposal network into three different functional components body neck head <eos> further propose weight shared residual neck module well scale tolerant attentional head module efficient one shot inference <eos> ms coco benchmark proposed fastmask outperforms all state art segment proposal method average recall being times faster <eos> moreover slight trade off accuracy fastmask segment object near real time fps resolution image demonstrating its potential practical applications <eos> implementation available github <eos> com voidrank fastmask <eos> <eop> combinatorial solution non rigid three dimensional shape image matching <eos> propose combinatorial solution problem non rigidly matching three dimensional shape three dimensional image data <eos> end model shape triangular mesh allow each triangle mesh rigidly transformed achieve suitable matching image <eos> penalising distance relative rotation between neighbouring triangles matching compromises between image shape information <eos> paper resolve two major challenges firstly address resulting large np hard combinatorial problem suitable graph theoretic approach <eos> secondly propose efficient discretisation unbounded dimensional lie group se <eos> knowledge first combinatorial formulation non rigid three dimensional shape image matching <eos> contrast existing local gradient descent optimisation method obtain solutions require good initialisation within bound optimal solution <eos> evaluate proposed combinatorial method two problems non rigid three dimensional shape shape non rigid three dimensional shape image registration demonstrate provides promising result <eos> <eop> interpretable structure evolving lstm <eos> paper develops general framework learning interpretable data representation via long short term memory lstm recurrent neural network over hierarchal graph structures <eos> instead learning lstm models over pre fixed structures propose further learn intermediate interpretable multi level graph structures progressive stochastic way data during lstm network optimization <eos> thus call model structure evolving lstm <eos> particular starting initial element level graph representation each node small data element structure evolving lstm gradually evolves multi level graph representations stochastically merging graph nodes high compatibilities along stacked lstm layer <eos> each lstm layer estimate compatibility two connected nodes their corresponding lstm gate outputs used generate merging probability <eos> candidate graph structures accordingly generated nodes grouped into cliques their merging probabilities <eos> then produce new graph structure metropolis hasting algorithm alleviates risk getting stuck local optimums stochastic sampling acceptance probability <eos> once graph structure accepted higher level graph then constructed taking partitioned cliques its nodes <eos> during evolving process representation becomes more abstracted higher levels redundant information filtered out allowing more efficient propagation long range data dependencies <eos> evaluate effectiveness structure evolving lstm application semantic object parsing demonstrate its advantage over state art lstm models standard benchmarks <eos> <eop> generating future adversarial transformers <eos> learn models generate immediate future video <eos> problem two main challenges <eos> firstly since future uncertain models should multi modal difficult learn <eos> secondly since future similar past models store low level details complicates learning high level semantics <eos> propose framework tackle both challenges <eos> present model generates future transforming pixels past <eos> approach explicitly disentangles model memory prediction helps model learn desirable invariances <eos> experiments suggest model generate short video plausible futures <eos> believe predictive models many applications robotics health care video understanding <eos> <eop> budget aware deep semantic video segmentation <eos> work study poorly understood trade off between accuracy runtime costs deep semantic video segmentation <eos> while recent work demonstrated advantages learning speed up deep activity detection clear if similar advantages will hold very different segmentation loss function defined over individual pixels across frames <eos> deep video segmentation most time consuming step represents application cnn every frame assigning class labels every pixel typically taking times video footage <eos> motivates new budget aware framework learns optimally select small subset frames pixelwise labeling cnn then efficiently interpolates obtained segmentations yet unprocessed frames <eos> interpolation may use either simple optical flow guided mapping pixel labels another significantly less complex thus faster cnn <eos> formalize frame selection markov decision process specify long short term memory lstm network model policy selecting frames <eos> training lstm develop policy gradient reinforcement learning approach approximating gradient non decomposable non differentiable objective <eos> evaluation two benchmark video datasets show new framework able significantly reduce computation time maintain competitive video segmentation accuracy under varying budgets <eos> <eop> spatially adaptive computation time residual network <eos> paper proposes deep learning architecture based residual network dynamically adjusts number executed layer region image <eos> architecture end end trainable deterministic problem agnostic <eos> therefore applicable without any modifications wide range computer vision problems such image classification object detection image segmentation <eos> present experimental result showing model improves computational efficiency residual network challenging imagenet classification coco object detection datasets <eos> additionally evaluate computation time maps visual saliency dataset cat find they correlate surprisingly well human eye fixation positions <eos> <eop> order preserving wasserstein distance sequence matching <eos> present new distance measure between sequences tackle local temporal distortion periodic sequences arbitrary starting point <eos> through viewing instances sequences empirical sample unknown distribution cast calculation distance between sequences optimal transport problem <eos> preserve inherent temporal relationships instances sequences smooth optimal transport problem two novel temporal regularization terms <eos> inverse difference moment regularization enforces transport local homogeneous structures kl divergence prior distribution regularization prevents transport between instances far temporal positions <eos> show problem efficiently optimized through matrix scaling algorithm <eos> extensive experiments different datasets different classifiers show proposed distance outperforms traditional dtw variants smoothed optimal transport distance without temporal regularization <eos> <eop> split brain autoencoders unsupervised learning cross channel prediction <eos> propose split brain autoencoders straightforward modification traditional autoencoder architecture unsupervised representation learning <eos> method adds split network resulting two disjoint sub network <eos> each sub network trained perform difficult task predicting one subset data channels another <eos> together sub network extract feature entire input signal <eos> forcing network solve cross channel prediction tasks induce representation within network transfers well other unseen tasks <eos> method achieves state art performance several large scale transfer learning benchmarks <eos> <eop> srn side output residual network object symmetry detection wild <eos> paper establish baseline object symmetry detection complex backgrounds presenting new benchmark end end deep learning approach opening up promising direction symmetry detection wild <eos> new benchmark named sym pascal spans challenges including object diversity multi object part invisibility various complex backgrounds far beyond existing datasets <eos> proposed symmetry detection approach named side output residual network srn leverages output residual units rus fit errors between object symmetry ground truth outputs rus <eos> stacking rus deep shallow manner srn exploits flow errors among multiple scales ease problems fitting complex outputs limited layer suppressing complex backgrounds effectively matching object symmetry different scales <eos> experimental result validate both benchmark its challenging aspects related real world image state art performance symmetry detection approach <eos> benchmark code srn publicly available github <eos> com kevinkecc srn <eos> <eop> spindle net person re identification human body region guided feature decomposition fusion <eos> person re identification reid important task video surveillance various applications <eos> non trivial due complex background clutters varying illumination conditions uncontrollable camera settings <eos> moreover person body misalignment caused detectors pose variations sometimes too severe feature matching across image <eos> study propose novel convolutional neural network cnn called spindle net based human body region guided multi stage feature decomposition tree structured competitive feature fusion <eos> first time human body structure information considered cnn framework facilitate feature learning <eos> proposed spindle net brings unique advantages separately captures semantic feature different body region thus macro micro body feature well aligned across image learned region feature different semantic region merged competitive scheme discriminative feature well preserved <eos> state art performance achieved multiple datasets large margins <eos> further demonstrate robustness effectiveness proposed spindle net proposed dataset sensereid without fine tuning <eos> <eop> borrowing treasures wealthy deep transfer learning through selective joint fine tuning <eos> deep neural network require large amount labeled training data during supervised learning <eos> however collecting labeling so much data might infeasible many cases <eos> paper introduce deep transfer learning scheme called selective joint fine tuning improving performance deep learning tasks insufficient training data <eos> scheme target learning task insufficient training data carried out simultaneously another source learning task abundant training data <eos> however source learning task use all existing training data <eos> core idea identify use subset training image original source learning task whose low level characteristics similar target learning task jointly fine tune shared convolutional layer both tasks <eos> specifically compute descriptors linear nonlinear filter bank responses training image both tasks use such descriptors search desired subset training sample source learning task <eos> experiments demonstrate deep transfer learning scheme achieves state art performance multiple visual classification tasks insufficient training data deep learning <eos> such tasks include caltech mit indoor fine grained classification problems oxford flowers stanford dogs <eos> comparison fine tuning without source domain proposed method improve classification accuracy using single model <eos> codes models available github <eos> com zyyszj selective joint fine tuning <eos> <eop> unified embedding metric learning zero exemplar event detection <eos> event detection unconstrained video conceived content based video retrieval two modalities textual visual <eos> given text describing novel event goal rank related video accordingly <eos> task zero exemplar no video examples given novel event <eos> related works train bank concept detectors external data sources <eos> detectors predict confidence scores test video ranked retrieved accordingly <eos> contrast learn joint space visual textual representations embedded <eos> space casts novel event probability pre defined events <eos> also learns measure distance between event its related video <eos> model trained end end publicly available eventnet <eos> when applied trecvid multimedia event detection dataset outperforms state art considerable margin <eos> <eop> practical method fully automatic intrinsic camera calibration using directionally encoded light <eos> calibrating intrinsic properties camera one fundamental tasks required variety computer vision image processing tasks <eos> precise measurement focal length location principal point well distortion parameters lens crucial example three dimensional reconstruction <eos> although variety method exist achieve goal they often cumbersome carry out require substantial manual interaction expert knowledge significant operating volume <eos> propose novel calibration method based usage directionally encoded light rays estimating intrinsic parameters <eos> enables fully automatic calibration small device mounted close front lens element still enables accuracy comparable standard method even when lens focused up infinity <eos> method overcomes mentioned limitations since guarantees accurate calibration without any human intervention while requiring only limited amount space <eos> besides approach also allows estimate distance focal plane well size aperture <eos> demonstrate advantages proposed method evaluating several camera lens configurations using prototypical devices <eos> <eop> modeling relationships referential expressions compositional modular network <eos> people often refer entities image terms their relationships other entities <eos> example black cat sitting under table refers both black cat entity its relationship another table entity <eos> understanding relationships essential interpreting grounding such natural language expressions <eos> most prior work focuses either grounding entire referential expressions holistically one region localizing relationships based fixed set categories <eos> paper instead present modular deep architecture capable analyzing referential expressions into their component parts identifying entities relationships mentioned input expression grounding them all scene <eos> call approach compositional modular network cmns novel architecture learns linguistic analysis visual inference end end <eos> approach built around two types neural modules inspect local region pairwise interactions between region <eos> evaluate cmns multiple referential expression datasets outperforming state art approaches all tasks <eos> <eop> image image translation conditional adversarial network <eos> investigate conditional adversarial network general purpose solution image image translation problems <eos> network only learn mapping input image output image but also learn loss function train mapping <eos> makes possible apply same generic approach problems traditionally would require very different loss formulations <eos> demonstrate approach effective synthesizing photos label maps reconstructing object edge maps colorizing image among other tasks <eos> moreover since release pix pix software associated paper hundreds twitter users posted their own artistic experiments using system <eos> community no longer hand engineer mapping functions work suggests achieve reasonable result without handengineering loss functions either <eos> <eop> counting everyday object everyday scenes <eos> interested counting number instances object classes natural everyday image <eos> previous counting approaches tackle problem restricted domains such counting pedestrians surveillance video <eos> counts also estimated outputs other vision tasks like object detection <eos> work build dedicated models counting designed tackle large variance counts appearances scales object found natural scenes <eos> approach inspired phenomenon subitizing ability humans make quick assessments counts given perceptual signal small count values <eos> given natural scene employ divide conquer strategy while incorporating context across scene adapt subitizing idea counting <eos> approach offers consistent improvements over numerous baseline approaches counting pascal voc coco datasets <eos> subsequently study how counting used improve object detection <eos> then show proof concept application counting method task visual question answering studying how many questions vqa coco qa datasets <eos> <eop> hand keypoint detection single image using multiview bootstrapping <eos> present approach uses multi camera system train fine grained detectors keypoints prone occlusion such joints hand <eos> call procedure multiview bootstrapping first initial keypoint detector used produce noisy labels multiple views hand <eos> noisy detections then triangulated three dimensional using multiview geometry marked outliers <eos> finally reprojected triangulations used new labeled training data improve detector <eos> repeat process generating more labeled data each iteration <eos> derive result analytically relating minimum number views achieve target true false positive rates given detector <eos> method used train hand keypoint detector single image <eos> resulting keypoint detector runs realtime rgb image accuracy comparable method use depth sensors <eos> single view detector triangulated over multiple views enables three dimensional markerless hand motion capture complex object interactions <eos> <eop> knowledge acquisition visual question answering via iterative querying <eos> humans possess extraordinary ability learn new skills new knowledge problem solving <eos> such learning ability also required automatic model deal arbitrary open ended questions visual world <eos> propose neural based approach acquiring task driven information visual question answering vqa <eos> model proposes queries actively acquire relevant information external auxiliary data <eos> supporting evidence either human curated automatic sources encoded stored into memory bank <eos> show acquiring task driven evidence effectively improves model performance both visual vqa datasets moreover queries offer certain level interpretability iterative qa model <eos> <eop> few shot object recognition machine labeled web image <eos> tremendous advances made convolutional neural network convnets object recognition now easily obtain adequately reliable machine labeled annotations easily predictions off shelf convnets <eos> work present abstraction memory based framework few shot learning building upon machine labeled image annotations <eos> method takes large scale machine annotated dataset <eos> openimages external memory bank <eos> external memory bank information stored memory slots form key value image feature regarded key label embedding serves value <eos> when queried few shot examples model selects visually similar data external memory bank writes useful information obtained related external data into another memory bank <eos> long short term memory lstm controllers attention mechanisms utilized guarantee data written abstraction memory correlates query example <eos> abstraction memory concentrates information external memory bank make few shot recognition effective <eos> experiments first confirm model learn conduct few shot object recognition clean human labeled data imagenet dataset <eos> then demonstrate model machine labeled image annotations very effective abundant resources performing object recognition novel categories <eos> experimental result show proposed model machine labeled annotations achieves great result only difference accuracy between machine labeled annotations human labeled annotations <eos> <eop> vqa machine learning how use existing vision algorithms answer new questions <eos> one most intriguing feature visual question answering vqa challenge unpredictability questions <eos> extracting information required answer them demands variety image operations detection counting segmentation reconstruction <eos> train method perform even one operations accurately image question answer tuples would challenging but aim achieve them all limited set such training data seems ambitious best <eos> method thus learns how exploit set external off shelf algorithms achieve its goal approach something common neural turing machine <eos> core proposed method new co attention model <eos> addition proposed approach generates human readable reasons its decision still trained end end without ground truth reasons being given <eos> demonstrate effectiveness two publicly available datasets visual genome vqa show produces state art result both cases <eos> <eop> learning deep binary descriptor multi quantization <eos> paper propose unsupervised feature learning method called deep binary descriptor multi quantization dbd mq visual matching <eos> existing learning based binary descriptors such compact binary face descriptor cbfd deepbit utilize rigid sign function binarization despite data distributions thereby suffering severe quantization loss <eos> order address limitation dbd mq considers binarization multi quantization task <eos> specifically apply autoencoders kaes network jointly learn parameters binarization functions under deep learning framework so discriminative binary descriptors obtained fine grained multi quantization <eos> extensive experimental result different visual analysis including patch retrieval image matching image retrieval show dbd mq outperforms most existing binary feature descriptors <eos> <eop> joint discriminative bayesian dictionary classifier learning <eos> propose jointly learn discriminative bayesian dictionary along linear classifier using coupled beta bernoulli processes <eos> representation model uses separate base measures dictionary classifier but associates them class specific training data using same bernoulli distributions <eos> bernoulli distributions control frequency factors <eos> dictionary atoms used data representations they inferred while accounting class labels approach <eos> further encourage discrimination dictionary model uses separate set bernoulli distributions represent data different classes <eos> approach adaptively learns association between dictionary atoms class labels while tailoring classifier relation joint inference over dictionary classifier <eos> once test sample represented over dictionary its representation accurately labelled classifier due strong coupling between dictionary classifier <eos> derive gibbs sampling equations joint representation model test approach face object scene action recognition establish its effectiveness <eos> <eop> graph regularized deep neural network unsupervised image representation learning <eos> deep auto encoder dae shown its promising power high level representation learning <eos> perspective manifold learning propose graph regularized deep neural network gr dnn endue traditional daes ability retaining local geometric structure <eos> deep structured regularizer formulated upon multi layer perceptions capture structure <eos> robust discriminative embedding space learned simultaneously preserve high level semantics geometric structure within local manifold tangent space <eos> theoretical analysis presents close relationship between proposed graph regularizer graph laplacian regularizer terms optimization objective <eos> also alleviate growth network complexity introducing anchor based bipartite graph guarantees good scalability large scale data <eos> experiments four datasets show comparable result proposed gr dnn state art method <eos> <eop> hsfm hybrid structure motion <eos> structure motion sfm method broadly categorized incremental global according their ways estimate initial camera poses <eos> while incremental system advanced robustness accuracy efficiency remains its key challenge <eos> solve problem global reconstruction system simultaneously estimates all camera poses epipolar geometry graph but usually sensitive outliers <eos> work propose new hybrid sfm method tackle issues efficiency accuracy robustness unified framework <eos> more specifically propose adaptive community based rotation averaging method first estimate camera rotations global manner <eos> then based estimated camera rotations camera centers computed incremental way <eos> extensive experiments show hybrid method performs similarly better than many state art global sfm approaches terms computational efficiency while achieves similar reconstruction accuracy robustness two other state art incremental sfm approaches <eos> <eop> perceptual generative adversarial network small object detection <eos> detecting small object notoriously challenging due their low resolution noisy representation <eos> existing object detection pipelines usually detect small object through learning representations all object multiple scales <eos> however performance gain such ad hoc architectures usually limited pay off computational cost <eos> work address small object detection problem developing single architecture internally lifts representations small object super resolved ones achieving similar characteristics large object thus more discriminative detection <eos> purpose propose new perceptual generative adversarial network perceptual gan model improves small object detection through narrowing representation difference small object large ones <eos> specifically its generator learns transfer perceived poor representations small object super resolved ones similar enough real large object fool competing discriminator <eos> meanwhile its discriminator competes generator identify generated representation imposes additional perceptual requirement generated representations small object must beneficial detection purpose generator <eos> extensive evaluations challenging tsinghua tencent caltech benchmark well demonstrate superiority perceptual gan detecting small object including traffic signs pedestrians over well established state arts <eos> <eop> deep roots improving cnn efficiency hierarchical filter groups <eos> propose new method creating computationally efficient compact convolutional neural network cnn using novel sparse connection structure resembles tree root <eos> allows significant reduction computational cost number parameters compared state art deep cnn without compromising accuracy exploiting sparsity inter layer filter dependencies <eos> validate approach using train more efficient variants state art cnn architectures evaluated cifar ilsvrc datasets <eos> result show similar higher accuracy than baseline architectures much less computation measured cpu gpu timings <eos> example resnet model fewer parameters fewer floating point operations faster cpu gpu <eos> deeper resnet model fewer parameters fewer floating point operations while maintaining state art accuracy <eos> googlenet model fewer parameters faster cpu gpu <eos> <eop> anti glare tightly constrained optimization eyeglass reflection removal <eos> absence clear eye visibility only degrades aesthetic value entire face image but also creates difficulties many computer vision tasks <eos> even mild reflections produce undesired superpositions visual information whose decomposition into background reflection layer using single image highly ill posed problem <eos> work enforce tight constraints derived thoroughly analysing properties eyeglass reflection <eos> addition strategy regularizes gradients reflection layer highly sparse proposes facial symmetry prior via formulating non convex optimization scheme removes reflections within few iterations <eos> experiments frontal face image inputs demonstrate high quality reflection removal result improvement iris detection rate <eos> <eop> xception deep learning depthwise separable convolutions <eos> present interpretation inception modules convolutional neural network being intermediate step between regular convolution depthwise separable convolution operation depthwise convolution followed pointwise convolution <eos> light depthwise separable convolution understood inception module maximally large number towers <eos> observation leads propose novel deep convolutional neural network architecture inspired inception inception modules replaced depthwise separable convolutions <eos> show architecture dubbed xception slightly outperforms inception imagenet dataset inception was designed significantly outperforms inception larger image classification dataset comprising million image classes <eos> since xception architecture same number parameters inception performance gains due increased capacity but rather more efficient use model parameters <eos> <eop> learning detailed face reconstruction single image <eos> reconstructing detailed geometric structure face given image key many computer vision graphics applications such motion capture reenactment <eos> reconstruction task challenging human faces vary extensively when considering expressions poses textures intrinsic geometries <eos> while many approaches tackle complexity using additional data reconstruct face single subject extracting facial surface single image remains difficult problem <eos> result single image based method usually provide only rough estimate facial geometry <eos> contrast propose leverage power convolutional neural network produce highly detailed face reconstruction single image <eos> purpose introduce end end cnn framework derives shape coarse fine fashion <eos> proposed architecture composed two main blocks network recovers coarse facial geometry coarsenet followed cnn refines facial feature geometry finenet <eos> proposed network connected novel layer renders depth image given mesh three dimensional unlike object recognition detection problems there no suitable datasets training cnn perform face geometry reconstruction <eos> therefore training regime begins supervised phase based synthetic image followed unsupervised phase uses only unconstrained facial image <eos> accuracy robustness proposed model demonstrated both qualitative quantitative evaluation tests <eos> <eop> stereo based three dimensional reconstruction dynamic fluid surfaces global optimization <eos> three dimensional reconstruction dynamic fluid surfaces open challenging problem computer vision <eos> unlike previous approaches reconstruct each surface point independently often return noisy depth maps propose novel global optimization based approach recovers both depths normals all three dimensional point simultaneously <eos> using traditional refraction stereo setup capture wavy appearance pre generated random pattern then estimate correspondences between captured image known background tracking pattern <eos> assuming light refracted only once through fluid interface minimize objective function incorporates both cross view normal consistency constraint single view normal consistency constraints <eos> key idea normals required light refraction based snell law one view should agree only ones second view but also ones estimated local three dimensional geometry <eos> moreover effective reconstruction error metric designed estimating refractive index fluid <eos> report experimental result both synthetic real data demonstrating proposed approach accurate shows superiority over conventional stereo based method <eos> <eop> deep video deblurring hand held cameras <eos> motion blur camera shake major problem video captured hand held devices <eos> unlike single image deblurring video based approaches take advantage abundant information exists across neighboring frames <eos> result best performing method rely alignment nearby frames <eos> however aligning image computationally expensive fragile procedure method aggregate information must therefore able identify region accurately aligned task requires high level scene understanding <eos> work introduce deep learning solution video deblurring cnn trained end end learn how accumulate information across frames <eos> train network collected dataset real video recorded high frame rate camera use generate synthetic motion blur supervision <eos> show feature learned dataset extend deblurring motion blur arises due camera shake wide range video compare quality result number other baselines <eos> <eop> accurate optical flow via direct cost volume processing <eos> present optical flow estimation approach operates full four dimensional cost volume <eos> direct approach shares structural benefits leading stereo matching pipelines known yield high accuracy <eos> day such approaches considered impractical due size cost volume <eos> show full four dimensional cost volume constructed fraction second due its regularity <eos> then exploit regularity further adapting semi global matching four dimensional setting <eos> yields pipeline achieves significantly higher accuracy than state art optical flow method while being faster than most <eos> approach outperforms all published general purpose optical flow method both sintel kitti benchmarks <eos> <eop> weakly supervised actor action segmentation via robust multi task ranking <eos> fine grained activity understanding video attracted considerable recent attention shift action classification detailed actor action understanding provides compelling result perceptual needs cutting edge autonomous systems <eos> however current method detailed understanding actor action significant limitations they require large amounts finely labeled data they fail capture any internal relationship among actors actions <eos> address issues paper propose novel robust multi task ranking model weakly supervised actor action segmentation only video level tags given training sample <eos> model able share useful information among different actors actions while learning ranking matrix select representative supervoxels actors actions respectively <eos> final segmentation result generated conditional random field considers various ranking scores different video parts <eos> extensive experimental result actor action dataset demonstrate proposed approach outperforms state art weakly supervised method performs well top performing fully supervised method <eos> <eop> feedback network <eos> urrently most successful learning models computer vision based learning successive representations followed decision layer <eos> usually actualized through feedforward multilayer neural network <eos> convnets each layer forms one such successive representations <eos> however alternative achieve same goal feedback based approach representation formed iterative manner based feedback received previous iteration output <eos> establish feedback based approach several core advantages over feedforward enables making early predictions query time its output naturally conforms hierarchical structure label space <eos> taxonomy provides new basis curriculum learning <eos> observe feedback develops considerably different representation compared feedforward counterparts line aforementioned advantages <eos> provide general feedback based learning architecture instantiated using existing rnns endpoint result par better than existing feedforward network addition above advantages <eos> <eop> re ranking person re identification reciprocal encoding <eos> when considering person re identification re id retrieval process re ranking critical step improve its accuracy <eos> yet re id community limited effort devoted re ranking especially fully automatic unsupervised solutions <eos> paper propose reciprocal encoding method re rank re id result <eos> hypothesis if gallery image similar probe reciprocal nearest neighbors more likely true match <eos> specifically given image reciprocal feature calculated encoding its reciprocal nearest neighbors into single vector used re ranking under jaccard distance <eos> final distance computed combination original distance jaccard distance <eos> re ranking method require any human interaction any labeled data so applicable large scale datasets <eos> experiments large scale market cuhk mars prw datasets confirm effectiveness method <eos> <eop> deep visual semantic quantization efficient image retrieval <eos> compact coding widely applied approximate nearest neighbor search large scale image retrieval due its computation efficiency retrieval quality <eos> paper presents compact coding solution focus deep learning quantization approach improves retrieval quality end end representation learning compact encoding already shown superior performance over hashing solutions similarity retrieval <eos> propose deep visual semantic quantization dvsq first approach learning deep quantization models labeled image data well semantic information underlying general text domains <eos> main contribution lies jointly learning deep visual semantic embeddings visual semantic quantizers using carefully designed hybrid network well specified loss functions <eos> dvsq enables efficient effective image retrieval supporting maximum inner product search computed based learned codebooks fast distance table lookup <eos> comprehensive empirical evidence shows dvsq generate compact binary codes yield state art similarity retrieval performance standard benchmarks <eos> <eop> sequential person recognition photo albums recurrent network <eos> recognizing identities people everyday photos still very challenging problem machine vision due issues such non frontal faces changes clothing location lighting <eos> recent studies shown rich relational information between people same photo help recognizing their identities <eos> work propose model relational information between people sequence prediction task <eos> core work novel recurrent network architecture relational information between instances labels appearance modeled jointly <eos> addition relational cues scene context incorporated sequence prediction model no additional cost <eos> sense approach unified framework modeling both contextual cues visual appearance person instances <eos> model trained end end sequence annotated instances photo inputs sequence corresponding labels targets <eos> demonstrate simple but elegant formulation achieves state art performance newly released people photo albums pipa dataset <eos> <eop> vip cnn visual phrase guided convolutional neural network <eos> intermediate level task connecting image captioning object detection visual relationship detection started catch researchers attention because its descriptive power clear structure <eos> detects object captures their pair wise interactions subject predicate object triplet <eos> person ride horse <eos> paper each visual relationship considered phrase three components <eos> formulate visual relationship detection three inter connected recognition problems propose visual phrase guided convolutional neural network vip cnn address them simultaneously <eos> vip cnn present phrase guided message passing structure pmps establish connection among relationship components help model consider three problems jointly <eos> corresponding non maximum suppression method model training strategy also proposed <eos> experimental result show vip cnn outperforms state art method both speed accuracy <eos> further pretrain vip cnn cleansed visual genome relationship dataset found perform better than pretraining imagenet task <eos> <eop> deep joint rain detection removal single image <eos> paper address rain removal problem single image even presence heavy rain rain streak accumulation <eos> core ideas lie new rain image model new deep learning architecture <eos> add binary map provides rain streak locations existing model comprises rain streak layer background layer <eos> create model consisting component representing rain streak accumulation individual streaks cannot seen thus visually similar mist fog another component representing various shapes directions overlapping rain streaks usually happen heavy rain <eos> based model develop multi task deep learning architecture learns binary rain streak map appearance rain streaks clean background ultimate output <eos> additional binary map critically beneficial since its loss function provide additional strong information network <eos> handle rain streak accumulation again phenomenon visually similar mist fog various shapes directions overlapping rain streaks propose recurrent rain detection removal network removes rain streaks clears up rain accumulation iteratively progressively <eos> each recurrence method new contextualized dilated network developed exploit regional contextual information produce better representations rain detection <eos> evaluation real image particularly heavy rain shows effectiveness models architecture <eos> <eop> person re identification wild <eos> paper presents novel large scale dataset comprehensive baselines end end pedestrian detection person recognition raw video frames <eos> baselines address three issues performance various combinations detectors recognizers mechanisms pedestrian detection help improve overall re identification re id accuracy assessing effectiveness different detectors re id <eos> make three distinct contributions <eos> first new dataset prw introduced evaluate person re identification wild using video acquired through six synchronized cameras <eos> contains identities frames pedestrians annotated their bounding box positions identities <eos> extensive benchmarking result presented dataset <eos> second show pedestrian detection aids re id through two simple yet effective improvements cascaded fine tuning strategy trains detection model first then classification model confidence weighted similarity cws metric incorporates detection scores into similarity measurement <eos> third derive insights evaluating detector performance particular scenario accurate person re id <eos> <eop> deep self taught learning weakly supervised object localization <eos> most existing weakly supervised localization wsl approaches learn detectors finding positive bounding boxes based feature learned image level supervision <eos> however feature contain spatial location related information usually provide poor quality positive sample training detector <eos> overcome issue propose deep self taught learning approach makes detector learn object level feature reliable acquiring tight positive sample afterwards re train itself based them <eos> consequently detector progressively improves its detection ability localizes more informative positive sample <eos> implement such self taught learning propose seed sample acquisition method via image object transferring dense subgraph discovery find reliable positive sample initializing detector <eos> online supportive sample harvesting scheme further proposed dynamically select most confident tight positive sample train detector mutual boosting way <eos> prevent detector being trapped poor optima due overfitting propose new relative improvement predicted cnn scores guiding self taught learning process <eos> extensive experiments pascal show approach outperforms state arts strongly validating its effectiveness <eos> <eop> killingfusion non rigid three dimensional reconstruction without correspondences <eos> introduce geometry driven approach real time three dimensional reconstruction deforming surfaces single rgb stream without any templates shape priors <eos> end tackle problem non rigid registration level set evolution without explicit correspondence search <eos> given pair signed distance fields sdfs representing shapes interest estimate dense deformation field aligns them <eos> defined displacement vector field same resolution sdfs determined iteratively via variational minimization <eos> ensure generates plausible shapes propose novel regularizer imposes local rigidity requiring deformation smooth approximately killing vector field <eos> generating nearly isometric motions <eos> moreover enforce level set property unity gradient magnitude preserved over iterations <eos> result killingfusion reliably reconstructs object undergoing topological changes fast inter frame motion <eos> addition incrementally building model scratch system also deform complete surfaces <eos> demonstrate capabilities several public datasets introduce own sequences permit both qualitative quantitative comparison related approaches <eos> <eop> context aware correlation filter tracking <eos> correlation filter cf based trackers recently gained lot popularity due their impressive performance benchmark datasets while maintaining high frame rates <eos> significant amount recent research focuses incorporation stronger feature richer representation tracking target <eos> however only helps discriminate target background within small neighborhood <eos> paper present framework allows explicit incorporation global context within cf trackers <eos> reformulate original optimization problem provide closed form solution single multi dimensional feature primal dual domain <eos> extensive experiments demonstrate framework significantly improves performance many cf trackers only modest impact frame rate <eos> <eop> missing modalities imputation via cascaded residual autoencoder <eos> affordable sensors lead increasing interest acquiring modeling data multiple modalities <eos> learning multiple modalities shown significantly improve performance object recognition <eos> however practice common sensing equipment experiences unforeseeable malfunction configuration issues leading corrupted data missing modalities <eos> most existing multi modal learning algorithms could handle missing modalities would discard either all modalities missing values all corrupted data <eos> leverage valuable information corrupted data propose impute missing data leveraging relatedness among different modalities <eos> specifically propose novel cascaded residual autoencoder cra impute missing modalities <eos> stacking residual autoencoders cra grows iteratively model residual between current prediction original data <eos> extensive experiments demonstrate superior performance cra both data imputation object recognition task imputed data <eos> <eop> disentangled representation learning gan pose invariant face recognition <eos> large pose discrepancy between two face image one key challenges face recognition <eos> conventional approaches pose invariant face recognition either perform face frontalization learn pose invariant representation non frontal face image <eos> argue more desirable perform both tasks jointly allow them leverage each other <eos> end paper proposes disentangled representation learning generative adversarial network dr gan three distinct novelties <eos> first encoder decoder structure generator allows dr gan learn generative discriminative representation addition image synthesis <eos> second representation explicitly disentangled other face variations such pose through pose code provided decoder pose estimation discriminator <eos> third dr gan take one multiple image input generate one unified representation along arbitrary number synthetic image <eos> quantitative qualitative evaluation both controlled wild databases demonstrate superiority dr gan over state art <eos> <eop> discretely coding semantic rank orders supervised image hashing <eos> learning hash recognized accomplish highly efficient storage retrieval large scale visual data <eos> particularly ranking based hashing techniques recently attracted broad research attention because ranking accuracy among retrieved data well explored their objective more applicable realistic search tasks <eos> however directly optimizing discrete hash codes without continuous relaxations nonlinear ranking objective infeasible either traditional optimization method even recent discrete hashing algorithms <eos> address challenging issue paper introduce novel supervised hashing method dubbed discrete semantic ranking hashing dserh aims directly embed semantic rank orders into binary codes <eos> dserh generalized adaptive discrete minimization adm approach proposed discretely optimize binary codes quadratic nonlinear ranking objective iterative manner guaranteed converge quickly <eos> additionally instead using independent labels form rank orders previous works generate listwise rank orders high level semantic word embeddings quantitatively capture intrinsic correlation between different categories <eos> evaluate dserh coupled both linear deep convolutional neural network cnn hash functions three image datasets <eos> cifar sun imagenet result manifest dserh outperform state art ranking based hashing method <eos> <eop> nid slam robust monocular slam using normalised information distance <eos> propose direct monocular slam algorithm based normalised information distance nid metric <eos> contrast current state art direct method based photometric error minimisation information theoretic nid metric provides robustness appearance variation due lighting weather structural changes scene <eos> demonstrate successful localisation mapping across changes lighting synthetic indoor scene across changes weather direct sun rain snow using real world data collected vehicle mounted camera <eos> approach runs real time consumer gpu using opengl provides comparable localisation accuracy state art photometric method but significantly outperforms both direct feature based method robustness appearance changes <eos> <eop> efficient optimization hierarchically structured interacting segments hints <eos> propose effective optimization algorithm general hierarchical segmentation model geometric interactions between segments <eos> any given tree specify partial order over object labels defining hierarchy <eos> well established segment interactions such inclusion exclusion margin constraints make model significantly more discriminant <eos> however existing optimization method allow full use such models <eos> generic expansion result weak local minima while common binary multi layered formulations lead non submodularity complex high order potentials polar domain unwrapping shape biases <eos> practice applying method arbitrary trees work except simple cases <eos> main contribution optimization method hierarchically structured interacting segments hints model arbitrary trees <eos> path moves algorithm based multi label mrf formulation seen combination well known expansion ishikawa techniques <eos> show state art biomedical segmentation many diverse examples complex trees <eos> <eop> scc semantic context cascade efficient action detection <eos> despite recent advances large scale video analysis action detection remains one most challenging unsolved problems computer vision <eos> snag part due large volume data needs analyzed detect actions video <eos> existing approaches mitigated computational cost but still method lack rich high level semantics helps them localize actions quickly <eos> paper introduce semantic cascade context scc model aims detect action long video sequences <eos> embracing semantic priors associated human activities scc produces high quality class specific action proposals prune unrelated activities cascade fashion <eos> experimental result activitynet unveils scc achieves state art performance action detection while operating real time <eos> <eop> semantic amodal segmentation <eos> common visual recognition tasks such classification object detection semantic segmentation rapidly reaching maturity given recent rate progress unreasonable conjecture techniques many problems will approach human levels performance next few years <eos> paper look future next frontier visual recognition offer one possible answer question <eos> propose detailed image annotation captures information beyond visible pixels requires complex reasoning about full scene structure <eos> specifically create amodal segmentation each image full extent each region marked just visible pixels <eos> annotators outline name all salient region image specify partial depth order <eos> result rich scene structure including visible occluded portions each region figure ground edge information semantic labels object overlap <eos> create two datasets semantic amodal segmentation <eos> first label image bsds dataset multiple annotators per image allowing study statistics human annotations <eos> show proposed full scene annotation surprisingly consistent between annotators including region edges <eos> second annotate image coco <eos> larger dataset allows explore number algorithmic ideas amodal segmentation depth ordering <eos> introduce novel metrics tasks along strong baselines define concrete new challenges community <eos> <eop> deep sequential context network action prediction <eos> paper proposes efficient powerful deep network action prediction partially observed video containing temporally incomplete action executions <eos> different after fact action recognition action prediction task requires action labels predicted partially observed video <eos> approach exploits abundant sequential context information enrich feature representations partial video <eos> reconstruct missing information feature extracted partial video learning fully observed action video <eos> amount information temporally ordered purpose modeling temporal orderings action segments <eos> label information also used better separate learned feature different categories <eos> develop new learning formulation enables efficient model training <eos> extensive experimental result ucf sports bit datasets demonstrate approach remarkably outperforms state art method up faster than method <eos> result also show actions differ their prediction characteristics some actions correctly predicted even though only beginning portion video observed <eos> <eop> comparative evaluation hand crafted learned local feature <eos> matching local image descriptors key step many computer vision applications <eos> more than decade hand crafted descriptors such sift used task <eos> recently multiple new descriptors learned data proposed shown improve sift terms discriminative power <eos> paper dedicated extensive experimental evaluation learned local feature establish single evaluation protocol ensures comparable result <eos> terms matching performance evaluate different descriptors regarding standard criteria <eos> however considering matching performance isolation only provides incomplete measure descriptor quality <eos> example finding additional correct matches between similar image necessarily lead better performance when trying match image under extreme viewpoint illumination changes <eos> besides pure descriptor matching thus also evaluate different descriptors context image based reconstruction <eos> enables study descriptor performance set more practical criteria including image retrieval ability register image under strong viewpoint illumination changes accuracy completeness reconstructed cameras scenes <eos> facilitate future research full evaluation pipeline made publicly available <eos> <eop> aggregated residual transformations deep neural network <eos> present simple highly modularized network architecture image classification <eos> network constructed repeating building block aggregates set transformations same topology <eos> simple design result homogeneous multi branch architecture only few hyper parameters set <eos> strategy exposes new dimension call cardinality size set transformations essential factor addition dimensions depth width <eos> imagenet dataset empirically show even under restricted condition maintaining complexity increasing cardinality able improve classification accuracy <eos> moreover increasing cardinality more effective than going deeper wider when increase capacity <eos> models named resnext foundations entry ilsvrc classification task secured nd place <eos> further investigate resnext imagenet set coco detection set also showing better result than its resnet counterpart <eos> code models publicly available online <eos> <eop> predicting behaviors basketball players first person video <eos> paper presents method predict future movements location gaze direction basketball players whole their first person video <eos> predicted behaviors reflect individual physical space affords take next actions while conforming social behaviors engaging joint attention <eos> key innovation use three dimensional reconstruction multiple first person cameras automatically annotate each other visual semantics social configurations <eos> leverage two learning signals uniquely embedded first person video <eos> individually first person video records visual semantics spatial social layout around person allows associating past similar situations <eos> collectively first person video follow joint attention link individuals group <eos> learn egocentric visual semantics group movements using siamese neural network retrieve future trajectories <eos> consolidate retrieved trajectories all players maximizing measure social compatibility gaze alignment towards joint attention predicted their social formation dynamics joint attention learned long term recurrent convolutional network <eos> allows characterize social configuration more plausible predict future group trajectories <eos> <eop> synthesizing three dimensional shapes via modeling multi view depth maps silhouettes deep generative network <eos> study problem learning generative models three dimensional shapes <eos> voxels three dimensional parts widely used underlying representations build complex three dimensional shapes however voxel based representations suffer high memory requirements parts based models require large collection cached richly parametrized parts <eos> take alternative approach learning generative model over multi view depth maps their corresponding silhouettes using deterministic rendering function produce three dimensional shapes image <eos> multi view representation shapes enables generation three dimensional models fine details depth maps silhouettes modeled much higher resolution than three dimensional voxels <eos> moreover approach naturally brings ability recover underlying three dimensional representation depth maps one few viewpoints <eos> experiments show framework generate three dimensional shapes variations details <eos> also demonstrate model out sample generalization power real world tasks occluded object <eos> <eop> memory augmented attribute manipulation network interactive fashion search <eos> introduce new fashion search protocol attribute manipulation allowed within interaction between users search engines <eos> manipulating color attribute clothing red blue <eos> particularly useful image based search when query image cannot perfectly match user expectation desired product <eos> build such search engine propose novel memory augmented attribute manipulation network amnet manipulate image representation attribute level <eos> given query image some attributes need modify amnet manipulate intermediate representation encoding unwanted attributes change them desired ones through following four novel components dual path cnn architecture discriminative deep attribute representation learning memory block internal memory neural controller prototype attribute representation learning hosting attribute manipulation network modify representation query image prototype feature retrieved memory block loss layer jointly optimizes attribute classification loss triplet ranking loss over triplet image facilitating precise attribute manipulation image retrieving <eos> extensive experiments conducted two large scale fashion search datasets <eos> darn deepfashion demonstrated amnet able achieve remarkably good performance compared well designed baselines terms effectiveness attribute manipulation search accuracy <eos> <eop> spatiotemporal pyramid network video action recognition <eos> two stream convolutional network shown strong performance video action recognition tasks <eos> key idea learn spatiotemporal feature fusing convolutional network spatially temporally <eos> however remains unclear how model correlations between spatial temporal structures multiple abstraction levels <eos> first spatial stream tends fail if two video share similar backgrounds <eos> second temporal stream may fooled if two actions resemble short snippets though appear distinct long term <eos> propose novel spatiotemporal pyramid network fuse spatial temporal feature pyramid structure such they reinforce each other <eos> architecture perspective network constitutes hierarchical fusion strategies trained whole using unified spatiotemporal loss <eos> series ablation experiments support importance each fusion strategy <eos> technical perspective introduce spatiotemporal compact bilinear operator into video analysis tasks <eos> operator enables efficient training bilinear fusion operations capture full interactions between spatial temporal feature <eos> final network achieves state art result standard video datasets <eos> <eop> reconstructing transient image single photon sensors <eos> computer vision algorithms build image three dimensional video capture dynamic events millisecond time scale <eos> however capturing analyzing transient image picosecond scale <eos> one trillion frames per second reveals unprecedented information about scene light transport within <eos> only crucial time flight range imaging but also helps further understanding light transport phenomena more fundamental level potentially allows revisit many assumptions made different computer vision algorithms <eos> work design evaluate imaging system builds single photon avalanche diode spad sensors capture multi path responses picosecond scale active illumination <eos> develop inverse method use modern approaches deconvolve denoise measurements presence poisson noise compute transient image higher quality than previously reported <eos> small form factor fast acquisition rates relatively low cost system potentially makes transient imaging more practical range applications <eos> <eop> dynamic facial analysis bayesian filtering recurrent neural network <eos> facial analysis video including head pose estimation facial landmark localization key many applications such facial animation capture human activity recognition human computer interaction <eos> paper propose use recurrent neural network rnn joint estimation tracking facial feature video <eos> inspired fact computation performed rnn bears resemblance bayesian filters used tracking many previous method facial analysis video <eos> bayesian filters used method however require complicated problem specific design tuning <eos> contrast proposed rnn based method avoids such tracker engineering learning training data similar how convolutional neural network cnn avoids feature engineering image classification <eos> end end network proposed rnn based method provides generic holistic solution joint estimation tracking various types facial feature consecutive video frames <eos> extensive experimental result head pose estimation facial landmark localization video demonstrate proposed rnn based method outperforms frame wise models bayesian filtering <eos> addition create large scale synthetic dataset head pose estimation achieve state art performance benchmark dataset <eos> <eop> polarimetric multi view stereo <eos> multi view stereo relies feature correspondences three dimensional reconstruction thus fundamentally flawed dealing featureless scenes <eos> paper propose polarimetric multi view stereo combines per pixel photometric information polarization epipolar constraints multiple views three dimensional reconstruction <eos> polarization reveals surface normal information thus helpful propagate depth featureless region <eos> polarimetric multi view stereo completely passive applied outdoors uncontrolled illumination since data capture done simply either polarizer polarization camera <eos> unlike previous work shape polarization limited either diffuse polarization specular polarization only propose novel polarization imaging model handle real world object mixed polarization <eos> prove there exactly two types ambiguities estimating surface azimuth angles polarization resolve them graph optimization iso depth contour tracing <eos> step significantly improves initial depth map estimate later fused together complete three dimensional reconstruction <eos> extensive experimental result demonstrate high quality three dimensional reconstruction better performance than state art multi view stereo method especially featureless three dimensional object such ceramic tiles office room white walls highly reflective cars outdoors <eos> <eop> object region mining adversarial erasing simple classification semantic segmentation approach <eos> investigate principle way progressively mine discriminative object region using classification network address weakly supervised semantic segmentation problems <eos> classification network only responsive small sparse discriminative region object interest deviates requirement segmentation task needs localize dense interior integral region pixel wise inference <eos> mitigate gap propose new adversarial erasing approach localizing expanding object region progressively <eos> starting single small object region proposed approach drives classification network sequentially discover new complement object region erasing current mined region adversarial manner <eos> localized region eventually constitute dense complete object region learning semantic segmentation <eos> further enhance quality discovered region adversarial erasing online prohibitive segmentation learning approach developed collaborate adversarial erasing providing auxiliary segmentation supervision modulated more reliable classification scores <eos> despite its apparent simplicity proposed approach achieves <eos> mean intersection over union miou scores pascal voc val test set new state arts <eos> <eop> miml fcn multi instance multi label learning via fully convolutional network privileged information <eos> multi instance multi label miml learning many interesting applications computer visions including multi object recognition automatic image tagging <eos> applications additional information such bounding boxes image captions descriptions often available during training phrase referred privileged information pi <eos> however existing works learning using pi only consider instance level pi privileged instances they fail make use bag level pi privileged bags available miml learning <eos> therefore paper propose two stream fully convolutional network named miml fcn unified novel pi loss solve problem miml learning privileged bags <eos> compared previous works pi proposed miml fcn utilizes readily available privileged bags instead hard obtain privileged instances making system more general practical real world applications <eos> proposed pi loss convex sgd compatible framework itself fully convolutional network miml fcn easily integrated state art deep learning network <eos> moreover flexibility convolutional layer allows exploit structured correlations among instances facilitate more effective training testing <eos> experimental result three benchmark datasets demonstrate effectiveness proposed miml fcn outperforming state art method application multi object recognition <eos> <eop> benchmarking denoising algorithms real photographs <eos> lacking realistic ground truth data image denoising techniques traditionally evaluated image corrupted synthesized <eos> aim obviate unrealistic setting developing methodology benchmarking denoising techniques real photographs <eos> capture pairs image different iso values appropriately adjusted exposure times nearly noise free low iso image serves reference <eos> derive ground truth careful post processing needed <eos> correct spatial misalignment cope inaccuracies exposure parameters through linear intensity transform based novel heteroscedastic tobit regression model remove residual low frequency bias stems <eos> minor illumination changes <eos> then capture novel benchmark dataset darmstadt noise dataset dnd consumer cameras differing sensor sizes <eos> one interesting finding various recent techniques perform well synthetic noise clearly outperformed bm photographs real noise <eos> benchmark delineates realistic evaluation scenarios deviate strongly commonly used scientific literature <eos> <eop> dual ascent framework lagrangean decomposition combinatorial problems <eos> propose general dual ascent message passing framework lagrangean dual decomposition combinatorial problems <eos> although method type shown their efficiency number problems so far there was no general algorithm applicable multiple problem types <eos> work propose such general algorithm <eos> depends several parameters used optimize its performance each particular setting <eos> demonstrate efficiency method graph matching multicut problems outperforms state art solvers including based subgradient optimization off shelf linear programming solvers <eos> <eop> study lagrangean decompositions dual ascent solvers graph matching <eos> study quadratic assignment problem computer vision also known graph matching <eos> two leading solvers problem optimize lagrange decomposition duals sub gradient dual ascent also known message passing updates <eos> explore direction further propose several additional lagrangean relaxations graph matching problem along corresponding algorithms all based common dual ascent framework <eos> extensive empirical evaluation gives several theoretical insights suggests new state art anytime solver considered problem <eos> improvement over state art particularly visible new dataset large scale sparse problem instances containing more than graph nodes each <eos> <eop> message passing algorithm minimum cost multicut problem <eos> propose dual decomposition linear program relaxation np hard minimum cost multicut problem <eos> unlike other polyhedral relaxations multicut polytope amenable efficient optimization message passing <eos> like other polyhedral relaxations tightened efficiently cutting planes <eos> define algorithm alternates between message passing efficient separation cycle odd wheel inequalities <eos> algorithm more efficient than state art algorithms based linear programming including algorithms written framework leading commercial software show experiments large instances problem applications computer vision biomedical image analysis data mining <eos> <eop> zero shot learning conventional supervised classification unseen visual data synthesis <eos> robust object recognition systems usually rely powerful feature extraction mechanisms large number real image <eos> however many realistic applications collecting sufficient image ever growing new classes unattainable <eos> paper propose new zero shot learning zsl framework synthesise visual feature unseen classes without acquiring real image <eos> using proposed unseen visual data synthesis uvds algorithm semantic attributes effectively utilised intermediate clue synthesise unseen visual feature training stage <eos> hereafter zsl recognition converted into conventional supervised problem <eos> synthesised visual feature straightforwardly fed typical classifiers such svm <eos> four benchmark datasets demonstrate benefit using synthesised unseen data <eos> extensive experimental result manifest proposed approach significantly improve state art result <eos> <eop> large scale three dimensional models really necessary accurate visual localization <eos> accurate visual localization key technology autonomous navigation <eos> three dimensional structure based method employ three dimensional models scene estimate full dof pose camera very accurately <eos> however constructing extending large scale three dimensional models still significant challenge <eos> contrast image retrieval based method only require database geo tagged image trivial construct maintain <eos> they often considered inaccurate since they only approximate positions cameras <eos> yet exact camera pose theoretically recovered when enough relevant database image retrieved <eos> paper demonstrate experimentally large scale three dimensional models strictly necessary accurate visual localization <eos> create reference poses large challenging urban dataset <eos> using poses show combining image based method local reconstructions result pose accuracy similar state art structure based method <eos> result suggest might want reconsider current approach accurate large scale localization <eos> <eop> global context aware attention lstm network three dimensional action recognition <eos> long short term memory lstm network shown superior performance three dimensional human action recognition due their power modeling dynamics dependencies sequential data <eos> since all joints informative action analysis irrelevant joints often bring lot noise need pay more attention informative ones <eos> however original lstm strong attention capability <eos> hence propose new class lstm network global context aware attention lstm gca lstm three dimensional action recognition able selectively focus informative joints action sequence assistance global contextual information <eos> order achieve reliable attention representation action sequence further propose recurrent attention mechanism gca lstm network attention performance improved iteratively <eos> experiments show end end network reliably focus most informative joints each frame skeleton sequence <eos> moreover network yields state art performance three challenging datasets three dimensional action recognition <eos> <eop> hierarchical boundary aware neural encoder video captioning <eos> use recurrent neural network video captioning recently gained lot attention since they used both encode input video generate corresponding description <eos> paper present recurrent video encoding scheme discover leverage hierarchical structure video <eos> unlike classical encoder decoder approach video encoded continuously recurrent layer propose novel lstm cell identify discontinuity point between frames segments modify temporal connections encoding layer accordingly <eos> evaluate approach three large scale datasets montreal video annotation dataset mpii movie description dataset microsoft video description corpus <eos> experiments show approach discover appropriate hierarchical representations input video improve state art result movie description datasets <eos> <eop> emotion recognition context <eos> understanding person experiencing her frame reference essential everyday life <eos> reason one think machines type ability would interact better people <eos> however there no current systems capable understanding detail people emotional states <eos> previous research computer vision recognize emotions mainly focused analyzing facial expression usually classifying into basic emotions <eos> however context plays important role emotion perception when context incorporated infer more emotional states <eos> paper present emotions context database emco dataset image containing people context non controlled environments <eos> image people annotated emotional categories also continuous dimensions valence arousal dominance <eos> emco dataset trained convolutional neural network model jointly analyzes person whole scene recognize rich information about emotional states <eos> show importance considering context recognizing people emotions image provide benchmark task emotion recognition visual context <eos> <eop> deep learning human visual sensitivity image quality assessment framework <eos> since human observers ultimate receivers digital image image quality metrics should designed human oriented perspective <eos> conventionally number full reference image quality assessment fr iqa method adopted various computational models human visual system hvs psychological vision science research <eos> paper propose novel convolutional neural network cnn based fr iqa model named deep image quality assessment deepqa behavior hvs learned underlying data distribution iqa databases <eos> different previous studies model seeks optimal visual weight based understanding database information itself without any prior knowledge hvs <eos> through experiments show predicted visual sensitivity maps agree human subjective opinions <eos> addition deepqa achieves state art prediction accuracy among fr iqa models <eos> <eop> learning non lambertian object intrinsics across shapenet categories <eos> focus non lambertian object level intrinsic problem recovering diffuse albedo shading specular highlights single image object <eos> based existing three dimensional models shapenet database large scale object intrinsics database rendered hdr environment maps <eos> millions synthetic image object their corresponding albedo shading specular ground truth image used train encoder decoder cnn decompose image into product albedo shading components along additive specular component <eos> cnn delivers accurate sharp result classical inverse problem computer vision <eos> evaluated realistically synthetic dataset method consistently outperforms state art large margin <eos> train test cnn across different object categories <eos> perhaps surprising especially cnn classification perspective intrinsics cnn generalizes very well across categories <eos> analysis shows feature learning encoder stage more crucial developing universal representation across categories <eos> apply model real image video internet observe robust realistic intrinsics result <eos> quality non lambertian intrinsics could open up many interesting applications such realistic product search based material properties image based albedo specular editing <eos> <eop> collaborative deep reinforcement learning joint object search <eos> examine problem joint top down active search multiple object under interaction <eos> person riding bicycle cups held table etc <eos> such object under interaction often provide contextual cues each other facilitate more efficient search <eos> treating each detector agent present first collaborative multi agent deep reinforcement learning algorithm learn optimal policy joint active object localization effectively exploits such beneficial contextual information <eos> learn inter agent communication through cross connections gates between network facilitated novel multi agent deep learning algorithm joint exploitation sampling <eos> verify proposed method multiple object detection benchmarks <eos> only model help improve performance state art active localization models also reveals interesting co detection patterns intuitively interpretable <eos> <eop> automatic understanding image video advertisements <eos> there more image than their objective physical content example advertisements created persuade viewer take certain action <eos> propose novel problem automatic advertisement understanding <eos> enable research problem create two datasets image dataset image ads video dataset ads <eos> data contains rich annotations encompassing topic sentiment ads questions answers describing actions viewer prompted take reasoning ad presents persuade viewer should according ad why should symbolic references ads make <eos> dove symbolizes peace <eos> also analyze most common persuasive strategies ads use capabilities computer vision systems should understand strategies <eos> present baseline classification result several prediction tasks including automatically answering questions about messages ads <eos> <eop> fftlasso large scale lasso fourier domain <eos> paper revisit lasso sparse representation problem studied used variety different areas ranging signal processing information theory computer vision machine learning <eos> vision community found its way into many important applications including face recognition tracking super resolution image denoising name few <eos> despite advances efficient sparse algorithms solving large scale lasso problems remains challenge <eos> circumvent difficulty people tend downsample subsample problem <eos> via dimensionality reduction maintain manageable sized lasso usually comes cost losing solution accuracy <eos> paper proposes novel circulant reformulation lasso lifts problem higher dimension admm efficiently applied its dual form <eos> because lifting all optimization variables updated using only basic element wise operations most computationally expensive fft <eos> way there no need linear system solver nor matrix vector multiplication <eos> since all operations fftlasso method element wise subproblems completely independent trivially parallelized <eos> attractive computational properties fftlasso verified extensive experiments synthetic real data face recognition task <eos> they demonstrate fftlasso scales much more effectively than state art solver <eos> <eop> multi modal mean fields via cardinality based clamping <eos> mean field inference central statistical physics <eos> attracted much interest computer vision community efficiently solve problems expressible terms large conditional random fields <eos> however since models posterior probability distribution product marginal probabilities may fail properly account important dependencies between variables <eos> therefore replace fully factorized distribution mean field weighted mixture such distributions similarly minimizes kl divergence true posterior <eos> introducing two new ideas namely conditioning groups variables instead single ones using parameter conditional random field potentials identify temperature sense statistical physics select such groups perform minimization efficiently <eos> extension clamping method proposed previous works allows both produce more descriptive approximation true posterior inspired diverse map paradigms fit mixture mean field approximations <eos> demonstrate positively impacts real world algorithms initially relied mean fields <eos> <eop> unified approach multi scale deep hand crafted feature defocus estimation <eos> paper introduce robust synergetic hand crafted feature simple but efficient deep feature convolutional neural network cnn architecture defocus estimation <eos> paper systematically analyzes effectiveness different feature shows how each feature compensate weaknesses other feature when they concatenated <eos> full defocus map estimation extract image patches strong edges sparsely after use them deep hand crafted feature extraction <eos> order reduce degree patch scale dependency also propose multi scale patch extraction strategy <eos> sparse defocus map generated using neural network classifier followed probability joint bilateral filter <eos> final defocus map obtained sparse defocus map guidance edge preserving filtered input image <eos> experimental result show algorithm superior state art algorithms terms defocus estimation <eos> work used applications such segmentation blur magnification all focus image generation estimation <eos> <eop> semantic scene completion single depth image <eos> paper focuses semantic scene completion task producing complete three dimensional voxel representation volumetric occupancy semantic labels scene single view depth map observation <eos> previous work considered scene completion semantic labeling depth maps separately <eos> however observe two problems tightly intertwined <eos> leverage coupled nature two tasks introduce semantic scene completion network sscnet end end three dimensional convolutional network takes single depth image input simultaneously outputs occupancy semantic labels all voxels camera view frustum <eos> network uses dilation based three dimensional context module efficiently expand receptive field enable three dimensional context learning <eos> train network construct suncg manually created largescale dataset synthetic three dimensional scenes dense volumetric annotations <eos> experiments demonstrate joint model outperforms method addressing each task isolation outperforms alternative approaches semantic scene completion task <eos> dataset code available sscnet <eos> <eop> fine coarse global registration rgb scans <eos> rgb scanning indoor environments important many applications including real estate interior design virtual reality <eos> however still challenging register rgb image hand held camera over long video sequence into globally consistent three dimensional model <eos> current method often lose tracking drift thus fail reconstruct salient structures large environments <eos> parallel walls different rooms <eos> address problem propose fine coarse global registration algorithm leverages robust registrations finer scales seed detection enforcement new correspondence structural constraints coarser scales <eos> test global registration algorithms provide benchmark manually clicked point correspondences scenes sun dataset <eos> during experiments benchmark find fine coarse algorithm registers long rgb sequences better than previous method <eos> <eop> universal adversarial perturbations <eos> given state art deep neural network classifier show existence universal image agnostic very small perturbation vector causes natural image misclassified high probability <eos> propose systematic algorithm computing universal perturbations show state art deep neural network highly vulnerable such perturbations albeit being quasi imperceptible human eye <eos> further empirically analyze universal perturbations show particular they generalize very well across neural network <eos> surprising existence universal perturbations reveals important geometric correlations among high dimensional decision boundary classifiers <eos> further outlines potential security breaches existence single directions input space adversaries possibly exploit break classifier most natural image <eos> <eop> saliency revisited analysis mouse movements versus fixations <eos> paper revisits visual saliency prediction evaluating recent advancements field such crowd sourced mouse tracking based databases contextual annotations <eos> pursue critical quantitative approach towards some new challenges including quality mouse tracking versus eye tracking model training evaluation <eos> extend quantitative evaluation models order incorporate contextual information proposing evaluation methodology allows accounting contextual factors such text faces object attributes <eos> proposed contextual evaluation scheme facilitates detailed analysis models helps identify their pros cons <eos> through several experiments find mouse tracking data lower inter participant visual congruency higher dispersion compared eye tracking data mouse tracking data totally agree eye tracking general terms different contextual region specific mouse tracking data leads acceptable result training current existing models mouse tracking data less reliable model selection evaluation <eos> contextual evaluation also reveals among studied models there no single model performs best all tested annotations <eos> <eop> online summarization via submodular convex optimization <eos> consider problem subset selection online setting data arrive incrementally <eos> instead storing running subset selection entire dataset propose incremental subset selection framework each time instant uses previously selected set representatives new batch data order update set representatives <eos> cast problem integer binary optimization minimizing encoding cost data via representatives regularized number selected items <eos> proposed optimization general np hard non convex study greedy approach based unconstrained submodular optimization also propose efficient convex relaxation <eos> show under appropriate conditions solution proposed convex algorithm achieves global optimal solution non convex problem <eos> result also address conventional problem subset selection offline setting special case <eos> extensive experiments problem video summarization demonstrate proposed online subset selection algorithms perform well real data capturing diverse representative events video while they obtain objective function values close offline setting <eos> <eop> red wine red tomato composition context <eos> compositionality contextuality key building blocks intelligence <eos> they allow compose known concepts generate new complex ones <eos> however traditional learning method model both properties require copious amounts labeled data learn new concepts <eos> large fraction existing techniques <eos> using late fusion compose concepts but fail model contextuality <eos> example red red wine different red red tomatoes <eos> paper present simple method respects contextuality order compose classifiers known visual concepts <eos> method builds upon intuition classifiers lie smooth space compositional transforms modeled <eos> show how generalize unseen combinations concepts <eos> result composing attributes object well composing subject predicate object demonstrate its strong generalization performance compared baselines <eos> finally present detailed analysis method highlight its properties <eos> <eop> dmatch learning local geometric descriptors rgb reconstructions <eos> matching local geometric feature real world depth image challenging task due noisy low resolution incomplete nature three dimensional scan data <eos> difficulties limit performance current state art method typically based histograms over geometric properties <eos> paper present dmatch data driven model learns local volumetric patch descriptor establishing correspondences between partial three dimensional data <eos> amass training data model propose self supervised feature learning method leverages millions correspondence labels found existing rgb reconstructions <eos> experiments show descriptor only able match local geometry new scenes reconstruction but also generalize different tasks spatial scales <eos> instance level object model alignment amazon picking challenge mesh surface correspondence <eos> result show dmatch consistently outperforms other state art approaches significant margin <eos> code data benchmarks pre trained models available online dmatch <eos> edu <eop> superpixel based tracking segmentation using markov chains <eos> propose simple but effective tracking segmentation algorithm using absorbing markov chain amc superpixel segmentation target state estimated combination bottom up top down approaches target segmentation propagated subsequent frames recursive manner <eos> algorithm constructs graph amc using superpixels identified two consecutive frames background superpixels previous frame correspond absorbing vertices while all other superpixels create transient ones <eos> weight each edge depends similarity scores end superpixels learned support vector regression <eos> once graph construction completed target segmentation estimated using absorption time each superpixel <eos> proposed tracking algorithm achieves substantially improved performance compared state art segmentation based tracking techniques multiple challenging datasets <eos> <eop> quad network unsupervised learning rank interest point detection <eos> several machine learning tasks require represent data using only sparse set interest point <eos> ideal detector able find corresponding interest point even if data undergo transformation typical given domain <eos> since task high practical interest computer vision many hand crafted solutions were proposed <eos> paper ask fundamental question learn such detectors scratch since often unclear point interesting human labelling cannot used find truly unbiased solution <eos> therefore task requires unsupervised formulation <eos> first propose such formulation training neural network rank point transformation invariant manner <eos> interest point then extracted top bottom quantiles ranking <eos> validate approach two tasks standard rgb image interest point detection challenging cross modal interest point detection between rgb depth image <eos> quantitatively show unsupervised method performs better par baselines <eos> <eop> multi context attention human pose estimation <eos> paper propose incorporate convolutional neural network multi context attention mechanism into end end framework human pose estimation <eos> adopt stacked hourglass network generate attention maps feature multiple resolutions various semantics <eos> conditional random field crf utilized model correlations among neighboring region attention map <eos> further combine holistic attention model focuses global consistency full human body body part attention model focuses detailed descriptions different body parts <eos> hence model ability focus different granularity local salient region global semantic consistent spaces <eos> additionally design novel hourglass residual units hrus increase receptive field network <eos> units extensions residual units side branch incorporating filters larger receptive field hence feature various scales learned combined within hrus <eos> effectiveness proposed multi context attention mechanism hourglass residual units evaluated two widely used human pose estimation benchmarks <eos> approach outperforms all existing method both benchmarks over all body parts <eos> code made publicly available <eos> <eop> action unit detection region adaptation multi labeling learning optimal temporal fusing <eos> action unit au detection becomes essential facial analysis <eos> many proposed approaches face challenging problems dealing alignments different face region effective fusion temporal information training model multiple au labels <eos> better address problems propose deep learning framework au detection region interest roi adaptation integrated multi label learning optimal lstm based temporal fusing <eos> first roi cropping net designed make sure specific interested region faces learned independently each sub region local convolutional neural network cnn whose convolutional filters will only trained corresponding region <eos> second multi label learning employed integrate outputs individual roi cropping nets learns inter relationships various aus acquires global feature across sub region au detection <eos> finally optimal selection multiple lstm layer carried out best fuse temporal feature order make au prediction most accurate <eos> proposed approach evaluated two popular au detection datasets bp disfa outperforming state art significantly average improvement around bp disfa respectively <eos> <eop> unsupervised learning depth ego motion video <eos> present unsupervised learning framework task dense three dimensional geometry camera motion estimation unstructured video sequences <eos> common recent work use end end learning approach view synthesis supervisory signal <eos> contrast works method completely unsupervised requiring only sequence image input <eos> achieve network estimates dof camera pose parameters input set along dense depth reference view using single view inference <eos> loss constructed projecting nearby posed views into reference view via depth map <eos> result using kitti dataset demonstrate effectiveness approach performs par another deep learning approach assumes ground truth pose information training time <eos> <eop> joint geometrical statistical alignment visual domain adaptation <eos> paper presents novel unsupervised domain adaptation method cross domain visual recognition <eos> propose unified framework reduces shift between domains both statistically geometrically referred joint geometrical statistical alignment jgsa <eos> specifically learn two coupled projections project source domain target domain data into low dimensional subspaces geometrical shift distribution shift reduced simultaneously <eos> objective function solved efficiently closed form <eos> extensive experiments verified proposed method significantly outperforms several state art domain adaptation method synthetic dataset three different real world cross domain visual recognition tasks <eos> <eop> jointly learning energy expenditures activities using egocentric multimodal signals <eos> physiological signals such heart rate provide valuable information about individual state activity <eos> however existing work computer vision yet explored leveraging signals enhance egocentric video understanding <eos> work propose model reasoning multimodal data jointly predict activities energy expenditures <eos> use heart rate signals privileged self supervision derive energy expenditure training stage <eos> multitask objective used jointly optimize two tasks <eos> additionally introduce dataset contains hours egocentric video augmented heart rate acceleration signals <eos> study lead new applications such visual calorie counter <eos> <eop> attend groups weakly supervised deep learning framework learning web data <eos> large scale datasets driven rapid development deep neural network visual recognition <eos> however annotating massive dataset expensive time consuming <eos> web image their labels comparison much easier obtain but direct training such automatially harvested image lead unsatisfactory performance because noisy labels web image adversely affect learned recognition models <eos> address drawback propose end end weakly supervised deep learning framework robust label noise web image <eos> proposed framework relies two unified strategies random grouping attention effectively reduce negative impact noisy web image annotations <eos> specifically random grouping stacks multiple image into single training instance thus increases labeling accuracy instance level <eos> attention other hand suppresses noisy signals both incorrectly labeled image less discriminative image region <eos> conducting intensive experiments two challenging datasets including newly collected fine grained dataset web image different car models superior performance proposed method over competitive baselines clearly demonstrated <eos> <eop> exact penalty method locally convergent maximum consensus <eos> maximum consensus estimation plays critically important role computer vision <eos> currently most prevalent approach draws class non deterministic hypothesize verify algorithms cheap but guarantee solution quality <eos> other extreme there global algorithms exhaustive search nature costly practical sized inputs <eos> paper aims fill gap between two extremes proposing locally convergent maximum consensus algorithm <eos> method based formulating problem linear complementarity constraints then defining penalized version provably equivalent original problem <eos> based penalty problem develop frank wolfe algorithm deterministically solve maximum consensus problem <eos> compared randomized techniques method deterministic locally convergent relative global algorithms method much more practical realistic input sizes <eos> further approach naturally applicable problems geometric residuals <eos> <eop> stylebank explicit representation neural image style transfer <eos> propose stylebank composed multiple convolution filter banks each filter bank explicitly represents one style neural image style transfer <eos> transfer image specific style corresponding filter bank operated top intermediate feature embedding produced single auto encoder <eos> stylebank auto encoder jointly learnt learning conducted such way auto encoder encode any style information thanks flexibility introduced explicit filter bank representation <eos> also enables conduct incremental learning add new image style learning new filter bank while holding auto encoder fixed <eos> explicit style representation along flexible network design enables fuse styles only image level but also region level <eos> method first style transfer network links back traditional texton mapping method hence provides new understanding neural style transfer <eos> method easy train runs real time produces result qualitatively better least comparable existing method <eos> <eop> multi view three dimensional object detection network autonomous driving <eos> paper aims high accuracy three dimensional object detection autonomous driving scenario <eos> propose multi view three dimensional network mv sensory fusion framework takes both lidar point cloud rgb image input predicts oriented three dimensional bounding boxes <eos> encode sparse three dimensional point cloud compact multi view representation <eos> network composed two subnetworks one three dimensional object proposal generation another multi view feature fusion <eos> proposal network generates three dimensional candidate boxes efficiently bird eye view representation three dimensional point cloud <eos> design deep fusion scheme combine region wise feature multiple views enable interactions between intermediate layer different paths <eos> experiments challenging kitti benchmark show approach outperforms state art around ap tasks three dimensional localization three dimensional detection <eos> addition detection approach obtains <eos> higher ap than state art hard data among lidar based method <eos> <eop> weakly supervised dense video captioning <eos> paper focuses novel challenging vision task dense video captioning aims automatically describe video clip multiple informative diverse caption sentences <eos> proposed method trained without explicit annotation fine grained sentence video region sequence correspondence but only based weak video level sentence annotations <eos> differs existing video captioning systems three technical aspects <eos> first propose lexical fully convolutional neural network lexical fcn weakly supervised multi instance multi label learning weakly link video region lexical labels <eos> second introduce novel submodular maximization scheme generate multiple informative diverse region sequences based lexical fcn outputs <eos> winner takes all scheme adopted weakly associate sentences region sequences training phase <eos> third sequence sequence learning based language model trained weakly supervised information obtained through association process <eos> show proposed method only produce informative diverse dense captions but also outperform state art single video captioning method large margin <eos> <eop> refinenet multi path refinement network high resolution semantic segmentation <eos> recently very deep convolutional neural network cnn shown outstanding performance object recognition also first choice dense classification problems such semantic segmentation <eos> however repeated subsampling operations like pooling convolution striding deep cnn lead significant decrease initial image resolution <eos> here present refinenet generic multi path refinement network explicitly exploits all information available along down sampling process enable high resolution prediction using long range residual connections <eos> way deeper layer capture high level semantic feature directly refined using fine grained feature earlier convolutions <eos> individual components refinenet employ residual connections following identity mapping mindset allows effective end end training <eos> further introduce chained residual pooling captures rich background context efficient manner <eos> carry out comprehensive experiments set new state art result seven public datasets <eos> particular achieve intersection over union score <eos> challenging pascal voc dataset best reported result date <eos> <eop> general models rational cameras case two slit projections <eos> rational camera model recently introduced provides general methodology studying abstract nonlinear imaging systems their multi view geometry <eos> paper builds framework study physical realizations rational cameras <eos> more precisely give explicit account mapping between between physical visual rays image point missing original description allows give simple analytical expressions direct inverse projections <eos> also consider primitive camera models orbits under action various projective transformations lead general notion intrinsic parameters <eos> methodology general but illustrated concretely depth study two slit cameras model using pairs linear projections <eos> simple analytical form allows describe models corresponding primitive cameras introduce intrinsic parameters clear geometric meaning define epipolar tensor characterizing two view correspondences <eos> turn leads new algorithms structure motion self calibration <eos> <eop> making deep neural network robust label noise loss correction approach <eos> present theoretically grounded approach train deep neural network including recurrent network subject class dependent label noise <eos> propose two procedures loss correction agnostic both application domain network architecture <eos> they simply amount most matrix inversion multiplication provided know probability each class being corrupted into another <eos> further show how one estimate probabilities adapting recent technique noise estimation multi class setting thus providing end end framework <eos> extensive experiments mnist imdb cifar cifar large scale dataset clothing image employing diversity architectures stacking dense convolutional pooling dropout batch normalization word embedding lstm residual layer demonstrate noise robustness proposals <eos> incidentally also prove when relu only non linearity loss curvature immune class dependent label noise <eos> <eop> semantic segmentation via structured patch prediction context crf guidance crf <eos> paper describes fast accurate semantic image segmentation approach encodes only segmentation specified feature but also high order context compatibilities boundary guidance constraints <eos> introduce structured patch prediction technique make trade off between classification discriminability boundary sensibility feature <eos> both label feature contexts embedded ensure recognition accuracy compatibility while complexity high order cliques reduced distance aware sampling pooling strategy <eos> proposed joint model also employs guidance crf further enhance segmentation performance <eos> message passing step augmented guided filtering enables efficient joint training whole system end end fashion <eos> proposed joint model outperforms state art pascal voc cityscapes miou <eos> also reaches leading performance ade dataset scene parsing track ilsvrc <eos> <eop> deep matching prior network toward tighter multi oriented text detection <eos> detecting incidental scene text challenging task because multi orientation perspective distortion variation text size color scale <eos> retrospective research only focused using rectangular bounding box horizontal sliding window localize text may result redundant background noise unnecessary overlap even information loss <eos> address issues propose new convolutional neural network cnn based method named deep matching prior network dmpnet detect text tighter quadrangle <eos> first use quadrilateral sliding windows several specific intermediate convolutional layer roughly recall text higher overlapping area then shared monte carlo method proposed fast accurate computing polygonal areas <eos> after designed sequential protocol relative regression exactly predict text compact quadrangle <eos> moreover auxiliary smooth ln loss also proposed further regressing position text better overall performance than loss smooth loss terms robustness stability <eos> effectiveness approach evaluated public word level multi oriented scene text database icdar robust reading competition challenge incidental scene text localization <eos> performance method evaluated using measure found <eos> outperforming existing state art method measure <eos> <eop> person search natural language description <eos> searching persons large scale image databases query natural language description important applications video surveillance <eos> existing method mainly focused searching persons image based attribute based queries major limitations practical usage <eos> paper study problem person search natural language description <eos> given textual description person algorithm person search required rank all sample person database then retrieve most relevant sample corresponding queried description <eos> since there no person dataset benchmark textual description available collect large scale person description dataset detailed natural language annotations person sample various sources termed cuhk person description dataset cuhk pedes <eos> wide range possible models baselines evaluated compared person search benchmark <eos> recurrent neural network gated neural attention mechanism gna rnn proposed establish state art performance person search <eos> <eop> analyzing computer vision data good bad ugly <eos> recent years great number datasets were published train evaluate computer vision cv algorithms <eos> valuable contributions helped push cv solutions level they used safety relevant applications such autonomous driving <eos> however major questions concerning quality usefulness test data cv evaluation still unanswered <eos> researchers engineers try cover all test cases using much test data possible <eos> paper propose different solution challenge <eos> introduce method dataset analysis builds upon improved version cv hazop checklist list potential hazards within cv domain <eos> picking stereo vision example provide extensive survey datasets covering last two decades <eos> create tailored checklist apply datasets middlebury kitti sintel freiburg hci present thorough characterization quantitative comparison <eos> confirm usability checklist identification challenging stereo situations applying nine state art stereo matching algorithms analyzed datasets showing hazard frames correlate difficult frames <eos> show challenging datasets still allow meaningful algorithm evaluation even small subsets <eos> finally provide list missing test cases still covered current datasets inspiration researchers who want participate future dataset creation <eos> <eop> convolutional neural network efficient robust hand pose estimation single depth image <eos> propose simple yet effective approach real time hand pose estimation single depth image using three dimensional convolutional neural network three dimensional cnn <eos> image based feature extracted cnn directly suitable three dimensional hand pose estimation due lack three dimensional spatial information <eos> proposed three dimensional cnn taking three dimensional volumetric representation hand depth image input capture three dimensional spatial structure input accurately regress full three dimensional hand pose single pass <eos> order make three dimensional cnn robust variations hand sizes global orientations perform three dimensional data augmentation training data <eos> experiments show proposed three dimensional cnn based approach outperforms state art method two challenging hand pose datasets very efficient implementation runs over fps standard computer single gpu <eos> <eop> icarl incremental classifier representation learning <eos> major open problem road artificial intelligence development incrementally learning systems learn about more more concepts over time stream data <eos> work introduce new training strategy icarl allows learning such class incremental way only training data small number classes present same time new classes added progressively <eos> icarl learns strong classifiers data representation simultaneously <eos> distinguishes earlier works were fundamentally limited fixed data representations therefore incompatible deep learning architectures <eos> show experiments cifar imagenet ilsvrc data icarl learn many classes incrementally over long period time other strategies quickly fail <eos> <eop> posetrack joint multi person pose estimation tracking <eos> work introduce challenging problem joint multi person pose estimation tracking unknown number persons unconstrained video <eos> existing method multi person pose estimation image cannot applied directly problem since also requires solve problem person association over time addition pose estimation each person <eos> therefore propose novel method jointly models multi person pose estimation tracking single formulation <eos> end represent body joint detections video spatio temporal graph solve integer linear program partition graph into sub graphs correspond plausible body pose trajectories each person <eos> proposed approach implicitly handles occlusion truncation persons <eos> since problem addressed quantitatively literature introduce challenging multi person posetrack dataset also propose completely unconstrained evaluation protocol make any assumptions about scale size location number persons <eos> finally evaluate proposed approach several baseline method new dataset <eos> <eop> learning deep embedding model zero shot learning <eos> zero shot learning zsl models rely learning joint embedding space both textual semantic description object classes visual representation object image projected nearest neighbour search <eos> despite success deep neural network learn end end model between text image other vision problems such image captioning very few deep zsl model exists they show little advantage over zsl models utilise deep feature representations but learn end end embedding <eos> paper argue key make deep zsl models succeed choose right embedding space <eos> instead embedding into semantic space intermediate space propose use visual space embedding space <eos> because space subsequent nearest neighbour search would suffer much less hubness problem thus become more effective <eos> model design also provides natural mechanism multiple semantic modalities <eos> attributes sentence descriptions fused optimised jointly end end manner <eos> extensive experiments four benchmarks show model significantly outperforms existing models <eos> <eop> mcmlsd dynamic programming approach line segment detection <eos> prior approaches line segment detection typically involve perceptual grouping image domain global accumulation hough domain <eos> here propose probabilistic algorithm merges advantages both approaches <eos> first stage lines detected using global probabilistic hough approach <eos> second stage each detected line analyzed image domain localize line segments generated peak hough map <eos> limiting search line distribution segments over sequence point line modeled markov chain probabilistically optimal labelling computed exactly using standard dynamic programming algorithm linear time <eos> markov assumption also leads intuitive ranking method uses local marginal posterior probabilities estimate expected number correctly labelled point segment <eos> assess resulting markov chain marginal line segment detector mcmlsd develop apply novel quantitative evaluation methodology controls under over segmentation <eos> evaluation yorkurbandb dataset shows proposed mcmlsd method outperforms state art substantial margin <eos> <eop> deep manta coarse fine many task network joint three dimensional vehicle analysis monocular image <eos> paper present novel approach called deep manta deep many tasks many task vehicle analysis given image <eos> robust convolutional network introduced simultaneous vehicle detection part localization visibility characterization three dimensional dimension estimation <eos> its architecture based new coarse fine object proposal boosts vehicle detection <eos> moreover deep manta network able localize vehicle parts even if parts visible <eos> inference network outputs used real time robust pose estimation algorithm fine orientation estimation three dimensional vehicle localization <eos> show experiments method outperforms monocular state art approaches vehicle detection orientation three dimensional location tasks very challenging kitti benchmark <eos> <eop> low rank embedded ensemble semantic dictionary zero shot learning <eos> zero shot learning visual recognition received much interest most recent years <eos> however semantic gap across visual feature their underlying semantics still biggest obstacle zero shot learning <eos> fight off hurdle propose effective low rank embedded semantic dictionary learning lesd through ensemble strategy <eos> specifically formulate novel framework jointly seek low rank embedding semantic dictionary link visual feature their semantic representations manages capture shared feature across different observed classes <eos> moreover ensemble strategy adopted learn multiple semantic dictionaries constitute latent basis unseen classes <eos> consequently model could extract variety visual characteristics within object well generalized unknown categories <eos> extensive experiments several zero shot benchmarks verify proposed model outperform state art approaches <eos> <eop> nonnegative matrix underapproximation robust multiple model fitting <eos> work introduce highly efficient algorithm address nonnegative matrix underapproximation nmu problem <eos> nonnegative matrix factorization nmf additional underapproximation constraint <eos> nmu result interesting compared traditional nmf they present additional sparsity part based behavior explaining unique data feature <eos> show feature practice first present application analysis climate data <eos> then present nmu based algorithm robustly fit multiple parametric models dataset <eos> proposed approach delivers state art result estimation multiple fundamental matrices homographies outperforming other alternatives literature exemplifying use efficient nmu computations <eos> <eop> bind binary integrated net descriptors texture less object recognition <eos> paper presents bind binary integrated net descriptor texture less object detector encodes multi layered binary represented nets high precision edge based description <eos> proposed concept aligns layer object sized patches nets onto highly fragmented occlusion resistant line segment midpoints linelets encode regional information into efficient binary strings <eos> lightweight nets encourage discriminative object description through their high spatial resolution enabling highly precise encoding object edges internal texture less information <eos> bind achieved various invariant properties such rotation scale edge polarity through its unique binary logical operated encoding matching techniques while performing remarkably well occlusion clutter <eos> apart yielding efficient computational performance bind also attained remarkable recognition rates surpassing recent state art texture less object detectors such border bold line <eos> <eop> efficient diffusion region manifolds recovering small object compact cnn representations <eos> query expansion popular method improve quality image retrieval both conventional cnn representations <eos> so far limited global image similarity <eos> work focuses diffusion mechanism captures image manifold feature space <eos> efficient off line stage allows optional reduction number stored region <eos> line stage proposed handling unseen queries indexing stage removes additional computation adjust precomputed data <eos> perform diffusion through sparse linear system solver yielding practical query times well below one second <eos> experimentally observe significant boost performance image retrieval compact cnn descriptors standard benchmarks especially when query object covers only small part image <eos> small object common failure case cnn based retrieval <eos> <eop> slow fast interpolator flow <eos> introduce method compute optical flow multiple scales motion without resorting multi resolution combinatorial method <eos> addresses key problem small object moving fast resolves artificial binding between how large object how fast move before being diffused away clas sical scale space <eos> even no learning achieves top performance most challenging optical flow benchmark <eos> moreover result interpretable indeed list assumptions underlying method explicitly <eos> key approach matching pro gression slow fast well choice terpolation method equivalently prior fill region data allows <eos> use several off shelf components relatively low sensitivity parameter tuning <eos> computational cost comparable state art <eos> <eop> chestx ray hospital scale chest ray database benchmarks weakly supervised classification localization common thorax diseases <eos> chest ray one most commonly accessible radiological examinations screening diagnosis many lung diseases <eos> tremendous number ray imaging studies accompanied radiological reports accumulated stored many modern hospitals picture archiving communication systems pacs <eos> other side still open question how type hospital size knowledge database containing invaluable imaging informatics <eos> loosely labeled used facilitate data hungry deep learning paradigms building truly large scale high precision computer aided diagnosis cad systems <eos> paper present new chest ray database namely chestx ray comprises frontal view ray image unique patients text mined eight disease image labels each image multi labels associated radiological reports using natural language processing <eos> importantly demonstrate commonly occurring thoracic diseases detected even spatially located via unified weakly supervised multi label image classification disease localization framework validated using proposed dataset <eos> although initial quantitative result promising reported deep convolutional neural network based reading chest rays <eos> recognizing locating common disease patterns trained only image level labels remains strenuous task fully automated high precision cad systems <eos> <eop> learning simulated unsupervised image through adversarial training <eos> recent progress graphics become more tractable train models synthetic image potentially avoiding need expensive annotations <eos> however learning synthetic image may achieve desired performance due gap between synthetic real image distributions <eos> reduce gap propose simulated unsupervised learning task learn model improve realism simulator output using unlabeled real data while preserving annotation information simulator <eos> develop method learning uses adversarial network similar generative adversarial network gans but synthetic image inputs instead random vectors <eos> make several key modifications standard gan algorithm preserve annotations avoid artifacts stabilize training self regularization term ii local adversarial loss iii updating discriminator using history refined image <eos> show enables generation highly realistic image demonstrate both qualitatively user study <eos> quantitatively evaluate generated image training models gaze estimation hand pose estimation <eos> show significant improvement over using synthetic image achieve state art result mpiigaze dataset without any labeled real data <eos> <eop> feature pyramid network object detection <eos> feature pyramids basic component recognition systems detecting object different scales <eos> but pyramid representations avoided recent object detectors based deep convolutional network partially because they slow compute memory intensive <eos> paper exploit inherent multi scale pyramidal hierarchy deep convolutional network construct feature pyramids marginal extra cost <eos> top down architecture lateral connections developed building high level semantic feature maps all scales <eos> architecture called feature pyramid network fpn shows significant improvement generic feature extractor several applications <eos> using basic faster cnn system method achieves state art single model result coco detection benchmark without bells whistles surpassing all existing single model entries including coco challenge winners <eos> addition method run fps gpu thus practical accurate solution multi scale object detection <eos> code will made publicly available <eos> <eop> loss max pooling semantic image segmentation <eos> work introduce novel loss max pooling concept handling imbalanced training data distributions applicable alternative loss layer context deep neural network semantic image segmentation tasks <eos> most real world semantic segmentation datasets exhibit long tail distributions few object categories comprising majority data consequently biasing classifiers towards them <eos> method adaptively re weights contributions each pixel based their observed losses targeting under performing classification result often encountered under represented object classes <eos> moreover approach goes beyond conventional cost sensitive learning attempts through adaptive considerations allow indirectly address both inter intra class imbalances <eos> provide theoretical justification approach complementary experimental analyses standard semantic segmentation datasets <eos> experiments challenging cityscapes pascal voc segmentation benchmarks find consistently improved result demonstrating efficacy approach <eos> <eop> learned contextual feature reweighting image geo localization <eos> address problem large scale image geo localization location image estimated identifying geo tagged reference image depicting same place <eos> propose novel model learning image representations integrates context aware feature reweighting order effectively focus region positively contribute geo localization <eos> particular introduce contextual reweighting network crn predicts importance each region feature map based image context <eos> model learned end end image geo localization task requires no annotation other than image geo tags training <eos> experimental result proposed approach significantly outperforms previous state art standard geo localization benchmark datasets <eos> also demonstrate crn discovers task relevant contexts without any additional supervision <eos> <eop> effectiveness visible watermarks <eos> visible watermarking widely used technique marking protecting copyrights many millions image web yet suffers inherent security flaw watermarks typically added consistent manner many image <eos> show consistency allows automatically estimate watermark recover original image high accuracy <eos> specifically present generalized multi image matting algorithm takes watermarked image collection input automatically estimates foreground watermark its alpha matte background original image <eos> since such attack relies consistency watermarks across image collection explore evaluate how affected various types inconsistencies watermark embedding could potentially used make watermarking more secured <eos> demonstrate algorithm stock imagery available web provide extensive quantitative analysis synthetic watermarked data <eos> key takeaway message paper visible watermarks should designed only robust against removal single image but more resistant mass scale removal image collections well <eos> <eop> deep view morphing <eos> recently convolutional neural network cnn successfully applied view synthesis problems <eos> however such cnn based method suffer lack texture details shape distortions high computational complexity <eos> paper propose novel cnn architecture view synthesis called deep view morphing suffer issues <eos> synthesize middle view two input image rectification network first rectifies two input image <eos> encoder decoder network then generates dense correspondences between rectified image blending masks predict visibility pixels rectified image middle view <eos> view morphing network finally synthesizes middle view using dense correspondences blending masks <eos> experimentally show proposed method significantly outperforms state art cnn based view synthesis method <eos> <eop> designing illuminant spectral power distributions surface classification <eos> there many scientific medical industrial imaging applications users full control scene illumination color reproduction primary objective example possible co design sensors spectral illumination order classify detect changes biological tissues organic inorganic materials object surface properties <eos> paper propose two different approaches illuminant spectrum selection surface classification <eos> supervised framework formulate biconvex optimization problem alternate between optimizing support vector classifier weights optimal illuminants <eos> also describe sparse principal component analysis pca dimensionality reduction approach used unlabeled data <eos> efficiently solve non convex pca problem using convex relaxation alternating direction method multipliers admm <eos> compare classification accuracy monochrome imaging sensor optimized illuminants classification accuracy conventional rgb cameras natural broadband illumination <eos> <eop> end end learning driving models large scale video datasets <eos> robust perception action models should learned training data diverse visual appearances realistic behaviors yet current approaches deep visuomotor policy learning generally limited situ models learned single vehicle simulation environment <eos> advocate learning generic vehicle motion model large scale crowd sourced video data develop end end trainable architecture learning predict distribution over future vehicle egomotion instantaneous monocular camera observations previous vehicle state <eos> model incorporates novel fcn lstm architecture learned large scale crowd sourced vehicle action data leverages available scene segmentation side tasks improve performance under privileged learning paradigm <eos> provide novel large scale dataset crowd sourced driving behavior suitable training model report result predicting driver action held out sequences across diverse conditions <eos> <eop> unsupervised visual linguistic reference resolution instructional video <eos> propose unsupervised method reference resolution instructional video goal temporally link entity <eos> mix yogurt produced <eos> key challenge inevitable visual linguistic ambiguities arising changes both visual appearance referring expression entity video <eos> challenge amplified fact aim resolve references no supervision <eos> address challenges learning joint visual linguistic model linguistic cues help resolve visual ambiguities vice versa <eos> verify approach learning model unsupervisedly using more than two thousand unstructured cooking video youtube show visual linguistic model substantially improve upon state art linguistic only model reference resolution instructional video <eos> <eop> dense captioning joint inference visual context <eos> dense captioning newly emerging computer vision topic understanding image dense language descriptions <eos> goal densely detect visual concepts <eos> object object parts interactions between them image labeling each short descriptive phrase <eos> identify two key challenges dense captioning need properly addressed when tackling problem <eos> first dense visual concept annotations each image associated highly overlapping target region making accurate localization each visual concept challenging <eos> second large amount visual concepts makes hard recognize each them appearance alone <eos> propose new model pipeline based two novel ideas joint inference context fusion alleviate two challenges <eos> design model architecture methodical manner thoroughly evaluate variations architecture <eos> final model compact efficient achieves state art accuracy visual genome dense captioning relative gain compared previous best algorithm <eos> qualitative experiments also reveal semantic capabilities model dense captioning <eos> <eop> unsupervised learning long term motion dynamics video <eos> present unsupervised representation learning approach compactly encodes motion dependencies video <eos> given pair image video clip framework learns predict long term three dimensional motions <eos> reduce complexity learning framework propose describe motion sequence atomic three dimensional flows computed rgb modality <eos> use recurrent neural network based encoder decoder framework predict sequences flows <eos> argue order decoder reconstruct sequences encoder must learn robust video representation captures long term motion dependencies spatial temporal relations <eos> demonstrate effectiveness learned temporal representations activity classification across multiple modalities datasets such ntu rgb msr daily activity three dimensional framework generic any input modality <eos> rgb depth rgb video <eos> <eop> clkn cascaded lucas kanade network image alignment <eos> paper proposes data driven approach image alignment <eos> main contribution novel network architecture combines strengths convolutional neural network cnn lucas kanade algorithm <eos> main component architecture lucas kanade layer performs inverse compositional algorithm convolutional feature maps <eos> train network develop cascaded feature learning method incorporates coarse fine strategy into training process <eos> method learns pyramid representation convolutional feature cascaded manner yields cascaded network performs coarse fine alignment feature pyramids <eos> apply model task homography estimation perform training evaluation large labeled dataset generated ms coco dataset <eos> experimental result show proposed approach significantly outperforms other method <eos> <eop> agent centric risk assessment accident anticipation risky region localization <eos> survival living agent <eos> must ability assess risk temporally anticipating accidents before they occur fig <eos> spatially localizing risky region fig <eos> environment move away threats <eos> paper take agent centric approach study accident anticipation risky region localization tasks <eos> propose novel soft attention recurrent neural network rnn explicitly models both spatial appearance wise non linear interaction between agent triggering event another agent static region involved <eos> order test proposed method introduce epic fail ef dataset consisting viral video capturing various accidents <eos> experiments evaluate risk assessment accuracy both temporal domain accident anticipation spatial domain risky region localization ef dataset street accident sa dataset <eos> method consistently outperforms other baselines both datasets <eos> <eop> shapeodds variational bayesian learning generative shape models <eos> shape models provide compact parameterization class shapes shown important variety vision problems including object detection tracking image segmentation <eos> learning generative shape models grid structured representations aka silhouettes usually hindered data likelihoods intractable marginals posteriors high dimensional shape spaces limited training sample associated risk overfitting estimation hyperparameters relating model complexity often entails computationally expensive grid searches <eos> paper propose bayesian treatment relies direct probabilistic formulation learning generative shape models silhouettes space <eos> propose variational approach learning latent variable model make use extend recent works variational bounds logistic gaussian integrals circumvent intractable marginals posteriors <eos> spatial coherency sparsity priors also incorporated lend stability optimization problem regularizing solution space while avoiding overfitting high dimensional low sample size scenario <eos> deploy type ii maximum likelihood estimate model hyperparameters avoid grid searches <eos> demonstrate proposed model generates realistic sample generalizes unseen examples able handle missing region background clutter while comparing favorably recent neural network based approaches <eos> <eop> expecting unexpected training detectors unusual pedestrians adversarial imposters <eos> autonomous vehicles become every day reality high accuracy pedestrian detection paramount practical importance <eos> pedestrian detection highly researched topic mature method but most datasets both training evaluation focus common scenes people engaged typical walking poses sidewalks <eos> but performance most crucial dangerous scenarios rarely observed such children playing street people using bicycles skateboards unexpected ways <eos> such tail data notoriously hard observe making both training testing difficult <eos> analyze problem collected novel annotated dataset dangerous scenarios called precarious pedestrian dataset <eos> even given dedicated collection effort relatively small contemporary standards image <eos> explore large scale data driven learning explore use synthetic data generated game engine <eos> significant challenge selected right priors parameters synthesis would like realistic data realistic poses object configurations <eos> inspired generative adversarial network generate massive amount synthetic data train discriminative classifier select realistic subset fools classifier deem synthetic imposters <eos> demonstrate pipeline allows one generate realistic adverserial training data making use rendering animation engines <eos> interestingly also demonstrate such data used rank algorithms suggesting synthetic imposters also used tail validation test time notoriously difficult challenge real world deployment <eos> <eop> er unified framework event retrieval recognition recounting <eos> develop unified framework complex event retrieval recognition recounting <eos> framework based compact video representation exploits temporal correlations image feature <eos> feature alignment procedure identifies removes feature redundancies across frames outputs intermediate tensor representation call video imprint <eos> video imprint then fed into reasoning network whose attention mechanism parallels memory network used language modeling <eos> reasoning network simultaneously recognizes event category locates key pieces evidence event recounting <eos> event retrieval tasks show compact video representation aggregated video imprint achieves significantly better retrieval accuracy compared existing method <eos> also set new state art result event recognition tasks additional benefit latent structure reasoning network highlights areas video imprint directly used event recounting <eos> video imprint maps back locations video frames network allows only identification key frames but also specific areas inside each frame most influential decision process <eos> <eop> outlier robust tensor pca <eos> low rank tensor analysis important various real applications computer vision <eos> however existing method focus recovering low rank tensor contaminated gaussian gross sparse noise hence cannot effectively handle outliers common practical tensor data <eos> solve issue propose outlier robust tensor principle component analysis tpca method simultaneous low rank tensor recovery outlier detection <eos> intrinsically low rank tensor observations arbitrary outlier corruption tpca first method provable performance guarantee exactly recovering tensor subspace detecting outliers under mild conditions <eos> since tensor data naturally high dimensional multi way further develop fast randomized algorithm requires small sampling size yet substantially accelerate tpca without performance drop <eos> experimental result four tasks outlier detection clustering semi supervised supervised learning clearly demonstrate advantages method <eos> <eop> mind class weight bias weighted maximum mean discrepancy unsupervised domain adaptation <eos> domain adaptation maximum mean discrepancy mmd widely adopted discrepancy metric between distributions source target domains <eos> however existing mmd based domain adaptation method generally ignore changes class prior distributions <eos> class weight bias across domains <eos> remains open problem but ubiquitous domain adaptation caused changes sample selection criteria application scenarios <eos> show mmd cannot account class weight bias result degraded domain adaptation performance <eos> address issue weighted mmd model proposed paper <eos> specifically introduce class specific auxiliary weights into original mmd exploiting class prior probability source target domains whose challenge lies fact class label target domain unavailable <eos> account proposed weighted mmd model defined introducing auxiliary weight each class source domain classification em algorithm suggested alternating between assigning pseudo labels estimating auxiliary weights updating model parameters <eos> extensive experiments demonstrate superiority weighted mmd over conventional mmd domain adaptation <eos> <eop> syncspeccnn synchronized spectral cnn three dimensional shape segmentation <eos> paper study problem semantic annotation three dimensional models represented shape graphs <eos> functional view taken represent localized information graphs so annotations such part segment keypoint nothing but indicator vertex functions <eos> compared image grids shape graphs irregular non isomorphic data structures <eos> enable prediction vertex functions them convolutional neural network resort spectral cnn method enables weight sharing parametrizing kernels spectral domain spanned graph laplacian eigenbases <eos> under setting network named syncspeccnn strives overcome two key challenges how share coefficients conduct multi scale analysis different parts graph single shape how share information across related but different shapes may represented very different graphs <eos> towards goals introduce spectral parametrization dilated convolutional kernels spectral transformer network <eos> experimentally tested syncspeccnn various tasks including three dimensional shape part segmentation keypoint prediction <eos> state art performance achieved all benchmark datasets <eos> <eop> unrolling shutter cnn correct motion distortions <eos> row wise exposure delay present cmos cameras responsible skew curvature distortions known rolling shutter rs effect while imaging under camera motion <eos> existing rs correction method resort using multiple image tailor scene specific correction schemes <eos> propose convolutional neural network cnn architecture automatically learns essential scene feature single rs image estimate row wise camera motion undo rs distortions back time first row exposure <eos> employ long rectangular kernels specifically learn effects produced row wise exposure <eos> experiments reveal proposed architecture performs better than conventional cnn employing square kernels <eos> single image correction method fares well even operating frame frame manner against video based method performs better than scene specific correction schemes even under challenging situations <eos> <eop> deep level set salient object detection <eos> deep learning applied saliency detection recent years <eos> superior performance proved deep network model semantic properties salient object <eos> yet difficult deep network discriminate pixels belonging similar receptive fields around object boundaries thus deep network may output maps blurred saliency inaccurate boundaries <eos> tackle such issue work propose deep level set network produce compact uniform saliency maps <eos> method drives network learn level set function salient object so output more accurate boundaries compact saliency <eos> besides propagate saliency information among pixels recover full resolution saliency map extend superpixel based guided filter layer network <eos> proposed network simple structure trained end end <eos> during testing network produce saliency maps efficiently feedforwarding testing image speed over fps gpus <eos> evaluations benchmark datasets show proposed method achieves state art performance <eos> <eop> instance aware image sentence matching selective multimodal lstm <eos> effective image sentence matching depends how well measure their global visual semantic similarity <eos> based observation such global similarity arises complex aggregation multiple local similarities between pairwise instances image object sentence words propose selective multimodal long short term memory network sm lstm instance aware image sentence matching <eos> sm lstm includes multimodal context modulated attention scheme each timestep selectively attend pair instances image sentence predicting pairwise instance aware saliency maps image sentence <eos> selected pairwise instances their representations obtained based predicted saliency maps then compared measure their local similarity <eos> similarly measuring multiple local similarities within few timesteps sm lstm sequentially aggregates them hidden states obtain final matching score desired global similarity <eos> extensive experiments show model well match image sentence complex content achieve state art result two public benchmark datasets <eos> <eop> motion blur motion flow deep learning solution removing heterogeneous motion blur <eos> removing pixel wise heterogeneous motion blur challenging due ill posed nature problem <eos> predominant solution estimate blur kernel adding prior but extensive literature subject indicates difficulty identifying prior suitably informative general <eos> rather than imposing prior based theory propose instead learn one data <eos> learning prior over latent image would require modeling all possible image content <eos> critical observation underpinning approach however learning motion flow instead allows model focus cause blur irrespective image content <eos> much easier learning task but also avoids iterative process through latent image priors typically applied <eos> approach directly estimates motion flow blurred image through fully convolutional deep neural network fcn recovers unblurred image estimated motion flow <eos> fcn first universal end end mapping blurred image dense motion flow <eos> train fcn simulate motion flows generate synthetic blurred image motion flow pairs thus avoiding need human labeling <eos> extensive experiments challenging realistic blurred image demonstrate proposed method outperforms state art <eos> <eop> deep temporal linear encoding network <eos> cnn encoding feature entire video representation human actions rarely addressed <eos> instead cnn work focused approaches fuse spatial temporal network but were typically limited processing shorter sequences <eos> present new video representation called temporal linear encoding tle embedded inside cnn new layer captures appearance motion throughout entire video <eos> encodes aggregated information into robust video feature representation via end end learning <eos> advantages tles they encode entire video into compact feature representation learning semantics discriminative feature space they applicable all kinds network like three dimensional cnn video classification they model feature interactions more expressive way without loss information <eos> conduct experiments two challenging human action datasets hmdb ucf <eos> experiments show tle outperforms current state art method both datasets <eos> <eop> end end training hybrid cnn crf models stereo <eos> propose novel principled hybrid cnn crf model stereo estimation <eos> model allows exploit advantages both convolutional neural network cnn conditional random fields crfs unified approach <eos> cnn compute expressive feature matching distinctive color edges turn used compute unary binary costs crf <eos> inference apply recently proposed highly parallel dual block descent algorithm only needs small fixed number iterations compute high quality approximate minimizer <eos> main contribution paper propose theoretically sound method based structured output support vector machine ssvm train hybrid cnn crf model large scale data end end <eos> trained models perform very well despite fact using shallow cnn apply any kind post processing final output crf <eos> evaluate combined models challenging stereo benchmarks such middlebury kitti also investigate performance each individual component <eos> <eop> deep feature flow video recognition <eos> deep convolutional neutral network achieved great success image recognition tasks <eos> yet non trivial transfer state art image recognition network video per frame evaluation too slow unaffordable <eos> present deep feature flow fast accurate framework video recognition <eos> runs expensive convolutional sub network only sparse key frames propagates their deep feature maps other frames via flow field <eos> achieves significant speedup flow computation relatively fast <eos> end end training whole architecture significantly boosts recognition accuracy <eos> deep feature flow flexible general <eos> validated two recent large scale video datasets <eos> makes large step towards practical video recognition <eos> code would released <eos> <eop> fully convolutional instance aware semantic segmentation <eos> present first fully convolutional end end solution instance aware semantic segmentation task <eos> inherits all merits fcns semantic segmentation instance mask proposal <eos> performs instance mask prediction classification jointly <eos> underlying convolutional representation fully shared between two sub tasks well between all region interest <eos> network architecture highly integrated efficient <eos> achieves state art performance both accuracy efficiency <eos> wins coco segmentation competition large margin <eos> code would released <eos> <eop> truncated max convex models <eos> truncated convex models tcm special case pair wise random fields widely used computer vision <eos> however restricting order potentials most two they fail capture useful image statistics <eos> propose natural generalization tcm high order random fields call truncated max convex models tmcm <eos> energy function tmcm consists two types potentials unary potential no restriction its form ii high order potential sum truncation largest convex distances over disjoint pairs random variables arbitrary size clique <eos> use convex distance function encourages smoothness while truncation allows discontinuities labeling <eos> using tmcm provides robustness towards errors definition cliques <eos> order minimize energy function tmcm over all possible labelings design efficient st mincut based range expansion algorithm <eos> prove accuracy algorithm establishing strong multiplicative bounds several special cases interest <eos> using synthetic standard real datasets demonstrate benefit high order tmcm over pairwise tcm well benefit range expansion algorithm over other st mincut based approaches <eos> <eop> asymmetric feature maps application sketch based retrieval <eos> propose novel concept asymmetric feature maps afm allows evaluate multiple kernels between query database entries without increasing memory requirements <eos> demonstrate advantages afm method derive short vector image representation due asymmetric feature maps supports efficient scale translation invariant sketch based image retrieval <eos> unlike most short code based retrieval systems proposed method provides query localization retrieved image <eos> efficiency search boosted approximating translation search via trigonometric polynomial scores projections <eos> projections special case afm <eos> order magnitude speed up achieved compared traditional trigonometric polynomials <eos> result boosted image based average query expansion exceeding significantly state art standard benchmarks <eos> <eop> instance level salient object segmentation <eos> image saliency detection recently witnessed rapid progress due deep convolutional neural network <eos> however none existing method able identify object instances detected salient region <eos> paper present salient instance segmentation method produces saliency mask distinct object instance labels input image <eos> method consists three steps estimating saliency map detecting salient object contours identifying salient object instances <eos> first two steps propose multiscale saliency refinement network generates high quality salient region masks salient object contours <eos> once integrated multiscale combinatorial grouping map based subset optimization framework method generate very promising salient object instance segmentation result <eos> promote further research evaluation salient instance segmentation also construct new database image their pixelwise salient instance annotations <eos> experimental result demonstrate proposed method capable achieving state art performance all public benchmarks salient region detection well new dataset salient instance segmentation <eos> <eop> kernel square loss exemplar machines image retrieval <eos> zepeda perez recently demonstrated promise exemplar svm esvm feature encoder image retrieval <eos> paper extends approach several directions first show replacing hinge loss square loss esvm cost function significantly reduces encoding time negligible effect accuracy <eos> call model square loss exemplar machine slem <eos> then introduce kernelized slem implemented efficiently through low rank matrix decomposition displays improved performance <eos> both slem variants exploit fact negative examples fixed so most slem computational complexity relegated offline process independent positive examples <eos> experiments establish performance computational advantages approach using large array base feature standard image retrieval datasets <eos> <eop> direct photometric alignment mesh deformation <eos> choice motion models vital applications like image video stitching video stabilization <eos> conventional method explored different approaches ranging simple global parametric models complex per pixel optical flow <eos> mesh based warping method achieve good balance between computational complexity model flexibility <eos> however they typically require high quality feature correspondences suffer mismatches low textured image content <eos> paper propose mesh based photometric alignment method minimizes pixel intensity difference instead euclidean distance known feature correspondences <eos> proposed method combines superior performance dense photometric alignment efficiency mesh based image warping <eos> achieves better global alignment quality than feature based counterpart textured image more importantly also robust low textured image content <eos> abundant experiments show method handle variety image video outperforms representative state art method both image stitching video stabilization tasks <eos> <eop> semantic multi view stereo jointly estimating object voxels <eos> dense three dimensional reconstruction rgb image highly ill posed problem due occlusions textureless reflective surfaces well other challenges <eos> propose object level shape priors address ambiguities <eos> towards goal formulate probabilistic model integrates multi view image evidence three dimensional shape information multiple object <eos> inference model yields dense three dimensional reconstruction scene well existence precise three dimensional pose object <eos> approach able recover fine details captured input shapes while defaulting input models occluded region image evidence weak <eos> due its probabilistic nature approach able cope approximate geometry three dimensional models well input shapes present scene <eos> evaluate approach quantitatively several challenging indoor outdoor datasets <eos> <eop> hope hierarchical object prototype encoding efficient object instance search video <eos> paper tackles problem efficient effective object instance search video <eos> effectively capture relevance between query video frames precisely localize particular object leverage object proposals improve quality object instance search video <eos> however hundreds object proposals obtained each frame could result unaffordable memory computational cost <eos> end present simple yet effective hierarchical object prototype encoding hope model accelerate object instance search without sacrificing accuracy exploits both spatial temporal self similarity property existing object proposals generated video frames <eos> design two types sphere means method <eos> spatially constrained sphere means temporally constrained sphere means learn frame level object prototypes dataset level object prototypes respectively <eos> way object instance search problem cast sparse matrix vector multiplication problem <eos> thanks sparsity codes both memory computational cost significantly reduced <eos> experimental result two video datasets demonstrate approach significantly improves performance video object instance search over other state art fast search schemes <eos> <eop> learning adaptive receptive fields deep image parsing network <eos> paper introduce novel approach regulate receptive field deep image parsing network automatically <eos> unlike previous works stressed much importance obtaining better receptive fields using manually selected dilated convolutional kernels approach uses two affine transformation layer network backbone operates feature maps <eos> feature maps will inflated shrinked new layer therefore receptive fields following layer changed accordingly <eos> end end training whole framework data driven without laborious manual intervention <eos> proposed method generic across dataset different tasks <eos> conduct extensive experiments both general parsing task face parsing task concrete examples demonstrate method superior regulation ability over manual designs <eos> <eop> contour constrained superpixels image video processing <eos> novel contour constrained superpixel ccs algorithm proposed work <eos> initialize superpixels region regular grid then refine superpixel label each region hierarchically block pixel levels <eos> make superpixel boundaries compatible object contours propose notion contour pattern matching formulate objective function including contour constraint <eos> furthermore extend ccs algorithm generate temporal superpixels video processing <eos> initialize superpixel labels each frame transferring previous frame refine labels make superpixels temporally consistent well compatible object contours <eos> experimental result demonstrate proposed algorithm provides better performance than state art superpixel method <eos> <eop> learning predict stereo reliability enforcing local consistency confidence maps <eos> confidence measures estimate unreliable disparity assignments performed stereo matching algorithm recently proved used several purposes <eos> paper aims increasing means deep network effectiveness state art confidence measures exploiting local consistency assumption <eos> exhaustively evaluated proposal confidence measures including top performing ones based random forests cnn training network two popular stereo algorithms small subset out frames kitti dataset <eos> experimental result show approach dramatically increases effectiveness all confidence measures remaining frames <eos> moreover without re training report further cross evaluation kitti middlebury confirming proposal provides remarkable improvements each confidence measure even when dealing significantly different input data <eos> best knowledge first method move beyond conventional pixel wise confidence estimation <eos> <eop> flownet evolution optical flow estimation deep network <eos> flownet demonstrated optical flow estimation <eop> growing brain fine tuning increasing model capacity <eos> cnn made undeniable impact computer vision through ability learn high capacity models large annotated training set <eos> one their remarkable properties ability transfer knowledge large source dataset typically smaller target dataset <eos> usually accomplished through fine tuning fixed size network new target data <eos> indeed virtually every contemporary visual recognition system makes use fine tuning transfer knowledge imagenet <eos> work analyze components parameters change during fine tuning discover increasing model capacity allows more natural model adaptation through fine tuning <eos> making analogy developmental learning demonstrate growing cnn additional units either widening existing layer deepening overall network significantly outperforms classic fine tuning approaches <eos> but order properly grow network show newly added units must appropriately normalized allow pace learning consistent existing units <eos> empirically validate approach several benchmark datasets producing state art result <eos> <eop> dynamic attention controlled cascaded shape regression exploiting training data augmentation fuzzy set sample weighting <eos> present new cascaded shape regression csr architecture namely dynamic attention controlled csr dac csr robust facial landmark detection unconstrained faces <eos> dac csr divides facial landmark detection into three cascaded sub tasks face bounding box refinement general csr attention controlled csr <eos> first two stages refine initial face bounding boxes output intermediate facial landmarks <eos> then online dynamic model selection method used choose appropriate domain specific csrs further landmark refinement <eos> key innovation dac csr fault tolerant mechanism using fuzzy set sample weighting attention controlled domain specific model training <eos> moreover advocate data augmentation simple but effective profile face generator context aware feature extraction better facial feature representation <eos> experimental result obtained challenging datasets demonstrate merits dac csr over state art method <eos> <eop> additive component analysis <eos> principal component analysis pca one most versatile tools unsupervised learning applications ranging dimensionality reduction exploratory data analysis visualization <eos> while much effort devoted encouraging meaningful representations through regularization <eos> non negativity sparsity underlying linearity assumptions limit their effectiveness <eos> address issue propose additive component analysis aca novel nonlinear extension pca <eos> inspired multivariate nonparametric regression additive models aca fits smooth manifold data learning explicit mapping low dimensional latent space input space trivially enables applications like denoising <eos> furthermore aca used drop replacement many algorithms use linear component analysis method subroutine via local tangent space learned manifold <eos> unlike many other nonlinear dimensionality reduction techniques aca efficiently applied large datasets since require computing pairwise similarities storing training data during testing <eos> multiple aca layer also composed learned jointly essentially same procedure improved representational power demonstrating encouraging potential nonparametric deep learning <eos> evaluate aca variety datasets showing improved robustness reconstruction performance interpretability <eos> <eop> lifting deep convolutional three dimensional pose estimation single image <eos> propose unified formulation problem three dimensional human pose estimation single raw rgb image reasons jointly about joint estimation three dimensional pose reconstruction improve both tasks <eos> take integrated approach fuses probabilistic knowledge three dimensional human pose multi stage cnn architecture uses knowledge plausible three dimensional landmark locations refine search better locations <eos> entire process trained end end extremely efficient obtains state art result human <eos> outperforming previous approaches both three dimensional errors <eos> <eop> attentional push deep convolutional network augmenting image salience shared attention modeling social scenes <eos> present novel visual attention tracking technique based shared attention modeling <eos> considering viewer participant activity occurring scene model learns loci attention scene actors use augment image salience <eos> go beyond image salience instead only computing power image region pull attention also consider strength scene actors push attention region question thus term attentional push <eos> present convolutional neural network cnn augments standard saliency models attentional push <eos> model contains two pathways attentional push pathway learns gaze location scene actors saliency pathway <eos> followed shallow augmented saliency cnn combines them generates augmented saliency <eos> training use transfer learning initialize train attentional push cnn minimizing classification error following actors gaze location grid using large scale gaze following dataset <eos> attentional push cnn then fine tuned along augmented saliency cnn minimize euclidean distance between augmented saliency ground truth fixations using eye tracking dataset annotated head gaze location scene actors <eos> evaluate model three challenging eye fixation datasets salicon isun cat illustrate significant improvements predicting viewers fixations social scenes <eos> <eop> fine grained recognition hsnet search informative image parts <eos> work addresses fine grained image classification <eos> work based hypothesis when dealing subtle differences among object classes critical identify only account few informative image parts remaining image context may only uninformative but may also hurt recognition <eos> motivates formulate problem sequential search informative parts over deep feature map produced deep convolutional neural network cnn <eos> state search set proposal bounding boxes image whose informativeness evaluated heuristic function used generating new candidate states successor function <eos> two functions unified via long short term memory network lstm into new deep recurrent architecture called hsnet <eos> thus hsnet generates proposals informative image parts ii fuses all proposals toward final fine grained recognition <eos> specify both supervised weakly supervised training hsnet depending availability object part annotations <eos> evaluation benchmark caltech ucsd birds cars datasets demonstrate competitive performance relative state art <eos> <eop> scalable person re identification supervised smoothed manifold <eos> most existing person re identification algorithms either extract robust visual feature learn discriminative metrics person image <eos> however underlying manifold image reside rarely investigated <eos> arises problem learned metric smooth respect local geometry structure data manifold <eos> paper study person re identification manifold based affinity learning did receive enough attention area <eos> unconventional manifold preserving algorithm proposed make best use supervision training data whose label information given pairwise constraints scale up large repositories low line time complexity plunged into most existing algorithms serving generic postprocessing procedure further boost identification accuracies <eos> extensive experimental result five popular person re identification benchmarks consistently demonstrate effectiveness method <eos> especially largest cuhk market method outperforms state art alternatives large margin high efficiency more appropriate practical applications <eos> <eop> riemannian nonlinear mixed effects models analyzing longitudinal deformations neuroimaging <eos> statistical machine learning models operate manifold valued data being extensively studied vision motivated applications activity recognition feature tracking medical imaging <eos> while non parametric method relatively well studied literature efficient formulations parametric models may offer benefits small sample size regimes only emerged recently <eos> so far manifold valued regression models such geodesic regression restricted analysis cross sectional data <eos> so called fixed effects statistics <eos> but most longitudinal analysis <eos> when participant provides multiple measurements over time application fixed effects models problematic <eos> effort answer need paper generalizes non linear mixed effects model regime response variable manifold valued <eos> derive underlying model estimation schemes demonstrate immediate benefits such model provide both group level individual level analysis longitudinal brain imaging data <eos> direct consequence result longitudinal analysis manifold valued measurements especially symmetric positive definite manifold conducted computationally tractable manner <eos> <eop> detecting oriented text natural image linking segments <eos> most state art text detection method specific horizontal latin text fast enough real time applications <eos> introduce segment linking seglink oriented text detection method <eos> main idea decompose text into two locally detectable elements namely segments links <eos> segment oriented box covering part word text line link connects two adjacent segments indicating they belong same word text line <eos> both elements detected densely multiple scales end end trained fully convolutional neural network <eos> final detections produced combining segments connected links <eos> compared previous method seglink improves along dimensions accuracy speed ease training <eos> standard icdar incidental challenge benchmark outperforming previous best large margin <eos> runs over fps image <eos> moreover without modification seglink able detect long lines non latin text such chinese <eos> <eop> diverse image annotation <eos> work study task image annotation goal describe image using few tags <eos> instead predicting full list tags here target providing short list tags under limited number <eos> cover much information possible image <eos> tags such short list should representative diverse <eos> means they required only corresponding contents image but also different each other <eos> end treat image annotation subset selection problem based conditional determinantal point process dpp model formulates representation diversity jointly <eos> further explore semantic hierarchy synonyms among candidate tags require two tags semantic hierarchy pair synonyms should selected simultaneously <eos> requirement then embedded into sampling algorithm according learned conditional dpp model <eos> besides find traditional metrics image annotation <eos> precision recall score only consider representation but ignore diversity <eos> thus propose new metrics evaluate quality selected subset <eos> tag list based semantic hierarchy synonyms <eos> human study through amazon mechanical turk verifies proposed metrics more close human judgment than traditional metrics <eos> experiments two benchmark datasets show proposed method produce more representative diverse tags compared existing image annotation method <eos> <eop> inverse compositional spatial transformer network <eos> paper establish theoretical connection between classical lucas kanade lk algorithm emerging topic spatial transformer network stns <eos> stns interest vision learning communities due their natural ability combine alignment classification within same theoretical framework <eos> inspired inverse compositional ic variant lk algorithm present inverse compositional spatial transformer network ic stns <eos> demonstrate ic stns achieve better performance than conventional stns less model capacity particular show superior performance pure image alignment tasks well joint alignment classification problems real world problems <eos> <eop> factorized variational autoencoders modeling audience reactions movies <eos> matrix tensor factorization method often used finding underlying low dimensional patterns noisy data <eos> paper study non linear tensor factoriza tion method based deep variational autoencoders <eos> approach well suited settings relationship between latent representation learned raw data representation highly complex <eos> apply ap proach large dataset facial expressions movie watching audiences over million faces <eos> experi ments show compared conventional linear factoriza tion method method achieves better reconstruction data further discovers interpretable latent factors <eos> <eop> adversarially tuned scene generation <eos> generalization performance trained computer vision cv systems use computer graphics cg generated data yet effective due concept domain shift between virtual real data <eos> although simulated data augmented few real world sample shown mitigate domain shift improve transferability trained models guiding bootstrapping virtual data generation distributions learnt target real world domain desired especially fields annotating even few real image laborious such semantic labeling optical flow intrinsic image etc <eos> order address problem unsupervised manner work combines recent advances cg aims generating stochastic scene layouts using large collections three dimensional object models generative adversarial training aims training generative models measuring discrepancy between generated real data terms their separability space deep discriminatively trained classifier <eos> method uses iterative estimation posterior density prior distributions generative graphical model <eos> done within rejection sampling framework <eos> initially assume uniform distributions priors over parameters scene described generative graphical model <eos> iterations proceed uniform prior distributions updated sequentially distributions closer unknown distributions target data <eos> demonstrate utility adversarially tuned scene generation two real world benchmark datasets cityscapes camvid traffic scene semantic labeling deep convolutional net deeplab <eos> obtained performance improvements <eos> point iou metric between deeplab models trained simulated set prepared scene generation models before after tuning cityscapes camvid respectively <eos> <eop> binge watching scaling affordance learning sitcoms <eos> recent years there renewed interest jointly modeling perception action <eos> core investigation idea modeling affordances <eos> however when comes predicting affordances even state art approaches still use any convnets <eos> why unlike semantic three dimensional tasks there still exist any large scale dataset affordances <eos> paper tackle challenge creating one biggest dataset learning affordances <eos> use seven sitcoms extract diverse set scenes how actors interact different object scenes <eos> dataset consists more than scenes ways humans interact image <eos> also propose two step approach predict affordances new scene <eos> first step given location scene classify pose classes likely affordance pose <eos> given pose class scene then use variational autoencoder vae extract scale deformation pose <eos> vae allows sample distribution possible poses test time <eos> finally show importance large scale data learning generalizable robust model affordances <eos> <eop> fast rcnn hard positive generation via adversary object detection <eos> how learn object detector invariant occlusions deformations current solution use data driven strategy collect large scale datasets object instances under different conditions <eos> hope final classifier use examples learn invariances <eos> but really possible see all occlusions dataset argue like categories occlusions object deformations also follow long tail <eos> some occlusions deformations so rare they hardly happen yet want learn model invariant such occurrences <eos> paper propose alternative solution <eos> propose learn adversarial network generates examples occlusions deformations <eos> goal adversary generate examples difficult object detector classify <eos> framework both original detector adversary learned joint manner <eos> experimental result indicate <eos> map boost voc <eos> map boost voc object detection challenge compared fast rcnn pipeline <eos> <eop> cognitive mapping planning visual navigation <eos> introduce neural architecture navigation novel environments <eos> proposed architecture learns map first person views plans sequence actions towards goals environment <eos> cognitive mapper planner cmp based two key ideas unified joint architecture mapping planning such mapping driven needs planner spatial memory ability plan given incomplete set observations about world <eos> cmp constructs top down belief map world applies differentiable neural net planner produce next action each time step <eos> accumulated belief world enables agent track visited region environment <eos> experiments demonstrate cmp outperforms both reactive strategies standard memory based architectures performs well novel environments <eos> furthermore show cmp also achieve semantically specified goals such go chair <eos> <eop> multi view supervision single view reconstruction via differentiable ray consistency <eos> study notion consistency between three dimensional shape observation propose differentiable formulation allows computing gradients three dimensional shape given observation arbitrary view <eos> so reformulating view consistency using differentiable ray consistency drc term <eos> show formulation incorporated learning framework leverage different types multi view observations <eos> foreground masks depth color image semantics etc <eos> supervision learning single view three dimensional prediction <eos> present empirical analysis technique controlled setting <eos> also show approach allows improve over existing techniques single view reconstruction object pascal voc dataset <eos> <eop> learning shape abstractions assembling volumetric primitives <eos> present learning framework abstracting complex shapes learning assemble object using three dimensional volumetric primitives <eos> addition generating simple geometrically interpretable explanations three dimensional object framework also allows automatically discover exploit consistent structure data <eos> demonstrate using method allows predicting shape representations leveraged obtaining consistent parsing across instances shape collection constructing interpretable shape similarity measure <eos> also examine applications image based prediction well shape manipulation <eos> <eop> amc attention guided multi modal correlation learning image search <eos> given user query traditional image search systems rank image according its relevance single modality <eos> image content surrounding text <eos> nowadays increasing number image internet available associated meta data rich modalities <eos> titles keywords tags etc <eos> exploited better similarity measure queries <eos> paper leverage visual textual modalities image search learning their correlation input query <eos> according intent query attention mechanism introduced adaptively balance importance different modalities <eos> propose novel attention guided multi modal correlation amc learning method consists jointly learned hierarchy intra inter attention network <eos> conditioned query intent intra attention network <eos> visual intra attention network language intra attention network attend informative parts within each modality multi modal inter attention network promotes importance most query relevant modalities <eos> experiments evaluate amc models search logs two real world image search engines show significant boost ranking user clicked image search result <eos> additionally extend amc models caption ranking task coco dataset achieve competitive result compared recent state arts <eos> <eop> bidirectional multirate reconstruction temporal modeling video <eos> despite recent success neural network image feature learning major problem video domain lack sufficient labeled data learning model temporal information <eos> paper propose unsupervised temporal modeling method learns untrimmed video <eos> speed motion varies constantly <eos> man may run quickly slowly <eos> therefore train multirate visual recurrent model mvrm encoding frames clip different intervals <eos> learning process makes learned model more capable dealing motion speed variance <eos> given clip sampled video use its past future neighboring clips temporal context reconstruct two temporal transitions <eos> present past transition present future transition reflecting temporal information different views <eos> proposed method exploits two transitions simultaneously incorporating bidirectional reconstruction consists backward reconstruction forward reconstruction <eos> apply proposed method two challenging video tasks <eos> complex event detection video captioning achieves state art performance <eos> notably method generates best single feature event detection relative improvement <eos> medtest dataset achieves best performance video captioning across all evaluation metrics youtube text dataset <eos> <eop> learning video object segmentation static image <eos> inspired recent advances deep learning instance segmentation object tracking introduce concept convnet based guidance applied video object segmentation <eos> model proceeds per frame basis guided output previous frame towards object interest next frame <eos> demonstrate highly accurate object segmentation video enabled using convolutional neural network convnet trained static image only <eos> key component approach combination offline online learning strategies former produces refined mask previous frame estimate latter allows capture appearance specific object instance <eos> method handle different types input annotations such bounding boxes segments while leveraging arbitrary amount annotated frames <eos> therefore system suitable diverse applications different requirements terms accuracy efficiency <eos> extensive evaluation obtain competitive result three different datasets independently type input annotation <eos> <eop> more you know using knowledge graphs image classification <eos> one characteristic set humans apart modern learning based computer vision algorithms ability acquire knowledge about world use knowledge reason about visual world <eos> humans learn about characteristics object relationships occur between them learn large variety visual concepts often few examples <eos> paper investigates use structured prior knowledge form knowledge graphs shows using knowledge improves performance image classification <eos> build recent work end end learning graphs introducing graph search neural network way efficiently incorporating large knowledge graphs into vision classification pipeline <eos> show number experiments method outperforms standard neural network baselines multi label classification <eos> <eop> detecting masked faces wild lle cnn <eos> detecting masked faces <eos> faces occlusions challenging task due two main reasons absence large datasets masked faces absence facial cues masked region <eos> address issues paper first introduces dataset internet image annotated masked faces denoted mafa <eos> different many previous datasets each annotated face mafa partially occluded mask <eos> analyzing characteristics masked faces propose lle cnn detect masked face via three major modules <eos> proposal module first combines two pre trained cnn extract candidate facial region input image represent them high dimensional descriptors <eos> after embedding module turns such descriptors into vectors weights respect components pre trained dictionaries representative normal faces non faces using locally linear embedding <eos> manner missing facial cues masked region largely recovered influences noisy cues introduced diversified masks greatly alleviated <eos> finally verification module takes weight vectors input identifies real facial region well their accurate positions jointly performing classification regression tasks within unified cnn <eos> experimental result mafa show proposed approach significantly outperforms state arts least <eos> detecting masked faces <eos> <eop> ultrastereo efficient learning based matching active stereo systems <eos> efficient estimation depth pairs stereo image one core problems computer vision <eos> efficiently solve specialized problem stereo matching under active illumination using new learning based algorithm <eos> type active stereo <eos> stereo matching scene texture augmented active light projector proving compelling designing depth cameras largely due improved robustness when compared time flight traditional structured light techniques <eos> algorithm uses unsupervised greedy optimization scheme learns feature discriminative estimating correspondences infrared image <eos> proposed method optimizes series sparse hyperplanes used test time remap all image patches into compact binary representation <eos> proposed algorithm cast patchmatch stereo like framework producing depth maps hz <eos> contrast standard structured light method approach generalizes different scenes require tedious per camera calibration procedures adversely affected interference overlapping sensors <eos> extensive evaluations show surpass quality overcome limitations current depth sensing technologies <eos> <eop> learning feature watching object move <eos> paper presents novel yet intuitive approach unsupervised feature learning <eos> inspired human visual system explore whether low level motion based grouping cues used learn effective visual representation <eos> specifically use unsupervised motion based segmentation video obtain segments use pseudo ground truth train convolutional network segment object single frame <eos> given extensive evidence motion plays key role development human visual system hope straightforward approach unsupervised learning will more effective than cleverly designed pretext tasks studied literature <eos> indeed extensive experiments show case <eos> when used transfer learning object detection representation significantly outperforms previous unsupervised approaches across multiple settings especially when training data target task scarce <eos> <eop> action decision network visual tracking deep reinforcement learning <eos> paper proposes novel tracker controlled sequentially pursuing actions learned deep reinforcement learning <eos> contrast existing trackers using deep network proposed tracker designed achieve light computation well satisfactory tracking accuracy both location scale <eos> deep network control actions pre trained using various training sequences fine tuned during tracking online adaptation target background changes <eos> pre training done utilizing deep reinforcement learning well supervised learning <eos> use reinforcement learning enables even partially labeled data successfully utilized semi supervised learning <eos> through evaluation otb dataset proposed tracker validated achieve competitive performance three times faster than state art deep network based trackers <eos> fast version proposed method operates real time gpu outperforms state art real time trackers <eos> <eop> multi attention network one shot learning <eos> one shot learning challenging problem aim recognize class identified single training image <eos> given practical importance one shot learning seems surprising rich information present class tag itself largely ignored <eos> most existing approaches restrict use class tag finding similar classes transferring classifiers metrics learned thereon <eos> demonstrate here contrast class tag inform one shot learning guide visual attention training image creating image representation <eos> motivated fact human beings better interpret training image if class tag image understood <eos> specifically design neural network architecture takes semantic embedding class tag generate attention maps uses attention maps create image feature one shot learning <eos> note unlike other applications task requires learned attention generator generalized novel classes <eos> show realized representing class tags distributed word embeddings learning attention map generator auxiliary training set <eos> also design multiple attention scheme extract richer information exemplar image leads substantial performance improvement <eos> through comprehensive experiments show proposed approach leads superior performance over baseline method <eos> <eop> denet global gaussian distribution embedding network its application visual recognition <eos> recently plugging trainable structural layer into deep convolutional neural network cnn image representations made promising progress <eos> however there little work inserting parametric probability distributions effectively model feature statistics into deep cnn end end manner <eos> paper proposes global gaussian distribution embedding network denet take step towards addressing problem <eos> core denet novel trainable layer global gaussian image representation plugged into deep cnn end end learning <eos> challenge proposed layer involves gaussian distributions whose space linear space makes its forward backward propagations non intuitive non trivial <eos> tackle issue employ gaussian embedding strategy respects structures both riemannian manifold smooth group gaussians <eos> based strategy construct proposed global gaussian embedding layer decompose into two sub layer matrix partition sub layer decoupling mean vector covariance matrix entangled embedding matrix square rooted symmetric positive definite matrix sub layer <eos> way derive partial derivatives associated proposed structural layer thus allow backpropagation gradients <eos> experimental result large scale region classification fine grained recognition tasks show denet superior its counterparts capable achieving state art performance <eos> <eop> depth defocus wild <eos> consider problem two frame depth defocus conditions unsuitable existing method yet typical everyday photography handheld cellphone camera small aperture non stationary scene sparse surface texture <eos> approach combines global analysis image content three dimensional surfaces deformations figure ground relations textures local estimation joint depth flow likelihoods tiny patches <eos> enable local estimation derive novel defocus equalization filters induce brightness constancy across frames impose tight upper bound defocus blur just three pixels radius through appropriate choice second frame <eos> global analysis use novel piecewise spline scene representation propagate depth flow across large irregularly shaped region <eos> experiments show combination preserves sharp boundaries yields good depth flow maps face significant noise uncertainty non rigidity data sparsity <eos> <eop> fried binary embedding high dimensional visual feature <eos> most existing binary embedding method prefer compact binary codes dimensional avoid high computational memory cost projecting high dimensional visual feature dimensional <eos> argue long binary codes critical fully utilize discriminative power high dimensional visual feature achieve better result various tasks such approximate nearest neighbour search <eos> generating long binary codes involves large projection matrix high dimensional matrix vector multiplication thus memory compute intensive <eos> tackle problems propose fried binary embedding fbe decompose projection matrix using adaptive fastfood transform multiplication several structured matrices <eos> result fbe reduce computational complexity dlogd memory cost respectively <eos> more importantly using structured matrices fbe regulate projection matrix against over fitting lead even better accuracy than using unconstrained projection matrix like itq same long code length <eos> experimental comparisons state art method over various visual applications demonstrate both efficiency performance advantages fbe <eos> <eop> tgif qa toward spatio temporal reasoning visual question answering <eos> vision language understanding emerged subject undergoing intense study artificial intelligence <eos> among many tasks line research visual question answering vqa one most successful ones goal learn model understands visual content region level details finds their associations pairs questions answers natural language form <eos> despite rapid progress past few years most existing work vqa focused primarily image <eos> paper focus extending vqa video domain contribute literature three important ways <eos> first propose three new tasks designed specifically video vqa require spatio temporal reasoning video answer questions correctly <eos> next introduce new large scale dataset video vqa named tgif qa extends existing vqa work new tasks <eos> finally propose dual lstm based approach both spatial temporal attention show its effectiveness over conventional vqa techniques through empirical evaluations <eos> <eop> joint registration representation learning unconstrained face identification <eos> recent advances deep learning resulted human level performances popular unconstrained face datasets including labeled faces wild youtube faces <eos> further advance research ijb benchmark was recently introduced more challenges especially form extreme head poses <eos> registration such faces quite demanding often requires laborious procedures like facial landmark localization <eos> paper propose convolutional neural network based data driven approach learns simultaneously register represent faces <eos> validate proposed scheme template based unconstrained face identification <eos> here template contains multiple media form image video frames <eos> unlike existing method synthesize all template media information feature level propose keep template media intact <eos> instead represent gallery templates their trained one vs rest discriminative models then employ bayesian strategy optimally fuses decisions all medias query template <eos> demonstrate efficacy proposed scheme ijb youtube celebrities cox datasets approach achieves significant relative performance boosts <eos> <eop> object aware dense semantic correspondence <eos> work aims build pixel pixel correspondences between image same visual class but different geometries visual similarities <eos> task particularly challenging because their visual content similar only high level structure ii background clutters keep bringing noises <eos> address problems paper proposes object aware method estimate per pixel correspondences semantic low level learning classifier each selected discriminative grid cell guiding localization every pixel under semantic constraint <eos> specifically object aware hierarchical graph ohg model constructed regulate matching consistency one coarse grid cell containing whole object fine grid cells covering smaller semantic elements finally every pixel <eos> guidance layer introduced semantic constraint local structure matching <eos> addition propose learn important high level structure each grid cell objectness driven way alternative handcrafted descriptors defining better visual similarity <eos> proposed method extensively evaluated various challenging benchmarks real world image <eos> result show method significantly outperforms state arts terms semantic flow accuracy <eos> <eop> misty three point algorithm relative pose <eos> there significant interest scene reconstruction underwater image given its utility oceanic research recreational image manipulation <eos> paper propose novel algorithm two view camera motion estimation underwater imagery <eos> method leverages constraints provided attenuation properties water its effects appearance color determine depth difference point respect two observing views underwater cameras <eos> additionally propose algorithm leveraging depth differences three such observed point estimate relative pose cameras <eos> given unknown underwater attenuation coefficients method estimates relative motion up scale <eos> result represented generalized camera <eos> evaluate method both real data simulated data <eos> <eop> weakly supervised affordance detection <eos> localizing functional region object affordances important aspect scene understanding relevant many robotics applications <eos> work introduce pixel wise annotated affordance dataset image containing object instances <eos> since parts object multiple affordances address convo lutional neural network multilabel affordance segmen tation <eos> also propose approach train network very few keypoint annotations <eos> approach achieves higher affordance detection accuracy than other weakly supervised method also rely keypoint annotations image annotations weak supervision <eos> <eop> end end representation learning correlation filter based tracking <eos> correlation filter algorithm trains linear template discriminate between image their translations <eos> well suited object tracking because its formulation fourier domain provides fast solution enabling detector re trained once per frame <eos> previous works use correlation filter however adopted feature were either manually designed trained different task <eos> work first overcome limitation interpreting correlation filter learner closed form solution differentiable layer deep neural network <eos> enables learning deep feature tightly coupled correlation filter <eos> experiments illustrate method important practical benefit allowing lightweight architectures achieve state art performance high framerates <eos> <eop> accurate depth normal maps occlusion aware focal stack symmetry <eos> introduce novel approach jointly estimate consistent depth normal maps light fields two main contributions <eos> first build cost volume focal stack symmetry <eos> however contrast previous approaches introduce partial focal stacks order able robustly deal occlusions <eos> idea already yields significanly better disparity maps <eos> second even recent sublabel accurate method multi label optimization recover only piecewise flat disparity map cost volume normals pointing mostly towards image plane <eos> renders normal maps recovered approaches unsuitable potential subsequent applications <eos> therefore propose regularization novel prior linking depth normals imposing smoothness resulting normal field <eos> then jointly optimize over depth normals achieve estimates both surpass previous work accuracy recent benchmark <eos> <eop> human pose estimation single image via distance matrix regression <eos> paper addresses problem three dimensional human pose estimation single image <eos> follow standard two step pipeline first detecting position body joints then using observations infer three dimensional pose <eos> first step use recent cnn based detector <eos> second step most existing approaches perform regression cartesian joint coordinates <eos> show more precise pose estimates obtained representing both three dimensional human poses using nxn distance matrices formulating problem three dimensional distance matrix regression <eos> learning such regressor leverage simple neural network architectures construction enforce positivity symmetry predicted matrices <eos> approach also advantage naturally handle missing observations allowing hypothesize position non observed joints <eos> quantitative result humaneva human <eos> datasets demonstrate consistent performance gains over state art <eos> qualitative evaluation image wild lsp dataset using regressor learned human <eos> reveals very promising generalization result <eos> <eop> zero shot action recognition error correcting output codes <eos> recently zero shot action recognition zsar emerged explosive growth action categories <eos> paper explore zsar novel perspective adopting error correcting output codes dubbed zsecoc <eos> zsecoc equips conventional ecoc additional capability zsar addressing domain shift problem <eos> particular learn discriminative zsecoc seen categories both category level semantics intrinsic data structures <eos> procedure deals domain shift implicitly transferring well established correlations among seen categories unseen ones <eos> moreover simple semantic transfer strategy developed explicitly transforming learned embeddings seen categories better fit underlying structure unseen categories <eos> consequence zsecoc inherits promising characteristics ecoc well overcomes domain shift making more discriminative zsar <eos> systematically evaluate zsecoc three realistic action benchmarks <eos> olympic sports hmdb ucf <eos> experimental result clearly show superiority zsecoc over state art method <eos> <eop> multiple instance detection network online instance classifier refinement <eos> late weakly supervised object detection great importance object recognition <eos> based deep learning weakly supervised detectors achieved many promising result <eos> however compared fully supervised detection more challenging train deep network based detectors weakly supervised manner <eos> here formulate weakly supervised detection multiple instance learning mil problem instance classifiers object detectors put into network hidden nodes <eos> propose novel online instance classifier refinement algorithm integrate mil instance classifier refinement procedure into single deep network train network end end only image level supervision <eos> without object location information <eos> more precisely instance labels inferred weak supervision propagated their spatially overlapped instances refine instance classifier online <eos> iterative instance classifier refinement procedure implemented using multiple streams deep network each stream supervises its latter stream <eos> weakly supervised object detection experiments carried out challenging pascal voc benchmarks <eos> obtain map voc significantly outperforms previous state art <eos> <eop> reliable crowdsourcing deep locality preserving learning expression recognition wild <eos> past research facial expressions used relatively limited datasets makes unclear whether current method employed real world <eos> paper present novel database raf db contains about facial image thousands individuals <eos> each image individually labeled about times then em algorithm was used filter out unreliable labels <eos> crowdsourcing reveals real world faces often express compound emotions even mixture ones <eos> all know raf db first database contains compound expressions wild <eos> cross database study shows action units basic emotions raf db much more diverse than even deviate lab controlled ones <eos> address problem propose new dlp cnn deep locality preserving cnn method aims enhance discriminative power deep feature preserving locality closeness while maximizing inter class scatters <eos> benchmark experiments class basic expressions class compound expressions well additional experiments sfew ck databases show proposed dlp cnn outperforms state art handcrafted feature deep learning based method expression recognition wild <eos> <eop> deep sketch hashing fast free hand sketch based image retrieval <eos> free hand sketch based image retrieval sbir specific cross view retrieval task queries abstract ambiguous sketches while retrieval database formed natural image <eos> work area mainly focuses extracting representative shared feature sketches natural image <eos> however neither cope well geometric distortion between sketches image nor feasible large scale sbir due heavy continuous valued distance computation <eos> paper speed up sbir introducing novel binary coding method named deep sketch hashing dsh semi heterogeneous deep architecture proposed incorporated into end end binary coding framework <eos> specifically three convolutional neural network utilized encode free hand sketches natural image especially auxiliary sketch tokens adopted bridges mitigate sketch image geometric distortion <eos> learned dsh codes effectively capture cross view similarities well intrinsic semantic correlations between different categories <eos> best knowledge dsh first hashing work specifically designed category level sbir end end deep architecture <eos> proposed dsh comprehensively evaluated two large scale datasets tu berlin extension sketchy experiments consistently show dsh superior sbir accuracies over several state art method while achieving significantly reduced retrieval time memory footprint <eos> <eop> semantic regularisation recurrent image annotation <eos> cnn rnn design pattern increasingly widely applied variety image annotation tasks including multi label classification captioning <eos> existing models use weakly semantic cnn hidden layer its transform image embedding provides interface between cnn rnn <eos> leaves rnn overstretched two jobs predicting visual concepts modelling their correlations generating structured annotation output <eos> importantly makes end end training cnn rnn slow ineffective due difficulty back propagating gradients through rnn train cnn <eos> propose simple modification design pattern makes learning more effective efficient <eos> specifically propose use semantically regularised embedding layer interface between cnn rnn <eos> regularising interface partially completely decouple learning problems allowing each more effectively trained jointly training much more efficient <eos> extensive experiments show state art performance achieved multi label classification well image captioning <eos> <eop> pyramid scene parsing network <eos> scene parsing challenging unrestricted open vocabulary diverse scenes <eos> paper exploit capability global context information different region based context aggregation through pyramid pooling module together proposed pyramid scene parsing network pspnet <eos> global prior representation effective produce good quality result scene parsing task while pspnet provides superior framework pixel level prediction <eos> proposed approach achieves state art performance various datasets <eos> came first imagenet scene parsing challenge pascal voc benchmark cityscapes benchmark <eos> single pspnet yields new record miou accuracy <eos> pascal voc accuracy <eos> <eop> human motion prediction using recurrent neural network <eos> human motion modelling classical problem intersection graphics computer vision applications spanning human computer interaction motion synthesis motion prediction virtual augmented reality <eos> following success deep learning method several computer vision tasks recent work focused using deep recurrent neural network rnns model human motion goal learning time dependent representations perform tasks such short term motion prediction long term human motion synthesis <eos> examine recent work focus evaluation methodologies commonly used literature show surprisingly state art performance achieved simple baseline attempt model motion all <eos> investigate result analyze recent rnn method looking architectures loss functions training procedures used state art approaches <eos> propose three changes standard rnn models typically used human motion result simple scalable rnn architecture obtains state art performance human motion prediction <eos> <eop> clevr diagnostic dataset compositional language elementary visual reasoning <eos> when building artificial intelligence systems reason answer questions about visual data need diagnostic tests analyze progress discover short comings <eos> existing benchmarks visual question answer ing help but strong biases models exploit correctly answer questions without reasoning <eos> they also conflate multiple sources error making hard pinpoint model weaknesses <eos> present diagnostic dataset tests range visual reasoning abilities <eos> contains minimal biases detailed annotations describing kind reasoning each question requires <eos> use dataset analyze variety modern visual reasoning systems providing novel insights into their abilities limitations <eos> <eop> sst single stream temporal action proposals <eos> paper presents new approach temporal detection human actions long untrimmed video sequences <eos> introduce single stream temporal action proposals sst new effective efficient deep architecture generation temporal action proposals <eos> network run continuously single stream over very long input video sequences without need divide input into short overlapping clips temporal windows batch processing <eos> demonstrate empirically model outperforms state art task temporal action proposal generation while achieving some fastest processing speeds literature <eos> finally demonstrate using sst proposals conjunction existing action classifiers result improved state art temporal action detection performance <eos> <eop> kernel pooling convolutional neural network <eos> convolutional neural network cnn bilinear pooling initially their full form later using compact representations yielded impressive performance gains wide range visual tasks including fine grained visual categorization visual question answering face recognition description texture style <eos> key their success lies spatially invariant modeling pairwise nd order feature interactions <eos> work propose general pooling framework captures higher order interactions feature form kernels <eos> demonstrate how approximate kernels such gaussian rbf up given order using compact explicit feature maps parameter free manner <eos> combined cnn composition kernel learned data end end fashion via error back propagation <eos> proposed kernel pooling scheme evaluated terms both kernel approximation error visual recognition accuracy <eos> experimental evaluations demonstrate state art performance commonly used fine grained recognition datasets <eos> <eop> subspace clustering via variance regularized ridge regression <eos> spectral clustering based subspace clustering method emerged recently <eos> when inputs dimensional data most existing clustering method convert such data vectors preprocessing severely damages spatial information data <eos> paper propose novel subspace clustering method data enhanced capability retaining spatial information clustering <eos> seeks two projection matrices simultaneously constructs linear representation projected data such sought projections help construct most expressive representation most variational information <eos> regularize method based covariance matrices directly obtained data much smaller size more computationally amiable <eos> moreover exploit nonlinear structures data nonlinear version proposed constructs adaptive manifold according updated projections <eos> learning processes projections representation manifold thus mutually enhance each other leading powerful data representation <eos> efficient optimization procedures proposed generate non increasing objective value sequence theoretical convergence guarantee <eos> extensive experimental result confirm effectiveness proposed method <eos> <eop> efficient global point cloud alignment using bayesian nonparametric mixtures <eos> point cloud alignment common problem computer vision robotics applications ranging three dimensional object recognition reconstruction <eos> propose novel approach alignment problem utilizes bayesian nonparametrics describe point cloud surface normal densities branch bound bb optimization recover relative transformation <eos> bb uses novel refinable near uniform tessellation rotation space using tetrahedra leading more efficient optimization compared common axis angle tessellation <eos> provide objective function bounds pruning given proposed tessellation prove bb converges optimum cost function along providing its computational complexity <eos> finally empirically demonstrate efficiency proposed approach well its robustness real world conditions such missing data partial overlap <eos> <eop> incremental multiresolution matrix factorization algorithm <eos> multiresolution analysis matrix factorization foundational tools computer vision <eos> work study interface between two distinct topics obtain techniques uncover hierarchical block structure symmetric matrices important aspect success many vision problems <eos> new algorithm incremental multiresolution matrix factorization uncovers such structure one feature time hence scales well large matrices <eos> describe how multiscale analysis goes much farther than direct global factorization data identify <eos> evaluate efficacy resulting factorizations relative leveraging within regression tasks using medical imaging data <eos> also use factorization representations learned popular deep network providing evidence their ability infer semantic relationships even when they explicitly trained so <eos> show algorithm used exploratory tool improve network architecture within numerous other settings vision <eos> <eop> cats color thermal stereo benchmark <eos> stereo matching well researched area using visible band color cameras <eos> thermal image typically lower resolution less texture noisier compared their visible band counterparts more challenging stereo matching algorithms <eos> previous benchmarks stereo matching either focus entirely visible band cameras contain only single thermal camera <eos> present color thermal stereo cats benchmark dataset consisting stereo thermal stereo color cross modality image pairs high accuracy ground truth mm generated lidar <eos> scanned cluttered indoor outdoor scenes featuring challenging environments conditions <eos> cats contains approximately image pedestrians vehicles electronics other thermally interesting object different environmental conditions including nighttime daytime foggy scenes <eos> ground truth was projected each four cameras generate color color thermal thermal cross modality disparity maps <eos> develop semi automatic lidar camera alignment procedure require calibration target <eos> compare state art algorithms baseline dataset show thermal cross modalities there still much room improvement <eos> expect dataset provide researchers more diverse set imaged locations object modalities than previous benchmarks stereo matching <eos> <eop> deep image matting <eos> image matting fundamental computer vision problem many applications <eos> previous algorithms poor performance when image similar foreground background colors complicated textures <eos> main reasons prior method only use low level feature lack high level context <eos> paper propose novel deep learning based algorithm tackle both problems <eos> deep model two parts <eos> first part deep convolutional encoder decoder network takes image corresponding trimap inputs predict alpha matte image <eos> second part small convolutional network refines alpha matte predictions first network more accurate alpha values sharper edges <eos> addition also create large scale image matting dataset including training image testing image <eos> evaluate algorithm image matting benchmark testing set wide variety real image <eos> experimental result clearly demonstrate superiority algorithm over previous method <eos> <eop> surfacing multiview three dimensional drawings via lofting occlusion reasoning <eos> three dimensional reconstruction scenes multiple views made impressive strides recent years chiefly method correlating isolated feature point intensities curvilinear structure <eos> without requiring controlled acquisition limited number object abundant patterns object object curves follow particular models majority method produce unorganized point clouds meshes voxel representations reconstructed scene some exceptions producing three dimensional drawings network curves <eos> robotics urban planning industrial design hard surface modeling however require structured representations make explicit three dimensional curves surfaces their spatial relationships <eos> reconstructing surface representations now constrained three dimensional drawing acting like scaffold hang computed representations leading increased robustness quality reconstruction <eos> paper presents one way completing such three dimensional drawings surface reconstructions exploring occlusion reasoning through lofting algorithms <eos> <eop> one shot metric learning person re identification <eos> re identification people surveillance footage must cope drastic variations color background viewing angle person pose <eos> supervised techniques often most effective but require extensive annotation infeasible large camera network <eos> unlike previous supervised learning approaches require hundreds annotated subjects learn metric using novel one shot learning approach <eos> first learn deep texture representation intensity image convolutional neural network cnn <eos> when training cnn using only intensity image learned embedding color invariant shows high performance even unseen datasets without fine tuning <eos> account differences camera color distributions learn color metric using single pair colorchecker image <eos> proposed one shot learning achieves performance competitive supervised method but uses only single example rather than hundreds required fully supervised case <eos> compared semi supervised unsupervised state art method approach yields significantly higher accuracy <eos> <eop> richer convolutional feature edge detection <eos> paper propose accurate edge detector using richer convolutional feature rcf <eos> since object natural image possess various scales aspect ratios learning rich hierarchical representations very critical edge detection <eos> cnn proved effective task <eos> addition convolutional feature cnn gradually become coarser increase receptive fields <eos> according observations attempt adopt richer convolutional feature such challenging vision task <eos> proposed network fully exploits multiscale multilevel information object perform image image prediction combining all meaningful convolutional feature holistic manner <eos> using vgg network achieve state art performance several available datasets <eos> when evaluating well known bsds benchmark achieve ods measure <eos> while retaining fast speed fps <eos> besides fast version rcf achieves ods measure <eos> <eop> video segmentation via multiple granularity analysis <eos> introduce multiple granularity analysis framework video segmentation coarse fine manner <eos> cast video segmentation spatio temporal superpixel labeling problem <eos> benefited bounding volume provided off shelf object trackers estimate foreground background super pixel labeling using spatiotemporal multiple instance learning algorithm obtain coarse foreground background separation within volume <eos> further refine segmentation mask pixel level using graph cut model <eos> extensive experiments benchmark video datasets demonstrate superior performance proposed video segmentation algorithm <eos> <eop> learning cross modal embeddings cooking recipes food image <eos> paper introduce recipe new large scale structured corpus over cooking recipes food image <eos> largest publicly available collection recipe data recipe affords ability train high capacity models aligned multi modal data <eos> accordingly train neural network find joint embedding recipes image yields impressive result image recipe retrieval task <eos> additionally demonstrate regularization via addition high level semantic classification objective improves performance rival humans enables semantic vector arithmetic <eos> postulate embeddings will provide basis further exploration recipe dataset food cooking general <eos> <eop> locality sensitive deconvolution network gated fusion rgb indoor semantic segmentation <eos> paper focuses indoor semantic segmentation using rgb data <eos> although commonly used deconvolution network deconvnet achieved impressive result task find there still room improvements two aspects <eos> one about boundary segmentation <eos> deconvnet aggregates large context predict label each pixel inherently limiting segmentation precision object boundaries <eos> other about rgb fusion <eos> recent state art method generally fuse rgb depth network equal weight score fusion regardless varying contributions two modalities delineating different categories different scenes <eos> address two problems first propose locality sensitive deconvnet ls deconvnet refine boundary segmentation over each modality <eos> ls deconvnet incorporates locally visual geometric cues raw rgb data into each deconvnet able learn upsample coarse convolutional maps large context whilst recovering sharp object boundaries <eos> towards rgb fusion introduce gated fusion layer effectively combine two ls deconvnets <eos> layer learn adjust contributions rgb depth over each pixel high performance object recognition <eos> experiments large scale sun rgb dataset popular nyu depth dataset show approach achieves new state art result rgb indoor semantic segmentation <eos> <eop> one many network visually pleasing compression artifacts reduction <eos> consider compression artifacts reduction problem compressed image transformed into artifact free image <eos> recent approaches problem typically train one one mapping using per pixel loss between outputs ground truths <eos> point out approaches used produce overly smooth result psnr doesn reflect their real performance <eos> paper propose one many network measures output quality using perceptual loss naturalness loss jpeg loss <eos> also avoid grid like artifacts during deconvolution using shift average strategy <eos> extensive experimental result demonstrate dramatic visual improvement approach over state arts <eos> <eop> recurrent modeling interaction context collective activity recognition <eos> modeling high order interactional context <eos> group interaction lies central collective group activity recognition <eos> however most previous activity recognition method offer flexible scalable scheme handle high order context modeling problem <eos> explicitly address fundamental bottleneck propose recurrent interactional context modeling scheme based lstm network <eos> utilizing information propagation aggregation capability lstm proposed scheme unifies interactional feature modeling process single person dynamics intra group <eos> persons within group inter group <eos> group group interactions <eos> proposed high order context modeling scheme produces more discriminative descriptive interactional feature <eos> very flexible handle varying number input instances <eos> different number persons group different number groups linearly scalable high order context modeling problem <eos> extensive experiments two benchmark collective group activity datasets demonstrate effectiveness proposed method <eos> <eop> hierarchical multimodal metric learning multimodal classification <eos> multimodal classification arises many computer vision tasks such object classification image retrieval <eos> idea utilize multiple sources modalities measuring same instance improve overall performance compared using single source modality <eos> varying characteristics exhibited multiple modalities make necessary simultaneously learn corresponding metrics <eos> paper propose multiple metrics learning algorithm multimodal data <eos> metric each modality product two matrices one matrix modality specific other enforced shared all modalities <eos> learned metrics improve multimodal classification accuracy experimental result four datasets show proposed algorithm outperforms existing learning algorithms based multiple metrics well other approaches tested datasets <eos> object instance recognition accuracy <eos> object category recognition accuracy multi view rgb dataset <eos> scene category recognition accuracy sun rgb dataset <eos> <eop> probabilistic temporal subspace clustering <eos> subspace clustering common modeling paradigm used identify constituent modes variation data locally linear structure <eos> structures common many problems computer vision including modeling time series complex human motion <eos> however classical subspace clustering algorithms learn relationships within set data without considering temporal dependency then use separate clustering step <eos> spectral clustering final segmentation <eos> moreover frequently optimization based algorithms assume all observations complete feature <eos> contrast real world applications some feature often missing result incomplete data substantial performance degeneration approaches <eos> paper propose unified non parametric generative framework temporal subspace clustering segment data drawn sequentially ordered union subspaces deals missing feature principled way <eos> non parametric nature generative model makes possible infer number subspaces their dimension automatically data <eos> experimental result human action datasets demonstrate proposed model consistently outperforms other state art subspace clustering approaches <eos> <eop> detecting visual relationships deep relational network <eos> relationships among object play crucial role image understanding <eos> despite great success deep learning techniques recognizing individual object reasoning about relationships among object remains challenging task <eos> previous method often treat classification problem considering each type relationship <eos> ride each distinct visual phrase <eos> person ride horse category <eos> such approaches faced significant difficulties caused high diversity visual appearance each kind relationships large number distinct visual phrases <eos> propose integrated framework tackle problem <eos> heart framework deep relational network novel formulation designed specifically exploiting statistical dependencies between object their relationships <eos> two large data set proposed method achieves substantial improvement over state art <eos> <eop> discover learn new object documentaries <eos> despite remarkable progress recent years detecting object new context remains challenging task <eos> detectors learned public dataset only work fixed list categories while training scratch usually requires large amount training data detailed annotations <eos> work aims explore novel approach learning object detectors documentary films weakly supervised manner <eos> inspired observation documentaries often provide dedicated exposition certain object categories visual presentations aligned subtitles <eos> believe object detectors learned such rich source information <eos> towards goal develop joint probabilistic framework individual pieces information including video frames subtitles brought together via both visual linguistic links <eos> top formulation further derive weakly supervised learning algorithm object model learning training set mining unified optimization procedure <eos> experimental result real world dataset demonstrate effective approach learning new object detectors <eos> <eop> spatio temporal vector locally max pooled feature action recognition video <eos> introduce spatio temporal vector locally max pooled feature st vlmpf super vector based encoding method specifically designed local deep feature encoding <eos> proposed method addresses important problem video understanding how build video representation incorporates cnn feature over entire video <eos> feature assignment carried out two levels using similarity spatio temporal information <eos> each assignment build specific encoding focused nature deep feature goal capture highest feature responses highest neuron activation network <eos> st vlmpf clearly provides more reliable video representation than some most widely used powerful encoding approaches improved fisher vectors vector locally aggregated descriptors while maintaining low computational complexity <eos> conduct experiments three action recognition datasets hmdb ucf ucf <eos> pipeline obtains state art result <eos> <eop> specular highlight removal facial image <eos> present method removing specular highlight reflections facial image may contain varying illumination colors <eos> accurately achieved through use physical statistical properties human skin faces <eos> employ melanin hemoglobin based model represent diffuse color variations facial skin utilize model constrain highlight removal solution manner effective even partially saturated pixels <eos> removal highlights further facilitated through estimation directionally variant illumination colors over face done while taking advantage statistically based approximation facial geometry <eos> important practical feature proposed method skin color model utilized way require color calibration camera <eos> moreover approach require assumptions commonly needed previous highlight removal techniques such uniform illumination color piecewise constant surface colors <eos> validate technique through comparisons existing method removing specular highlights <eos> <eop> radiometric calibration faces image <eos> present method radiometric calibration cameras single image contains human face <eos> technique takes advantage low rank property exists among certain skin albedo gradients because pigments within skin <eos> property becomes distorted image captured non linear camera response function perform radiometric calibration solving inverse response function best restores low rank property image <eos> although work makes use color properties skin pigments show calibration unaffected color scene illumination sensitivities camera color filters <eos> experiments validate approach variety image containing human faces show faces provide important source calibration data image existing radiometric calibration techniques perform poorly <eos> <eop> help pedestrian detection <eos> aggregating extra feature considered effective approach boost traditional pedestrian detection method <eos> however there still lack studies whether how cnn based pedestrian detectors benefit extra feature <eos> first contribution paper exploring issue aggregating extra feature into cnn based pedestrian detection framework <eos> through extensive experiments evaluate effects different kinds extra feature quantitatively <eos> moreover propose novel network architecture namely hyperlearner jointly learn pedestrian detection well given extra feature <eos> multi task training hyperlearner able utilize information given feature improve detection performance without extra inputs inference <eos> experimental result multiple pedestrian benchmarks validate effectiveness proposed hyperlearner <eos> <eop> stylenet generating attractive visual captions styles <eos> propose novel framework named stylenet address task generating attractive captions image video different styles <eos> end devise novel model component named factored lstm automatically distills style factors monolingual text corpus <eos> then runtime explicitly control style caption generation process so produce attractive visual captions desired style <eos> approach achieves goal leveraging two set data factual image video caption paired data stylized monolingual text data <eos> romantic humorous sentences <eos> show experimentally stylenet outperforms existing approaches generating visual captions different styles measured both automatic human evaluation metrics newly collected flickrstyle image caption dataset contains flickr image corresponding humorous romantic captions <eos> <eop> image super resolution via deep recursive residual network <eos> recently convolutional neural network cnn based models achieved great success single image super resolution sisr <eos> owing strength deep network cnn models learn effective nonlinear mapping low resolution input image high resolution target image cost requiring enormous parameters <eos> paper proposes very deep cnn model up convolutional layer named deep recursive residual network drrn strives deep yet concise network <eos> specifically residual learning adopted both global local manners mitigate difficulty training very deep network recursive learning used control model parameters while increasing depth <eos> extensive benchmark evaluation shows drrn significantly outperforms state art sisr while utilizing far fewer parameters <eos> code available github <eos> com tyshiwo drrn cvpr <eos> <eop> residual attention network image classification <eos> work propose residual attention network convolutional neural network using attention mechanism incorporate state art feed forward network architecture end end training fashion <eos> residual attention network built stacking attention modules generate attention aware feature <eos> attention aware feature different modules change adaptively layer going deeper <eos> inside each attention module bottom up top down feedforward structure used unfold feedforward feedback attention process into single feedforward process <eos> importantly propose attention residual learning train very deep residual attention network easily scaled up hundreds layer <eos> extensive analyses conducted cifar cifar datasets verify effectiveness every module mentioned above <eos> residual attention network achieves state art object recognition performance three benchmark datasets including cifar <eos> single model single crop top error <eos> note method achieves <eos> top accuracy improvement trunk depth forward flops comparing resnet <eos> experiment also demonstrates network robust against noisy labels <eos> <eop> end end concept word detection video captioning retrieval question answering <eos> propose high level concept word detector integrated any video language models <eos> takes video input generates list concept words useful semantic priors language generation models <eos> proposed word detector two important properties <eos> first require any external knowledge sources training <eos> second proposed word detector trainable end end manner jointly any video language models <eos> effectively exploit detected words also develop semantic attention mechanism selectively focuses detected concept words fuse them word encoding decoding language model <eos> order demonstrate proposed approach indeed improves performance multiple video language tasks participate all four tasks lsmdc <eos> approach won three them including fill blank multiple choice test movie retrieval <eos> <eop> semantic autoencoder zero shot learning <eos> existing zero shot learning zsl models typically learn projection function feature space semantic embedding space <eos> however such projection function only concerned predicting training seen class semantic representation <eos> attribute prediction classification <eos> when applied test data context zsl contains different unseen classes without training data zsl model typically suffers project domain shift problem <eos> work present novel solution zsl based learning semantic autoencoder sae <eos> taking encoder decoder paradigm encoder aims project visual feature vector into semantic space existing zsl models <eos> however decoder exerts additional constraint projection code must able reconstruct original visual feature <eos> show additional reconstruction constraint learned projection function seen classes able generalise better new unseen classes <eos> importantly encoder decoder linear symmetric enable develop extremely efficient learning algorithm <eos> extensive experiments six benchmark datasets demonstrate proposed sae outperforms significantly existing zsl models additional benefit lower computational cost <eos> furthermore when sae applied supervised clustering problem also beats state art <eos> <eop> co occurrence filter <eos> co occurrence filter cof boundary preserving filter <eos> based bilateral filter bf but instead using gaussian range values preserve edges relies co occurrence matrix <eos> pixel values co occur frequently image <eos> inside textured region will high weight co occurrence matrix <eos> turn means such pixel pairs will averaged hence smoothed regardless their intensity differences <eos> other hand pixel values rarely co occur <eos> across texture boundaries will low weight co occurrence matrix <eos> result they will averaged boundary between them will preserved <eos> cof therefore extends bf deal boundaries just edges <eos> learns co occurrences directly image <eos> achieve various filtering result directing learn co occurrence matrix part image different image <eos> give definition filter discuss how use color image show several use cases <eos> <eop> all pixels equal difficulty aware semantic segmentation via deep layer cascade <eos> propose novel deep layer cascade lc method improve accuracy speed semantic segmentation <eos> unlike conventional model cascade mc composed multiple independent models lc treats single deep model cascade several sub models <eos> earlier sub models trained handle easy confident region they progressively feed forward harder region next sub model processing <eos> convolutions only calculated region reduce computations <eos> proposed method possesses several advantages <eos> first lc classifies most easy region shallow stage makes deeper stage focuses few hard region <eos> such adaptive difficulty aware learning improves segmentation performance <eos> second lc accelerates both training testing deep network thanks early decisions shallow stage <eos> third comparison mc lc end end trainable framework allowing joint learning all sub models <eos> evaluate method pascal voc cityscapes datasets achieving state art performance fast speed <eos> <eop> deeply supervised salient object detection short connections <eos> recent progress saliency detection substantial benefiting mostly explosive development convolutional neural network cnn <eos> semantic segmentation saliency detection algorithms developed lately mostly based fully convolutional neural network fcns <eos> there still large room improvement over generic fcn models explicitly deal scale space problem <eos> holisitcally nested edge detector hed provides skip layer structure deep supervision edge boundary detection but performance gain hed saliency detection obvious <eos> paper propose new saliency method introducing short connections skip layer structures within hed architecture <eos> framework provides rich multi scale feature maps each layer property critically needed perform segment detection <eos> method produces state art result widely tested salient object detection benchmarks advantages terms efficiency <eos> seconds per image effectiveness simplicity over existing algorithms <eos> <eop> citypersons diverse dataset pedestrian detection <eos> convnets enabled significant progress pedestrian detection recently but there still open questions regard ing suitable architectures training data <eos> revisit cnn design point out key adaptations enabling plain fas terrcnn obtain state art result caltech dataset <eos> achieve further improvement more better data introduce citypersons new set person annotations top cityscapes dataset <eos> di versity citypersons allows first time train one single cnn model generalizes well over mul tiple benchmarks <eos> moreover additional training citypersons obtain top result using fasterrcnn caltech improving especially more difficult cases heavy occlusion small scale providing higher loc alization quality <eos> <eop> generalized rank pooling activity recognition <eos> most popular deep models action recognition split video sequences into short sub sequences consisting few frames frame based feature then pooled recognizing activity <eos> usually pooling step discards temporal order frames could otherwise used better recognition <eos> towards end propose novel pooling method generalized rank pooling grp takes input feature intermediate layer cnn trained tiny sub sequences produces output parameters subspace provides low rank approximation feature ii preserves their temporal order <eos> propose use parameters compact representation video sequence then used classification setup <eos> formulate objective computing subspace riemannian optimization problem grassmann manifold propose efficient conjugate gradient scheme solving <eos> experiments several activity recognition datasets show scheme leads state art performance <eos> <eop> deep cross modal hashing <eos> due its low storage cost fast query speed cross modal hashing cmh widely used similarity search multimedia retrieval applications <eos> however most existing cmh method based hand crafted feature might optimally compatible hash code learning procedure <eos> result existing cmh method hand crafted feature may achieve satisfactory performance <eos> paper propose novel cmh method called deep cross modal hashing dcmh integrating feature learning hash code learning intothe same framework <eos> dcmh end end learning framework deep neural network one each modality perform feature learning scratch <eos> experiments three real datasets image text modalities show dcmh outperform other baselines achieve state art performance cross modal retrieval applications <eos> <eop> revisiting metric learning spd matrix based visual representation <eos> success many visual recognition tasks largely depends good similarity measure distance metric learning plays important role regard <eos> meanwhile symmetric positive definite spd matrix receiving increased attention feature representation multiple computer vision applications <eos> however distance metric learning spd matrices sufficiently researched <eos> few existing works approached learning either xp xk transformation matrix dxd spd matrices <eos> different method paper proposes new member family distance metric learning spd matrices <eos> learns only parameters adjust eigenvalues spd matrices through efficient optimisation scheme <eos> also shown proposed method interpreted learning sample specific transformation matrix instead fixed transformation matrix learned all sample existing works <eos> optimised parameters used massage spd matrices better discrimination while still keeping them original space <eos> perspective proposed method complements rather than competes existing linear transformation based method latter always applied output former perform distance metric learning further <eos> proposed method tested multiple spd based visual representation data set used literature result demonstrate its interesting properties attractive performance <eos> <eop> cnn based patch matching optical flow thresholded hinge embedding loss <eos> learning based approaches yet achieved their full potential optical flow estimation their performance still trails heuristic approaches <eos> paper present cnn based patch matching approach optical flow estimation <eos> important contribution approach novel thresholded loss siamese network <eos> demonstrate loss performs clearly better than existing losses <eos> also allows speed up training factor tests <eos> furthermore present novel way calculating cnn based feature different image scales performs better than existing method <eos> also discuss new ways evaluating robustness trained feature application patch matching optical flow <eos> interesting discovery paper low pass filtering feature maps increase robustness feature created cnn <eos> proved competitive performance approach submitting kitti kitti mpi sintel evaluation portals obtained state art result all three datasets <eos> <eop> multi view stereo benchmark high resolution image multi camera video <eos> motivated limitations existing multi view stereo benchmarks present novel dataset task <eos> towards goal recorded variety indoor outdoor scenes using high precision laser scanner captured both high resolution dslr imagery well synchronized low resolution stereo video varying fields view <eos> align image laser scans propose robust technique minimizes photometric errors conditioned geometry <eos> contrast previous datasets benchmark provides novel challenges covers diverse set viewpoints scene types ranging natural scenes man made indoor outdoor environments <eos> furthermore provide data significantly higher temporal spatial resolution <eos> benchmark first cover important use case hand held mobile devices while also providing high resolution dslr camera image <eos> make datasets online evaluation server available www <eos> <eop> snapshot hyperspectral light field imaging <eos> paper presents first snapshot hyperspectral light field imager practice <eos> specifically design novel hybrid camera system obtain two complementary measurements sample angular spectral dimensions respectively <eos> recover full hyperspectral light field severely undersampled measurements then propose efficient computational reconstruction algorithm exploiting large correlations across angular spectral dimensions through self learned dictionaries <eos> simulation elaborate hyperspectral light field dataset validates effectiveness proposed approach <eos> hardware experimental result demonstrate first time knowledge hyperspectral light field containing angular views spectral bands acquired single shot <eos> <eop> zero shot recognition using dual visual semantic mapping paths <eos> zero shot recognition aims accurately recognize object unseen classes using shared visual semantic mapping between image feature space semantic embedding space <eos> mapping learned training data seen classes expected transfer ability unseen classes <eos> paper tackle problem exploiting intrinsic relationship between semantic space manifold transfer ability visual semantic mapping <eos> formalize their connection cast zero shot recognition joint optimization problem <eos> motivated propose novel framework zero shot recognition contains dual visual semantic mapping paths <eos> analysis shows framework only apply prior semantic knowledge infer underlying semantic manifold image feature space but also generate optimized semantic embedding space enhance transfer ability visual semantic mapping unseen classes <eos> proposed method evaluated zero shot recognition four benchmark datasets achieving outstanding result <eos> <eop> new representation skeleton sequences three dimensional action recognition <eos> paper presents new method three dimensional action recognition skeleton sequences <eos> three dimensional trajectories human skeleton joints <eos> proposed method first transforms each skeleton sequence into three clips each consisting several frames spatial temporal feature learning using deep neural network <eos> each clip generated one channel cylindrical coordinates skeleton sequence <eos> each frame generated clips represents temporal information entire skeleton sequence incorporates one particular spatial relationship between joints <eos> entire clips include multiple frames different spatial relationships provide useful spatial structural information human skeleton <eos> propose use deep convolutional neural network learn long term temporal information skeleton sequence frames generated clips then use multi task learning network mtln jointly process all frames clips parallel incorporate spatial structural information action recognition <eos> experimental result clearly show effectiveness proposed new representation feature learning method three dimensional action recognition <eos> <eop> efficient linear programming dense crfs <eos> fully connected conditional random field crf gaussian pairwise potentials proven popular effective multi class semantic segmentation <eos> while energy dense crf minimized accurately using linear programming lp relaxation state art algorithm too slow useful practice <eos> alleviate deficiency introduce efficient lp minimization algorithm dense crfs <eos> end develop proximal minimization framework dual each proximal problem optimized via block coordinate descent <eos> show each block variables efficiently optimized <eos> specifically one block problem decomposes into significantly smaller subproblems each defined over single pixel <eos> other block problem optimized via conditional gradient descent <eos> two advantages conditional gradient computed time linear number pixels labels optimal step size computed analytically <eos> experiments standard datasets provide compelling evidence approach outperforms all existing baselines including previous lp based approach dense crfs <eos> <eop> learning deep match kernels image set classification <eos> image set classification recently generated great popularity due its widespread applications computer vision <eos> great challenges arise effectively efficiently measuring similarity between image set high inter class ambiguity huge intra class variability <eos> paper propose deep match kernels dmk directly measure similarity between image set match kernel framework <eos> specifically build deep local match kernels between image upon arc cosine kernels faithfully characterize similarity between image mimicking deep neural network introduce anchors aggregate deep local match kernels into global match kernel between image set learned supervised way kernel alignment therefore more discriminative <eos> dmk provides first match kernel framework image set classification removes specific assumptions usually required previous approaches computationally more efficient <eos> conduct extensive experiments four datasets three diverse image set classification tasks <eos> dmk achieves high performance consistently surpasses state art method showing its great effectiveness image set classification <eos> <eop> deep regression architecture two stage re initialization high performance facial landmark detection <eos> regression based facial landmark detection method usually learns series regression functions update landmark positions initial estimation <eos> most existing approaches focus learning effective mapping functions robust image feature improve performance <eos> approach dealing initialization issue however receives relatively fewer attentions <eos> paper present deep regression architecture two stage re initialization explicitly deal initialization problem <eos> global stage given image rough face detection result full face region firstly re initialized supervised spatial transformer network canonical shape state then trained regress coarse landmark estimation <eos> local stage different face parts further separately re initialized their own canonical shape states followed another regression subnetwork get final estimation <eos> proposed deep architecture trained end end obtains promising result using different kinds unstable initialization <eos> also achieves superior performances over many competing algorithms <eos> <eop> product manifold filter non rigid shape correspondence via kernel density estimation product space <eos> many algorithms computation correspondences between deformable shapes rely some variant nearest neighbor matching descriptor space <eos> such example various point wise correspondence recovery algorithms used post processing stage functional correspondence framework <eos> such frequently used techniques implicitly make restrictive assumptions <eos> nearisometry considered shapes practice suffer lack accuracy result poor surjectivity <eos> propose alternative recovery technique capable guaranteeing bijective correspondence producing significantly higher accuracy smoothness <eos> unlike other method approach depend assumption analyzed shapes isometric <eos> derive proposed method statistical framework kernel density estimation demonstrate its performance several challenging deformable three dimensional shape matching datasets <eos> <eop> elastic shape template spatially sparse deforming forces <eos> current elastic sft shape template method based norm minimization <eos> none accurately recover spatial location acting forces since norm based minimization tends find best tradeoff among noisy data fit elastic model <eos> work study shapes deformed spatially sparse set forces <eos> propose two formulations new class sft problems dubbed here sle sft sparse linear elastic sft <eos> first ideal formulation uses norm minimize cardinal non zero components deforming forces <eos> second relaxed formulation uses norm minimize sum absolute values force components <eos> new formulations use solid boundary constraints sbc usually needed rigidly position shape frame deformed image <eos> introduce projective elastic space property pesp jointly encodes reprojection constraint elastic model <eos> prove filling property necessary sufficient relaxed formulation retrieve ground truth three dimensional deformed shape ii recover right spatial domain non zero deforming forces <eos> iii also proves rigidly place deformed shape image frame without using sbc <eos> finally prove when filling pesp resolving relaxed formulation provides same ground truth solution ideal formulation <eos> result simulated real data show substantial improvements recovering deformed shapes well spatial location deforming forces <eos> <eop> general framework curve surface comparison registration oriented varifolds <eos> paper introduces general setting construction data fidelity metrics between oriented non oriented geometric shapes like curves curve set surfaces <eos> metrics based representation shapes distributions their local tangent normal vectors definition reproducing kernels spaces <eos> construction combines one common setting extends previous frameworks currents varifolds provides very large class kernel metrics easily computed without requiring any kind parametrization shapes smooth enough give robustness certain imperfections could result <eos> then give sense synthetic examples versatility potentialities such metrics when used various problems like shape comparison clustering diffeomorphic registration <eos> <eop> branchout regularization online ensemble tracking convolutional neural network <eos> propose extremely simple but effective regularization technique convolutional neural network cnn referred branchout online ensemble tracking <eos> algorithm employs cnn target representation common convolutional layer but multiple branches fully connected layer <eos> better regularization subset branches cnn selected randomly online learning whenever target appearance models need updated <eos> each branch may different number layer maintain variable abstraction levels target appearances <eos> branchout multi level target representation allows learn robust target appearance models diversity handle various challenges visual tracking problem effectively <eos> proposed algorithm evaluated standard tracking benchmarks shows state art performance even without additional pretraining external tracking sequences <eos> <eop> expert gate lifelong learning network experts <eos> paper introduce model lifelong learning based network experts <eos> new tasks experts learned added model sequentially building was learned before <eos> ensure scalability process data previous tasks cannot stored hence available when learning new task <eos> critical issue such context addressed literature so far relates decision expert deploy test time <eos> introduce set gating autoencoders learn representation task hand test time automatically forward test sample relevant expert <eos> also brings memory efficiency only one expert network loaded into memory any given time <eos> further autoencoders inherently capture relatedness one task another based most relevant prior model used training new expert fine tuning learning without forgetting selected <eos> evaluate method image classification video prediction problems <eos> <eop> adascan adaptive scan pooling deep convolutional neural network human action recognition video <eos> propose novel method temporally pooling frames video task human action recognition <eos> method motivated observation there only small number frames together contain sufficient information discriminate action class present video rest <eos> proposed method learns pool such discriminative informative frames while discarding majority non informative frames single temporal scan video <eos> algorithm so continuously predicting discriminative importance each video frame subsequently pooling them deep learning framework <eos> show effectiveness proposed pooling method standard benchmarks consistently improves baseline pooling method both rgb optical flow based convolutional network <eos> further combination complementary video representations show result competitive respect state art result two challenging publicly available benchmark datasets <eos> <eop> learning motion patterns video <eos> problem determining whether object motion irrespective camera motion far being solved <eos> address challenging task learning motion patterns video <eos> core approach fully convolutional network learned entirely synthetic video sequences their ground truth optical flow motion segmentation <eos> encoder decoder style architecture first learns coarse representation optical flow field feature then refines iteratively produce motion labels original high resolution <eos> further improve labeling objectness map conditional random field account errors optical flow also focus moving things rather than stuff <eos> output label each pixel denotes whether undergone independent motion <eos> irrespective camera motion <eos> demonstrate benefits learning framework moving object segmentation task goal segment all object motion <eos> approach outperforms top method recently released davis benchmark dataset comprising real world sequences <eos> also evaluate berkeley motion segmentation database achieving state art result <eos> <eop> provable self representation based outlier detection union subspaces <eos> many computer vision tasks involve processing large amounts data contaminated outliers need detected rejected <eos> while outlier detection method based robust statistics existed decades only recently method based sparse low rank representation developed along guarantees correct outlier detection when inliers lie one more low dimensional subspaces <eos> paper proposes new outlier detection method combines tools sparse representation random walks graph <eos> exploiting property data point expressed sparse linear combinations each other obtain asymmetric affinity matrix among data point use construct weighted directed graph <eos> defining suitable markov chain graph establish connection between inliers outliers essential inessential states markov chain allows detect outliers using random walks <eos> provide theoretical analysis justifies correctness method under geometric connectivity assumptions <eos> experimental result image databases demonstrate its superiority respect state art sparse low rank outlier detection method <eos> <eop> deep structured learning facial action unit intensity estimation <eos> consider task automated estimation facial expression intensity <eos> involves estimation multiple output variables facial action units aus structurally dependent <eos> their structure arises statistically induced co occurrence patterns au intensity levels <eos> modeling structure critical improving estimation performance however performance bounded quality input feature extracted face image <eos> goal paper model structures estimate complex feature representations simultaneously combining conditional random field crf encoded au dependencies deep learning <eos> end propose novel copula cnn deep learning approach modeling multivariate ordinal variables <eos> model accounts ordinal structure output variables their non linear dependencies via copula functions modeled cliques crf <eos> jointly optimized deep cnn feature encoding layer using newly introduced balanced batch iterative training algorithm <eos> demonstrate effectiveness approach task au intensity estimation two benchmark datasets <eos> show joint learning deep feature target output structure result significant performance gains compared existing structured deep models deep models analysis facial expressions <eos> <eop> joint detection identification feature learning person search <eos> existing person re identification benchmarks method mainly focus matching cropped pedestrian image between queries candidates <eos> however different real world scenarios annotations pedestrian bounding boxes unavailable target person needs searched gallery whole scene image <eos> close gap propose new deep learning framework person search <eos> instead breaking down into two separate tasks pedestrian detection person re identification jointly handle both aspects single convolutional neural network <eos> online instance matching oim loss function proposed train network effectively scalable datasets numerous identities <eos> validate approach collect annotate large scale benchmark dataset person search <eos> contains image identities pedestrian bounding boxes <eos> experiments show framework outperforms other separate approaches proposed oim loss function converges much faster better than conventional softmax loss <eos> <eop> learning align semantic segmentation maps geolocalization <eos> present efficient method geolocalization urban environments starting coarse estimate location provided gps using simple untextured <eos> model surrounding buildings <eos> key contribution novel efficient robust method optimize pose train deep network predict best direction improve pose estimate given semantic segmentation input image rendering buildings estimate <eos> then iteratively apply cnn until converging good pose <eos> approach avoids use reference image surroundings difficult acquire match while <eos> models broadly available <eos> therefore apply places unseen during training <eos> <eop> lcr net localization classification regression human pose <eos> propose end end architecture joint three dimensional human pose estimation natural image <eos> key approach generation scoring number pose proposals per image allows predict three dimensional pose multiple people simultaneously <eos> hence approach require approximate localization humans initialization <eos> architecture named lcr net contains main components pose proposal generator suggests potential poses different locations image classifier scores different pose proposals regressor refines pose proposals both three dimensional all three stages share convolutional feature layer trained jointly <eos> final pose estimation obtained integrating over neighboring pose hypotheses shown improve over standard non maximum suppression algorithm <eos> approach significantly outperforms state art three dimensional pose estimation human <eos> moreover shows promising result real image both single multi person subsets mpii pose benchmark <eos> <eop> primary object segmentation video based region augmentation reduction <eos> novel algorithm segment primary object video sequence proposed work <eos> first generate candidate region primary object using both color motion edges <eos> second estimate initial primary object region exploiting recurrence property primary object <eos> third augment initial region missing parts reducing them excluding noisy parts repeatedly <eos> augmentation reduction process arp identifies primary object region each frame <eos> experimental result demonstrate proposed algorithm significantly outperforms state art conventional algorithms recent benchmark datasets <eos> <eop> deep pilot learning deep agent piloting through deg sports video <eos> watching sports video requires viewer continuously select viewing angle either through sequence mouse clicks head movements <eos> relieve viewer piloting task propose deep pilot deep learning based agent piloting through sports video automatically <eos> each frame agent observes panoramic image knowledge previously selected viewing angles <eos> task agent shift current viewing angle <eos> action next preferred one <eos> propose directly learn online policy agent data <eos> specifically leverage state art object detector propose few candidate object interest yellow boxes fig <eos> then recurrent neural network used select main object green dash boxes fig <eos> given main object previously selected viewing angles method regresses shift viewing angle move next one <eos> use policy gradient technique jointly train pipeline minimizing regression loss measuring distance between selected ground truth viewing angles smoothness loss encouraging smooth transition viewing angle maximizing expected reward focusing foreground object <eos> evaluate method built new sports video dataset consisting five sports domains <eos> trained domain specific agents achieved best performance viewing angle selection accuracy users preference compared other baselines <eos> <eop> learning refining privileged information based rnns action recognition depth sequences <eos> existing rnn based approaches action recognition depth sequences require either skeleton joints hand crafted depth feature inputs <eos> end end manner mapping raw depth maps action classes non trivial design due fact single channel map lacks texture thus weakens discriminative power relatively small set depth training data <eos> address challenges propose learn rnn driven privileged information pi three steps encoder pre trained learn joint embedding depth appearance pi <eos> learned embedding layer then tuned learning step aiming optimize network exploiting pi form multi task loss <eos> however exploiting pi secondary task provides little help improve performance primary task <eos> classification due gap between them <eos> finally bridging matrix defined connect two tasks discovering latent pi refining step <eos> pi based classification loss maintains consistency between latent pi predicted distribution <eos> latent pi network iteratively estimated updated expectation maximization procedure <eos> proposed learning process provides greater discriminative power model subtle depth difference while helping avoid overfitting scarcer training data <eos> experiments show significant performance gains over state art method three public benchmark datasets newly collected blanket dataset <eos> <eop> simultaneous facial landmark detection pose deformation estimation under facial occlusion <eos> facial landmark detection head pose estimation facial deformation analysis typical facial behavior analysis tasks computer vision <eos> existing method usually perform each task independently sequentially ignoring their interactions <eos> tackle problem propose unified framework simultaneous facial landmark detection head pose estimation facial deformation analysis proposed model robust facial occlusion <eos> following cascade procedure augmented model based head pose estimation iteratively update facial landmark locations facial occlusion head pose facial deformation until convergence <eos> experimental result benchmark databases demonstrate effectiveness proposed method simultaneous facial landmark detection head pose facial deformation estimation even if image under facial occlusion <eos> <eop> domain based approach social relation recognition <eos> social relations foundation human daily life <eos> developing techniques analyze such relations visual data bears great potential build machines better understand capable interacting social level <eos> previous investigations remained partial due overwhelming diversity complexity topic consequently only focused handful social relations <eos> paper argue domain based theory social psychology great starting point systematically approach problem <eos> theory provides coverage all aspects social relations equally concrete predictive about visual attributes behaviors defining relations included each domain <eos> provide first dataset built holistic conceptualization social life composed hierarchical label space social domains social relations <eos> also contribute first models recognize such domains relations find superior performance attribute based feature <eos> beyond encouraging performance attribute based approach also find interpretable feature accordance predictions social psychology literature <eos> beyond findings believe contributions more tightly interleave visual recognition social psychology theory potential complement theoretical work area empirical data driven models social life <eos> <eop> fractal dimension invariant filtering its cnn based implementation <eos> fractal analysis widely used computer vision especially texture image processing texture analysis <eos> key concept fractal based image model fractal dimension invariant bi lipschitz transformation image thus capable representing intrinsic structural information image robustly <eos> however invariance fractal dimension generally hold after filtering limits application fractal based image model <eos> paper propose novel fractal dimension invariant filtering fdif method extending invariance fractal dimension filtering operations <eos> utilizing notion local self similarity first develop local fractal model image <eos> adding nonlinear post processing step behind anisotropic filter banks demonstrate proposed filtering method capable preserving local invariance fractal dimension image <eos> meanwhile show fdif method re instantiated approximately via cnn based architecture convolution layer extracts anisotropic structure image nonlinear layer enhances structure via preserving local fractal dimension image <eos> proposed filtering method provides novel geometric interpretation cnn based image model <eos> focusing challenging image processing task detecting complicated curves texture like image proposed method obtains superior result state art approaches <eos> <eop> transformation grounded image generation network novel three dimensional view synthesis <eos> present transformation grounded image generation network novel three dimensional view synthesis single image <eos> approach first explicitly infers parts geometry visible both input novel views then casts remaining synthesis problem image completion <eos> specifically both predict flow move pixels input novel view along novel visibility map helps deal occulsion disocculsion <eos> next conditioned intermediate result hallucinate infer parts object invisible input image <eos> addition new network structure training combination adversarial perceptual loss result reduction common artifacts novel view synthesis such distortions holes while successfully generating high frequency details preserving visual aspects input image <eos> evaluate approach wide range synthetic real examples <eos> both qualitative quantitative result show method achieves significantly better result compared existing method <eos> <eop> noise blind image deblurring <eos> present novel approach noise blind deblurring problem deblurring image known blur but unknown noise level <eos> introduce efficient robust solution based bayesian framework using smooth generalization loss <eos> novel bound allows calculation very high dimensional integrals closed form <eos> avoids degeneracy maximum posteriori map estimates leads effective noise adaptive scheme <eos> moreover drastically accelerate algorithm using majorization minimization mm without introducing any approximation boundary artifacts <eos> further speed up convergence turning algorithm into neural network termed gradnet highly parallelizable efficiently trained <eos> demonstrate noise blind formulation integrated different priors significantly improves existing deblurring algorithms noise blind known noise case <eos> furthermore gradnet leads state art performance across different noise levels while retaining high computational efficiency <eos> <eop> multi scale fcn cascaded instance aware segmentation arbitrary oriented word spotting wild <eos> scene text detection attracted great attention years <eos> text potentially exist wide variety image video play important role understanding scene <eos> paper present novel text detection algorithm composed two cascaded steps multi scale fully convolutional neural network fcn proposed extract text block region novel instance word line aware segmentation designed further remove false positives obtain word instances <eos> proposed algorithm accurately localize word text line arbitrary orientations including curved text lines cannot handled lot other frameworks <eos> algorithm achieved state art performance icdar ic icdar ic cute street view text svt benchmark datasets <eos> <eop> combining bottom up top down smoothness cues weakly supervised image segmentation <eos> paper addresses problem weakly supervised semantic image segmentation <eos> goal label every pixel new image given only image level object labels associated training image <eos> problem statement differs common semantic segmentation pixel wise annotations typically assumed available training <eos> specify novel deep architecture fuses three distinct computation processes toward semantic segmentation namely bottom up computation neural activations cnn image level prediction object classes ii top down estimation conditional likelihoods cnn activations given predicted object resulting probabilistic attention maps per object class iii lateral attention message passing neighboring neurons same cnn layer <eos> fusion iii realized via conditional random field recurrent network aimed generating smooth boundary preserving segmentation <eos> unlike existing work formulate unified end end learning all components deep architecture <eos> evaluation benchmark pascal voc dataset demonstrates outperform reasonable weakly supervised baselines state art approaches <eos> <eop> multiple people tracking lifted multicut person re identification <eos> tracking multiple persons monocular video crowded scene challenging task <eos> humans master even if they loose track person locally re identifying same person based their appearance <eos> care must taken across long distances similar looking persons need identical <eos> work propose novel graph based formulation links clusters person hypotheses over time solving instance minimum cost lifted multicut problem <eos> model generalizes previous works introducing mechanism adding long range attractive connections between nodes graph without modifying original set feasible solutions <eos> allows reward tracks assign detections similar appearance same person way introduce implausible solutions <eos> effectively match hypotheses over longer temporal gaps develop new deep architectures re identification people <eos> they combine holistic representations extracted deep network body pose layout obtained state art pose estimation model <eos> demonstrate effectiveness formulation reporting new state art mot benchmark <eos> <eop> filter flow made practical massively parallel lock free <eos> paper inspired relatively recent work seitz baker introduced so called filter flow model <eos> filter flow finds transformation relating pair multiple image identifying large set local linear filters imposing additional constraints certain structural properties filters enables filter flow serve general one stop construction spectrum problems vision optical flow defocus stereo affine alignment <eos> idea beautiful yet benefits borne out practice because significant computational challenges <eos> issue makes most if all deployments practical vision problems out reach <eos> key thrust work identify mathematically near equivalent reformulations model eliminate serious limitation <eos> demonstrate via detailed optimization focused development filter flow indeed solved fairly efficiently wide range instantiations <eos> derive efficient algorithms perform extensive theoretical analysis focused convergence parallelization show how result competitive state art many applications achieved negligible application specific adjustments post processing <eos> actual numerical scheme easy understand implement lines matlab development will enable filter flow viable general solver testbed numerous applications community going forward <eos> <eop> acquiring axially symmetric transparent object using single view transmission imaging <eos> propose novel practical solution high quality reconstruction axially symmetric transparent object <eos> while special case such transparent object ubiquitous real world <eos> common examples glasses goblets tumblers carafes etc <eos> very unique visually appealing forms making their reconstruction interesting vision graphics applications <eos> acquisition setup involves imaging such object single viewpoint while illuminating them directly behind few patterns emitted lcd panel <eos> reconstruction step then based optimization object geometry its refractive index minimize difference between observed simulated transmission refraction rays passing through object <eos> exploit object axial symmetry strong shape prior allows achieve robust reconstruction single viewpoint using simple commodity acquisition setup <eos> demonstrate high quality reconstruction several common rotationally symmetric well more complex fold symmetric transparent object approach <eos> <eop> online graph completion multivariate signal recovery computer vision <eos> adoption human loop paradigms computer vision machine learning leading various applications actual data acquisition <eos> human supervision underlying inference algorithms closely interwined <eos> while classical work active learning provides effective solutions when learning module involves classification regression tasks many practical issues such partially observed measurements financial constraints even additional distributional structural aspects data typically fall outside scope treatment <eos> instance sequential acquisition partial measurements data manifest matrix tensor novel strategies completion collaborative filtering remaining entries only studied recently <eos> motivated vision problems seek annotate large dataset image via crowdsourced platform alternatively complement result state art object detector using human feedback study completion problem defined graphs requests additional measurements must made sequentially <eos> design optimization model fourier domain graph describing how ideas based adaptive submodularity provide algorithms work well practice <eos> large set image collected imgur see promising result image otherwise difficult categorize <eos> also show applications experimental design problem neuroimaging <eos> <eop> octnet learning deep three dimensional representations high resolutions <eos> present octnet representation deep learning sparse three dimensional data <eos> contrast existing models representation enables three dimensional convolutional network both deep high resolution <eos> towards goal exploit sparsity input data hierarchically partition space using set unbalanced octrees each leaf node stores pooled feature representation <eos> allows focus memory allocation computation relevant dense region enables deeper network without compromising resolution <eos> demonstrate utility octnet representation analyzing impact resolution several three dimensional tasks including three dimensional object classification orientation estimation point cloud labeling <eos> <eop> non local color image denoising convolutional neural network <eos> propose novel deep network architecture grayscale color image denoising based non local image model <eos> motivation overall design proposed network stems variational method exploit inherent non local self similarity property natural image <eos> build concept introduce deep network perform non local processing same time they significantly benefit discriminative learning <eos> experiments berkeley segmentation dataset comparing several state art method show proposed non local models achieve best reported denoising performance both grayscale color image all tested noise levels <eos> also worth noting increase performance comes no extra cost capacity network compared existing alternative deep network architectures <eos> addition highlight direct link proposed non local models convolutional neural network <eos> connection significant importance since allows models take full advantage latest advances gpu computing deep learning makes them amenable efficient implementations through their inherent parallelism <eos> <eop> slow flow exploiting high speed cameras accurate diverse optical flow reference data <eos> existing optical flow datasets limited size variability due difficulty capturing dense ground truth <eos> paper tackle problem tracking pixels through densely sampled space time volumes recorded high speed video camera <eos> model exploits linearity small motions reasons about occlusions multiple frames <eos> using technique able establish accurate reference flow fields outside laboratory natural environments <eos> besides show how predictions used augment input image realistic motion blur <eos> demonstrate quality produced flow fields synthetic real world datasets <eos> finally collect novel challenging optical flow dataset applying technique data high speed camera analyze performance state art optical flow under various levels motion blur <eos> <eop> cross view image matching geo localization urban environments <eos> paper address problem cross view image geo localization <eos> specifically aim estimate gps location query street view image finding matching image reference database geo tagged bird eye view image vice versa <eos> end present new framework cross view image geo localization taking advantage tremendous success deep convolutional neural network cnn image classification object detection <eos> first employ faster cnn detect buildings query reference image <eos> next each building query image retrieve nearest neighbors reference buildings using siamese network trained both positive matching image pairs negative pairs <eos> find correct nn each query building develop efficient multiple nearest neighbors matching method based dominant set <eos> evaluate proposed framework new dataset consists pairs street view bird eye view image <eos> experimental result show proposed method achieves better geo localization accuracy than other approaches able generalize image unseen locations <eos> <eop> improving pairwise ranking multi label image classification <eos> learning rank recently emerged attractive technique train deep convolutional neural network various computer vision tasks <eos> pairwise ranking particular successful multi label image classification achieving state art result various benchmarks <eos> however most existing approaches use hinge loss train their models non smooth thus difficult optimize especially deep network <eos> furthermore they employ simple heuristics such top thresholding determine labels include output ranked list labels limits their use real world setting <eos> work propose two techniques improve pairwise ranking based multi label image classification solving aforementioned problems propose novel loss function pairwise ranking smooth everywhere incorporate label decision module into model estimating optimal confidence thresholds each visual concept <eos> provide theoretical analyses loss function point view bayes consistency risk minimization show its benefit over existing pairwise ranking formulations <eos> also demonstrate effectiveness approach two large scale datasets nus wide ms coco achieving best reported result literature <eos> <eop> webly supervised semantic segmentation <eos> propose weakly supervised semantic segmentation algorithm uses image tags supervision <eos> apply tags queries collect three set web image encode clean foregrounds common back grounds realistic scenes classes <eos> introduce novel three stage training pipeline progressively learn semantic segmentation models <eos> first train refine class specific shallow neural network obtain segmentation masks each class <eos> shallow neural network all classes then assembled into one deep convolutional neural network end end training testing <eos> experiments show method notably outperforms previous state art weakly supervised semantic segmentation approaches pascal voc segmentation bench mark <eos> further apply class specific shallow neural network object segmentation obtain excellent result <eos> <eop> self supervised video representation learning odd one out network <eos> propose new self supervised cnn pre training technique based novel auxiliary task called odd one out learning <eos> task machine asked identify unrelated odd element set otherwise related elements <eos> apply technique self supervised video representation learning sample subsequences video ask network learn predict odd video subsequence <eos> odd video subsequence sampled such wrong temporal order frames while even ones correct temporal order <eos> therefore generate odd one out question no manual annotation required <eos> learning machine implemented multi stream convolutional neural network learned end end <eos> using odd one out network learn temporal representations video generalizes other related tasks such action recognition <eos> action classification method obtains <eos> ucf dataset using only ucf data training approximately better than current state art self supervised learning method <eos> similarly hmdb dataset outperform self supervised state art method <eos> action classification task <eos> <eop> fast video classification via adaptive cascading deep models <eos> recent advances enabled oracle classifiers classify across many classes input distributions high accuracy without retraining <eos> however classifiers relatively heavyweight so applying them classify video costly <eos> show day day video exhibits highly skewed class distributions over short term distributions classified much simpler models <eos> formulate problem detecting short term skews online exploiting models based new sequential decision making problem dubbed online bandit problem present new algorithm solve <eos> when applied recognizing faces tv shows movies realize end end classification speedups <eos> gpu cpu relative state art convolutional neural network competitive accuracy <eos> <eop> non contact full field vibration measurement based phase shifting <eos> vibration measurement systems widely used industry <eos> variety vibration measurement techniques proposed including method using acceleration sensor laser displacement meter tracking marker using camera <eos> however method limitations allow only one point measured require markers <eos> present novel non contact full field joint measurement technique both vibrations shape based phase shifting <eos> key idea acquire frequency vibrating object using fft analyze phase shift error vibrating object <eos> proposed algorithm estimates phase shift error iterating frame frame optimization pixel pixel optimization <eos> feature approach measure surface vibrating different frequencies without markers texture full fields <eos> developed system low cost system composed digital light processing dlp projector camera frames per second <eos> result experiments show low frequency vibration object measured high accuracy non contact <eos> also reconstruction vibrating object surface performed high accuracy <eos> <eop> fusionseg learning combine motion appearance fully automatic segmentation generic object video <eos> propose end end learning framework segmenting generic object video <eos> method learns combine appearance motion information produce pixel level segmentation masks all prominent object video <eos> formulate task structured prediction problem design two stream fully convolutional neural network fuses together motion appearance unified framework <eos> since large scale video datasets pixel level segmentations problematic show how bootstrap weakly annotated video together existing image recognition datasets training <eos> through experiments three challenging video segmentation benchmarks method substantially improves state art segmenting generic unseen object <eos> code pre trained models available project website <eos> <eop> variational autoencoded regression high dimensional regression visual data complex manifold <eos> paper proposes new high dimensional regression method merging gaussian process regression into variational autoencoder framework <eos> contrast other regression method proposed method focuses case output responses complex high dimensional manifold such image <eos> contributions summarized follows new regression method estimating high dimensional image responses handled existing regression algorithms proposed <eos> ii proposed regression method introduces strategy learn latent space well encoder decoder so result regressed response latent space coincide corresponding response data space <eos> iii proposed regression embedded into generative model whole procedure developed variational autoencoder framework <eos> demonstrate robustness effectiveness method through number experiments various visual data regression problems <eos> <eop> temporal action localization structured maximal sums <eos> address problem temporal action localization video <eos> pose action localization structured prediction over arbitrary length temporal windows each window scored sum frame wise classification scores <eos> additionally model classifies start middle end each action separate components allowing system explicitly model each action temporal evolution take advantage informative temporal dependencies present structure <eos> framework localize actions searching structured maximal sum problem develop novel provably efficient algorithmic solution <eos> frame wise classification scores computed using feature deep convolutional neural network cnn trained end end directly optimize novel structured objective <eos> evaluate system thumos action detection benchmark achieve competitive performance <eos> <eop> dynamic edge conditioned filters convolutional neural network graphs <eos> number problems formulated prediction graph structured data <eos> work generalize convolution operator regular grids arbitrary graphs while avoiding spectral domain allows handle graphs varying size connectivity <eos> move beyond simple diffusion filter weights conditioned specific edge labels neighborhood vertex <eos> together proper choice graph coarsening explore constructing deep neural network graph classification <eos> particular demonstrate generality formulation point cloud classification set new state art graph classification dataset outperform other deep learning approaches <eos> <eop> synthesizing normalized faces facial identity feature <eos> present method synthesizing frontal neutral expression image person face given input face photograph <eos> achieved learning generate facial landmarks textures feature extracted facial recognition network <eos> unlike previous generative approaches encoding feature vector largely invariant lighting pose facial expression <eos> exploiting invariance train decoder network using only frontal neutral expression photographs <eos> since photographs well aligned decompose them into sparse set landmark point aligned texture maps <eos> decoder then predicts landmarks textures independently combines them using differentiable image warping operation <eos> resulting image used number applications such analyzing facial attributes exposure white balance adjustment creating avatar <eos> <eop> task driven dynamic fusion reducing ambiguity video description <eos> integrating complementary feature multiple channels expected solve description ambiguity problem video captioning whereas inappropriate fusion strategies often harm rather than help performance <eos> existing static fusion method video captioning such concatenation summation cannot attend appropriate feature channels thus fail adaptively support recognition various kinds visual entities such actions object <eos> paper contributes first depth study weakness inherent data driven static fusion method video captioning <eos> establishment task driven dynamic fusion tddf method <eos> adaptively choose different fusion patterns according model status <eos> improvement video captioning <eos> extensive experiments conducted two well known benchmarks demonstrate dynamic fusion method outperforms state art result msvd meteor scores <eos> achieves superior meteor scores <eos> compared single feature relative improvement derived fusion method <eos> respectively two datasets <eos> <eop> unsupervised pixel level domain adaptation generative adversarial network <eos> collecting well annotated image datasets train modern machine learning algorithms prohibitively expensive many tasks <eos> one appealing alternative rendering synthetic data ground truth annotations generated automatically <eos> unfortunately models trained purely rendered image fail generalize real image <eos> address shortcoming prior work introduced unsupervised domain adaptation algorithms tried either map representations between two domains learn extract feature domain invariant <eos> work approach problem new light learning unsupervised manner transformation pixel space one domain other <eos> generative adversarial network gan based method adapts source domain image appear if drawn target domain <eos> approach only produces plausible sample but also outperforms state art number unsupervised domain adaptation scenarios large margins <eos> finally demonstrate adaptation process generalizes object classes unseen during training <eos> <eop> simultaneous visual data completion denoising based tensor rank total variation minimization its primal dual splitting algorithm <eos> tensor completion attracted attention because its promising ability generality <eos> however there few studies noisy scenarios directly solve optimization problem consisting noise inequality constraint <eos> paper propose new tensor completion denoising model including tensor total variation tensor nuclear norm minimization range values noise inequalities <eos> furthermore developed its solution algorithm based primal dual splitting method computationally efficient compared tensor decomposition based non convex optimization <eos> lastly extensive experiments demonstrated advantages proposed method visual data retrieval such color image movies three dimensional volumetric data <eos> <eop> point set similarity based deep feature learning person re identification <eos> person re identification re id remains challenging problem due significant appearance changes caused variations view angle background clutter illumination condition mutual occlusion <eos> address issues conventional method usually focus proposing robust feature representation learning metric transformation based pairwise similarity using fisher type criterion <eos> recent development deep learning based approaches address two processes joint fashion achieved promising progress <eos> one key issues deep learning based person re id selection proper similarity comparison criteria performance learned feature using existing criterion based pairwise similarity still limited because only distances mostly considered <eos> paper present novel person re id method based similarity comparison <eos> metric jointly minimize intra class distance maximize inter class distance while back propagating gradient optimize parameters deep model <eos> utilizing proposed metric learned deep model effectively distinguish different persons learning discriminative stable feature representations <eos> comprehensive experimental evaluations dpes cuhk prid market datasets demonstrate advantages method over state art approaches <eos> <eop> gated feedback refinement network dense image labeling <eos> effective integration local global contextual information crucial dense labeling problems <eos> most existing method based encoder decoder architecture simply concatenate feature earlier layer obtain higher frequency details refinement stages <eos> however there limits quality refinement possible if ambiguous information passed forward <eos> paper propose gated feedback refinement network frnet end end deep learning framework dense labeling tasks addresses limitation existing method <eos> initially frnet makes coarse prediction then progressively refines details efficiently integrating local global contextual information during refinement stages <eos> introduce gate units control information passed forward order filter out ambiguity <eos> experiments three challenging dense labeling datasets camvid pascal voc horse cow parsing show effectiveness method <eos> proposed approach achieves state art result camvid horse cow parsing datasets produces competitive result pascal voc dataset <eos> <eop> hallucinating very low resolution unaligned noisy face image transformative discriminative autoencoders <eos> most conventional face hallucination method assume input image sufficiently large aligned all require input image noise free <eos> their performance degrades drastically if input image tiny unaligned contaminated noise <eos> paper introduce novel transformative discriminative autoencoder super resolve unaligned noisy tiny low resolution face image <eos> contrast encoder decoder based autoencoders method uses decoder encoder decoder network <eos> first employ transformative discriminative decoder network upsample denoise simultaneously <eos> then use transformative encoder network project intermediate hr faces aligned noise free lr faces <eos> finally use second decoder generate hallucinated hr image <eos> extensive evaluations very large face dataset show method achieves superior hallucination result outperforms state art large margin <eos> <eop> learning dynamic guidance depth image enhancement <eos> depth image acquired consumer depth sensors <eos> kinect tof usually low resolution insufficient quality <eos> one natural solution incorporate high resolution rgb camera exploiting their statistical correlation <eos> however most existing method intuitive limited characterizing complex dynamic dependency between intensity depth image <eos> address limitations propose weighted analysis representation model guided depth image enhancement advances conventional method two aspects task driven learning ii dynamic guidance <eos> first generalize analysis representation model including guided weight function dependency modeling <eos> task driven learning formulation introduced obtain optimized guidance tailored specific enhancement task <eos> second depth image gradually enhanced along iterations thus guidance should also dynamically adjusted account updating depth image <eos> end stage wise parameters learned dynamic guidance <eos> experiments guided depth image upsampling noisy depth image restoration validate effectiveness method <eos> <eop> shape segmentation projective convolutional network <eos> paper introduces deep architecture segmenting three dimensional object into their labeled semantic parts <eos> architecture combines image based fully convolutional network fcns surface based conditional random fields crfs yield coherent segmentations three dimensional shapes <eos> image based fcns used efficient view based reasoning about three dimensional object parts <eos> through special projection layer fcn outputs effectively aggregated across multiple views scales then projected onto three dimensional object surfaces <eos> finally surface based crf combines projected outputs geometric consistency cues yield coherent segmentations <eos> whole architecture multi view fcns crf trained end end <eos> approach significantly outperforms existing state art method currently largest segmentation benchmark shapenet <eos> finally demonstrate promising segmentation result noisy three dimensional shapes acquired consumer grade depth cameras <eos> <eop> deep image harmonization <eos> compositing one most common operations photo editing <eos> generate realistic composites appearances foreground background need adjusted make them compatible <eos> previous approaches harmonize composites focused learning statistical relationships between hand crafted appearance feature foreground background unreliable especially when contents two layer vastly different <eos> work propose end end deep convolutional neural network image harmonization capture both context semantic information composite image during harmonization <eos> also introduce efficient way collect large scale high quality training data facilitate training process <eos> experiments synthesized dataset real composite image show proposed network outperforms previous state art method <eos> <eop> matrix tri factorization manifold regularizations zero shot learning <eos> zero shot learning zsl aims recognize object unseen classes available training data another set seen classes <eos> existing solutions focused exploring knowledge transfer via intermediate semantic embedding <eos> attributes shared between seen unseen classes <eos> paper propose novel projection framework based matrix tri factorization manifold regularizations <eos> specifically learn semantic embedding projection decomposing visual feature matrix under guidance semantic embedding class label matrices <eos> additionally introducing manifold regularizations visual data semantic embeddings learned projection effectively captures geometrical manifold structure residing both visual semantic spaces <eos> avoid projection domain shift problem devise effective prediction scheme exploiting test time manifold structure <eos> extensive experiments four benchmark datasets show approach significantly outperforms state arts yielding average improvement ratio <eos> recognition retrieval task respectively <eos> <eop> spatio temporal alignment non overlapping sequences independently panning cameras <eos> paper addresses problem spatio temporal alignment multiple video sequences <eos> identify tackle novel scenario problem referred nonoverlapping sequences nos <eos> nos captured multiple freely panning handheld cameras whose field views fov might no direct spatial overlap <eos> popularity mobile sensors nos rise when multiple cooperative users capture public event create panoramic video when consolidating multiple footages incident into single video <eos> tackle novel scenario first spatially align sequences reconstructing background each sequence registering backgrounds even if backgrounds overlapping <eos> given spatial alignment temporally synchronize sequences such trajectories moving object <eos> cars pedestrians consistent across sequences <eos> experimental result demonstrate performance algorithm novel challenging scenario quantitatively qualitatively <eos> <eop> learning fully convolutional network iterative non blind deconvolution <eos> paper propose fully convolutional network iterative non blind deconvolution <eos> decompose non blind deconvolution problem into image denoising image deconvolution <eos> train fcnn remove noise gradient domain use learned gradients guide image deconvolution step <eos> contrast existing deep neural network based method iteratively deconvolve blurred image multi stage framework <eos> proposed method able learn adaptive image prior keeps both local details global structures information <eos> both quantitative qualitative evaluations benchmark datasets demonstrate proposed method performs favorably against state art algorithms terms quality speed <eos> <eop> seeing into darkness scotopic visual recognition <eos> image formed counting how many photons traveling given set directions hit image sensor during given time interval <eos> when photons few far between concept image breaks down best consider directly flow photons <eos> computer vision regime call scotopic radically different classical image based paradigm visual computations classification control search take place while stream photons captured decisions may taken soon enough information available <eos> scotopic regime important biomedical imaging security astronomy many other fields <eos> here develop framework allows machine classify object few photons possible while maintaining error rate below acceptable threshold <eos> dynamic asymptotically optimal speed accuracy tradeoff key feature framework <eos> propose study algorithm optimize tradeoff convolutional network directly lowlight image evaluate simulated image standard datasets <eos> surprisingly scotopic systems achieve comparable classification performance traditional vision systems while using less than <eos> photons conventional image <eos> addition demonstrate algorithms work even when illuminance environment unknown varying <eos> last outline spiking neural network coupled photon counting sensors power efficient hardware realization scotopic algorithms <eos> <eop> distinguishing indistinguishable exploring structural ambiguities via geodesic context <eos> perennial problem structure motion sfm visual ambiguity posed repetitive structures <eos> recent disambiguating algorithms infer ambiguities mainly via explicit background context thus face limitations highly ambiguous scenes visually indistinguishable <eos> instead analyzing local visual information propose novel algorithm sfm disambiguation explores global topology encoded photo collections <eos> important adaptation work approximate available imagery using manifold viewpoints <eos> note while ambiguous image appear deceptively similar appearance they actually located far apart geodesics <eos> establish manifold adaptively identifying cameras adjacent viewpoint detect ambiguities via new measure geodesic consistency <eos> demonstrate accuracy efficiency proposed approach range complex ambiguity datasets even including challenging scenes without background conflicts <eos> <eop> learning invariant hilbert space domain adaptation <eos> paper introduces learning scheme construct hilbert space <eos> vector space along its inner product address both unsupervised semi supervised domain adaptation problems <eos> achieved learning projections each domain latent space along mahalanobis metric latent space simultaneously minimizing notion domain variance while maximizing measure discriminatory power <eos> particular make use riemannian optimization techniques match statistical properties <eos> first second order statistics between sample projected into latent space different domains <eos> upon availability class labels further deem sample sharing same label form more compact clusters while pulling away sample coming different classes <eos> extensively evaluate contrast proposal against state art method task visual domain adaptation using both handcrafted deep net feature <eos> experiments show even simple nearest neighbor classifier proposed method outperform several state art method benefitting more involved classification schemes <eos> <eop> removing rain single image via deep detail network <eos> propose new deep network architecture removing rain streaks individual image based deep convolutional neural network cnn <eos> inspired deep residual network resnet simplifies learning process changing mapping form propose deep detail network directly reduce mapping range input output makes learning process easier <eos> further improve de rained result use priori image domain knowledge focusing high frequency detail during training removes background interference focuses model structure rain image <eos> demonstrates deep architecture only benefits high level vision tasks but also used solve low level imaging problems <eos> though train network synthetic data find learned network generalizes well real world test image <eos> experiments show proposed method significantly outperforms state art method both synthetic real world image terms both qualitative quantitative measures <eos> discuss applications structure denoising jpeg artifact reduction end paper <eos> <eop> binarized mode seeking scalable visual pattern discovery <eos> paper studies visual pattern discovery large scale image collections via binarized mode seeking image only represented binary codes efficient storage computation <eos> address problem perspective binary space mode seeking <eos> first binary mean shift bms proposed discover frequent patterns via mode seeking directly binary space <eos> binomial based kernel binary constraint introduced binarized analysis <eos> second further extend bms more general form namely contrastive binary mean shift cbms maximizes contrastive density binary space finding informative patterns both frequent discriminative dataset <eos> binarized algorithm optimization method demonstrate significant computation storage improvement compared standard techniques operating euclidean space while performance largely degenerate <eos> furthermore cbms discovers more informative patterns suppressing low discriminative modes <eos> evaluate method both annotated ilsvrc image un annotated blind flickr image datasets million scale image demonstrates both scalability effectiveness algorithms discovering frequent informative patterns large scale collection <eos> <eop> fast person re identification via cross camera semantic binary transformation <eos> numerous method proposed person re identification most however neglect matching efficiency <eos> recently several hashing based approaches developed make re identification more scalable large scale gallery set <eos> despite their efficiency works ignore cross camera variations severely deteriorate final matching accuracy <eos> address above issues propose novel hashing based method fast person re identification namely cross camera semantic binary transformation csbt <eos> csbt aims transform original high dimensional feature vectors into compact identity preserving binary codes <eos> end csbt first employs subspace projection mitigate cross camera variations maximizing intra person similarities inter person discrepancies <eos> subsequently binary coding scheme proposed via seamlessly incorporating both semantic pairwise relationships local affinity information <eos> finally joint learning framework proposed simultaneous subspace projection learning binary coding based discrete alternating optimization <eos> experimental result four benchmarks clearly demonstrate superiority csbt over state art method <eos> <eop> deep multi scale convolutional neural network dynamic scene deblurring <eos> non uniform blind deblurring general dynamic scenes challenging computer vision problem blurs arise only multiple object motions but also camera shake scene depth variation <eos> remove complicated motion blurs conventional energy optimization based method rely simple assumptions such blur kernel partially uniform locally linear <eos> moreover recent machine learning based method also depend synthetic blur datasets generated under assumptions <eos> makes conventional deblurring method fail remove blurs blur kernel difficult approximate parameterize <eos> object motion boundaries <eos> work propose multi scale convolutional neural network restores sharp image end end manner blur caused various sources <eos> together present multi scale loss function mimics conventional coarse fine approaches <eos> furthermore propose new large scale dataset provides pairs realistic blurry image corresponding ground truth sharp image obtained high speed camera <eos> proposed model trained dataset demonstrate empirically method achieves state art performance dynamic scene deblurring only qualitatively but also quantitatively <eos> <eop> deep crisp boundaries <eos> edge detection had made significant progress help deep convolutional network convnet <eos> convnet based edge detectors approached human level performance standard benchmarks <eos> provide systematical study detector outputs show they failed accurately localize edges adversarial tasks require crisp edge inputs <eos> addition propose novel refinement architecture address challenging problem learning crisp edge detector using convnet <eos> method leverages top down backward refinement pathway progressively increases resolution feature maps generate crisp edges <eos> result achieve promising performance bsds surpassing human accuracy when using standard criteria largely outperforming state art method when using more strict criteria <eos> further demonstrate benefit crisp edge maps estimating optical flow generating object proposals <eos> <eop> learning multifunctional binary codes both category attribute oriented retrieval tasks <eos> paper propose unified framework address multiple realistic image retrieval tasks concerning both category attributes <eos> considering scale modern datasets hashing favorable its low complexity <eos> however most existing hashing method designed preserve one single kind similarity thus incapable dealing different tasks simultaneously <eos> overcome limitation propose new hashing method named dual purpose hashing dph jointly preserves category attribute similarities exploiting convolutional network cnn hierarchically capture correlations between category attributes <eos> since image both category attribute labels scarce method designed take abundant partially labelled image internet training inputs <eos> such framework binary codes new coming image readily obtained quantizing network outputs binary like layer attributes recovered codes easily <eos> experiments two large scale datasets show dual purpose hash codes achieve comparable even better performance than state art method specifically designed each individual retrieval task while being more compact than compared method <eos> <eop> generative face completion <eos> paper propose effective face completion algorithm using deep generative model <eos> different well studied background completion face completion task more challenging often requires generate semantically new pixels missing key components <eos> eyes mouths contain large appearance variations <eos> unlike existing nonparametric algorithms search patches synthesize algorithm directly generates contents missing region based neural network <eos> model trained combination reconstruction loss two adversarial losses semantic parsing loss ensures pixel faithfulness local global contents consistency <eos> extensive experimental result demonstrate qualitatively quantitatively model able deal large area missing pixels arbitrary shapes generate realistic face completion result <eos> <eop> diversified texture synthesis feed forward network <eos> recent progresses deep discriminative generative modeling shown promising result texture synthesis <eos> however existing feed forward based method trade off generality efficiency suffer many issues such shortage generality <eos> build one network per texture lack diversity <eos> always produce visually identical output suboptimality <eos> generate less satisfying visual effects <eos> work focus solving issues improved texture synthesis <eos> propose deep generative feed forward network enables efficient synthesis multiple textures within one single network meaningful interpolation between them <eos> meanwhile suite important techniques introduced achieve better convergence diversity <eos> extensive experiments demonstrate effectiveness proposed model techniques synthesizing large number textures show its applications stylization <eos> <eop> learning deep cnn denoiser prior image restoration <eos> model based optimization method discriminative learning method two dominant strategies solving various inverse problems low level vision <eos> typically two kinds method their respective merits drawbacks <eos> model based optimization method flexible handling different inverse problems but usually time consuming sophisticated priors purpose good performance meanwhile discriminative learning method fast testing speed but their application range greatly restricted specialized task <eos> recent works revealed aid variable splitting techniques denoiser prior plugged modular part model based optimization method solve other inverse problems <eos> such integration induces considerable advantage when denoiser obtained via discriminative learning <eos> however study integration fast discriminative denoiser prior still lacking <eos> end paper aims train set fast effective cnn convolutional neural network denoisers integrate them into model based optimization method solve other inverse problems <eos> experimental result demonstrate learned set denoisers only achieve promising gaussian denoising result but also used prior deliver good performance various low level vision applications <eos> <eop> fast multi frame stereo scene flow motion segmentation <eos> propose new multi frame method efficiently computing scene flow dense depth optical flow camera ego motion dynamic scene observed moving stereo camera rig <eos> technique also segments out moving object rigid scene <eos> method first estimate disparity map dof camera motion using stereo matching visual odometry <eos> then identify region inconsistent estimated camera motion compute per pixel optical flow only region <eos> flow proposal fused camera motion based flow proposal using fusion moves obtain final optical flow motion segmentation <eos> unified framework benefits all four tasks stereo optical flow visual odometry motion segmentation leading overall higher accuracy efficiency <eos> method currently ranked third kitti scene flow benchmark <eos> furthermore cpu implementation runs seconds per frame orders magnitude faster than top six method <eos> also report thorough evaluation challenging sintel sequences fast camera object motion method consistently outperforms osf menze currently ranked second kitti benchmark <eos> <eop> deeppermnet visual permutation learning <eos> present principled approach uncover structure visual data solving novel deep learning task coined visual permutation learning <eos> goal task find permutation recovers structure data shuffled versions <eos> case natural image task boils down recovering original image patches shuffled unknown permutation matrix <eos> unfortunately permutation matrices discrete thereby posing difficulties gradient based method <eos> end resort continuous approximation matrices using doubly stochastic matrices generate standard cnn predictions using sinkhorn iterations <eos> unrolling iterations sinkhorn network layer propose deeppermnet end end cnn model task <eos> utility deeppermnet demonstrated two challenging computer vision problems namely relative attributes learning ii self supervised representation learning <eos> result show state art performance public figures osr benchmarks classification segmentation tasks pascal voc dataset ii <eos> <eop> light field blind motion deblurring <eos> study problem deblurring light fields general three dimensional scenes captured under three dimensional camera motion present both theoretical practical contributions <eos> analyzing motion blurred light field primal fourier domains develop intuition into effects camera motion light field show advantages capturing light field instead conventional image motion deblurring derive simple analytical method motion deblurring certain cases <eos> then present algorithm blindly deblur light fields general scenes without any estimation scene geometry demonstrate recover both sharp light field three dimensional camera motion path real synthetically blurred light fields <eos> <eop> wetness color single multispectral image <eos> visual recognition wet surfaces their degrees wetness important many computer vision applications <eos> inform slippery spots road autonomous vehicles muddy areas trail humanoid robots freshness groceries <eos> past monochromatic appearance change fact surfaces darken when wet modeled recognize wet surfaces <eos> paper show color change particularly its spectral behavior carries rich information about wet surface <eos> derive analytical spectral appearance model wet surfaces expresses characteristic spectral sharpening due multiple scattering absorption surface <eos> derive novel method estimating key parameters spectral appearance model enables recovery original surface color degree wetness single observation <eos> applied multispectral image method estimates spatial map wetness together dry spectral distribution surface <eos> knowledge work first model leverage spectral characteristics wet surfaces revert its appearance <eos> conduct comprehensive experimental validation number wet real surfaces <eos> result demonstrate accuracy model effectiveness method surface wetness color estimation <eos> <eop> seeing invisible poses estimating three dimensional body pose egocentric video <eos> understanding camera wearer activity central egocentric vision yet one key facet activity inherently invisible camera wearer body pose <eos> prior work focuses estimating pose hands arms when they come into view but gives incomplete view full body posture prevents any pose estimate all many frames since hands only visible fraction daily life activities <eos> propose infer invisible pose person behind egocentric camera <eos> given single video efficient learning based approach returns full body three dimensional joint positions each frame <eos> method exploits cues dynamic motion signatures surrounding scene change predictably function body pose well static scene structures reveal viewpoint <eos> further introduce novel energy minimization scheme infer pose sequence <eos> uses soft predictions poses per time instant together non parametric model human pose dynamics over longer windows <eos> method outperforms array possible alternatives including typical deep learning approaches direct pose regression image <eos> <eop> controlling perceptual factors neural style transfer <eos> neural style transfer shown very exciting result enabling new forms image manipulation <eos> here extend existing method introduce control over spatial location colour information across spatial scale <eos> demonstrate how enhances method allowing high resolution controlled stylisation helps alleviate common failure cases such applying ground textures sky region <eos> furthermore decomposing style into perceptual factors enable combination style information multiple sources generate new perceptually appealing styles existing ones <eos> also describe how method used more efficiently produce large size high quality stylisation <eos> finally show how introduced control measures applied recent method fast neural style transfer <eos> <eop> learning rank retargeted image <eos> image retargeting techniques adjust image into different sizes attracted much attention recently <eos> objective quality assessment oqa image retargeting result often desired automatically select best result <eos> existing oqa method output absolute score each retargeted image use scores compare different result <eos> observing challenging even human subjects give consistent scores retargeting result different source image paper propose learning based oqa method predicts ranking set retargeted image same source image <eos> show more manageable task helps achieve more consistent prediction human preference sufficient most application scenarios <eos> compute ranking propose simple yet efficient machine learning framework uses general regression neural network grnn model combination seven elaborate oqa metrics <eos> then propose simple scheme transform relative scores output grnn into global ranking <eos> train grnn model using human preference data collected elaborate retargetme benchmark evaluate method based subjective study retargetme <eos> moreover introduce further subjective benchmark evaluate generalizability different oqa method <eos> experimental result demonstrate method outperforms eight representative oqa method ranking prediction better generalizability different datasets <eos> <eop> image deblurring via extreme channels prior <eos> camera motion introduces motion blur affecting many computer vision tasks <eos> dark channel prior dcp helps blind deblurring scenes including natural face text low illumination image <eos> however limitations less likely support kernel estimation while bright pixels dominate input image <eos> observe bright pixels clear image likely bright after blur process <eos> based observation first illustrate phenomenon mathematically define bright channel prior bcp <eos> then propose technique deblurring such image elevates performance existing motion deblurring algorithms <eos> proposed method takes advantage both bright dark channel prior <eos> joint prior named extreme channels prior crucial achieving efficient restorations leveraging both bright dark information <eos> extensive experimental result demonstrate proposed method more robust performs favorably against state art image deblurring method both synthesized natural image <eos> <eop> fixed point factorized network <eos> recent years deep neural network dnn based method achieved remarkable performance wide range tasks among most powerful widely used techniques computer vision <eos> however dnn based method both computational intensive resource consuming hinders application method embedded systems like smart phones <eos> alleviate problem introduce novel fixed point factorized network ffn pretrained models reduce computational complexity well storage requirement network <eos> resulting network only weights significantly eliminates most resource consuming multiply accumulate operations macs <eos> extensive experiments large scale imagenet classification task show proposed ffn only requires one thousandth multiply operations comparable accuracy <eos> <eop> large margin object tracking circulant feature maps <eos> structured output support vector machine svm based tracking algorithms shown favorable performance recently <eos> nonetheless time consuming candidate sampling complex optimization limit their real time applications <eos> paper propose novel large margin object tracking method absorbs strong discriminative ability structured output svm speeds up correlation filter algorithm significantly <eos> secondly multimodal target detection technique proposed improve target localization precision prevent model drift introduced similar object background noise <eos> thirdly exploit feedback high confidence tracking result avoid model corruption problem <eos> implement two versions proposed tracker representations both conventional hand crafted deep convolution neural network cnn based feature validate strong compatibility algorithm <eos> experimental result demonstrate proposed tracker performs superiorly against several state art algorithms challenging benchmark sequences while runs speed excess frames per second <eos> <eop> learning residual image face attribute manipulation <eos> face attributes interesting due their detailed description human faces <eos> unlike prior researches working attribute prediction address inverse more challenging problem called face attribute manipulation aims modifying face image according given attribute value <eos> instead manipulating whole image propose learn corresponding residual image defined difference between image before after manipulation <eos> way manipulation operated efficiently modest pixel modification <eos> framework approach based generative adversarial network <eos> consists two image transformation network discriminative network <eos> transformation network responsible attribute manipulation its dual operation discriminative network used distinguish generated image real image <eos> also apply dual learning allow transformation network learn each other <eos> experiments show residual image effectively learned used attribute manipulations <eos> generated image remain most details attribute irrelevant areas <eos> <eop> one shot hyperspectral imaging using faced reflectors <eos> hyperspectral imaging useful technique various computer vision tasks such material recognition <eos> however such technique usually requires expensive professional setup time consuming because conventional hyperspectral image consists large number observations <eos> paper propose novel technique one shot hyperspectral imaging using faced reflectors color filters attached <eos> key idea based principle each multiple reflections filters different spectrum allows observe multiple intensities through different spectra <eos> technique implemented either coupled mirror kaleidoscope geometry <eos> experimental result show technique capable accurately capturing hyperspectral image using coupled mirror setup readily available <eos> <eop> video shop exact matching clothes video online shopping image <eos> recent years both online retail video hosting service exponentially grown <eos> paper novel deep neural network called asymnet proposed explore new cross domain task video shop targeting matching clothes appeared video exactly same items online shops <eos> image side well established method used detect extract feature clothing patches arbitrary sizes <eos> video side deep visual feature extracted detected object region each frame further fed into long short term memory lstm framework sequence modeling captures temporal dynamics video <eos> conduct exact matching between video online shopping image lstm hidden states video image feature extracted static image jointly modeled under similarity network reconfigurable deep tree structure <eos> moreover approximate training method proposed achieve efficiency when training <eos> extensive experiments conducted large cross domain dataset demonstrated effectiveness efficiency proposed asymnet outperforms state art method <eos> <eop> novel tensor based video rain streaks removal approach via utilizing discriminatively intrinsic priors <eos> rain streaks removal important issue outdoor vision system recently investigated extensively <eos> paper propose novel tensor based video rain streaks removal approach fully considering discriminatively intrinsic characteristics rain streaks clean video needs neither rain detection nor time consuming dictionary learning stage <eos> specific one hand rain streaks sparse smooth along raindrops direction other hand clean video possess smoothness along rain perpendicular direction global local correlation along time direction <eos> use norm enhance sparsity underlying rain two unidirectional total variation tv regularizers guarantee different discriminative smoothness tensor nuclear norm time directional difference operator characterize exclusive correlation clean video along time <eos> alternation direction method multipliers admm employed solve proposed concise tensor based convex model <eos> experiments implemented synthetic real data substantiate effectiveness efficiency proposed method <eos> under comprehensive quantitative performance measures approach outperforms other state art method <eos> <eop> deshadownet multi context embedding deep network shadow removal <eos> shadow removal challenging task requires detection annotation shadows well semantic understanding scene <eos> paper propose automatic end end deep neural network deshadownet tackle problems unified manner <eos> deshadownet designed multi context architecture output shadow matte predicted embedding information three different perspectives <eos> first global network extracts shadow feature global view <eos> two levels feature derived global network transferred two parallel network <eos> while one extracts appearance input image other one involves semantic understanding final prediction <eos> two complementary network generate multi context feature obtain shadow matte fine local details <eos> evaluate performance proposed method construct first large scale benchmark image pairs <eos> extensive experiments two publicly available benchmarks large scale benchmark show proposed method performs favorably against several state art method <eos> <eop> generalized semantic preserving hashing label cross modal retrieval <eos> due availability large amounts multimedia data cross modal matching gaining increasing importance <eos> hashing based techniques provide attractive solution problem when data size large <eos> different scenarios cross modal matching possible example data different modalities associated single label multiple labels addition may may one one correspondence <eos> most existing approaches developed case there one one correspondence between data two modalities <eos> paper propose simple yet effective generalized hashing framework work all different scenarios while preserving semantic distance between data point <eos> approach first learns optimum hash codes two modalities simultaneously so preserve semantic similarity between data point then learns hash functions map feature hash codes <eos> extensive experiments single label dataset like wiki multi label datasets like nus wide pascal labelme under all different scenarios comparisons state art shows effectiveness proposed approach <eos> <eop> fc fully convolutional color constancy confidence weighted pooling <eos> improvements color constancy arisen use convolutional neural network cnn <eos> however patch based cnn exist problem faced issue estimation ambiguity patch may contain insufficient information establish unique even limited possible range illumination colors <eos> image patches estimation ambiguity only appear great frequency photographs but also significantly degrade quality network training inference <eos> overcome problem present fully convolutional network architecture patches throughout image carry different confidence weights according value they provide color constancy estimation <eos> confidence weights learned applied within novel pooling layer local estimates merged into global solution <eos> formulation network able determine learn how pool automatically color constancy datasets without additional supervision <eos> proposed network also allows end end training achieves higher efficiency accuracy <eos> standard benchmarks network outperforms previous state art while achieving greater efficiency <eos> <eop> template based monocular three dimensional recovery elastic shapes using lagrangian multipliers <eos> present paper efficient template based method three dimensional recovery elastic shapes fixed monocular camera <eos> exploiting object elasticity contrast isometric method use inextensibility constraints large range deformations handled <eos> method expressed saddle point problem using lagrangian multipliers resulting linear system unifies both mechanical optical constraints integrates dirichlet boundary conditions whether they fixed free <eos> experimentally show no prior knowledge material properties needed exhibit generic usability method elastic inelastic object different kinds materials <eos> comparisons existing techniques conducted synthetic real elastic object strains ranging resulting low errors <eos> <eop> discriminative optimization theory applications point cloud registration <eos> many computer vision problems formulated optimization cost function <eos> approach faces two main challenges designing cost function local optimum acceptable solution developing efficient numerical method search one multiple local optima <eos> while designing such functions feasible noiseless case stability location local optima mostly unknown under noise occlusion missing data <eos> practice result undesirable local optima having local optimum expected place <eos> other hand numerical optimization algorithms high dimensional spaces typically local often rely expensive first second order information guide search <eos> overcome limitations paper proposes discriminative optimization method learns search directions data without need cost function <eos> specifically explicitly learns sequence updates search space leads stationary point correspond desired solutions <eos> provide formal analysis illustrate its benefits problem three dimensional point cloud registration both synthetic range scan data <eos> show outperforms state art algorithms large margin terms accuracy robustness perturbations computational efficiency <eos> <eop> fine grained recognition thousands object categories single example training <eos> approach problem fast detection recognition large number thousands object categories while training very limited amount examples usually one per category <eos> examples task include detection retail products only one studio image each product available training ii detection brand logos iii detection three dimensional object their respective poses within static image only sparse subset partial object views available training single example each view <eos> building detector based so few examples presents significant challenge current top performing deep learning based techniques require large amounts data train <eos> approach task based non parametric probabilistic model initial detection cnn based refinement temporal integration applicable <eos> successfully demonstrate its usefulness variety experiments both existing own benchmarks achieving state art performance <eos> <eop> deep co occurrence feature learning visual object recognition <eos> paper addresses three issues integrating part based representations into convolutional neural network cnn object recognition <eos> first most part based models rely few pre specified object parts <eos> however optimal object parts recognition often vary category category <eos> second acquiring training data part level annotation labor intensive <eos> third modeling spatial relationships between parts cnn often involves exhaustive search part templates over multiple network streams <eos> tackle three issues introducing new network layer called co occurrence layer <eos> extend convolutional layer encode co occurrence between visual parts detected numerous neurons instead few pre specified parts <eos> end feature maps serve both filters image mutual correlation filtering conducted between them <eos> co occurrence layer end end trainable <eos> resultant co occurrence feature rotation translation invariant robust object deformation <eos> applying new layer vgg resnet achieve recognition rates <eos> caltech ucsd bird benchmark respectively <eos> source code available github <eos> com yafangshih deep cooc <eos> <eop> gift knowledge distillation fast optimization network minimization transfer learning <eos> introduce novel technique knowledge transfer knowledge pretrained deep neural network dnn distilled transferred another dnn <eos> dnn performs mapping input space output space through many layer sequentially define distilled knowledge transferred terms flow between layer calculated computing inner product between feature two layer <eos> when compare student dnn original network same size student dnn but trained without teacher network proposed method transferring distilled knowledge flow between two layer exhibits three important phenomena student dnn learns distilled knowledge optimized much faster than original model student dnn outperforms original dnn student dnn learn distilled knowledge teacher dnn trained different task student dnn outperforms original dnn trained scratch <eos> <eop> salient object learning salient object detector ensembling linear exemplar regressors <eos> finding salient object helpful developing better feature models salient object detection sod <eos> paper investigate image selected discarded constructing new sod dataset find many similar candidates complex shape low objectness three main attributes many non salient object <eos> moreover object may diversified attributes make them salient <eos> result propose novel salient object detector ensembling linear exemplar regressors <eos> first select reliable foreground background seeds using boundary prior then adopt locally linear embedding lle conduct manifold preserving foregroundness propagation <eos> manner foregroundness map generated roughly pop out salient object suppress non salient ones many similar candidates <eos> moreover extract shape foregroundness attention descriptors characterize extracted object proposals linear exemplar regressor trained encode how detect salient proposals specific image <eos> finally various linear exemplar regressors ensembled form single detector adapts various scenarios <eos> extensive experimental result dataset new sod dataset show approach outperforms state art method <eos> <eop> full resolution residual network semantic segmentation street scenes <eos> semantic image segmentation essential component modern autonomous driving systems accurate understanding surrounding scene crucial navigation action planning <eos> current state art approaches semantic image segmentation rely pre trained network were initially developed classifying image whole <eos> while network exhibit outstanding recognition performance <eos> visible they lack localization accuracy <eos> precisely something located <eos> therefore additional processing steps performed order obtain pixel accurate segmentation masks full image resolution <eos> alleviate problem propose novel resnet like architecture exhibits strong localization recognition performance <eos> combine multi scale context pixel level accuracy using two processing streams within network one stream carries information full image resolution enabling precise adherence segment boundaries <eos> other stream undergoes sequence pooling operations obtain robust feature recognition <eos> two streams coupled full image resolution using residuals <eos> without additional processing steps without pre training approach achieves intersection over union score <eos> <eop> optical flow estimation using spatial pyramid network <eos> learn compute optical flow combining classical spatial pyramid formulation deep learning <eos> estimates large motions coarse fine approach warping one image pair each pyramid level current flow estimate computing update flow <eos> instead standard minimization objective function each pyramid level train one deep network per level compute flow update <eos> unlike recent flownet approach network need deal large motions dealt pyramid <eos> first spatial pyramid network spynet much simpler smaller than flownet terms model parameters <eos> makes more efficient appropriate embedded applications <eos> second since flow each pyramid level small pixel convolutional approach applied pairs warped image appropriate <eos> third unlike flownet learned convolution filters appear similar classical spatio temporal filters giving insight into method how improve <eos> result more accurate than flownet most standard benchmarks suggesting new direction combining classical flow method deep learning <eos> <eop> spatio temporal naive bayes nearest neighbor st nbnn skeleton based action recognition <eos> motivated previous success using non parametric method recognize object <eos> nbnn extend recognize actions using skeletons <eos> each three dimensional action presented sequence three dimensional poses <eos> similar nbnn proposed spatio temporal nbnn applies stage class distance classify actions <eos> however st nbnn takes spatio temporal structure three dimensional actions into consideration relaxes naive bayes assumption nbnn <eos> specifically st nbnn adopts bilinear classifiers identify both key temporal stages well spatial joints action classification <eos> although only using linear classifier experiments three benchmark datasets show combining strength both non parametric parametric models st nbnn achieve competitive performance compared state art result using sophisticated models such deep learning <eos> moreover identifying key skeleton joints temporal stages each action class st nbnn capture essential spatio temporal patterns play key roles recognizing actions always achievable using end end models <eos> <eop> gms grid based motion statistics fast ultra robust feature correspondence <eos> incorporating smoothness constraints into feature matching known enable ultra robust matching <eos> however such formulations both complex slow making them unsuitable video applications <eos> paper proposes gms grid based motion statistics simple means encapsulating motion smoothness statistical likelihood certain number matches region <eos> gms enables translation high match numbers into high match quality <eos> provides real time ultra robust correspondence system <eos> evaluation video low textures blurs wide baselines show gms consistently out performs other real time matchers achieve parity more sophisticated much slower techniques <eos> <eop> detailed accurate human shape estimation clothed three dimensional scan sequences <eos> address problem estimating human pose body shape three dimensional scans over time <eos> reliable estimation three dimensional body shape necessary many applications including virtual try health monitoring avatar creation virtual reality <eos> scanning bodies minimal clothing however presents practical barrier applications <eos> address problem estimating body shape under clothing sequence three dimensional scans <eos> previous method exploited body models produce smooth shapes lacking personalized details <eos> contribute new approach recover personalized shape person <eos> estimated shape deviates parametric model fit three dimensional scans <eos> demonstrate method using high quality data well sequences visual hulls extracted multi view image <eos> also make available buff new dataset enables quantitative evaluation buff <eos> method outperforms state art both pose estimation shape estimation qualitatively quantitatively <eos> <eop> active convolution learning shape convolution image classification <eos> recent years deep learning achieved great success many computer vision applications <eos> convolutional neural network cnn lately emerged major approach image classification <eos> most research cnn thus far focused developing architectures such inception residual network <eos> convolution layer core cnn but few studies addressed convolution unit itself <eos> paper introduce convolution unit called active convolution unit acu <eos> new convolution no fixed shape because define any form convolution <eos> its shape learned through backpropagation during training <eos> proposed unit few advantages <eos> first acu generalization convolution define only all conventional convolutions but also convolutions fractional pixel coordinates <eos> freely change shape convolution provides greater freedom form cnn structures <eos> second shape convolution learned while training there no need tune hand <eos> third acu learn better than conventional unit obtained improvement simply changing conventional convolution acu <eos> tested proposed method plain residual network result showed significant improvement using method various datasets architectures comparison baseline <eos> code available github <eos> com jyh active convolution <eos> <eop> video desnowing deraining based matrix decomposition <eos> existing snow rain removal method often fail heavy snow rain dynamic scene <eos> one reason failure due assumption all snowflakes rain streaks sparse snow rain scenes <eos> other existing method often differentiate moving object snowflakes rain streaks <eos> paper propose model based matrix decomposition video desnowing deraining solve problems mentioned above <eos> divide snowflakes rain streaks into two categories sparse ones dense ones <eos> background fluctuations optical flow information detection moving object sparse snowflakes rain streaks formulated multi label markov random fields mrfs <eos> dense snowflakes rain streaks they considered obey gaussian distribution <eos> snowflakes rain streaks including sparse ones dense ones scene backgrounds removed low rank representation backgrounds <eos> meanwhile group sparsity term model designed filter snow rain pixels within moving object <eos> experimental result show proposed model performs better than state art method snow rain removal <eos> <eop> thin slicing network deep structured model pose estimation video <eos> deep convnets shown effective task human pose estimation single image <eos> however several challenging issues arise video based case such self occlusion motion blur uncommon poses few no examples training data <eos> temporal information provide additional cues about location body joints help alleviate issues <eos> paper propose deep structured model estimate sequence human poses unconstrained video <eos> model efficiently trained end end manner capable representing appearance body joints their spatio temporal relationships simultaneously <eos> domain knowledge about human body explicitly incorporated into network providing effective priors regularize skeletal structure enforce temporal consistency <eos> proposed end end architecture evaluated two widely used benchmarks video based pose estimation penn action jhmdb datasets <eos> approach outperforms several state art method <eos> <eop> self supervised learning visual feature through embedding image into text topic spaces <eos> end end training scratch current deep architectures new computer vision problems would require imagenet scale datasets always possible <eos> paper present method able take advantage freely available multi modal content train computer vision algorithms without human supervision <eos> put forward idea performing self supervised learning visual feature mining large scale corpus multi modal text image documents <eos> show discriminative visual feature learnt efficiently training cnn predict semantic context particular image more probable appear illustration <eos> leverage hidden semantic structures discovered text corpus well known topic modeling technique <eos> experiments demonstrate state art performance image classification object detection multi modal retrieval compared recent self supervised natural supervised approaches <eos> <eop> coarse fine segmentation shape tailored continuum scale spaces <eos> formulate energy segmentation designed preference segmenting coarse over fine structure image without smoothing across boundaries region <eos> energy formulated integrating continuum scales scale space computed heat equation within region <eos> show energy optimized without computing continuum scales but instead single scale <eos> makes method computationally efficient comparison energies using discrete set scales <eos> apply method texture motion segmentation <eos> experiments benchmark datasets show continuum scales leads better segmentation accuracy over discrete scales other competing method <eos> <eop> minimum delay moving object detection <eos> present general framework method detection object video based apparent motion <eos> object moves relative background motion some unknown time video goal detect segment object soon moves online manner <eos> due unreliability motion between frames more than two frames needed reliably detect object <eos> method designed detect object minimum delay <eos> frames after object moves constraining false alarms <eos> experiments new extensive dataset moving object detection show method achieves less delay all false alarm constraints than existing state art <eos> <eop> hyper laplacian regularized unidirectional low rank tensor recovery multispectral image denoising <eos> recent low rank based matrix tensor recovery method widely explored multispectral image msi denoising <eos> method however ignore difference intrinsic structure correlation along spatial sparsity spectral correlation non local self similarity mode <eos> paper go further giving detailed analysis about rank properties both matrix tensor cases figure out non local self similarity key ingredient while low rank assumption others may hold <eos> motivates design simple yet effective unidirectional low rank tensor recovery model capable truthfully capturing intrinsic structure correlation reduced computational burden <eos> however low rank models suffer ringing artifacts due aggregation overlapped patches cubics <eos> while previous method resort spatial information offer new perspective utilizing exclusively spectral information msis address issue <eos> analysis based hyper laplacian prior introduced model global spectral structures so indirectly alleviate ringing artifacts spatial domain <eos> advantages proposed method over existing ones multi fold more reasonably structure correlation representability less processing time less artifacts overlapped region <eos> proposed method extensively evaluated several benchmarks significantly outperforms state art msi denoising method <eos> <eop> online asymmetric similarity learning cross modal retrieval <eos> cross modal retrieval attracted intensive attention recent years <eos> measuring semantic similarity between heterogeneous data object essential yet challenging problem cross modal retrieval <eos> paper propose online learning method learn similarity function between heterogeneous modalities preserving relative similarity training data modeled set bi directional hinge loss constraints cross modal training triplets <eos> overall online similarity function learning problem optimized margin based passive aggressive algorithm <eos> further extend approach learn similarity function reproducing kernel hilbert spaces kernelizing approach combining multiple kernels derived different layer cnn feature using hedging algorithm <eos> theoretical mistake bounds given method <eos> experiments conducted real world datasets well demonstrate effectiveness method <eos> <eop> latent multi view subspace clustering <eos> paper propose novel latent multi view subspace clustering lmsc method clusters data point latent representation simultaneously explores underlying complementary information multiple views <eos> unlike most existing single view subspace clustering method reconstruct data point using original feature method seeks underlying latent representation simultaneously performs data reconstruction based learned latent representation <eos> complementarity multiple views latent representation could depict data themselves more comprehensively than each single view individually accordingly makes subspace representation more accurate robust well <eos> proposed method intuitive optimized efficiently using augmented lagrangian multiplier alternating direction minimization alm adm algorithm <eos> extensive experiments benchmark datasets validated effectiveness proposed method <eos> <eop> unsupervised vanishing point detection camera calibration single manhattan image radial distortion <eos> article concerns automatic calibration camera radial distortion single image <eos> known under mild assumption square pixels zero skew lines scene project into circles image three lines suffice calibrate camera up ambiguity between focal length radial distortion <eos> calibration result highly depend accurate circle estimation hard accomplish because lines tend project into short circular arcs <eos> overcome problem show given short circular arc edge possible robustly determine line goes through center corresponding circle <eos> lines henceforth called lines circle centres lccs used new method detects set parallel lines estimates calibration parameters including center amount distortion focal length camera orientation respect manhattan frame <eos> extensive experiments both semi synthetic real image show algorithm outperforms state art approaches unsupervised calibration single image while providing more information <eos> <eop> re sign re aligned end end sequence modelling deep recurrent cnn hmms <eos> work presents iterative re alignment approach applicable visual sequence labelling tasks such gesture recognition activity recognition continuous sign language recognition <eos> previous method dealing video data usually rely given frame labels train their classifiers <eos> however looking recent data set labels often tend noisy commonly overseen <eos> propose algorithm treats provided training labels weak labels refines label image alignment fly weakly supervised fashion <eos> given series frames sequence level labels deep recurrent cnn blstm network trained end end <eos> embedded into hmm resulting deep model corrects frame labels continuously improves its performance several re alignments <eos> evaluate two challenging publicly available sign recognition benchmark data set featuring over classes <eos> outperform state art up absolute relative <eos> <eop> improving interpretability deep neural network semantic information <eos> interpretability deep neural network dnns essential since enables users understand overall strengths weaknesses models conveys understanding how models will behave future how diagnose correct potential problems <eos> however challenging reason about dnn actually due its opaque black box nature <eos> address issue propose novel technique improve interpretability dnns leveraging rich semantic information embedded human descriptions <eos> concentrating video captioning task first extract set semantically meaningful topics human descriptions cover wide range visual concepts integrate them into model interpretive loss <eos> then propose prediction difference maximization algorithm interpret learned feature each neuron <eos> experimental result demonstrate its effectiveness video captioning using interpretable feature also transferred video action recognition <eos> clearly understanding learned feature users easily revise false predictions via human loop procedure <eos> <eop> social scene understanding end end multi person action localization collective activity recognition <eos> present unified framework understanding human social behaviors raw image sequences <eos> model jointly detects multiple individuals infers their social actions estimates collective actions single feed forward pass through neural network <eos> propose single architecture rely external detection algorithms but rather trained end end generate dense proposal maps refined via novel inference scheme <eos> temporal consistency handled via person level matching recurrent neural network <eos> complete model takes input sequence frames outputs detections along estimates individual actions collective activities <eos> demonstrate state art performance algorithm multiple publicly available benchmarks <eos> <eop> untrimmednets weakly supervised action recognition detection <eos> current action recognition method heavily rely trimmed video model training <eos> however expensive time consuming acquire large scale trimmed video dataset <eos> paper presents new weakly supervised architecture called untrimmednet able directly learn action recognition models untrimmed video without requirement temporal annotations action instances <eos> untrimmednet couples two important components classification module selection module learn action models reason about temporal duration action instances respectively <eos> two components implemented feed forward network untrimmednet therefore end end trainable architecture <eos> exploit learned models action recognition wsr detection wsd untrimmed video datasets thumos activitynet <eos> although untrimmednet only employs weak supervision method achieves performance superior comparable strongly supervised approaches two datasets <eos> <eop> multi task correlation particle filter robust object tracking <eos> paper propose multi task correlation particle filter mcpf robust visual tracking <eos> first present multi task correlation filter mcf takes interdependencies among different feature into account learn correlation filters jointly <eos> proposed mcpf designed exploit complement strength mcf particle filter <eos> compared existing tracking method based correlation filters particle filters proposed tracker several advantages <eos> first shepherd sampled particles toward modes target state distribution via mcf thereby resulting robust tracking performance <eos> second effectively handle large scale variation via particle sampling strategy <eos> third effectively maintain multiple modes posterior density using fewer particles than conventional particle filters thereby lowering computational cost <eos> extensive experimental result three benchmark datasets demonstrate proposed mcpf performs favorably against state art method <eos> <eop> improving training deep neural network via singular value bounding <eos> deep learning method achieve great success recently many computer vision problems <eos> spite practical successes optimization deep network remains active topic deep learning research <eos> work focus investigation network solution properties potentially lead good performance <eos> research inspired theoretical empirical result use orthogonal matrices initialize network but interested investigating how orthogonal weight matrices perform when network training converges <eos> end propose constrain solutions weight matrices orthogonal feasible set during whole process network training achieve simple yet effective method called singular value bounding svb <eos> svb all singular values each weight matrix simply bounded narrow band around value <eos> based same motivation also propose bounded batch normalization bbn improves batch normalization removing its potential risk ill conditioned layer transform <eos> present both theoretical empirical result justify proposed method <eos> experiments benchmark image classification datasets show efficacy proposed svb bbn <eos> particular achieve state art result <eos> error rate cifar <eos> cifar using off shelf network architectures wide resnets <eos> preliminary result imagenet also show promise large scale learning <eos> release implementation code method www <eos> net research svb <eos> <eop> large kernel matters improve semantic segmentation global convolutional network <eos> convolution neural network cnn boosted per formanceofalotofcomputervisiontasks likeimageclassi fication segmentation detection <eos> based observations recent model design ers prefer employ stacking small kernels like over large size filters <eos> however field semantic seg mentation need perform dense per pixel pre diction find large kernel plays important role relieve contradictories when optimizing classi fication localization tasks simultaneously <eos> following design principle large size kernel propose global convolutional network address both classi fication localization issue semantic segmentation task <eos> further refine object category boundaries presentboundaryrefinementblockbasedonresidualstruc ture <eos> qualitatively model achieves state art perfor mance two public benchmarks outperforms previous result large margin <eos> pascal voc dataset <eos> <eop> neural aggregation network video face recognition <eos> paper presents neural aggregation network nan video face recognition <eos> network takes face video face image set person variable number face image its input produces compact fixed dimension feature representation recognition <eos> whole network composed two modules <eos> feature embedding module deep convolutional neural network cnn maps each face image feature vector <eos> aggregation module consists two attention blocks adaptively aggregate feature vectors form single feature inside convex hull spanned them <eos> due attention mechanism aggregation invariant image order <eos> nan trained standard classification verification loss without any extra supervision signal found automatically learns advocate high quality face image while repelling low quality ones such blurred occluded improperly exposed faces <eos> experiments ijb youtube face celebrity video face recognition benchmarks show consistently outperforms naive aggregation method achieves state art accuracy <eos> <eop> deep future gaze gaze anticipation egocentric video using adversarial network <eos> introduce new problem gaze anticipation egocentric video <eos> substantially extends conventional gaze prediction problem future frames no longer confining current frame <eos> solve problem propose new generative adversarial neural network based model deep future gaze dfg <eos> dfg generates multiple future frames conditioned single current frame anticipates corresponding future gazes next few seconds <eos> consists two network generator discriminator <eos> generator uses two stream spatial temporal convolution architecture three dimensional cnn explicitly untangling foreground background generate future frames <eos> then attaches another three dimensional cnn gaze anticipation based synthetic frames <eos> discriminator plays against generator differentiating synthetic frames generator real frames <eos> through competition discriminator generator progressively improves quality future frames thus anticipates future gaze better <eos> experimental result publicly available egocentric datasets show dfg significantly outperforms all well established baselines <eos> moreover demonstrate dfg achieves better performance gaze prediction current frames than state art method <eos> due benefiting learning motion discriminative representations frame generation <eos> further contribute new egocentric dataset ost object search task <eos> dfg also achieves best performance challenging dataset <eos> <eop> simultaneous stereo video deblurring scene flow estimation <eos> video outdoor scene often show unpleasant blur effects due large relative motion between camera dynamic object large depth variations <eos> existing works typically focus monocular video deblurring <eos> paper propose novel approach deblurring stereo video <eos> particular exploit piece wise planar assumption about scene leverage scene flow information deblur image <eos> unlike existing approach used pre computed scene flow propose single framework jointly estimate scene flow deblur image motion cues scene flow estimation blur information could reinforce each other produce superior result than conventional scene flow estimation stereo deblurring method <eos> evaluate method extensively two available datasets achieve significant improvement flow estimation removing blur effect over state art method <eos> <eop> empirical evaluation visual question answering novel object <eos> study problem answering questions about image harder setting test questions corresponding image contain novel object were queried about training data <eos> such setting inevitable real world owing heavy tailed distribution visual categories there would some object would annotated train set <eos> show performance two popular existing method drop significantly when evaluated novel object cf <eos> propose method use large existing external corpora unlabeled text <eos> books ii image tagged classes achieve novel object based visual question answering <eos> systematically study both oracle case novel object known textually well fully automatic case without any explicit knowledge novel object but minimal assumption novel object semantically related existing object training <eos> proposed method novel object based visual question answering modular potentially used many visual question answering architectures <eos> show consistent improvements two popular architectures give qualitative analysis cases model well fails bring improvements <eos> <eop> binary constraint preserving graph matching <eos> graph matching fundamental problem computer vision pattern recognition area <eos> general formulated integer quadratic programming iqp problem <eos> since np hard approximate relaxations required <eos> paper new graph matching method proposed <eos> there three main contributions proposed method propose new graph matching relaxation model called binary constraint preserving graph matching bpgm aims incorporate discrete binary mapping constraints more graph matching relaxation <eos> bpgm motivated new observation discrete binary constraints iqp matching problem represented encoded exactly norm constraint <eos> effective projection algorithm derived solve bpgm model <eos> using bpgm propose path following strategy optimize iqp matching problem thus obtain desired discrete solution convergence <eos> promising experimental result show effectiveness proposed method <eos> <eop> exploiting saliency object segmentation image level labels <eos> there remarkable improvements semantic labelling task recent years <eos> however state art method rely large scale pixel level annotations <eos> paper studies problem training pixel wise semantic labeller network image level annotations present object classes <eos> recently shown high quality seeds indicating discriminative object region obtained image level labels <eos> without additional information obtaining full extent object inherently ill posed problem due co occurrences <eos> propose using saliency model additional information hereby exploit prior knowledge object extent image statistics <eos> show how combine both information sources order recover fully supervised performance new state art weakly supervised training pixel wise semantic labelling <eos> <eop> predicting salient face multiple face video <eos> although recent success convolutional neural network cnn advances state art saliency prediction static image few work addressed problem predicting attention video <eos> other hand find attention different subjects consistently focuses single face each frame video involving multiple faces <eos> therefore propose paper novel deep learning dl based method predict salient face multiple face video capable learning feature transition salient faces across video frames <eos> particular first learn cnn each frame locate salient face <eos> taking cnn feature input develop multiple stream long short term memory lstm network predict temporal transition salient faces video sequences <eos> evaluate dl based method build new eye tracking database multiple face video <eos> experimental result show method outperforms prior state art method predicting visual attention faces multiple face video <eos> <eop> spftn self paced fine tuning network segmenting object weakly labelled video <eos> object segmentation weakly labelled video interesting yet challenging task aims learning perform category specific video object segmentation only using video level tags <eos> existing works research area might still some limitations <eos> lack effective dnn based learning frameworks under exploring context information requiring leverage unstable negative video collection prevent them obtaining more promising performance <eos> end propose novel self paced fine tuning network spftn based framework could learn explore context information within video frames capture adequate object semantics without using negative video <eos> perform weakly supervised learning based deep neural network make earliest effort integrate self paced learning regime deep neural network into unified compatible framework leading self paced fine tuning network <eos> comprehensive experiments large scale youtube object davis datasets demonstrate proposed approach achieves superior performance compared other state art method well baseline network models <eos> <eop> look closer see better recurrent attention convolutional neural network fine grained image recognition <eos> recognizing fine grained categories <eos> bird species difficult due challenges discriminative region localization fine grained feature learning <eos> existing approaches predominantly solve challenges independently while neglecting fact region detection fine grained feature learning mutually correlated thus reinforce each other <eos> paper propose novel recurrent attention convolutional neural network ra cnn recursively learns discriminative region attention region based feature representation multiple scales mutual reinforced way <eos> learning each scale consists classification sub network attention proposal sub network apn <eos> apn starts full image iteratively generates region attention coarse fine taking previous prediction reference while finer scale network takes input amplified attended region previous scale recurrent way <eos> proposed ra cnn optimized intra scale classification loss inter scale ranking loss mutually learn accurate region attention fine grained representation <eos> ra cnn need bounding box part annotations trained end end <eos> conduct comprehensive experiments show ra cnn achieves best performance three fine grained tasks relative accuracy gains <eos> cub birds stanford dogs stanford cars respectively <eos> <eop> local global edge profiles camera motion blurred image <eos> work investigate relation between edge profiles present motion blurred image underlying camera motion responsible causing motion blur <eos> while related works camera motion estimation cme rely strong assumption space invariant blur handle challenging case general camera motion <eos> first show how edge profiles alone harnessed perform direct cme single observation <eos> while routine conventional method jointly estimate latent image too through alternating minimization above scheme best suited when such pursuit either impractical inefficacious <eos> applications actually favor alternating minimization strategy edge profiles serve valuable cue <eos> incorporate suitably derived constraint edge profiles into existing blind deblurring framework demonstrate improved restoration performance <eos> experiments reveal approach yields state art result blind deblurring problem <eos> <eop> fly adaptation regression forests online camera relocalisation <eos> camera relocalisation important problem computer vision applications simultaneous localisation mapping virtual augmented reality navigation <eos> common techniques either match current image against keyframes known poses coming tracker establish three dimensional correspondences between keypoints current image point scene order estimate camera pose <eos> recently regression forests become popular alternative establish such correspondences <eos> they achieve accurate result but must trained offline target scene preventing relocalisation new environments <eos> paper show how circumvent limitation adapting pre trained forest new scene fly <eos> adapted forests achieve relocalisation performance par offline forests approach runs under ms making desirable real time systems require online relocalisation <eos> <eop> plug play generative network conditional iterative generation image latent space <eos> generating high resolution photo realistic image long standing goal machine learning <eos> showed one interesting way synthesize novel image performing gradient descent latent space generator network maximize activations one multiple neurons separate classifier network <eos> paper extend method introducing additional prior latent code improving both sample quality sample diversity leading state art generative model produces high quality image higher resolutions than previous generative models so all imagenet categories <eos> addition provide unified probabilistic interpretation related activation maximization method call general class models plug play generative network <eos> ppgns composed generator network capable drawing wide range image types replaceable condition network tells generator draw <eos> demonstrate generation image conditioned class when imagenet classification network also conditioned caption when image captioning network <eos> method also improves state art deep multifaceted feature visualization involves synthetically generating set inputs activate neuron order better understand how deep neural network operate <eos> finally show model performs reasonably well task image inpainting <eos> while operate image paper approach modality agnostic applied many types data <eos> <eop> domain adaptation mixture alignments second higher order scatter tensors <eos> paper propose approach domain adaptation dubbed second higher order transfer knowledge so hot based mixture alignments second higher order scatter statistics between source target domains <eos> human ability learn few labeled sample recurring motivation literature domain adaptation <eos> towards end investigate supervised target scenario few labeled target training sample per category exist <eos> specifically utilize two cnn streams source target network fused classifier level <eos> feature fully connected layer fc each network used compute second even higher order scatter tensors one per network stream per class <eos> source target distributions somewhat different despite being related align scatters two network streams same class within class scatters desired degree bespoke loss while maintaining good separation between class scatters <eos> train entire network end end fashion <eos> provide evaluations standard office benchmark visual domains rgb combined caltech depth rgb transfer <eos> attain state art result <eos> <eop> generative model depth based robust three dimensional facial pose tracking <eos> consider problem depth based robust three dimensional facial pose tracking under unconstrained scenarios heavy occlusions arbitrary facial expression variations <eos> unlike previous depth based discriminative data driven method require sophisticated training manual intervention propose generative framework unifies pose tracking face model adaptation fly <eos> particularly propose statistical three dimensional face model owns flexibility generate predict distribution uncertainty underlying face model <eos> moreover unlike prior arts employing icp based facial pose estimation propose ray visibility constraint regularizes pose based face model visibility against input point cloud augments robustness against occlusions <eos> experimental result biwi ict dhp datasets reveal proposed framework effective outperforms state art depth based method <eos> <eop> single image reflection suppression <eos> reflections common artifact image taken through glass windows <eos> automatically removing reflection artifacts after picture taken ill posed problem <eos> attempts solve problem using optimization schemes therefore rely various prior assumptions physical world <eos> instead removing reflections single image met limited success so far propose novel approach suppress reflections <eos> based laplacian data fidelity term zero gradient sparsity term imposed output <eos> experiments artificial real world image show reflection suppression method performs better than state art reflection removal techniques <eos> <eop> learning non maximum suppression <eos> object detectors hugely profited moving towards end end learning paradigm proposals fea tures classifier becoming one neural network improved result two fold general object detection <eos> one indispensable component non maximum suppression nms post processing algorithm responsible merging all detections belong same object <eos> de facto standard nms algorithm still fully hand crafted suspiciously simple being based greedy clustering fixed distance threshold forces trade off between recall precision <eos> propose new network architecture designed perform nms using only boxes their score <eos> report experiments person detection pets general object categories coco dataset <eos> approach shows promise providing improved localization occlusion handling <eos> <eop> brisks binary feature spherical image geodesic grid <eos> paper develop interest point detector binary feature descriptor spherical image <eos> take inspiration recent framework developed planar image brisk binary robust invariant scalable keypoints adapt method operate spherical image <eos> all processing intrinsic sphere avoids distortion inherent storing indexing spherical image representation <eos> discretise image spherical geodesic grid formed recursive subdivision triangular mesh <eos> leads multiscale pixel grid comprising mainly hexagonal pixels lends itself naturally spherical image pyramid representation <eos> interest point detection use variant accelerated segment test ast corner detector operates geodesic grid <eos> estimate continuous scale location feature descriptors built sampling onto regular pattern tangent space <eos> evaluate repeatability precision recall both synthetic spherical image known ground truth correspondences real image <eos> <eop> gaze embeddings zero shot image classification <eos> zero shot image classification using auxiliary information such attributes describing discriminative object properties requires time consuming annotation domain experts <eos> instead propose method relies human gaze auxiliary information exploiting even non expert users natural ability judge class membership <eos> present data collection paradigm involves discrimination task increase information content obtained gaze data <eos> method extracts discriminative descriptors data learns compatibility function between image gaze using three novel gaze embeddings gaze histograms gh gaze feature grid gfg gaze feature sequence gfs <eos> introduce two new gaze annotated datasets fine grained image classification show human gaze data indeed class discriminative provides competitive alternative expert annotated attributes outperforms other baselines zero shot image classification <eos> <eop> lamp adaptive layout aware multi patch deep convolutional neural network photo aesthetic assessment <eos> deep convolutional neural network cnn recently shown generate promising result aesthetics assessment <eos> however performance deep cnn method often compromised constraint neural network only takes fixed size input <eos> accommodate requirement input image need transformed via cropping warping padding often alter image composition reduce image resolution cause image distortion <eos> thus aesthetics original image impaired because potential loss fine grained details holistic image layout <eos> however such fine grained details holistic image layout critical evaluating image aesthetics <eos> paper present adaptive layout aware multi patch convolutional neural network lamp cnn architecture photo aesthetic assessment <eos> novel scheme able accept arbitrary sized image learn both fined grained details holistic image layout simultaneously <eos> enable training hybrid inputs extend method developing dedicated double subnet neural network structure <eos> multi patch subnet layout aware subnet <eos> further construct aggregation layer effectively combine hybrid feature two subnets <eos> extensive experiments large scale aesthetics assessment benchmark ava demonstrate significant performance improvement over state art photo aesthetic assessment <eos> <eop> toroidal constraints two point localization under high outlier ratios <eos> localizing query image against three dimensional model large scale hard problem since three dimensional matches become more more ambiguous model size increases <eos> creates need pose estimation strategies handle very low inlier ratios <eos> paper draw new insights geometric information available three dimensional matching process <eos> modern descriptors invariant against large variations viewpoint able find rays space used triangulate given point closest query descriptor <eos> well known two correspondences constrain camera lie surface torus <eos> adding knowledge direction triangulation able approximate position camera two matches alone <eos> derive geometric solver compute position under microsecond <eos> using solver propose simple yet powerful outlier filter scales quadratically number matches <eos> validate accuracy solver demonstrate usefulness method real world settings <eos> <eop> hidden layer perceptual learning <eos> studies visual perceptual learning investigate way human performance improves practice context relatively simple therefore more manageable visual tasks <eos> building powerful tools currently available training convolution neural network cnn network whose original architecture was inspired visual system revisited some open computational questions perceptual learning <eos> first replicated two representative set perceptual learning experiments training shallow cnn perform relevant tasks <eos> network qualitatively showed most characteristic behavior observed perceptual learning including hallmark phenomena specificity its various manifestations forms transfer partial transfer learning enabling <eos> next analyzed dynamics weight modifications network identifying patterns appeared instrumental transfer generalization learned skills one task another simulated network <eos> patterns may identify ways domain search parameter space during network re training significantly reduced thereby accomplishing knowledge transfer <eos> <eop> interponet brain inspired neural network optical flow dense interpolation <eos> sparse dense interpolation optical flow fundamental phase pipeline most leading optical flow estimation algorithms <eos> current state art method interpolation epicflow local average method based edge aware geodesic distance <eos> propose new data driven sparse dense interpolation algorithm based fully convolutional network <eos> draw inspiration filling process visual cortex introduce lateral dependencies between neurons multi layer supervision into learning process <eos> also show importance image contour learning process <eos> method robust outperforms epicflow competitive optical flow benchmarks several underlying matching algorithms <eos> leads state art performance sintel kitti benchmarks <eos> <eop> learning category specific three dimensional shape models weakly labeled image <eos> recently researchers made great processes build category specific three dimensional shape models image manual annotations consisting class labels keypoints ground truth figure ground segmentations <eos> however annotation figure ground segmentations still labor intensive time consuming <eos> further alleviate burden providing such manual annotations make earliest effort learn category specific three dimensional shape models only using weakly labeled image <eos> revealing underlying relationship between tasks common object segmentation category specific three dimensional shape reconstruction propose novel framework jointly solve two problems along cluster level learning curriculum <eos> comprehensive experiments challenging pascal voc benchmark demonstrate category specific three dimensional shape models trained using weakly supervised learning framework could some extent approach performance state art method using expensive manual segmentation annotations <eos> addition experiments also demonstrate effectiveness using three dimensional shape models helping common object segmentation <eos> <eop> zero shot learning good bad ugly <eos> due importance zero shot learning number proposed approaches increased steadily recently <eos> argue time take step back analyze status quo area <eos> purpose paper three fold <eos> first given fact there no agreed upon zero shot learning benchmark first define new benchmark unifying both evaluation protocols data splits <eos> important contribution published result often comparable sometimes even flawed due <eos> pre training zero shot test classes <eos> second compare analyze significant number state art method depth both classic zero shot setting but also more realistic generalized zero shot setting <eos> finally discuss limitations current status area taken basis advancing <eos> <eop> learning multilinear structure visual data <eos> statistical decomposition method paramount importance discovering modes variations visual data <eos> probably most prominent linear decomposition method principal component analysis pca discovers single mode variation data <eos> however practice visual data exhibit several modes variations <eos> instance appearance faces varies identity expression pose etc <eos> extract modes variations visual data several supervised method such tensorfaces rely multilinear tensor decomposition <eos> higher order svd developed <eos> main drawbacks such method they require both labels regarding modes variations same number sample under all modes variations <eos> same face under different expressions poses etc <eos> therefore their applicability limited well organised data usually captured well controlled conditions <eos> paper propose first general multilinear method best knowledge discovers multilinear structure visual data unsupervised setting <eos> without presence labels <eos> demonstrate applicability proposed method two applications namely shape shading sfs expression transfer <eos> <eop> linking image text way nets <eos> linking two data sources basic building block numerous computer vision problems <eos> canonical correlation analysis cca achieves utilizing linear optimizer order maximize correlation between two views <eos> recent work makes use non linear models including deep learning techniques optimize cca loss some feature space <eos> paper introduce novel bi directional neural network architecture task matching vectors two data sources <eos> approach employs two tied neural network channels project two views into common maximally correlated space using euclidean loss <eos> show direct link between correlation based loss euclidean loss enabling use euclidean loss correlation maximization <eos> overcome common euclidean regression optimization problems modify well known techniques problem including batch normalization dropout <eos> show state art result number computer vision matching tasks including mnist image matching sentence image matching flickr flickr coco datasets <eos> <eop> unsupervised semantic scene labeling streaming data <eos> introduce unsupervised semantic scene labeling approach continuously learns adapts semantic models discovered within data stream <eos> while closely related unsupervised video segmentation algorithm designed early video processing strategy produces coherent over segmentations but instead directly learn higher level semantic concepts <eos> achieved ensemble based approach each learner clusters data local window data stream <eos> overlapping local windows processed encoded graph structure create label mapping across windows reconcile labelings reduce unsupervised learning noise <eos> additionally iteratively learn merging threshold criteria observed data similarities automatically determine number learned labels without human provided parameters <eos> experiments show approach semantically labels video streams high degree accuracy achieves better balance under over segmentation entropy than existing video segmentation algorithms given similar numbers label outputs <eos> <eop> fast fast automatic thumbnail generation using deep neural network <eos> fast automatic thumbnail generation system based deep neural network <eos> fully convolutional deep neural network learns specific filters thumbnails different sizes aspect ratios <eos> during inference appropriate filter selected depending dimensions target thumbnail <eos> unlike most previous work fast utilize saliency but addresses problem directly <eos> addition eliminates need conduct region search over saliency map <eos> model generalizes thumbnails different sizes including extreme aspect ratios generate thumbnails real time <eos> data set more than thumbnail annotations was collected train fast <eos> show competitive result comparison existing techniques <eos> <eop> point cloud registration localization using deep neural network auto encoder <eos> present algorithm registration between large scale point cloud close proximity scanned point cloud providing localization solution fully independent prior information about initial positions two point cloud coordinate systems <eos> algorithm denoted lorax selects super point local subsets point describes geometric structure each low dimensional descriptor <eos> descriptors then used infer potential matching region efficient coarse registration process followed fine tuning stage <eos> set super point selected covering point clouds overlapping spheres then filtering out low quality nonsalient region <eos> descriptors computed using state art unsupervised machine learning utilizing technology deep neural network based auto encoders <eos> abstract novel framework provides strong alternative common practice using manually designed key point descriptors coarse point cloud registration <eos> utilizing super point instead key point allows available geometrical data better exploited find correct transformation <eos> encoding local three dimensional geometric structures using deep neural network auto encoder instead traditional descriptors continues trend seen other computer vision applications indeed leads superior result <eos> algorithm tested challenging point cloud registration datasets its advantages over previous approaches well its robustness density changes noise missing data shown <eos> <eop> improved stereo matching constant highway network reflective confidence learning <eos> present improved three step pipeline stereo matching problem introduce multiple novelties each stage <eos> propose new highway network architecture computing matching cost each possible disparity based multilevel weighted residual shortcuts trained hybrid loss supports multilevel comparison image patches <eos> novel post processing step then introduced employs second deep convolutional neural network pooling global information multiple disparities <eos> network outputs both image disparity map replaces conventional winner takes all strategy confidence prediction <eos> confidence score achieved training network new technique call reflective loss <eos> lastly learned confidence employed order better detect outliers refinement step <eos> proposed pipeline achieves state art accuracy largest most competitive stereo benchmarks learned confidence shown outperform all existing alternatives <eos> <eop> superpixels polygons using simple non iterative clustering <eos> present improved version simple linear iterative clustering slic superpixel segmentation <eos> unlike slic algorithm non iterative enforces connectivity start requires lesser memory faster <eos> relying superpixel boundaries obtained using algorithm also present polygonal partitioning algorithm <eos> demonstrate superpixels well polygonal partitioning superior respective state art algorithms quantitative benchmarks <eos> <eop> poseidon face depth driver pose estimation <eos> fast accurate upper body head pose estimation key task automatic monitoring driver attention challenging context characterized severe illumination changes occlusions extreme poses <eos> work present new deep learning framework head localization pose estimation depth image <eos> core proposal regressive neural network called poseidon composed three independent convolutional nets followed fusion layer specially conceived understanding pose depth <eos> addition recover intrinsic value face appearance understanding head position orientation propose new face depth model learning image faces depth <eos> result face reconstruction qualitatively impressive <eos> test proposed framework two public datasets namely biwi kinect head pose ict dhp pandora new challenging dataset mainly inspired automotive setup <eos> result show method overcomes all recent state art works running real time more than frames per second <eos> <eop> optical flow mostly rigid scenes <eos> optical flow natural scenes combination motion observer independent motion object <eos> existing algorithms typically focus either recovering motion structure under assumption purely static world optical flow general unconstrained scenes <eos> combine approaches optical flow algorithm estimates explicit segmentation moving object appearance physical constraints <eos> static region take advantage strong constraints jointly estimate camera motion three dimensional structure scene over multiple frames <eos> allows also regularize structure instead motion <eos> formulation uses plane parallax framework works even under small baselines reduces motion estimation one dimensional search problem resulting more accurate estimation <eos> moving region flow treated unconstrained computed existing optical flow method <eos> resulting mostly rigid flow mr flow method achieves state art result both mpi sintel kitti benchmarks <eos> <eop> photo realistic single image super resolution using generative adversarial network <eos> despite breakthroughs accuracy speed single image super resolution using faster deeper convolutional neural network one central problem remains largely unsolved how recover finer texture details when super resolve large upscaling factors behavior optimization based super resolution method principally driven choice objective function <eos> recent work largely focused minimizing mean squared reconstruction error <eos> resulting estimates high peak signal noise ratios but they often lacking high frequency details perceptually unsatisfying sense they fail match fidelity expected higher resolution <eos> paper present srgan generative adversarial network gan image super resolution sr <eos> knowledge first framework capable inferring photo realistic natural image upscaling factors <eos> achieve propose perceptual loss function consists adversarial loss content loss <eos> adversarial loss pushes solution natural image manifold using discriminator network trained differentiate between super resolved image original photo realistic image <eos> addition use content loss motivated perceptual similarity instead similarity pixel space <eos> deep residual network able recover photo realistic textures heavily downsampled image public benchmarks <eos> extensive mean opinion score mos test shows hugely significant gains perceptual quality using srgan <eos> mos scores obtained srgan closer original high resolution image than obtained any state art method <eos> <eop> roam rich object appearance model application rotoscoping <eos> rotoscoping detailed delineation scene elements through video shot painstaking task tremendous importance professional post production pipelines <eos> while pixel wise segmentation techniques help task professional rotoscoping tools rely parametric curves offer artists much better interactive control definition editing manipulation segments interest <eos> sticking prevalent rotoscoping paradigm propose novel framework capture track visual aspect arbitrary object scene given first closed outline object <eos> model combines collection local foreground background appearance models spread along outline global appearance model enclosed object set distinctive foreground landmarks <eos> structure rich appearance model allows simple initialization efficient iterative optimization exact minimization each step line adaptation video <eos> demonstrate qualitatively quantitatively merit framework through comparisons tools based either dynamic segmentation closed curve pixel wise binary labelling <eos> <eop> densely connected convolutional network <eos> recent work shown convolutional network substantially deeper more accurate efficient train if they contain shorter connections between layer close input close output <eos> paper embrace observation introduce dense convolutional network densenet connects each layer every other layer feed forward fashion <eos> whereas traditional convolutional network layer connections one between each layer its subsequent layer network direct connections <eos> each layer feature maps all preceding layer used inputs its own feature maps used inputs into all subsequent layer <eos> densenets several compelling advantages they alleviate vanishing gradient problem strengthen feature propagation encourage feature reuse substantially reduce number parameters <eos> evaluate proposed architecture four highly competitive object recognition benchmark tasks cifar cifar svhn imagenet <eos> densenets obtain significant improvements over state art most them whilst requiring less memory computation achieve high performance <eos> code pre trained models available github <eos> com liuzhuang densenet <eos> <eop> multi level attention network visual question answering <eos> inspired recent success text based question answering visual question answering vqa proposed automatically answer natural language questions reference given image <eos> compared text based qa vqa more challenging because reasoning process visual domain needs both effective semantic embedding fine grained visual understanding <eos> existing approaches predominantly infer answers abstract low level visual feature while neglecting modeling high level image semantics rich spatial context region <eos> solve challenges propose multi level attention network visual question answering simultaneously reduce semantic gap semantic attention benefit fine grained spatial inference visual attention <eos> first generate semantic concepts high level semantics convolutional neural network cnn select question related concepts semantic attention <eos> second encode region based middle level outputs cnn into spatially embedded representation bidirectional recurrent neural network further pinpoint answer related region multiple layer perceptron visual attention <eos> third jointly optimize semantic attention visual attention question embedding softmax classifier infer final answer <eos> extensive experiments show proposed approach outperforms state arts two challenging vqa datasets <eos> <eop> spatial semantic image search visual feature synthesis <eos> performance image retrieval improved tremendously recent years through use deep feature representations <eos> most existing method however aim retrieve image visually similar semantically relevant query irrespective spatial configuration <eos> paper develop spatial semantic image search technology enables users search image both semantic spatial constraints manipulating concept text boxes query canvas <eos> train convolutional neural network synthesize appropriate visual feature captures spatial semantic constraints user canvas query <eos> directly optimize retrieval performance visual feature when training deep neural network <eos> visual feature then used retrieve image both spatially semantically relevant user query <eos> experiments large scale datasets such ms coco visual genome show method outperforms other baseline state art method spatial semantic image search <eos> <eop> temporal residual network dynamic scene recognition <eos> paper combines three contributions establish new state art dynamic scene recognition <eos> first present novel convnet architecture based temporal residual units fully convolutional spacetime <eos> model augments spatial resnets convolutions across time hierarchically add temporal residuals depth network increases <eos> second existing approaches video based recognition categorized baseline seven previously top performing algorithms selected comparative evaluation dynamic scenes <eos> third introduce new challenging video database dynamic scenes more than doubles size previously available <eos> dataset explicitly split into two subsets equal size contain video without camera motion allow systematic study how variable interacts defining dynamics scene per se <eos> evaluations verify particular strengths weaknesses baseline algorithms respect various scene classes camera motion parameters <eos> finally temporal resnet boosts recognition performance establishes new state art dynamic scene recognition well complementary task action recognition <eos> <eop> radiometric calibration internet photo collections <eos> radiometrically calibrating image internet photo collections brings photometric analysis lab data big image data wild but conventional calibration method cannot directly applied such image data <eos> paper presents method jointly perform radiometric calibration set image internet photo collection <eos> incorporating consistency scene reflectance corresponding pixels multiple image proposed method estimates radiometric response functions all image using rank minimization framework <eos> calibration aligns all response functions image set up same exponential ambiguity robust manner <eos> quantitative result using both synthetic real data show effectiveness proposed method <eos> <eop> see forest trees joint spatial temporal recurrent neural network video based person re identification <eos> surveillance cameras widely used different scenes <eos> accordingly demanding need recognize person under different cameras called person re identification <eos> topic gained increasing interests computer vision recently <eos> however less attention paid video based approaches compared image based ones <eos> two steps usually involved previous approaches namely feature learning metric learning <eos> but most existing approaches only focus either feature learning metric learning <eos> meanwhile many them take full use temporal spatial information <eos> paper concentrate video based person re identification build end end deep neural network architecture jointly learn feature metrics <eos> proposed method automatically pick out most discriminative frames given video temporal attention model <eos> moreover integrates surrounding information each location spatial recurrent model when measuring similarity another pedestrian video <eos> method handles spatial temporal information simultaneously unified manner <eos> carefully designed experiments three public datasets show effectiveness each component proposed deep network performing better comparison state art method <eos> <eop> procedural generation video train deep action recognition network <eos> deep learning human action recognition video making significant progress but slowed down its dependency expensive manual labeling large video collections <eos> work investigate generation synthetic training data action recognition recently shown promising result variety other computer vision tasks <eos> propose interpretable parametric generative model human action video relies procedural generation other computer graphics techniques modern game engines <eos> generate diverse realistic physically plausible dataset human action video called phav procedural human action video <eos> contains total video more than examples each action categories <eos> approach limited existing motion capture sequences procedurally define synthetic actions <eos> introduce deep multi task representation learning architecture mix synthetic real video even if action categories differ <eos> experiments ucf hmdb benchmarks suggest combining large set synthetic video small real world datasets boost recognition performance significantly outperforming fine tuning state art unsupervised generative models video <eos> <eop> spatiotemporal multiplier network video action recognition <eos> paper presents general convnet architecture video action recognition based multiplicative interactions spacetime feature <eos> model combines appearance motion pathways two stream architecture motion gating trained end end <eos> theoretically motivate multiplicative gating functions residual network empirically study their effect classification accuracy <eos> capture long term dependencies inject identity mapping kernels learning temporal relationships <eos> architecture fully convolutional spacetime able evaluate video single forward pass <eos> empirical investigation reveals model produces state art result two standard action recognition datasets <eos> <eop> real time video super resolution spatio temporal network motion compensation <eos> convolutional neural network enabled accurate image super resolution real time <eos> however recent attempts benefit temporal correlations video super resolution limited naive inefficient architectures <eos> paper introduce spatio temporal sub pixel convolution network effectively exploit temporal redundancies improve reconstruction accuracy while maintaining real time speed <eos> specifically discuss use early fusion slow fusion three dimensional convolutions joint processing multiple consecutive video frames <eos> also propose novel joint motion compensation video super resolution algorithm orders magnitude more efficient than competing method relying fast multi resolution spatial transformer module end end trainable <eos> contributions provide both higher accuracy temporally more consistent video confirm qualitatively quantitatively <eos> relative single frame models spatio temporal network either reduce computational cost whilst maintaining same quality provide <eos> db gain similar computational cost <eos> result publicly available datasets demonstrate proposed algorithms surpass current state art performance both accuracy efficiency <eos> <eop> query focused video summarization dataset evaluation memory network based approach <eos> recent years witnessed resurgence interest video summarization <eos> however one main obstacles research video summarization user subjectivity users various preferences over summaries <eos> subjectiveness causes least two problems <eos> first no single video summarizer fits all users unless interacts adapts individual users <eos> second very challenging evaluate performance video summarizer <eos> tackle first problem explore recently proposed query focused video summarization introduces user preferences form text queries about video into summarization process <eos> propose memory network parameterized sequential determinantal point process order attend user query onto different video frames shots <eos> address second challenge contend good evaluation metric video summarization should focus semantic information humans perceive rather than visual feature temporal overlaps <eos> end collect dense per video shot concept annotations compile new dataset suggest efficient evaluation method defined upon concept annotations <eos> conduct extensive experiments contrasting video summarizer existing ones present detailed analyses about dataset new evaluation method <eos> <eop> new rank constraint multi view fundamental matrices its application camera location recovery <eos> accurate estimation camera matrices important step structure motion algorithms <eos> paper introduce novel rank constraint collections fundamental matrices multi view settings <eos> show general selection proper scale factors matrix formed stacking fundamental matrices between pairs image rank <eos> moreover matrix forms symmetric part rank matrix whose factors relate directly corresponding camera matrices <eos> use new characterization produce better estimations fundamental matrices optimizing cost function using iterative re weighted least squares alternate direction method multiplier <eos> further show procedure improve recovery camera locations particularly multi view settings fewer image available <eos> <eop> attentional correlation filter network adaptive visual tracking <eos> propose new tracking framework attentional mechanism chooses subset associated correlation filters increased robustness computational efficiency <eos> subset filters adaptively selected deep attentional network according dynamic properties tracking target <eos> contributions manifold summarised follows introducing attentional correlation filter network allows adaptive tracking dynamic targets <eos> ii utilising attentional network shifts attention best candidate modules well predicting estimated accuracy currently inactive modules <eos> iii enlarging variety correlation filters cover target drift blurriness occlusion scale changes flexible aspect ratio <eos> iv validating robustness efficiency attentional mechanism visual tracking through number experiments <eos> method achieves similar performance non real time trackers state art performance amongst real time trackers <eos> <eop> deep mixture linear inverse regressions applied head pose estimation <eos> convolutional neural network convnets become state art many classification regression problems computer vision <eos> when comes regression approaches such measuring euclidean distance target predictions often employed output layer <eos> paper propose coupling gaussian mixture linear inverse regressions convnet describe methodological foundations associated algorithm jointly train deep network regression function <eos> test model head pose estimation problem <eos> particular problem show inverse regression outperforms regression models currently used state art computer vision method <eos> method require incorporation additional data often proposed literature thus able work well relatively small training datasets <eos> finally outperforms state art method head pose estimation using widely used head pose dataset <eos> best knowledge first incorporate inverse regression into deep learning computer vision applications <eos> <eop> human shape silhouettes using generative hks descriptors cross modal neural network <eos> work present novel method capturing human body shape single scaled silhouette <eos> combine deep correlated feature capturing different views embedding spaces based three dimensional cues novel convolutional neural network cnn based architecture <eos> first train cnn find richer body shape representation space pose invariant three dimensional human shape descriptors <eos> then learn mapping silhouettes representation space help novel architecture exploits correlation multi view data during training time improve prediction test time <eos> extensively validate result synthetic real data demonstrating significant improvements accuracy compared state art providing practical system detailed human body measurements single image <eos> <eop> std rgbd semantic segmentation using spatio temporal data driven pooling <eos> propose novel superpixel based multi view convolutional neural network semantic image segmentation <eos> proposed network produces high quality segmentation single image leveraging information additional views same scene <eos> particularly indoor video such captured robotic platforms handheld bodyworn rgbd cameras nearby video frames provide diverse viewpoints additional context object scenes <eos> leverage such information first compute region correspondences optical flow image boundary based superpixels <eos> given region correspondences propose novel spatio temporal pooling layer aggregate information over space time <eos> evaluate approach nyu depth sun datasets compare various state art single view multi view approaches <eos> besides general improvement over state art also show benefits making use unlabeled frames during training multi view well single view prediction <eos> <eop> two view geometry unsynchronized cameras <eos> present new method simultaneously estimating camera geometry time shift video sequences multiple unsynchronized cameras <eos> algorithms simultaneous computation fundamental matrix homography unknown time shift between image developed <eos> method use minimal correspondence set eight fundamental matrix four half homography therefore suitable robust estimation using ransac <eos> furthermore present iterative algorithm extends applicability sequences significantly unsynchronized finding correct time shift up several seconds <eos> evaluated method synthetic wide range real world datasets result show broad applicability problem camera synchronization <eos> <eop> using locally corresponding cad models dense three dimensional reconstructions single image <eos> investigate problem estimating dense three dimensional shape object given set landmarks silhouette single image <eos> obvious prior employ such problem dictionary dense cad models <eos> employing sufficiently large enough dictionary cad models however general computationally infeasible <eos> common strategy dictionary learning encourage generalization allow linear combinations dictionary elements <eos> too however problematic most cad models cannot readily placed global dense correspondence <eos> paper propose two step strategy <eos> first employ orthogonal matching pursuit rapidly choose closest single cad model dictionary projected image <eos> second employ novel graph embedding based local dense correspondence allow sparse linear combinations cad models <eos> validate framework experimentally both synthetic real world scenario demonstrate superiority approach both three dimensional mesh reconstruction volumetric representation <eos> <eop> bighand benchmark hand pose dataset state art analysis <eos> paper introduce large scale hand pose <eop> matrix splitting method composite function minimization <eos> composite function minimization captures wide spectrum applications both computer vision machine learning <eos> includes bound constrained optimization cardinality regularized optimization special cases <eos> paper proposes analyzes new matrix splitting method msm minimizing composite functions <eos> viewed generalization classical gauss seidel method successive over relaxation method solving linear systems literature <eos> incorporating new gaussian elimination procedure matrix splitting method achieves state art performance <eos> convex problems establish global convergence convergence rate iteration complexity msm while non convex problems prove its global convergence <eos> finally validate performance matrix splitting method two particular applications nonnegative matrix factorization cardinality regularized sparse coding <eos> extensive experiments show method outperforms existing composite function minimization techniques term both efficiency efficacy <eos> <eop> simultaneous geometric radiometric calibration projector camera pair <eos> present novel method allows simultaneous geometric radiometric calibration projector camera pair <eos> simple require specialized hardware <eos> prewarp align specially designed projection pattern onto printed pattern different colorimetric properties <eos> after capturing patterns several orientations perform geometric calibration estimating corner locations two patterns different color channels <eos> perform radiometric calibration projector using information contained inside projected squares <eos> show method performs par current approaches all require separate geometric radiometric calibration while being more efficient user friendly <eos> <eop> global geometry sphere constrained sparse blind deconvolution <eos> blind deconvolution problem recovering convolutional kernel activation signal their convolution <eos> problem ill posed without further constraints priors <eos> paper studies situation nonzero entries activation signal sparsely randomly populated <eos> normalize convolution kernel unit frobenius norm cast sparse blind deconvolution problem nonconvex optimization problem over sphere <eos> spherical constraint every spurious local minimum turns out close some signed shift truncation ground truth under certain hypotheses <eos> benign property motivates effective two stage algorithm recovers ground truth partial information offered suboptimal local minimum <eos> geometry inspired algorithm recovers ground truth certain microscopy problems also exhibits promising performance more challenging image deblurring problem <eos> insights into global geometry two stage algorithm extend convolutional dictionary learning problem superposition multiple convolution signals observed <eos> <eop> towards accurate multi person pose estimation wild <eos> propose method multi person detection pose estimation achieves state art result challenging coco keypoints task <eos> simple yet powerful top down approach consisting two stages <eos> first stage predict location scale boxes likely contain people use faster rcnn detector <eos> second stage estimate keypoints person potentially contained each proposed bounding box <eos> each keypoint type predict dense heatmaps offsets using fully convolutional resnet <eos> combine outputs introduce novel aggregation procedure obtain highly localized keypoint predictions <eos> also use novel form keypoint based non maximum suppression nms instead cruder box level nms novel form keypoint based confidence score estimation instead box level scoring <eos> trained coco data alone final system achieves average precision <eos> coco test dev set <eos> test standard set outperforming winner coco keypoints challenge other recent state art <eos> further using additional house labeled data obtain even higher average precision <eos> test dev set <eos> test standard set more than absolute improvement compared previous best performing method same dataset <eos> <eop> clever elimination strategy efficient minimal solvers <eos> present new insight into systematic generation minimal solvers computer vision leads smaller faster solvers <eos> many minimal problem formulations coupled set linear polynomial equations image measurements enter linear equations only <eos> show useful solve such systems first eliminating all unknowns appear linear equations then extending solutions rest unknowns <eos> generalized fully non linear systems linearization via lifting <eos> demonstrate approach leads more efficient solvers three problems partially calibrated relative camera pose computation unknown focal length radial distortion <eos> approach also generates new interesting constraints fundamental matrices partially calibrated cameras were known before <eos> <eop> adaptive move making auxiliary cuts binary pairwise energies <eos> many computer vision problems require optimization binary non submodular energies <eos> context iterative submodularization techniques based trust region lsa tr auxiliary functions lsa aux recently proposed <eos> they achieve state art result number computer vision applications <eos> paper extend lsa aux framework two directions <eos> first unlike lsa aux selects auxiliary functions based solely current solution propose incorporate several additional criteria <eos> result tighter bounds configurations more likely closer current solution <eos> second propose move making extensions lsa aux achieve tighter bounds restricting search space <eos> finally evaluate method several applications <eos> show each application least one extensions significantly outperforms original lsa aux <eos> moreover best extension lsa aux comparable better than lsa tr five out six applications achieving state arts result four out six applications <eos> <eop> space attenuation coefficients underwater computer vision <eos> underwater image reconstruction method require knowledge wideband attenuation coefficients per color channel <eos> current estimation method coefficients require specialized hardware multiple image none them leverage multitude existing ocean optical measurements priors <eos> here aim constrain set physically feasible wideband attenuation coefficients ocean utilizing water attenuation measured worldwide oceanographers <eos> calculate space valid wideband effective attenuation coefficients three dimensional rgb domain find bound manifold space sufficiently represents variation clearest murkiest waters <eos> validate model using situ experiments two different optical water bodies red sea mediterranean <eos> moreover show contradictory common image formation model coefficients depend imaging range object reflectance quantify errors resulting ignoring dependencies <eos> <eop> consensus maximization linear matrix inequality constraints <eos> consensus maximization proven useful tool robust estimation <eos> while randomized method like ransac fast they guarantee global optimality fail manage large amounts outliers <eos> other hand global method commonly slow because they exploit structure problem hand <eos> paper show solution space reduced introducing linear matrix inequality lmi constraints <eos> leads significant speed ups optimization time even large amounts outliers while maintaining global optimality <eos> study several cases objective variables special structure such rotation scaled rotation essential matrices posed lmi constraints <eos> very useful several standard computer vision problems such estimating similarity transformations absolute poses relative poses obtain compelling result both synthetic real datasets <eos> up percent outlier rate ransac often fails constrained approach consistently faster than non constrained one while finding same global solution <eos> <eop> optical flow requires multiple strategies but only one network <eos> show matching problem underlies optical flow requires multiple strategies depending amount image motion other factors <eos> then study implications observation training deep neural network representing image patches context descriptor based optical flow <eos> propose metric learning method selects suitable negative sample based nature true match <eos> type training produces network displays multiple strategies depending input leads state art result kitti kitti optical flow benchmarks <eos> <eop> convex global three dimensional registration lagrangian duality <eos> registration three dimensional models euclidean transformation fundamental task core many application computer vision <eos> problem non convex due presence rotational constraints making traditional local optimization method prone getting stuck local minima <eos> paper addresses finding globally optimal transformation various three dimensional registration problems unified formulation integrates common geometric registration modalities namely point point point line point plane <eos> formulation renders optimization problem independent both number nature correspondences <eos> main novelty proposal introduction strengthened lagrangian dual relaxation problem surpasses previous similar approaches effectiveness <eos> fact even though no theoretical guarantees exhaustive empirical evaluation both synthetic real experiments always resulted tight relaxation allowed recover guaranteed globally optimal solution exploiting duality theory <eos> thus approach allows effectively solving three dimensional registration global optimality guarantees while running fraction time state art alternative based more computationally intensive branch bound method <eos> eriksson solving quadratically constrained geometrical problems using lagrangian duality proc <eos> pattern recognition icpr pp <eos> branch bound method euclidean registration problems ieee trans <eos> <eop> pool pooling stochastic spatial sampling <eos> feature pooling layer <eos> max pooling convolutional neural network cnn serve dual purpose providing increasingly abstract representations well yielding computational savings subsequent convolutional layer <eos> view pooling operation cnn two step procedure first pooling window <eos> slides over feature map stride one leaves spatial resolution intact second downsampling performed selecting one pixel each non overlapping pooling window often uniform deterministic <eos> top left manner <eos> starting point work observation regularly spaced downsampling arising non overlapping windows although intuitive signal processing perspective goal signal reconstruction necessarily optimal learning goal generalize <eos> study aspect propose novel pooling strategy stochastic spatial sampling pool regular downsampling replaced more general stochastic version <eos> observe general stochasticity acts strong regularizer also seen doing implicit data augmentation introducing distortions feature maps <eos> further introduce mechanism control amount distortion suit different datasets architectures <eos> demonstrate effectiveness proposed approach perform extensive experiments several popular image classification benchmarks observing excellent improvements over baseline models <eos> <eop> generating descriptions grounded co referenced people <eos> learning how generate descriptions image video received major interest both computer vision natural language processing communities <eos> while few works proposed learn grounding during generation process unsupervised way via attention mechanism remains unclear how good quality grounding whether benefits description quality <eos> work propose movie description model learns generate description jointly ground localize mentioned characters well visual co reference resolution between pairs consecutive sentences clips <eos> also propose use weak localization supervision through character mentions provided movie descriptions learn character grounding <eos> training time first learn how localize characters relating their visual appearance mentions descriptions via semi supervised approach <eos> then provide noisy supervision into description model greatly improves its performance <eos> proposed description model improves over prior work <eos> generated description quality additionally provides grounding local co reference resolution <eos> evaluate mpii movie description dataset using automatic human evaluation measures using newly collected grounding co reference data characters <eos> <eop> deep photo style transfer <eos> paper introduces deep learning approach photographic style transfer handles large variety image content while faithfully transferring reference style <eos> approach builds upon recent work painterly transfer separates style content image considering different layer neural network <eos> however approach suitable photorealistic style transfer <eos> even when both input reference image photographs output still exhibits distortions reminiscent painting <eos> contribution constrain transformation input output locally affine colorspace express constraint custom fully differentiable energy term <eos> show approach successfully suppresses distortion yields satisfying photorealistic style transfers broad variety scenarios including transfer time day weather season artistic edits <eos> <eop> you smarter than sixth grader textbook question answering multimodal machine comprehension <eos> introduce task multi modal machine comprehension aims answering multimodal questions given context text diagrams image <eos> present textbook question answering tqa dataset includes lessons multi modal questions taken middle school science curricula <eos> analysis shows significant portion questions require complex parsing text diagrams reasoning indicating dataset more complex compared previous machine comprehension visual question answering datasets <eos> extend state art method textual machine comprehension visual question answering tqa dataset <eos> experiments show models perform well tqa <eos> presented dataset opens new challenges research question answering reasoning across multiple modalities <eos> <eop> instancecut edges instances multicut <eos> work addresses task instance aware semantic segmentation <eos> key motivation design simple method new modelling paradigm therefore different trade off between advantages disadvantages compared known approaches <eos> approach term instancecut represents problem two output modalities instance agnostic semantic segmentation ii all instance boundaries <eos> former computed standard convolutional neural network semantic segmentation latter derived new instance aware edge detection model <eos> reason globally about optimal partitioning image into instances combine two modalities into novel multicut formulation <eos> evaluate approach challenging cityscapes dataset <eos> despite conceptual simplicity approach achieve best result among all published method perform particularly well rare object classes <eos> <eop> deep hashing network unsupervised domain adaptation <eos> recent years deep neural network emerged dominant machine learning tool wide variety application domains <eos> however training deep neural network requires large amount labeled data expensive process terms time labor human expertise <eos> domain adaptation transfer learning algorithms address challenge leveraging labeled data different but related source domain develop model target domain <eos> further explosive growth digital data posed fundamental challenge concerning its storage retrieval <eos> due its storage retrieval efficiency recent years witnessed wide application hashing variety computer vision applications <eos> paper first introduce new dataset office home evaluate domain adaptation algorithms <eos> dataset contains image variety everyday object multiple domains <eos> then propose novel deep learning framework exploit labeled source data unlabeled target data learn informative hash codes accurately classify unseen target data <eos> best knowledge first research effort exploit feature learning capabilities deep neural network learn representative hash codes address domain adaptation problem <eos> extensive empirical studies multiple transfer tasks corroborate usefulness framework learning efficient hash codes outperform existing competitive baselines unsupervised domain adaptation <eos> <eop> harmonic network deep translation rotation equivariance <eos> translating rotating input image should affect result many computer vision tasks <eos> convolutional neural network cnn already translation equivariant input image translations produce proportionate feature map translations <eos> global rotation equivariance typically sought through data augmentation but patch wise equivariance more difficult <eos> present harmonic network nets cnn exhibiting equivariance patch wise translation rotation <eos> achieve replacing regular cnn filters circular harmonics returning maximal response orientation every receptive field patch <eos> nets use rich parameter efficient fixed computational complexity representation show deep feature maps within network encode complicated rotational invariants <eos> demonstrate layer general enough used conjunction latest architectures techniques such deep supervision batch normalization <eos> also achieve state art classification rotated mnist competitive result other benchmark challenges <eos> <eop> demon depth motion network learning monocular stereo <eos> paper formulate structure motion learning problem <eos> train convolutional network end end compute depth camera motion successive unconstrained image pairs <eos> architecture composed multiple stacked encoder decoder network core part being iterative network able improve its own predictions <eos> network estimates only depth motion but additionally surface normals optical flow between image confidence matching <eos> crucial component approach training loss based spatial relative differences <eos> compared traditional two frame structure motion method result more accurate more robust <eos> contrast popular depth single image network demon learns concept matching thus better generalizes structures seen during training <eos> <eop> wide field view monocentric light field camera <eos> light field lf capture processing important expanding range computer vision applications offering rich textural depth information simplification conventionally complex tasks <eos> although lf cameras commercially available no existing device offers wide field view fov imaging <eos> due part limitations fisheye lenses fundamentally constrained entrance pupil diameter severely limits depth sensitivity <eos> work describe novel compact optical design couples monocentric lens multiple sensors using microlens arrays allowing lf capture unprecedented fov <eos> leveraging capabilities lf representation propose novel method efficiently coupling spherical lens planar sensors replacing expensive bulky fiber bundles <eos> construct single sensor lf camera prototype rotating sensor relative fixed main lens emulate wide fov multi sensor scenario <eos> finally describe processing toolchain including convenient spherical lf parameterization demonstrate depth estimation post capture refocus indoor outdoor panoramas pixels mpix degree fov <eos> <eop> teaching compositionality cnn <eos> convolutional neural network cnn shown great success computer vision approaching human level performance when trained specific tasks via application specific loss functions <eos> paper propose method augmenting training cnn so their learned feature compositional <eos> encourages network form representations disentangle object their surroundings each other thereby promoting better generalization <eos> method agnostic specific de tails underlying cnn applied principle used any cnn <eos> show experiments learned representations lead feature activations more localized improve performance over non compositional baselines object recognition tasks <eos> <eop> learning barycentric representations three dimensional shapes sketch based three dimensional shape retrieval <eos> retrieving three dimensional shapes sketches challenging problem since sketches three dimensional shapes two heterogeneous domains result large discrepancy between them <eos> paper propose learn barycenters projections three dimensional shapes sketch based three dimensional shape retrieval <eos> specifically first use two deep convolutional neural network cnn extract deep feature sketches projections three dimensional shapes <eos> three dimensional shapes then compute wasserstein barycenters deep feature multiple projections form barycentric representation <eos> finally constructing metric network discriminative loss formulated wasserstein barycenters three dimensional shapes sketches deep feature space learn discriminative compact three dimensional shape sketch feature retrieval <eos> proposed method evaluated shrec shrec sketch track benchmark datasets <eos> compared state art method proposed method significantly improve retrieval performance <eos> <eop> stacked generative adversarial network <eos> paper propose novel generative model named stacked generative adversarial network sgan trained invert hierarchical representations bottom up discriminative network <eos> model consists top down stack gans each learned generate lower level representations conditioned higher level representations <eos> representation discriminator introduced each feature hierarchy encourage representation manifold generator align bottom up discriminative network leveraging powerful discriminative representations guide generative model <eos> addition introduce conditional loss encourages use conditional information layer above novel entropy loss maximizes variational lower bound conditional entropy generator outputs <eos> first train each stack independently then train whole model end end <eos> unlike original gan uses single noise vector represent all variations sgan decomposes variations into multiple levels gradually resolves uncertainties top down generative process <eos> based visual inspection inception scores visual turing test demonstrate sgan able generate image much higher quality than gans without stacking <eos> <eop> image splicing detection via camera response function analysis <eos> recent advances image manipulation techniques made image forgery detection increasingly more challenging <eos> important component such tools fake motion defocus blurs through boundary splicing copy move operators emulate wide aperture slow shutter effects <eos> paper present new technique based analysis camera response functions crf efficient robust splicing copy move forgery detection localization <eos> first analyze how non linear crfs affect edges terms intensity gradient bivariable histograms <eos> show distinguishable shape differences real vs <eos> forged blurs near edges after splicing operation <eos> based analysis introduce deep learning framework detect localize forged edges <eos> particular show problem transformed handwriting recognition problem resolved using convolutional neural network <eos> generate large dataset forged image produced splicing followed retouching comprehensive experiments show proposed method outperforms state art techniques accuracy robustness <eos> <eop> illuminant camera communication observe moving object under strong external light spread spectrum modulation <eos> many algorithms computer vision use light sources illuminate object actively create situation appropriate extract their characteristics <eos> example shape reflectance measured projector camera system some human machine vr systems use projectors displays interaction <eos> existing active lighting systems usually assume no severe external lights observe projected lights clearly one limitations active illumination <eos> paper propose method energy efficient active illumination environment severe external lights <eos> proposed method extracts light signals illuminants removing external light using spread spectrum modulation <eos> because image sequence needed observe modulated signals proposed method extends signal processing realize signal detection projected onto moving object combining spread spectrum modulation spatio temporal filtering <eos> experiments apply proposed method structured light system under sunlight photometric stereo external lights insensible image embedding <eos> <eop> building regular decision boundary deep network <eos> work build generic architecture convolutional neural network discover empirical properties neural network <eos> first contribution introduce state art framework depends upon few hyper parameters study network when vary them <eos> no max pooling no biases only layer purely convolutional yields up <eos> accuracy respectively cifar cifar <eos> show nonlinearity deep network need continuous non expansive point wise achieve good performance <eos> show increasing width network permits being competitive very deep network <eos> second contribution analysis contraction separation properties network <eos> indeed nearest neighbor classifier applied deep feature progressively improves depth indicates representation progressively more regular <eos> besides defined analyzed local support vectors separate classes locally <eos> all experiments reproducible code will available online based tensorflow <eos> <eop> geometric deep learning graphs manifolds using mixture model cnn <eos> deep learning achieved remarkable performance breakthrough several fields most notably speech recognition natural language processing computer vision <eos> particular convolutional neural network cnn architectures currently produce state art performance variety image analysis tasks such object detection recognition <eos> most deep learning research so far focused dealing three dimensional euclidean structured data such acoustic signals image video <eos> recently there increasing interest geometric deep learning attempting generalize deep learning method non euclidean structured data such graphs manifolds variety applications domains network analysis computational social science computer graphics <eos> paper propose unified framework allowing generalize cnn architectures non euclidean domains graphs manifolds learn local stationary compositional task specific feature <eos> show various non euclidean cnn method previously proposed literature considered particular instances framework <eos> test proposed method standard tasks realms image graph three dimensional shape analysis show consistently outperforms previous approaches <eos> <eop> identifying first person camera wearers third person video <eos> consider scenarios wish perform joint scene understanding object tracking activity recognition other tasks scenarios multiple people wearing body worn cameras while third person static camera also captures scene <eos> need establish person level correspondences across first third person video challenging because camera wearer visible his her own egocentric video preventing use direct feature matching <eos> paper propose new semi siamese convolutional neural network architecture address novel challenge <eos> formulate problem learning joint embedding space first third person video considers both spatial motion domain cues <eos> new triplet loss function designed minimize distance between correct first third person matches while maximizing distance between incorrect ones <eos> end end approach performs significantly better than several baselines part learning first third person feature optimized matching jointly distance measure itself <eos> <eop> im cad <eos> given single photo room large database furniture cad models goal reconstruct scene similar possible scene depicted photograph composed object drawn database <eos> present completely automatic system address im cad problem produces high quality result challenging imagery interior home design remodeling websites <eos> approach iteratively optimizes placement scale object room best match scene renderings input photo using image comparison metrics trained via deep convolutional neural nets <eos> operating jointly full scene once account inter object occlusions <eos> also show applicability method standard scene understanding benchmarks obtain significant improvement <eos> <eop> photorealistic facial texture inference using deep neural network <eos> present data driven inference method synthesize photorealistic texture map complete three dimensional face model given partial view person wild <eos> after initial estimation shape low frequency albedo compute high frequency partial texture map without shading component visible face area <eos> extract fine appearance details incomplete input introduce multi scale detail analysis technique based mid layer feature correlations extracted deep convolutional neural network <eos> demonstrate fitting convex combination feature correlations high resolution face database yield semantically plausible facial detail description entire face <eos> complete photorealistic texture map then synthesized iteratively optimizing reconstructed feature correlations <eos> using high resolution textures commercial rendering framework produce high fidelity three dimensional renderings visually comparable obtained state art multi view face capture systems <eos> demonstrate successful face reconstructions wide range low resolution input image including historical figures <eos> addition extensive evaluations validate realism result using crowdsourced user study <eos> <eop> learning learn noisy web video <eos> understanding simultaneously very diverse intricately fine grained set possible human actions critical open problem computer vision <eos> manually labeling training video feasible some action classes but doesn scale full long tailed distribution actions <eos> promising way address leverage noisy data web queries learn new actions using semi supervised webly supervised approaches <eos> however method typically learn domain specific knowledge rely iterative hand tuned data labeling policies <eos> work instead propose reinforcement learning based formulation selecting right examples training classifier noisy web search result <eos> method uses learning learn data labeling policy small labeled training dataset then uses automatically label noisy web data new visual concepts <eos> experiments challenging sports action recognition benchmark well additional fine grained newly emerging action classes demonstrate method able learn good labeling policies noisy data use learn accurate visual concept classifiers <eos> <eop> regressing robust discriminative three dimensional morphable models very deep neural network <eos> three dimensional shapes faces well known discriminative <eos> yet despite they rarely used face recognition always under controlled viewing conditions <eos> claim symptom serious but often overlooked problem existing method single view three dimensional face reconstruction when applied wild their three dimensional estimates either unstable change different photos same subject they over regularized generic <eos> response describe robust method regressing discriminative three dimensional morphable face models dmm <eos> use convolutional neural network cnn regress dmm shape texture parameters directly input photo <eos> overcome shortage training data required purpose offering method generating huge numbers labeled examples <eos> three dimensional estimates produced cnn surpass state art accuracy micc data set <eos> coupled three dimensional face matching pipeline show first competitive face recognition result lfw ytf ijb benchmarks using three dimensional face shapes representations rather than opaque deep feature vectors used other modern systems <eos> <eop> hpatches benchmark evaluation handcrafted learned local descriptors <eos> paper propose novel benchmark evaluating local image descriptors <eos> demonstrate existing datasets evaluation protocols specify unambiguously all aspects evaluation leading ambiguities inconsistencies result reported literature <eos> furthermore datasets nearly saturated due recent improvements local descriptors obtained learning them large annotated datasets <eos> therefore introduce new large dataset suitable training testing modern descriptors together strictly defined evaluation protocols several tasks such matching retrieval classification <eos> allows more realistic thus more reliable comparisons different application scenarios <eos> evaluate performance several state art descriptors analyse their properties <eos> show simple normalisation traditional hand crafted descriptors boost their performance level deep learning based descriptors within realistic benchmarks evaluation <eos> <eop> using ranking cnn age estimation <eos> human age considered important biometric trait human identification search <eos> recent research shows aging feature deeply learned large scale data lead significant performance improvement facial image based age estimation <eos> however age related ordinal information totally ignored approaches <eos> paper propose novel convolutional neural network cnn based framework ranking cnn age estimation <eos> ranking cnn contains series basic cnn each trained ordinal age labels <eos> then their binary outputs aggregated final age prediction <eos> theoretically obtain much tighter error bound ranking based age estimation <eos> moreover rigorously prove ranking cnn more likely get smaller estimation errors when compared multi class classification approaches <eos> through extensive experiments show statistically ranking cnn significantly outperforms other state art age estimation models benchmark datasets <eos> <eop> deepnav learning navigate large cities <eos> present deepnav convolutional neural network cnn based algorithm navigating large cities using locally visible street view image <eos> deepnav agent learns reach its destination quickly making correct navigation decisions intersections <eos> collect large scale dataset street view image organized graph nodes connected roads <eos> dataset contains city graphs total more than million street view image <eos> propose supervised learning approaches navigation task show how search city graph used generate labels image <eos> annotation process fully automated using publicly available mapping services requires no human input <eos> evaluate proposed deepnav models held out cities navigating different types destinations show algorithms outperform previous work uses hand crafted feature support vector regression svr <eos> <eop> world fast moving object <eos> notion fast moving object fmo <eos> object moves over distance exceeding its size within exposure time introduced <eos> fmos may typically rotate high angular speed <eos> fmos very common sports video but rare elsewhere <eos> single frame such object often barely visible appear semitransparent streaks <eos> method detection tracking fmos proposed <eos> method consists three distinct algorithms form efficient localization pipeline operates successfully broad range conditions <eos> show possible recover appearance object its axis rotation despite its blurred appearance <eos> proposed method evaluated new annotated dataset <eos> result show existing trackers inadequate problem fmo localization new approach required <eos> two applications localization temporal superresolution highlighting presented <eos> <eop> sports field localization via deep structured models <eos> work propose novel way efficiently localizing sports field single broadcast image game <eos> related work area relies manually annotating few key frames extending localization similar image installing fixed specialized cameras stadium layout field obtained <eos> contrast formulate problem branch bound inference markov random field energy function defined terms semantic cues such field surface lines circles obtained deep semantic segmentation network <eos> moreover approach fully automatic depends only single image broadcast video game <eos> demonstrate effectiveness method applying soccer hockey <eos> <eop> deep watershed transform instance segmentation <eos> most contemporary approaches instance segmentation use complex pipelines involving conditional random fields recurrent neural network object proposals template matching schemes <eos> paper present simple yet powerful end end convolutional neural network tackle task <eos> approach combines intuitions classical watershed transform modern deep learning produce energy map image object instances unambiguously represented energy basins <eos> then perform cut single energy level directly yield connected components corresponding object instances <eos> model achieves more than double performance over state art challenging cityscapes instance level segmentation task <eos> <eop> annotating object instances polygon rnn <eos> paper propose approach semi automatic annotation object instances <eos> while most current method treat object segmentation pixel labeling problem here cast polygon prediction task mimicking how most current datasets annotated <eos> particular approach takes input image crop produces vertex polygon one time allowing human annotator interfere any time correct point <eos> model easily integrates any correction producing accurate segmentations desired annotator <eos> show annotation method speeds up annotation process factor <eos> across all classes while achieving <eos> agreement iou original ground truth matching typical agreement between human annotators <eos> cars speed up factor even higher <eos> further show generalization capabilities approach unseen datasets <eos> <eop> multimodal transfer hierarchical deep convolutional neural network fast artistic style transfer <eos> transferring artistic styles onto everyday photographs become extremely popular task both academia industry <eos> recently offline training replaced online iterative optimization enabling nearly real time stylization <eos> when stylization network applied directly high resolution image however style localized region often appears less similar desired artistic style <eos> because transfer process fails capture small intricate textures maintain correct texture scales artworks <eos> here propose multimodal convolutional neural network takes into consideration faithful representations both color luminance channels performs stylization hierarchically multiple losses increasing scales <eos> compared state art network network also perform style transfer nearly real time performing much more sophisticated training offline <eos> properly handling style texture cues multiple scales using several modalities transfer just large scale obvious style cues but also subtle exquisite ones <eos> scheme generate result visually pleasing more similar multiple desired artistic styles color texture cues multiple scales <eos> <eop> detect replace refine deep structured prediction pixel wise labeling <eos> pixel wise image labeling interesting challenging problem great significance computer vision community <eos> order dense labeling algorithm able achieve accurate precise result consider dependencies exist joint space both input output variables <eos> implicit approach modeling dependencies training deep neural network given input initial estimate output labels input image will able predict new refined estimate labels <eos> context work concerned optimal architecture performing label improvement task <eos> argue prior approaches either directly predicting new label estimates predicting residual corrections <eos> initial labels feed forward deep network architectures sub optimal <eos> instead propose generic architecture decomposes label improvement task three steps detecting initial label estimates incorrect replacing incorrect labels new ones finally refining renewed labels predicting residual corrections <eos> furthermore explore compare various other alternative architectures consist aforementioned detection replace refine components <eos> extensively evaluate examined architectures challenging task dense disparity estimation stereo matching report both quantitative qualitative result three different datasets <eos> finally dense disparity estimation network implements proposed generic architecture achieves state art result kitti test surpassing prior approaches significant margin <eos> plan release torch code implements paper github <eos> com gidariss drr struct pred <eos> <eop> grassmannian manifold optimization assisted sparse spectral clustering <eos> spectral clustering one pioneered clustering method machine learning pattern recognition field <eos> relies spectral decomposition criterion learn low dimensonal embedding data basic clustering algorithm such means <eos> recent sparse spectral clustering ssc introduces sparsity similarity low dimensional space enforcing sparsity induced penalty resulting non convex optimization solution calculated through relaxed convex problem via standard admm alternative direction method multipliers rather than inferring latent representation eigen structure <eos> paper provides direct solution solving new grassmann optimization problem <eos> way calculating latent embedding becomes part optmization manifolds recently developed manifold optimization method applied <eos> turns out learned new feature only very informative clustering but also more intuitive effective visualization after dimensionality reduction <eos> conduct empirical studies simulated datasets several real world benchmark datasets validate proposed method <eos> experimental result exhibit effectiveness new manifold based clustering dimensionality reduction method <eos> <eop> robust joint individual variance explained <eos> discovering common joint individual subspaces crucial analysis multiple data set including multi view multi modal data <eos> several statistical machine learning method developed discovering common feature across multiple data set <eos> most well studied family method canonical correlation analysis cca its variants <eos> even though cca powerful tool several drawbacks render its application challenging computer vision applications <eos> discovers only common feature individual ones sensitive gross errors present visual data <eos> recently efforts made order develop method discover individual common components <eos> nevertheless method mainly applicable two set data <eos> paper investigate use recently proposed statistical method so called joint individual variance explained jive method recovery joint individual components arbitrary number data set <eos> since jive robust gross errors propose alternatives both robust non gaussian noise large magnitude well able automatically find rank individual components <eos> demonstrate effectiveness proposed approach two computer vision applications namely facial expression synthesis face age progression wild <eos> <eop> anchornet weakly supervised network learn geometry sensitive feature semantic matching <eos> despite significant progress deep learning recent years state art semantic matching method still rely legacy feature such sift hog <eos> argue strong invariance properties key success recent deep architectures classification task make them unfit dense correspondence tasks unless large amount supervision used <eos> work propose deep network termed anchornet produces image representations well suited semantic matching <eos> relies set filters whose response geometrically consistent across different object instances even presence strong intra class scale viewpoint variations <eos> trained only weak image level labels final representation successfully captures information about object structure improves result state art semantic matching method such deformable spatial pyramid proposal flow method <eos> show positive result cross instance matching task different instances same object category matched well new cross category semantic matching task aligning pairs instances each different object class <eos> <eop> physically based rendering indoor scene understanding using convolutional neural network <eos> indoor scene understanding central applications such robot navigation human companion assistance <eos> over last years data driven deep neural network outperformed many traditional approaches thanks their representation learning capabilities <eos> one bottlenecks training better representations amount available per pixel ground truth data required core scene understanding tasks such semantic segmentation normal prediction object boundary detection <eos> address problem number works proposed using synthetic data <eos> however systematic study how such synthetic data generated missing <eos> work introduce large scale synthetic dataset physically based rendered image realistic three dimensional indoor scenes <eos> study effects rendering method scene lighting training three computer vision tasks surface normal prediction semantic segmentation object boundary detection <eos> study provides insights into best practices training synthetic data more realistic rendering worth shows pretraining new synthetic dataset improve result beyond current state art all three tasks <eos> <eop> youtube boundingboxes large high precision human annotated data set object detection video <eos> introduce new large scale data set video urls densely sampled object bounding box annotations called youtube boundingboxes yt bb <eos> data set consists approximately video segments about long automatically selected feature object natural settings without editing post processing recording quality often akin hand held cell phone camera <eos> object represent subset coco label set <eos> all video segments were human annotated high precision classification labels bounding boxes frame per second <eos> use cascade increasingly precise human annotations ensures label accuracy above every class tight bounding boxes <eos> finally train evaluate well known deep network architectures report baseline figures per frame classification localization <eos> also demonstrate how temporal contiguity video potentially used improve such inferences <eos> data set found research <eos> com youtube bb <eos> hope availability such large curated corpus will spur new advances video object detection tracking <eos> <eop> full resolution image compression recurrent neural network <eos> paper presents set full resolution lossy image compression method based neural network <eos> each architectures describe provide variable compression rates during deployment without requiring retraining network each network need only trained once <eos> all architectures consist recurrent neural network rnn based encoder decoder binarizer neural network entropy coding <eos> compare rnn types lstm associative lstm introduce new hybrid gru resnet <eos> also study one shot versus additive reconstruction architectures introduce new scaled additive framework <eos> compare previous work showing improvements <eos> auc area under rate distortion curve depending perceptual metric used <eos> far know first neural network architecture able outperform jpeg image compression across most bitrates rate distortion curve kodak dataset image without aid entropy coding <eos> <eop> learning extract semantic structure documents using multimodal fully convolutional neural network <eos> present end end multimodal fully convolutional network extracting semantic structures document image <eos> consider document semantic structure extraction pixel wise segmentation task propose unified model classifies pixels based only their visual appearance traditional page segmentation task but also content underlying text <eos> moreover propose efficient synthetic document generation process use generate pretraining data network <eos> once network trained large set synthetic documents fine tune network unlabeled real documents using semi supervised approach <eos> systematically study optimum network architecture show both multimodal approach synthetic data pretraining significantly boost performance <eos> <eop> hardware efficient guided image filtering multi label problem <eos> guided filter gf well known its linear complexity <eos> however when filtering image channel guidance gf needs invert xn matrix each pixel <eos> best knowledge existing matrix inverse algorithms inefficient current hardwares <eos> shortcoming limits applications multichannel guidance computation intensive system such multi label system <eos> need new gf like filter perform fast multichannel image guided filtering <eos> since optimal linear complexity gf cannot minimized further only way thus bring all potentialities current parallel computing hardwares into full play <eos> paper propose hardware efficient guided filter hgf solves efficiency problem multichannel guided image filtering yields competent result when applying multi label problems synthesized polynomial multichannel guidance <eos> specifically order boost filtering performance hgf takes new matrix inverse algorithm only involves two hardware efficient operations element wise arithmetic calculations box filtering <eos> order break linear model restriction hgf synthesizes polynomial multichannel guidance introduce nonlinearity <eos> benefiting polynomial guidance hardware efficient matrix inverse algorithm hgf only more sensitive underlying structure guidance but also achieves fastest computing speed <eos> due merits hgf obtains state art result terms accuracy efficiency computation intensive multi label systems <eos> <eop> fully adaptive feature sharing multi task network applications person attribute classification <eos> multi task learning aims improve generalization performance multiple prediction tasks appropriately sharing relevant information across them <eos> context deep neural network idea often realized hand designed network architectures layer shared across tasks branches encode task specific feature <eos> however space possible multi task deep architectures combinatorially large often final architecture arrived manual exploration space both error prone tedious <eos> propose automatic approach designing compact multi task deep learning architectures <eos> approach starts thin multi layer network dynamically widens greedy manner during training <eos> doing so iteratively creates tree like deep architecture similar tasks reside same branch until top layer <eos> evaluation person attributes classification tasks involving facial clothing attributes suggests models produced proposed method fast compact closely match exceed state art accuracy strong baselines much more expensive models <eos> <eop> hyperspectral image super resolution via non local sparse tensor factorization <eos> hyperspectral image hsi super resolution fuses low resolution lr hsi high resolution hr multispectral image msi recently attracted much attention <eos> most current hsi super resolution approaches based matrix factorization unfolds three dimensional hsi matrix before processing <eos> general matrix data representation obtained after matrix unfolding operation makes hard fully exploit inherent hsi spatial spectral structures <eos> paper novel hsi super resolution method based non local sparse tensor factorization called nlstf proposed <eos> sparse tensor factorization directly decompose each cube hsi sparse core tensor dictionaries three modes reformulates hsi super resolution problem estimation sparse core tensor dictionaries each cube <eos> further exploit non local spatial self similarities hsi similar cubes grouped together they assumed share same dictionaries <eos> dictionaries learned lr hsi hr msi each group corresponding sparse core tensors estimated spare coding learned dictionaries each cube <eos> experimental result demonstrate superiority proposed nlstf approach over several state art hsi super resolution approaches <eos> <eop> multi scale continuous crfs sequential deep network monocular depth estimation <eos> paper addresses problem depth estimation single still image <eos> inspired recent works multi scale convolutional neural network cnn propose deep model fuses complementary information derived multiple cnn side outputs <eos> different previous method integration obtained means continuous conditional random fields crfs <eos> particular propose two different variations one based cascade multiple crfs other unified graphical model <eos> designing novel cnn implementation mean field updates continuous crfs show both proposed models regarded sequential deep network training performed end end <eos> through extensive experimental evaluation demonstrate effectiveness proposed approach establish new state art result publicly available datasets <eos> <eop> learning cross modal deep representations robust pedestrian detection <eos> paper presents novel method detecting pedestrians under adverse illumination conditions <eos> approach relies novel cross modality learning framework based two main phases <eos> first given multimodal dataset deep convolutional network employed learn non linear mapping modeling relations between rgb thermal data <eos> then learned feature representations transferred second deep network receives input rgb image outputs detection result <eos> way feature both discriminative robust bad illumination conditions learned <eos> importantly test time only second pipeline considered no thermal data required <eos> extensive evaluation demonstrates proposed approach outperforms state art challenging kaist multispectral pedestrian dataset competitive previous method popular caltech dataset <eos> <eop> noisy softmax improving generalization ability dcnn via postponing early softmax saturation <eos> over past few years softmax sgd become commonly used component default training strategy cnn frameworks respectively <eos> however when optimizing cnn sgd saturation behavior behind softmax always gives illusion training well then omitted <eos> paper first emphasize early saturation behavior softmax will impede exploration sgd sometimes reason model converging bad local minima then propose noisy softmax mitigating early saturation issue injecting annealed noise softmax during each iteration <eos> operation based noise injection aims postponing early saturation further bringing continuous gradients propagation so significantly encourage sgd solver more exploratory help find better local minima <eos> paper empirically verifies superiority early softmax desaturation method indeed improves generalization ability cnn model regularization <eos> experimentally find early desaturation helps optimization many tasks yielding state art competitive result several popular benchmark datasets <eos> <eop> deep metric learning via facility location <eos> learning image similarity metrics end end fashion deep network demonstrated excellent result tasks such clustering retrieval <eos> however current method all focus very local view data <eos> paper propose new metric learning scheme based structured prediction aware global structure embedding space designed optimize clustering quality metric nmi <eos> show state art performance standard datasets such cub cars stanford online products nmi evaluation metrics <eos> <eop> event based visual inertial odometry <eos> event based cameras provide new visual sensing model detecting changes image intensity asynchronously across all pixels camera <eos> providing events extremely high rates up mhz they allow sensing both high speed high dynamic range situations traditional cameras may fail <eos> paper present first algorithm fuse purely event based tracking algorithm inertial measurement unit provide accurate metric tracking camera full dof pose <eos> algorithm asynchronous provides measurement updates rate proportional camera velocity <eos> algorithm selects feature image plane tracks spatiotemporal windows around feature within event stream <eos> extended kalman filter structureless measurement model then fuses feature tracks output imu <eos> camera poses filter then used initialize next step tracker reject failed tracks <eos> show method successfully tracks camera motion event camera dataset number challenging situations <eos> <eop> scribbler controlling deep image synthesis sketch color <eos> recently there several promising method generate realistic imagery deep convolutional network <eos> method sidestep traditional computer graphics rendering pipeline instead generate imagery pixel level learning large collections photos <eos> however method limited utility because difficult user control network produces <eos> paper propose deep adverserial image synthesis architecture conditioned coarse sketches sparse color strokes generate realistic cars bedrooms faces <eos> demonstrate sketch based image synthesis system allows users scribble over sketch indicate preferred color object <eos> network then generate convincing image satisfy both color sketch constraints user <eos> network feed forward allows users see effect their edits real time <eos> compare recent work sketch image synthesis show approach generate more realistic more diverse more controllable outputs <eos> architecture also effective user guided colorization grayscale image <eos> <eop> scene graph generation iterative message passing <eos> understanding visual scene goes beyond recognizing individual object isolation <eos> relationships between object also constitute rich semantic information about scene <eos> work explicitly model object their relationships using scene graphs visually grounded graphical structure image <eos> propose novel end end model generates such structured scene representation input image <eos> key insight graph generation problem formulated message passing between primal node graph its dual edge graph <eos> joint inference model take advantage contextual cues make better predictions object their relationships <eos> experiments show model significantly outperforms previous method visual genome dataset well support relation inference nyu depth dataset <eos> <eop> accurate single stage detector using recurrent rolling convolution <eos> most recent successful method accurate object detection localization used some variants cnn style two stage convolutional neural network cnn plausible region were proposed first stage then followed second stage decision refinement <eos> despite simplicity training efficiency deployment single stage detection method competitive when evaluated benchmarks consider map high iou thresholds <eos> paper proposed novel single stage end end trainable object detection network overcome limitation <eos> achieved introducing recurrent rolling convolution rrc architecture over multi scale feature maps construct object classifiers bounding box regressors deep context <eos> evaluated method challenging kitti dataset measures method under iou threshold <eos> showed rrc single reduced vgg based model already significantly outperformed all previously published result <eos> time paper was written models ranked first kitti car detection hard level first cyclist detection second pedestrian detection <eos> result were reached previous single stage method <eos> code publicly available <eos> <eop> indoor scene parsing instance segmentation semantic labeling support relationship inference <eos> over years indoor scene parsing attracted growing interest computer vision community <eos> existing method typically focused diverse subtasks challenging problem <eos> particular while some them aim segmenting image into region such object surface instances others aim inferring semantic labels given region their support relationships <eos> different tasks typically treated separate ones <eos> however they bear strong connections good region should respect semantic labels support only defined meaningful region support relationships strongly depend semantics <eos> paper therefore introduce approach jointly segment instances infer their semantic labels support relationships single input image <eos> exploiting hierarchical segmentation formulate problem jointly finding region hierarchy correspond instances estimating their class labels pairwise support relationships <eos> express via markov random field allows further encode links between different types variables <eos> inference model done exactly via integer linear programming learn its parameters structural svm framework <eos> experiments nyuv demonstrate benefits reasoning jointly about all subtasks indoor scene parsing <eos> <eop> reflection removal using low rank matrix completion <eos> image taken through glass often capture target transmitted scene well undesired reflected scenes <eos> paper propose low rank matrix completion algorithm remove reflection artifacts automatically multiple glass image taken slightly different camera locations <eos> assume transmitted scenes more dominant than reflected scenes typical glass image <eos> first warp multiple glass image reference image gradients consistent transmission image while gradients varying across reflection image <eos> based observation compute gradient reliability such pixels belonging salient edges transmission image assigned high reliability <eos> then suppress gradients reflection image recover gradients transmission image only solving low rank matrix completion problem gradient domain <eos> reconstruct original transmission image using resulting optimal gradient map <eos> experimental result show proposed algorithm removes reflection artifacts glass image faithfully outperforms existing algorithms typical glass image <eos> <eop> deep multimodal representation learning temporal data <eos> recent years deep learning successfully applied multimodal learning problems aim learning useful joint representations data fusion applications <eos> when available modalities consist time series data such video audio sensor signals becomes imperative consider their temporal structure during fusion process <eos> paper propose correlational recurrent neural network corrrnn novel temporal fusion model fusing multiple input modalities inherently temporal nature <eos> key feature proposed model include simultaneous learning joint representation temporal dependencies between modalities ii use multiple loss terms objective function including maximum correlation loss term enhance learning cross modal information iii use attention model dynamically adjust contribution different input modalities joint representation <eos> validate model via experimentation two different tasks video sensor based activity classification audio visual speech recognition <eos> empirically analyze contributions different components proposed corrrnn model demonstrate its robustness effectiveness state art performance multiple datasets <eos> <eop> weighted entropy based quantization deep neural network <eos> quantization considered one most effective method optimize inference cost neural network models their deployment mobile embedded systems tight resource constraints <eos> such approaches critical provide low cost quantization under tight accuracy loss constraint <eos> paper propose novel method quantizing weights activations based concept weighted entropy <eos> unlike recent work binary weight neural network approach multi bit quantization weights activations quantized any number bits depending target accuracy <eos> facilitates much more flexible exploitation accuracy performance trade off provided different levels quantization <eos> moreover scheme provides automated quantization flow based conventional training algorithms greatly reduces design time effort quantize network <eos> according extensive evaluations based practical neural network models image classification alexnet googlenet resnet object detection fcn layer resnet language modeling lstm network method achieves significant reductions both model size amount computation minimal accuracy loss <eos> also compared existing quantization schemes ours provides higher accuracy similar resource constraint requires much lower design effort <eos> <eop> deep supervision shape concepts occlusion aware three dimensional object parsing <eos> monocular three dimensional object parsing highly desirable various scenarios including occlusion reasoning holistic scene interpretation <eos> present deep convolutional neural network cnn architecture localize semantic parts image three dimensional space while inferring their visibility states given single rgb image <eos> key insight exploit domain knowledge regularize network deeply supervising its hidden layer order sequentially infer intermediate concepts associated final task <eos> acquire training data desired quantities ground truth three dimensional shape relevant concepts render three dimensional object cad models generate large scale synthetic data simulate challenging occlusion configurations between object <eos> train network only synthetic data demonstrate state art performances real image benchmarks including extended version kitti pascal voc pascal ikea three dimensional keypoint localization instance segmentation <eos> empirical result substantiate utility deep supervision scheme demonstrating effective transfer knowledge synthetic data real image resulting less overfitting compared standard end end training <eos> <eop> spatio temporal self organizing map deep network dynamic object detection video <eos> dynamic object detection challenging construct effective model sufficiently characterize spatial temporal properties background <eos> paper proposes new spatio temporal self organizing map stsom deep network detect dynamic object complex scenarios <eos> proposed approach several contributions first novel stsom shared all pixels video frame presented efficiently model complex background <eos> exploit fact motions complex background global variation space local variation time train stsom using whole frames sequence pixel over time tackle variance complex background <eos> second bayesian parameter estimation based method presented learn thresholds automatically all pixels filter out background <eos> last order model complex background more accurately extend single layer stsom deep network <eos> then background filtered out layer layer <eos> experimental result cdnet dataset demonstrate proposed stsom deep network outperforms numerous recently proposed method overall performance most categories scenarios <eos> <eop> semantic image inpainting deep generative models <eos> semantic image inpainting challenging task large missing region filled based available visual data <eos> existing method extract information only single image generally produce unsatisfactory result due lack high level context <eos> paper propose novel method semantic image inpainting generates missing content conditioning available data <eos> given trained generative model search closest encoding corrupted image latent image manifold using context prior losses <eos> encoding then passed through generative model infer missing content <eos> method inference possible irrespective how missing content structured while state art learning based method requires specific information about holes training phase <eos> experiments three datasets show method successfully predicts information large missing region achieves pixel level photorealism significantly outperforming state art method <eos> <eop> unambiguous text localization retrieval cluttered scenes <eos> text instance one category self described object provides valuable information understanding describing cluttered scenes <eos> paper explore task unambiguous text localization retrieval accurately localize specific targeted text instance cluttered image given natural language description refers <eos> address issue first novel recurrent dense text localization network dtln proposed sequentially decode intermediate convolutional representations cluttered scene image into set distinct text instance detections <eos> approach avoids repeated detections multiple scales same text instance recurrently memorizing previous detections effectively tackles crowded text instances close proximity <eos> second propose context reasoning text retrieval crtr model jointly encodes text instances their context information through recurrent network ranks localized text bounding boxes scoring function context compatibility <eos> quantitative evaluations standard scene text localization benchmarks newly collected scene text retrieval dataset demonstrate effectiveness advantages models both scene text localization retrieval <eos> <eop> guesswhat visual object discovery through multi modal dialogue <eos> introduce guesswhat two player guessing game testbed research interplay computer vision dialogue systems <eos> goal game locate unknown object rich image scene asking sequence questions <eos> higher level image understanding like spatial reasoning language grounding required solve proposed task <eos> key contribution collection large scale dataset consisting human played games total visual question answer pairs image <eos> explain design decisions collecting dataset introduce oracle questioner tasks associated two players game <eos> prototyped deep learning models establish initial baselines introduced tasks <eos> <eop> learning spatial regularization image level supervisions multi label image classification <eos> multi label image classification fundamental but challenging task computer vision <eos> great progress achieved exploiting semantic relations between labels recent years <eos> however conventional approaches unable model underlying spatial relations between labels multi label image because spatial annotations labels generally provided <eos> paper propose unified deep neural network exploits both semantic spatial relations between labels only image level supervisions <eos> given multi label image proposed spatial regularization network srn generates attention maps all labels captures underlying relations between them via learnable convolutions <eos> aggregating regularized classification result original result resnet network classification performance consistently improved <eos> whole deep neural network trained end end only image level annotations thus requires no additional efforts image annotations <eos> extensive evaluations public datasets different types labels show approach significantly outperforms state arts strong generalization capability <eos> analysis learned srn model demonstrates effectively capture both semantic spatial relations labels improving classification performance <eos> <eop> cern confidence energy recurrent network group activity recognition <eos> work about recognizing human activities occurring video distinct semantic levels including individual actions interactions group activities <eos> recognition realized using two level hierarchy long short term memory lstm network forming feed forward deep architecture trained end end <eos> comparison existing architectures lstms make two key contributions giving name approach confidence energy recurrent network cern <eos> first instead using common softmax layer prediction specify novel energy layer el estimating energy predictions <eos> second rather than finding common minimum energy class assignment may numerically unstable under uncertainty specify el additionally computes values solutions way estimates most confident energy minimum <eos> evaluation collective activity volleyball datasets demonstrates advantages two contributions relative common softmax energy minimization formulations ii superior performance relative state art approaches <eos> <eop> visual translation embedding network visual relation detection <eos> visual relations such person ride bike bike next car offer comprehensive scene understanding image already shown their great utility connecting computer vision natural language <eos> however due challenging combinatorial complexity modeling subject predicate object relation triplets very little work done localize predict visual relations <eos> inspired recent advances relational representation learning knowledge bases convolutional object detection network propose visual translation embedding network vtranse visual relation detection <eos> vtranse places object low dimensional relation space relation modeled simple vector translation <eos> subject predicate object <eos> propose novel feature extraction layer enables object relation knowledge transfer fully convolutional fashion supports training inference single forward backward pass <eos> best knowledge vtranse first end end relation detection network <eos> demonstrate effectiveness vtranse over other state art method two large scale datasets visual relationship visual genome <eos> note even though vtranse purely visual model still competitive lu multi modal model language <eop> neural face editing intrinsic image disentangling <eos> traditional face editing method often require number sophisticated task specific algorithms applied one after other process tedious fragile computationally intensive <eos> paper propose end end generative adversarial network infers face specific disentangled representation intrinsic face properties including shape <eos> normals albedo lighting alpha matte <eos> show network trained wild image incorporating network physically based image formation module appropriate loss functions <eos> disentangling latent representation allows semantically relevant edits one aspect facial appearance manipulated while keeping orthogonal properties fixed demonstrate its use number facial editing applications <eos> <eop> east efficient accurate scene text detector <eos> previous approaches scene text detection already achieved promising performances across various benchmarks <eos> however they usually fall short when dealing challenging scenarios even when equipped deep neural network models because overall performance determined interplay multiple stages components pipelines <eos> work propose simple yet powerful pipeline yields fast accurate text detection natural scenes <eos> pipeline directly predicts words text lines arbitrary orientations quadrilateral shapes full image eliminating unnecessary intermediate steps <eos> candidate aggregation word partitioning single neural network <eos> simplicity pipeline allows concentrating efforts designing loss functions neural network architecture <eos> experiments standard datasets including icdar coco text msra td demonstrate proposed algorithm significantly outperforms state art method terms both accuracy efficiency <eos> icdar dataset proposed algorithm achieves score <eos> <eop> episodic camn contextual attention based memory network iterative feedback scene labeling <eos> scene labeling seen sequence sequence prediction task pixels labels quite important leverage relevant context enhance performance pixel classification <eos> paper introduce episodic attention based memory network achieve goal <eos> present unified framework mainly consists convolutional neural network cnn specifically fully convolutional network fcn attention based memory module feedback connections perform context selection refinement <eos> full model produces context aware representation each target patch aggregating activated context its original local representation produced convolution layer <eos> evaluate model pascal context sift flow pascal voc datasets achieve competitive result other state art method scene labeling <eos> <eop> robust energy minimization brdf invariant shape light fields <eos> highly effective optimization frameworks developed traditional multiview stereo relying lambertian photoconsistency <eos> however they account complex material properties <eos> other hand recent works explored pde invariants shape recovery complex brdfs but they incorporated into robust numerical optimization frameworks <eos> present variational energy minimization framework robust recovery shape multiview stereo complex unknown brdfs <eos> while formulation general demonstrate its efficacy shape recovery using single light field image microlens array may considered realization purely translational multiview stereo setup <eos> formulation automatically balances contributions texture gradients traditional lambertian photoconsistency appropriate brdf invariant pde smoothness prior <eos> unlike prior works energy function inherently handles spatially varying brdfs albedos <eos> extensive experiments synthetic real data show optimization framework consistently achieves errors lower than lambertian baselines further more robust than prior brdf invariant reconstruction method <eos> <eop> connecting look feel associating visual tactile properties physical materials <eos> machines interact physical world they must understand physical properties object materials they encounter <eos> use fabrics example deformable material rich set mechanical properties <eos> thin flexible fabric when draped tends look different heavy stiff fabric <eos> also feels different when touched <eos> using collection fabric sample captured color depth image draped fabrics along tactile data high resolution touch sensor <eos> then sought associate information vision touch jointly training cnn across three modalities <eos> through cnn each input regardless modality generates embedding vector records fabric physical property <eos> comparing embedding vectors system able look fabric image predict how will feel vice versa <eos> also show system jointly trained vision touch data outperform similar system trained only visual data when tested purely visual inputs <eos> <eop> robust visual tracking using oblique random forests <eos> random forest emerged powerful classification technique promising result various vision tasks including image classification pose estimation object detection <eos> however current techniques shown little improvements visual tracking they mostly rely piece wise orthogonal hyperplanes create decision nodes lack robust incremental learning mechanism much needed online tracking <eos> paper propose discriminative tracker based novel incremental oblique random forest <eos> unlike conventional orthogonal decision trees use single feature heuristic measures obtain split each node propose use more powerful proximal svm obtain oblique hyperplanes capture geometric structure data better <eos> resulting decision surface restricted axis aligned hence ability represent classify input data better <eos> furthermore order generalize online tracking scenarios derive incremental update steps enable hyperplanes each node updated recursively efficiently closed form fashion <eos> demonstrate effectiveness method using two large scale benchmark datasets otb otb show method gives competitive result several challenging cases relying simple hog feature well combination more sophisticated deep neural network based models <eos> implementations proposed random forest available github <eos> com zhangleuestc incremental oblique random forest <eos> <eop> discriminative covariance oriented representation learning face recognition image set <eos> face recognition image set while most existing works mainly focus building robust set models hand crafted feature remains research gap learn better image representations closely match subsequent image set modeling classification <eos> taking sample covariance matrix set model light its recent promising success present discriminative covariance oriented representation learning dcrl framework bridge above gap <eos> framework constructs feature learning network <eos> cnn project face image into target representation space network trained towards goal set covariance matrix calculated target space maximum discriminative ability <eos> encode discriminative ability set covariance matrices elaborately design two different loss functions respectively lead two different representation learning schemes <eos> graph embedding scheme softmax regression scheme <eos> both schemes optimize whole network containing both image representation mapping set model classification joint learning manner <eos> proposed method extensively validated three challenging large scale databases task face recognition image set <eos> youtube celebrities youtube face db point shoot challenge <eos> <eop> generalized deep image image regression <eos> present deep convolutional neural network architecture serves generic image image regressor trained end end without any further machinery <eos> proposed architecture recursively branched deconvolutional network rbdn develops cheap multi context image representation very early using efficient recursive branching scheme extensive parameter sharing learnable upsampling <eos> multi context representation subjected highly non linear locality preserving transformation remainder network comprising series convolutions deconvolutions without any spatial downsampling <eos> rbdn architecture fully convolutional handle variable sized image during inference <eos> provide qualitative quantitative result diverse tasks relighting denoising colorization show proposed rbdn architecture obtains comparable result state art each tasks when used off shelf without any post processing task specific architectural modifications <eos> <eop> multi object tracking quadruplet convolutional neural network <eos> propose quadruplet convolutional neural network quad cnn multi object tracking learn associate object detections across frames using quadruplet losses <eos> proposed network consider target appearances together their temporal adjacencies data association <eos> unlike conventional ranking losses quadruplet loss enforces additional constraint makes temporally adjacent detections more closely located than ones large temporal gaps <eos> also employ multi task loss jointly learn object association bounding box regression better localization <eos> whole network trained end end <eos> tracking target association performed minimax label propagation using metric learned proposed network <eos> evaluate performance multi object tracking algorithm public mot challenge datasets achieve outstanding result <eos> <eop> semantic compositional network visual captioning <eos> semantic compositional network scn developed image captioning semantic concepts <eos> tags detected image probability each tag used compose parameters long short term memory lstm network <eos> scn extends each weight matrix lstm ensemble tag dependent weight matrices <eos> degree each member ensemble used generate image caption tied image dependent probability corresponding tag <eos> addition captioning image also extend scn generate captions video clips <eos> qualitatively analyze semantic composition scns quantitatively evaluate algorithm three benchmark datasets coco flickr youtube text <eos> experimental result show proposed method significantly outperforms prior state art approaches across multiple evaluation metrics <eos> <eop> link head beak zero shot learning noisy text description part precision <eos> paper study learning visual classifiers unstructured text description part precision no training image <eos> show visual text terms encouraged attend its relevant parts while image connections non visual text terms vanishes without any supervision <eos> learning process enables terms like peak linked parts like only head instance while non visual terms like migrate affect classifier prediction without part text annotation <eos> image encoded part based cnn detect bird parts learn part specific learning representation <eos> part based visual classifiers predicted text descriptions unseen visual classifiers facilitate classification without training image also known zero shot recognition <eos> performed experiments cub dataset improves zero shot recognition result <eos> also created large scale benchmark north american bird image text descriptions also showed method outperforming existing method <eos> <eop> non local low rank framework ultrasound speckle reduction <eos> speckle refers granular patterns occur ultrasound image due wave interference <eos> speckle removal greatly improve visibility underlying structures ultrasound image enhance subsequent post processing <eos> present novel framework speckle removal based low rank non local filtering <eos> approach works first computing guidance image assists selection candidate patches non local filtering face significant speckles <eos> candidate patches further refined using low rank minimization estimated using truncated weighted nuclear norm twnn structured sparsity <eos> show proposed filtering framework produces result outperform state art method both qualitatively quantitatively <eos> framework also provides better segmentation result when used pre processing ultrasound image <eos> <eop> sca cnn spatial channel wise attention convolutional network image captioning <eos> visual attention successfully applied structural prediction tasks such visual captioning question answering <eos> existing visual attention models generally spatial <eos> attention modeled spatial probabilities re weight last conv layer feature map cnn encoding input image <eos> however argue such spatial attention necessarily conform attention mechanism dynamic feature extractor combines contextual fixations over time cnn feature naturally spatial channel wise multi layer <eos> paper introduce novel convolutional neural network dubbed sca cnn incorporates spatial channel wise attentions cnn <eos> task image captioning sca cnn dynamically modulates sentence generation context multi layer feature maps encoding <eos> attentive spatial locations multiple layer <eos> attentive channels visual attention <eos> evaluate proposed sca cnn architecture three benchmark image captioning datasets flickr flickr mscoco <eos> consistently observed sca cnn significantly outperforms state art visual attention based image captioning method <eos> <eop> compact dnn approaching googlenet level accuracy classification domain adaptation <eos> recently dnn model compression based network architecture design <eos> squeezenet attracted lot attention <eos> no accuracy drop image classification observed extremely compact network compared well known models <eos> emerging question however whether model compression techniques hurt dnn learning ability other than classifying image single dataset <eos> preliminary experiment shows compression method could degrade domain adaptation da ability though classification performance preserved <eos> therefore propose new compact network architecture unsupervised da method paper <eos> dnn built new basic module conv provides more diverse feature extractors without significantly increasing parameters <eos> unified framework da method will simultaneously learn invariance across domains reduce divergence feature representations adapt label prediction <eos> experiments show dnn obtains googlenet level accuracy both classification da da method slightly outperforms previous competitive ones <eos> put all together da strategy based dnn achieves state art sixteen total eighteen da tasks popular office office caltech datasets <eos> <eop> relationship proposal network <eos> image scene understanding requires learning relationships between object scene <eos> scene many object may only few individual interacting object <eos> party image many people only handful people might speaking each other <eos> detect all relationships would inefficient first detect all individual object then classify all pairs only number all pairs quadratic but classification requires limited object categories scalable real world image <eos> paper address challenges using pairs related region image train relationship proposer test time produces manageable number related region <eos> name model relationship proposal network rel pn <eos> like object proposals rel pn class agnostic thus scalable open vocabulary object <eos> demonstrate ability rel pn localize relationships only few thousand proposals <eos> demonstrate its performance visual genome dataset compare other baselines designed <eos> also conduct experiments smaller subset image over related region show promising result <eos> <eop> designing energy efficient convolutional neural network using energy aware pruning <eos> deep convolutional neural network cnn indispensable state art computer vision algorithms <eos> however they still rarely deployed battery powered mobile devices such smartphones wearable gadgets vision algorithms enable many revolutionary real world applications <eos> key limiting factor high energy consumption cnn processing due its high computational complexity <eos> while there many previous efforts try reduce cnn model size amount computation find they necessarily result lower energy consumption <eos> therefore targets serve good metric energy cost estimation <eos> close gap between cnn design energy consumption optimization propose energy aware pruning algorithm cnn directly uses energy consumption cnn guide pruning process <eos> energy estimation methodology uses parameters extrapolated actual hardware measurements <eos> proposed layer layer pruning algorithm also prunes more aggressively than previously proposed pruning method minimizing error output feature maps instead filter weights <eos> each layer weights first pruned then locally fine tuned closed form least square solution quickly restore accuracy <eos> after all layer pruned entire network globally fine tuned using back propagation <eos> proposed pruning method energy consumption alexnet googlenet reduced <eos> respectively less than top accuracy loss <eos> also show reducing number target classes alexnet greatly decreases number weights but limited impact energy consumption <eos> <eop> boundary aware instance segmentation <eos> address problem instance level semantic seg mentation aims jointly detecting segmenting classifying every individual object image <eos> con text existing method typically propose candidate object usually bounding boxes directly predict binary mask within each such proposal <eos> consequence they cannot recover errors object candidate genera tion process such too small shifted boxes <eos> paper introduce novel object segment rep resentation based distance transform object masks <eos> then design object mask network omn new residual deconvolution architecture infers such representation decodes into final binary object mask <eos> allows predict masks go beyond scope bounding boxes thus robust inaccu rate object candidates <eos> integrate omn into mul titask network cascade framework learn result ing boundary aware instance segmentation bais network end end manner <eos> experiments pas cal voc cityscapes datasets demonstrate benefits approach outperforms state art both object proposal generation instance segmentation <eos> <eop> joint intensity spatial metric learning robust gait recognition <eos> paper describes joint intensity metric learning method improve robustness gait recognition silhouette based descriptors such gait energy image <eos> because existing method often use difference image intensities between matching pair <eos> absolute difference gait energies norm measure dissimilarity large intrasubject differences derived covariate conditions <eos> large gait energies caused carried object vs <eos> small gait energies caused background may wash out subtle intersubject differences <eos> difference middle level gait energies derived motion differences <eos> therefore introduce metric joint intensity mitigate large intrasubject differences well leverage subtle intersubject differences <eos> more specifically formulate joint intensity spatial metric learning unified framework alternately optimize linear ranking support vector machines <eos> experiments using ou isir treadmill data set largest clothing variation large population data set bag version containing carrying status wild demonstrate effectiveness proposed method <eos> <eop> seeing there learning context determine object missing <eos> most computer vision focuses image <eos> propose train standalone object centric context representation perform opposite task seeing there <eos> given image context model predict object should exist even when no object instances present <eos> combined object detection result perform novel vision task finding object missing image <eos> model based convolutional neural network structure <eos> specially designed training strategy model learns ignore object focus context only <eos> fully convolutional thus highly efficient <eos> experiments show effectiveness proposed approach one important accessibility task finding city street region curb ramps missing could help millions people mobility disabilities <eos> <eop> joint gap detection inpainting line drawings <eos> propose novel data driven approach automatically detecting completing gaps line drawings convolutional neural network <eos> case existing inpainting approaches natural image masks indicating missing region generally required input <eos> here show line drawings enough structures learned cnn allow automatic detection completion gaps without any such input <eos> thus method find gaps line drawings complete them without user interaction <eos> furthermore completion realistically conserves thickness curvature line segments <eos> all necessary heuristics such realistic line completion learned naturally dataset line drawings various patterns line completion generated fly training pairs improve model generalization <eos> evaluate method qualitatively diverse set challenging line drawings also provide quantitative result user study significantly outperforms state art <eos> <eop> cdc convolutional de convolutional network precise temporal action localization untrimmed video <eos> temporal action localization important yet challenging problem <eos> given long untrimmed video consisting multiple action instances complex background contents need only recognize their action categories but also localize start time end time each instance <eos> many state art systems use segment level classifiers select rank proposal segments pre determined boundaries <eos> however desirable model should move beyond segment level make dense predictions fine granularity time determine precise temporal boundaries <eos> end design novel convolutional de convolutional cdc network places cdc filters top three dimensional convnets shown effective abstracting action semantics but reduce temporal length input data <eos> proposed cdc filter performs required temporal upsampling spatial downsampling operations simultaneously predict actions frame level granularity <eos> unique jointly modeling action semantics space time fine grained temporal dynamics <eos> train cdc network end end manner efficiently <eos> model only achieves superior performance detecting actions every frame but also significantly boosts precision localizing temporal boundaries <eos> finally cdc network demonstrates very high efficiency ability process frames per second single gpu server <eos> source code trained models available online bitbucket <eos> org columbiadvmm cdc <eos> <eop> switching convolutional neural network crowd counting <eos> propose novel crowd counting model maps given crowd scene its density <eos> crowd analysis compounded myriad factors like inter occlusion between people due extreme crowding high similarity appearance between people background elements large variability camera view point <eos> current state art approaches tackle factors using multi scale cnn architectures recurrent network late fusion feature multi column cnn different receptive fields <eos> propose switching convolutional neural network leverages variation crowd density within image improve accuracy localization predicted crowd count <eos> patches grid within crowd scene relayed independent cnn regressors based crowd count prediction quality cnn established during training <eos> independent cnn regressors designed different receptive fields switch classifier trained relay crowd scene patch best cnn regressor <eos> perform extensive experiments all major crowd counting datasets evidence better performance compared current state art method <eos> provide interpretable representations multichotomy space crowd scene patches inferred switch <eos> observed switch relays image patch particular cnn column based density crowd <eos> <eop> captioning image diverse object <eos> recent captioning models limited their ability scale describe concepts unseen paired image text corpora <eos> propose novel object captioner noc deep visual semantic captioning model describe large number object categories present existing image caption datasets <eos> model takes advantage external sources labeled image object recognition datasets semantic knowledge extracted unannotated text <eos> propose minimizing joint objective learn diverse data sources leverage distributional semantic embeddings enabling model generalize describe novel object outside image caption datasets <eos> demonstrate model exploits semantic information generate captions hundreds object categories imagenet object recognition dataset observed mscoco image caption training data well many categories observed very rarely <eos> both automatic evaluations human judgements show model considerably outperforms prior work being able describe many more categories object <eos> <eop> amodal detection three dimensional object inferring three dimensional bounding boxes ones rgb depth image <eos> paper addresses problem amodal perception three dimensional object detection <eos> task only find object localizations three dimensional world but also estimate their physical sizes poses even if only parts them visible rgb image <eos> recent approaches attempted harness point cloud depth channel exploit three dimensional feature directly three dimensional space demonstrated superiority over traditional <eos> revisit amodal three dimensional detection problem sticking <eos> representation framework directly relate <eos> visual appearance three dimensional object <eos> propose novel three dimensional object detection system simultaneously predicts object three dimensional locations physical sizes orientations indoor scenes <eos> experiments nyuv dataset show algorithm significantly outperforms state art indicates <eos> representation capable encoding feature three dimensional amodal object detection <eos> all source code data github <eos> com phoenixnn amodal det <eos> <eop> consistent aware deep learning person re identification camera network <eos> paper propose consistent aware deep learning cadl framework person re identification camera network <eos> unlike most existing person re identification method identify whether two body image same person approach aims obtain maximal correct matches whole camera network <eos> different recently proposed camera network based re identification method only consider consistent information matching stage obtain global optimal association exploit such consistent aware information under deep learning framework both feature representation image matching automatically learned certain consistent constraints <eos> specifically reach global optimal solution balance performance between different cameras optimizing similarity association iteratively <eos> experimental result show method obtains significant performance improvement outperforms state art method large margins <eos> <eop> enhancing video summarization via vision language embedding <eos> paper addresses video summarization problem distilling raw video into shorter form while still capturing original story <eos> show visual representations supervised freeform language make good fit application extending recent submodular summarization approach representativeness interestingness objectives computed feature joint vision language embedding space <eos> perform evaluation two diverse datasets ut egocentric tv episodes show new objectives give improved summarization ability compared standard visual feature alone <eos> experiments also show vision language embedding need trained domain specific data but learned standard still image vision language datasets transferred video <eos> further benefit model ability guide summary using freeform text input test time allowing user customization <eos> <eop> quality aware network set set recognition <eos> paper targets problem set set recognition learns metric between two image set <eos> image each set belong same identity <eos> since image set complementary they hopefully lead higher accuracy practical applications <eos> however quality each sample cannot guaranteed sample poor quality will hurt metric <eos> paper quality aware network qan proposed confront problem quality each sample automatically learned although such information explicitly provided training stage <eos> network two branches first branch extracts appearance feature embedding each sample other branch predicts quality score each sample <eos> feature quality scores all sample set then aggregated generate final feature embedding <eos> show two branches trained end end manner given only set level identity annotation <eos> analysis gradient spread mechanism indicates quality learned network beneficial set set recognition simplifies distribution network needs fit <eos> experiments both face verification person re identification show advantages proposed qan <eos> source code network structure downloaded github <eos> <eop> spatially varying blur detection based multiscale fused sorted transform coefficients gradient magnitudes <eos> detection spatially varying blur without having any information about blur type challenging task <eos> paper propose novel effective approach address blur detection problem single image without requiring any knowledge about blur type level camera settings <eos> approach computes blur detection maps based novel high frequency multiscale fusion sort transform hifst gradient magnitudes <eos> evaluations proposed approach diverse set blurry image different blur types levels contents demonstrate proposed algorithm performs favorably against state art method qualitatively quantitatively <eos> <eop> age progression regression conditional adversarial autoencoder <eos> if provide you face image mine without telling you actual age when took picture large amount face image crawled containing labeled faces different ages but necessarily paired you show me would look like when am was like when was answer probably no <eos> most existing face aging works attempt learn transformation between age groups thus would require paired sample well labeled query image <eos> paper look problem generative modeling perspective such no paired sample required <eos> addition given unlabeled image generative model directly produce image desired age attribute <eos> propose conditional adversarial autoencoder caae learns face manifold traversing smooth age progression regression realized simultaneously <eos> caae face first mapped latent vector through convolutional encoder then vector projected face manifold conditional age through deconvolutional generator <eos> latent vector preserves personalized face feature <eos> personality age condition controls progression vs <eos> two adversarial network imposed encoder generator respectively forcing generate more photo realistic faces <eos> experimental result demonstrate appealing performance flexibility proposed framework comparing state art ground truth <eos> <eop> residual expansion algorithm fast effective optimization nonconvex least squares problems <eos> propose residual expansion re algorithm global near global optimization method nonconvex least squares problems <eos> unlike most existing nonconvex optimization techniques re algorithm based either stochastic multi point searches therefore achieve fast global optimization <eos> moreover re algorithm easy implement successful high dimensional optimization <eos> re algorithm exhibits excellent empirical performance terms means clustering point set registration optimized product quantization blind image deblurring <eos> <eop> scannet richly annotated three dimensional reconstructions indoor scenes <eos> key requirement leveraging supervised deep learning method availability large labeled datasets <eos> unfortunately context rgb scene understanding very little data available current datasets cover small range scene views limited semantic annotations <eos> address issue introduce scannet rgb video dataset containing <eos> views scenes annotated three dimensional camera poses surface reconstructions semantic segmentations <eos> collect data designed easy use scalable rgb capture system includes automated surface reconstruction crowdsourced semantic annotation <eos> show using data helps achieve state art performance several three dimensional scene understanding tasks including three dimensional object classification semantic voxel labeling cad model retrieval <eos> <eop> more less more complicated network less inference complexity <eos> paper present novel general network structure towards accelerating inference process convolutional neural network more complicated network structure yet less inference complexity <eos> core idea equip each original convolutional layer another low cost collaborative layer lccl element wise multiplication relu outputs two parallel layer produces layer wise output <eos> combined layer potentially more discriminative than original convolutional layer its inference faster two reasons zero cells lccl feature maps will remain zero after element wise multiplication thus safe skip calculation corresponding high cost convolution original convolutional layer lccl very fast if implemented convolution only single filter shared all channels <eos> extensive experiments cifar cifar ilscrc benchmarks show proposed network structure accelerate inference process average negligible performance drop <eos> <eop> online video object segmentation via convolutional trident network <eos> semi supervised online video object segmentation algorithm accepts user annotations about target object first frame proposed work <eos> propagate segmentation labels previous frame current frame using optical flow vectors <eos> however propagation error prone <eos> therefore develop convolutional trident network ctn three decoding branches separative definite foreground definite background decoders <eos> then perform markov random field optimization based outputs three decoders <eos> sequentially carry out processes second last frames extract segment track target object <eos> experimental result demonstrate proposed algorithm significantly outperforms state art conventional algorithms davis benchmark dataset <eos> <eop> learning object interactions descriptions semantic image segmentation <eos> recent advanced deep convolutional network cnn achieved great successes many computer vision tasks because their compelling learning complexity presences large scale labeled data <eos> however obtaining per pixel annotations expensive performances cnn semantic image segmentation fully exploited <eos> work significantly increases segmentation accuracy cnn learning image descriptions wild idw dataset <eos> unlike previous image captioning datasets captions were manually densely annotated image their descriptions idw automatically downloaded internet without any manual cleaning refinement <eos> idw cnn proposed jointly train idw existing image segmentation dataset such pascal voc voc <eos> two appealing properties <eos> first knowledge different datasets fully explored transferred each other improve performance <eos> second segmentation accuracy voc constantly increased when selecting more data idw <eos> extensive experiments demonstrate effectiveness scalability idw cnn outperforms existing best performing system voc test set <eos> <eop> shape completion using three dimensional encoder predictor cnn shape synthesis <eos> introduce data driven approach complete partial three dimensional shapes through combination volumetric deep neural network three dimensional shape synthesis <eos> partially scanned input shape method first infers low resolution but complete output <eos> end introduce three dimensional encoder predictor network three dimensional epn composed three dimensional convolutional layer <eos> network trained predict fill missing data operates implicit surface representation encodes both known unknown space <eos> allows predict global structure unknown areas high accuracy <eos> then correlate intermediary result three dimensional geometry shape database test time <eos> final pass propose patch based three dimensional shape synthesis method imposes three dimensional geometry retrieved shapes constraints coarsely completed mesh <eos> synthesis process enables reconstruct fine scale detail generate high resolution output while respecting global mesh structure obtained three dimensional epn <eos> although three dimensional epn outperforms state art completion method main contribution work lies combination data driven shape predictor analytic three dimensional shape synthesis <eos> result show extensive evaluations newly introduced shape completion benchmark both real world synthetic data <eos> <eop> impact typicality informative representative selection <eos> computer vision selection most informative sample huge pool training data order learn good recognition model active research problem <eos> furthermore also useful reduce annotation cost time consuming annotate unlabeled sample <eos> paper motivated theories data compression propose novel sample selection strategy exploits concept typicality domain information theory <eos> typicality simple powerful technique applied compress training data learn good classification model <eos> work typicality used identify subset most informative sample labeling then used update model using active learning <eos> proposed model take advantage inter relationships between data sample <eos> approach leads significant reduction manual labeling cost while achieving similar better recognition performance compared model trained entire training set <eos> demonstrated through rigorous experimentation five datasets <eos> <eop> infinite variational autoencoder semi supervised learning <eos> paper presents infinite variational autoencoder vae whose capacity adapts suit input data <eos> achieved using mixture model mixing coefficients modeled dirichlet process allowing integrate over coefficients when performing inference <eos> critically then allows automatically vary number autoencoders mixture based data <eos> experiments show flexibility method particularly semi supervised learning only small number training sample available <eos> <eop> understanding traffic density large scale web camera data <eos> understanding traffic density large scale web camera webcam video challenging problem because such video low spatial temporal resolution high occlusion large perspective <eos> deeply understand traffic density explore both optimization based deep learning based method <eos> avoid individual vehicle detection tracking both method map dense image feature into vehicle density one based rank constrained regression other based fully convolutional network fcn <eos> regression based method learns different weights different blocks image embed road geometry significantly reduce error induced camera perspective <eos> fcn based method jointly estimates vehicle density vehicle count residual learning framework perform end end dense prediction allowing arbitrary image resolution adapting different vehicle scales perspectives <eos> analyze compare both method get insights optimization based method improve deep model <eos> since existing datasets cover all challenges work collected labelled large scale traffic video dataset containing million frames webcams <eos> both method extensively evaluated compared different counting tasks datasets <eos> fcn based method significantly reduces mean absolute error mae <eos> public dataset trancos compared state art baseline <eos> <eop> end end three dimensional face reconstruction deep neural network <eos> monocular three dimensional facial shape reconstruction single facial image active research area due its wide applications <eos> inspired success deep neural network dnn propose dnn based approach end end three dimensional face reconstruction uh far single image <eos> different recent works reconstruct refine three dimensional face iterative manner using both rgb image initial three dimensional facial shape rendering dnn model end end thus complicated three dimensional rendering process avoided <eos> moreover integrate dnn architecture two components namely multi task loss function fusion convolutional neural network cnn improve facial expression reconstruction <eos> multi task loss function three dimensional face reconstruction divided into neutral three dimensional facial shape reconstruction expressive three dimensional facial shape reconstruction <eos> neutral three dimensional facial shape class specific <eos> therefore higher layer feature useful <eos> comparison expressive three dimensional facial shape favors lower intermediate layer feature <eos> fusion cnn feature different intermediate layer fused transformed predicting three dimensional expressive facial shape <eos> through extensive experiments demonstrate superiority end end framework improving accuracy three dimensional face reconstruction <eos> <eop> deep learning low precision half wave gaussian quantization <eos> problem quantizing activations deep neural network considered <eos> examination popular binary quantization approach shows consists approximating classical non linearity hyperbolic tangent two functions piecewise constant sign function used feedforward network computations piecewise linear hard tanh function used backpropagation step during network learning <eos> problem approximating widely used relu non linearity then considered <eos> half wave gaussian quantizer hwgq proposed forward approximation shown efficient implementation exploiting statistics network activations batch normalization operations <eos> overcome problem gradient mismatch due use different forward backward approximations several piece wise backward approximators then investigated <eos> implementation resulting quantized network denoted hwgq net shown achieve much closer performance full precision network such alexnet resnet googlenet vgg net than previously available low precision network bit binary weights bit quantized activations <eos> <eop> deep pyramidal residual network <eos> deep convolutional neural network dcnns shown remarkable performance image classification tasks recent years <eos> generally deep neural network architectures stacks consisting large number convolutional layer they perform downsampling along spatial dimension via pooling reduce memory usage <eos> concurrently feature map dimension <eos> number channels sharply increased downsampling locations essential ensure effective performance because increases diversity high level attributes <eos> also applies residual network very closely related their performance <eos> research instead sharply increasing feature map dimension units perform downsampling gradually increase feature map dimension all units involve many locations possible <eos> design discussed depth together new insights proven effective means improving generalization ability <eos> furthermore propose novel residual unit capable further improving classification accuracy new network architecture <eos> experiments benchmark cifar cifar imagenet datasets shown network architecture superior generalization ability compared original residual network <eos> <eop> ron reverse connection objectness prior network object detection <eos> present ron efficient effective framework generic object detection <eos> motivation smartly associate best region based <eos> faster cnn region free <eos> under fully convolutional architecture ron mainly focuses two fundamental problems multi scale object localization negative sample mining <eos> address design reverse connection enables network detect object multi levels cnn <eos> deal propose objectness prior significantly reduce searching space object <eos> optimize reverse connection objectness prior object detector jointly multi task loss function thus ron directly predict final detection result all locations various feature maps <eos> extensive experiments challenging pascal voc pascal voc ms coco benchmarks demonstrate competitive performance ron <eos> specifically vgg low resolution input size network gets <eos> map pascal voc <eos> map pascal voc datasets <eos> its superiority increases when datasets become larger more difficult demonstrated result ms coco dataset <eos> gpu memory test phase speed network fps times faster than faster cnn counterpart <eos> code will made publicly available <eos> <eop> weakly supervised visual grounding phrases linguistic structures <eos> propose weakly supervised approach takes image sentence pairs input learns visually ground <eos> localize arbitrary linguistic phrases form spatial attention masks <eos> specifically model trained image their associated image level captions without any explicit region phrase correspondence annotations <eos> end introduce end end model learns visual groundings phrases two types carefully designed loss functions <eos> addition standard discriminative loss enforces attended image region phrases consistently encoded propose novel structural loss makes use parse tree structures induced sentences <eos> particular ensure complementarity among attention masks correspond sibling noun phrases compositionality attention masks among children parent phrases defined sentence parse tree <eos> validate effectiveness approach microsoft coco visual genome datasets <eos> <eop> network sketching exploiting binary structure deep cnn <eos> convolutional neural network cnn deep architectures substantially advanced state art computer vision tasks <eos> however deep network typically resource intensive thus difficult deployed mobile devices <eos> recently cnn binary weights shown compelling efficiency community whereas accuracy such models usually unsatisfactory practice <eos> paper introduce network sketching novel technique pursuing binary weight cnn targeting more faithful inference better trade off practical applications <eos> basic idea exploit binary structure directly pre trained filter banks produce binary weight models via tensor expansion <eos> whole process treated coarse fine model approximation akin pencil drawing steps outlining shading <eos> further speedup generated models namely sketches also propose associative implementation binary tensor convolutions <eos> experimental result demonstrate proper sketch alexnet resnet outperforms existing binary weight models large margins imagenet large scale classification task while committed memory network parameters only exceeds little <eos> <eop> casenet deep category aware semantic edge detection <eos> boundary edge cues highly beneficial improving wide variety vision tasks such semantic segmentation object recognition stereo object proposal generation <eos> recently problem edge detection revisited significant progress made deep learning <eos> while classical edge detection challenging binary problem itself category aware semantic edge detection nature even more challenging multi label problem <eos> model problem such each edge pixel associated more than one class they appear contours junctions belonging two more semantic classes <eos> end propose novel end end deep semantic edge learning architecture based resnet new skip layer architecture category wise edge activations top convolution layer share fused same set bottom layer feature <eos> then propose multi label loss function supervise fused activations <eos> show proposed architecture benefits problem better performance outperform current state art semantic edge detection method large margin standard data set such sbd cityscapes <eos> <eop> geometric loss functions camera pose regression deep learning <eos> deep learning shown effective robust real time monocular image relocalisation <eos> particular posenet deep convolutional neural network learns regress dof camera pose single image <eos> learns localize using high level feature robust difficult lighting motion blur unknown camera intrinsics point based sift registration fails <eos> however was trained using naive loss function hyper parameters require expensive tuning <eos> paper give problem more fundamental theoretical treatment <eos> explore number novel loss functions learning camera pose based geometry scene reprojection error <eos> additionally show how automatically learn optimal weighting simultaneously regress position orientation <eos> leveraging geometry demonstrate technique significantly improves posenet performance across datasets ranging indoor rooms small city <eos> <eop> model based iterative restoration binary document image compression dictionary learning <eos> inherent noise observed <eos> scanned binary document image degrades image quality harms compression ratio through breaking pattern repentance adding entropy document image <eos> paper design cost function bayesian framework dictionary learning <eos> minimizing cost function produces restored image better quality than observed noisy image dictionary representing encoding image <eos> after restoration use dictionary same cost function encode restored image following symbol dictionary framework jbig standard lossless mode <eos> experimental result variety document image demonstrate method improves image quality compared observed image simultaneously improves compression ratio <eos> test image synthetic noise method reduces number flipped pixels <eos> improves compression ratio <eos> compared best encoding method <eos> test image real noise method visually improves image quality outperforms cutting edge method <eos> terms compression ratio <eos> <eop> fine grained image classification via combining vision language <eos> fine grained image classification challenging task due large intra class variance small inter class variance aiming recognizing hundreds sub categories belonging same basic level category <eos> most existing fine grained image classification method generally learn part detection models obtain semantic parts better classification accuracy <eos> despite achieving promising result method mainly two limitations all parts obtained through part detection models beneficial indispensable classification fine grained image classification requires more detailed visual descriptions could provided part locations attribute annotations <eos> addressing above two limitations paper proposes two stream model combing vision language cvl learning latent semantic representations <eos> vision stream learns deep representations original visual information via deep convolutional neural network <eos> language stream utilizes natural language descriptions could point out discriminative parts characteristics each image provides flexible compact way encoding salient visual aspects distinguishing sub categories <eos> since two streams complementary combing two streams further achieves better classification accuracy <eos> comparing state art method widely used cub dataset fine grained image classification experimental result demonstrate cvl approach achieves best performance <eos> <eop> minimal solution two view focal length estimation using two affine correspondences <eos> minimal solution using two affine correspondences presented estimate common focal length fundamental matrix between two semi calibrated cameras known intrinsic parameters except common focal length <eos> best knowledge problem unsolved <eos> proposed approach extends point correspondence based techniques linear constraints derived local affine transformations <eos> obtained multivariate polynomial system efficiently solved hidden variable technique <eos> observing geometry local affinities introduce novel conditions eliminating invalid roots <eos> select best one out remaining candidates root selection technique proposed outperforming recent ones especially case high level noise <eos> proposed point algorithm validated both synthetic data publicly available real image pairs <eos> matlab implementation proposed solution included paper <eos> <eop> joint graph decomposition node labeling problem algorithms applications <eos> state combinatorial optimization problem whose feasible solutions define both decomposition node labeling given graph <eos> problem offers common mathematical abstraction seemingly unrelated computer vision tasks including instance separating semantic segmentation articulated human body pose estimation multiple object tracking <eos> conceptually generalizes unconstrained integer quadratic program minimum cost lifted multicut problem both np hard <eos> order find feasible solutions efficiently define two local search algorithms converge monotonously local optimum offering feasible solution any time <eos> demonstrate effectiveness algorithms tackling computer vision tasks apply them instances problem construct published data using published algorithms <eos> report state art application specific accuracy three above mentioned applications <eos> <eop> detangling people individuating multiple close people their body parts via region assembly <eos> today person detection method work best when people common upright poses appear reasonably well spaced out image <eos> however many real image people <eos> people often appear quite close each other <eos> limbs linked heads touching their poses often pedestrian like <eos> propose approach detangle people multi person image <eos> formulate task region assembly problem <eos> starting large set overlapping region body part semantic segmentation generic object proposals optimization approach reassembles pieces together into multiple person instances <eos> since optimal region assembly challenging combinatorial problem present lagrangian relaxation method accelerate lower bound estimation thereby enabling fast branch bound solution global optimum <eos> output method produces pixel level map indicating both body part labels arm leg torso head parts belong individual person <eos> result challenging datasets show method robust clutter occlusion complex poses <eos> outperforms variety competing method including existing detector crf method region cnn approaches <eos> addition demonstrate its impact proxemics recognition task demands precise representation whose body part crowded image <eos> <eop> flight dynamics based recovery uav trajectory using ground cameras <eos> propose new method estimate dof trajectory flying object such quadrotor uav within three dimensional airspace monitored using multiple fixed ground cameras <eos> based new structure motion formulation three dimensional reconstruction single moving point known motion dynamics <eos> main contribution new bundle adjustment procedure addition optimizing camera poses regularizes point trajectory using prior based motion dynamics specifically flight dynamics <eos> furthermore infer underlying control input sent uav autopilot determined its flight trajectory <eos> method requires neither perfect single view tracking nor appearance matching across views <eos> robustness allow tracker generate multiple detections per frame each video <eos> true detections data association across video estimated using robust multi view triangulation subsequently refined bundle adjustment formulation <eos> quantitative evaluation simulated data experiments real video indoor outdoor scenes shows technique superior existing method <eos> <eop> surfnet generating three dimensional shape surfaces using deep residual network <eos> three dimensional shape models naturally parameterized using vertices faces <eos> composed polygons forming surface <eos> however current three dimensional learning paradigms predictive generative tasks using convolutional neural network focus voxelized representation object <eos> lifting convolution operators traditional three dimensional result high computational overhead little additional benefit most geometry information contained surface boundary <eos> here study problem directly generating three dimensional shape surface rigid non rigid shapes using deep convolutional neural network <eos> develop procedure create consistent geometry image representing three dimensional shape surface category shapes <eos> then use consistent representation category specific shape generation parametric representation image developing novel extensions deep residual network task three dimensional surface generation <eos> experiments indicate network learns meaningful representation shape surfaces allowing interpolate between shape orientations poses invent new shape surfaces reconstruct three dimensional shape surfaces previously unseen image rectify noisy correspondence between three dimensional shapes belonging same class <eos> <eop> unite people closing loop between three dimensional human representations <eos> three dimensional models provide common ground different representations human bodies <eos> turn robust estimation proven powerful tool obtain three dimensional fits wild <eos> however depending level detail hard impossible acquire labeled data training estimators large scale <eos> propose hybrid approach problem extended version recently introduced smplify method obtain high quality three dimensional body model fits multiple human pose datasets <eos> human annotators solely sort good bad fits <eos> procedure leads initial dataset up three dimensional rich annotations <eos> comprehensive set experiments show how data used train discriminative models produce result unprecedented level detail models predict segments landmark locations body <eos> using landmark pose estimator present state art result three dimensional human pose shape estimation using order magnitude less training data without assumptions about gender pose fitting procedure <eos> show up three dimensional enhanced improved fits grow quantity quality makes system deployable large scale <eos> data code models available research purposes <eos> <eop> semantically consistent regularization zero shot recognition <eos> role semantics zero shot learning considered <eos> effectiveness previous approaches analyzed according form supervision provided <eos> while some learn semantics independently others only supervise semantic subspace explained training classes <eos> thus former able constrain whole space but lacks ability model semantic correlations <eos> latter addresses issue but leaves part semantic space unsupervised <eos> complementarity exploited new convolutional neural network cnn framework proposes use semantics constraints recognition <eos> although cnn trained classification no transfer ability encouraged learning hidden semantic layer together semantic code classification <eos> two forms semantic constraints then introduced <eos> first loss based regularizer introduces generalization constraint each semantic predictor <eos> second codeword regularizer favors semantic class mappings consistent prior semantic knowledge while allowing learned data <eos> significant improvements over state art achieved several datasets <eos> <eop> simultaneous super resolution cross modality synthesis three dimensional medical image using weakly supervised joint convolutional sparse coding <eos> magnetic resonance imaging mri offers high resolution vivo imaging rich functional anatomical multimodality tissue contrast <eos> practice however there challenges associated considerations scanning costs patient comfort scanning time constrain how much data acquired clinical research studies <eos> paper explore possibility generating high resolution multimodal image low resolution single modality imagery <eos> propose weakly supervised joint convolutional sparse coding simultaneously solve problems super resolution sr cross modality image synthesis <eos> learning process requires only few registered multimodal image pairs training set <eos> additionally quality joint dictionary learning improved using larger set unpaired image <eos> combine unpaired data different image resolutions modalities hetero domain image alignment term proposed <eos> local image neighborhoods naturally preserved operating whole image domain opposed image patches using joint convolutional sparse coding <eos> paired image enhanced joint learning process unpaired data additional maximum mean discrepancy term minimizes dissimilarity between their feature distributions <eos> experiments show proposed method outperforms state art techniques both sr reconstruction simultaneous sr cross modality synthesis <eos> <eop> viraliency pooling local virality <eos> overly connected world automatic recognition virality quality image video rapidly widely spread crucial importance recently awaken interest computer vision community concurrently recent progress deep learning architectures showed global average pooling strategies allow extract class activation maps highlight part image most likely contain certain class <eos> extend concept introducing pooling layer learns size average support learned top average lena pooling <eos> hypothesize latent concepts feature maps describing virality may require such rich pooling strategy perform extensive evaluation assess validity hypothesis <eos> moreover also appraise use objectness maps predicting localizing virality image <eos> experiments shown two publicly available datasets annotated virality <eos> <eop> generative attribute controller conditional filtered generative adversarial network <eos> present generative attribute controller gac novel functionality generating editing image while intuitively controlling large variations attribute <eos> controller based novel generative model called conditional filtered generative adversarial network cfgan extension conventional conditional gan cgan incorporates filtering architecture into generator input <eos> unlike conventional cgan represents attribute directly using observable variable <eos> binary indicator attribute presence so its controllability restricted attribute labeling <eos> restricted off control cfgan filtering architecture associates attribute multi dimensional latent variable enabling latent variations attribute represented <eos> also define filtering architecture training scheme considering controllability enabling variations attribute intuitively controlled using typical controllers radio buttons slide bars <eos> evaluated cfgan mnist cub celeba datasets show enables large variations attribute only represented but also intuitively controlled while retaining identity <eos> also show learned latent space enough expressive power conduct attribute transfer attribute based image retrieval <eos> <eop> deep learning lie groups skeleton based action recognition <eos> recent years skeleton based action recognition become popular three dimensional classification problem <eos> state art method typically first represent each motion sequence high dimensional trajectory lie group additional dynamic time warping then shallowly learn favorable lie group feature <eos> paper incorporate lie group structure into deep network architecture learn more appropriate lie group feature three dimensional action recognition <eos> within network structure design rotation mapping layer transform input lie group feature into desirable ones aligned better temporal domain <eos> reduce high feature dimensionality architecture equipped rotation pooling layer elements lie group <eos> furthermore propose logarithm mapping layer map resulting manifold data into tangent space facilitates application regular output layer final classification <eos> evaluations proposed network standard three dimensional human action recognition datasets clearly demonstrate its superiority over existing shallow lie group feature learning method well most conventional deep learning method <eos> <eop> dynamic time flight <eos> time flight tof depth cameras provide robust depth inference low power requirements wide variety consumer industrial applications <eos> cameras reconstruct single depth frame given set infrared ir frames captured over very short exposure period <eos> operating mode camera essentially forgets all information previously captured performs depth inference scratch every frame <eos> challenge practice propose using previously captured information when inferring depth <eos> inherent problem address camera motion over longer period collecting observations <eos> derive probabilistic framework combining simple but robust model camera object motion together observation model <eos> combination allows integrate information over multiple frames while remaining robust rapid changes <eos> operating camera manner implications terms both computational efficiency how information should captured <eos> address two issues demonstrate realtime tof system robust temporal integration improves depth accuracy over strong baseline method including adaptive spatio temporal filters <eos> <eop> walking measuring along chord bunches better describe leaf shapes <eos> effectively describing recognizing leaf shapes under arbitrary deformations particularly large database remains unsolved problem <eos> research attempted new strategy describing shape walking along bunch chords pass through shape measure region trespassed <eos> novel chord bunch walks cbw descriptor developed through chord walking effectively integrates shape image function over walked chord reflect contour feature inner properties shape <eos> each contour point chord bunch groups multiple pairs chord walks build hierarchical framework coarse fine description <eos> proposed cbw descriptor invariant rotation scaling translation mirror transforms <eos> instead using expensive optimal correspondence based matching improved hausdorff distance encoded correspondence information proposed efficient yet effective shape matching <eos> experimental studies proposed method obtained substantially higher accuracies low computational cost over benchmarks indicates research potential along direction <eos> <eop> ubernet training universal convolutional neural network low mid high level vision using diverse datasets limited memory <eos> work train end end manner convolutional neural network cnn jointly handles low mid high level vision tasks unified architecture <eos> such network act like swiss knife vision tasks call ubernet indicate its overarching nature <eos> main contribution work consists handling challenges emerge when scaling up many tasks <eos> introduce techniques facilitate training deep architecture while relying diverse training set ii training many potentially unlimited tasks limited memory budget <eos> allows train end end manner unified cnn architecture jointly handles boundary detection normal estimation saliency estimation semantic segmentation human part segmentation semantic boundary detection region proposal generation object detection <eos> obtain competitive performance while jointly addressing all tasks <eos> system will made publicly available <eos> <eop> parametric spline face morphable model detailed fitting shape subspace <eos> pre learnt subspace method <eos> dmms significant exploration synthesis three dimensional faces assuming faces linear class <eos> however human face nonlinear manifold new test always pre learnt subspace accurately because disparity brought ethnicity age gender etc <eos> paper propose parametric spline morphable model splinemm three dimensional face representation great advantages fitting data unknown source accurately <eos> model describe face spline surface divide face surface into several shape units sus according facial action coding system facs mesh instead surface directly <eos> fitting algorithm proposed optimize coefficients spline control point components along pre learnt identity expression subspaces well optimize details refinement progress <eos> any pre learnt subspace complete handle variety details faces expressions covers limited span morphing <eos> sus division detail refinement make model fitting facial muscle deformation larger span morphing subspace <eos> conduct experiments face scan data kinect data well space time data test performance detail fitting robustness missing data noise demonstrate effectiveness model <eos> convincing result illustrated demonstrate effectiveness model compared popular method <eos> <eop> convolutional neural network architecture geometric matching <eos> address problem determining correspondences between two image agreement geometric model such affine thin plate spline transformation estimating its parameters <eos> contributions work three fold <eos> first propose convolutional neural network architecture geometric matching <eos> architecture based three main components mimic standard steps feature extraction matching simultaneous inlier detection model parameter estimation while being trainable end end <eos> second demonstrate network parameters trained synthetically generated imagery without need manual annotation matching layer significantly increases generalization capabilities never seen before image <eos> finally show same model perform both instance level category level matching giving state art result challenging proposal flow dataset <eos> <eop> deep representation learning human motion prediction classification <eos> generative models three dimensional human motion often restricted small number activities therefore generalize well novel movements applications <eos> work propose deep learning framework human motion capture data learns generic representation large corpus motion capture data generalizes well new unseen motions <eos> using encoding decoding network learns predict future three dimensional poses most recent past extract feature representation human motion <eos> most work deep learning sequence prediction focuses video speech <eos> since skeletal data different structure present evaluate different network architectures make different assumptions about time dependencies limb correlations <eos> quantify learned feature use output different layer action classification visualize receptive fields network units <eos> method outperforms recent state art skeletal motion prediction even though use action specific training data <eos> result show deep feedforward network trained generic mocap database successfully used feature extraction human motion data representation used foundation classification prediction <eos> <eop> deep affordance grounded sensorimotor object recognition <eos> well established cognitive neuroscience human perception object constitutes complex process object appearance information combined evidence about so called object affordances namely types actions humans typically perform when interacting them <eos> fact recently motivated sensorimotor approach challenging task automatic object recognition both information sources fused improve robustness <eos> work aforementioned paradigm adopted surpassing current limitations sensorimotor object recognition research <eos> specifically deep learning paradigm introduced problem first time developing number novel neuro biologically neuro physiologically inspired architectures utilize state art neural network fusing available information sources multiple ways <eos> proposed method evaluated using large rgb corpus specifically collected task sensorimotor object recognition made publicly available <eos> experimental result demonstrate utility affordance information object recognition achieving up relative error reduction its inclusion <eos> <eop> all you need beyond good init exploring better solution training extremely deep convolutional neural network orthonormality modulation <eos> deep neural network difficult train predicament becomes worse depth increases <eos> essence problem exists magnitude backpropagated errors will result gradient vanishing exploding phenomenon <eos> show variant regularizer utilizes orthonormality among different filter banks alleviate problem <eos> moreover design backward error modulation mechanism based quasi isometry assumption between two consecutive parametric layer <eos> equipped two ingredients propose several novel optimization solutions utilized training specific structured repetitively triple modules conv bnrelu extremely deep convolutional neural network cnn without any shortcuts identity mappings scratch <eos> experiments show proposed solutions achieve distinct improvements layer layer plain network both cifar imagenet datasets <eos> moreover successfully train plain cnn match performance residual counterparts <eos> besides propose new principles designing network structure insights evoked orthonormality <eos> combined residual structure achieve comparative performance imagenet dataset <eos> <eop> scale aware face detection <eos> convolutional neural network cnn based face detectors inefficient handling faces diverse scales <eos> they rely either fitting large single model faces across large scale range multi scale testing <eos> both computationally expensive <eos> propose scale aware face detection safd handle scale explicitly using cnn achieve better performance less computation cost <eos> prior detection efficient cnn predicts scale distribution histogram faces <eos> then scale histogram guides zoom zoom out image <eos> since faces will approximately uniform scale after zoom they detected accurately even much smaller cnn <eos> actually more than faces afw covered less than two zooms per image <eos> extensive experiments fddb malf afw show advantages safd <eos> <eop> intrinsic grassmann averages online linear robust subspace learning <eos> principal component analysis pca fundamental method estimating linear subspace approximation high dimensional data <eos> many algorithms exist literature achieve statistically robust version pca called rpca <eos> paper present geometric framework computing principal linear subspaces both situations amounts computing intrinsic average space all subspaces grassmann manifold <eos> point manifold defined subspaces spanned tuples observations <eos> show intrinsic grassmann average subspaces coincide principal components observations when they drawn gaussian distribution <eos> similar result also shown hold rpca <eos> further propose efficient online algorithm subspace averaging linear complexity terms number sample linear convergence rate <eos> when data outliers proposed online robust subspace averaging algorithm shows significant performance accuracy computation time gain over recently published rpca method publicly accessible code <eos> demonstrated competitive performance proposed online subspace algorithm method one synthetic two real data set <eos> experimental result depicting stability proposed method also presented <eos> furthermore two real outlier corrupted datasets present comparison experiments showing lower reconstruction error using online rpca algorithm <eos> terms reconstruction error time required both algorithms outperform competition <eos> <eop> object co skeletonization co segmentation <eos> recent advances joint processing image certainly shown its advantages over individual processing <eos> different existing works geared towards co segmentation co localization paper explore new joint processing topic co skeletonization defined joint skeleton extraction common object set semantically similar image <eos> object skeletonization real world image challenging problem because there no prior knowledge object shape if consider only single image <eos> motivates resort idea object co skeletonization hoping commonness prior existing across similar image may help just other joint processing problems such co segmentation <eos> noting skeleton provide good scribbles segmentation skeletonization turn needs good segmentation propose coupled framework co skeletonization co segmentation tasks so they well informed each other benefit each other synergistically <eos> since new problem also construct benchmark dataset co skeletonization task <eos> extensive experiments demonstrate proposed method achieves very competitive result <eos> <eop> product split trees <eos> work introduce new kind spatial partition trees efficient nearest neighbor search <eos> approach first identifies set useful data splitting directions then learns codebook used encode such directions <eos> use product quantization idea order make effective codebook large evaluation scalar products between query encoded splitting direction very fast encoding itself compact <eos> result proposed data srtucture product split tree achieves compact clustering data point while keeping traversal very efficient <eos> nearest neighbor search experiments high dimensional data product split trees achieved state art performance demonstrating better speed accuracy tradeoff than other spatial partition trees <eos> <eop> pose aware person recognition <eos> person recognition method use multiple body region shown significant improvements over traditional face based recognition <eos> one primary challenges full body person recognition extreme variation pose view point <eos> work present approach tackles pose variations utilizing multiple models trained specific poses combined using pose aware weights during testing <eos> ii learning person representation propose network jointly optimizes single loss over multiple body region <eos> iii finally introduce new benchmarks evaluate person recognition diverse scenarios show significant improvements over previously proposed approaches all benchmarks including photo album setting pipa <eos> <eop> dynamic faust registering human bodies motion <eos> while ready availability three dimensional scan data influenced research throughout computer vision less attention focused data three dimensional scans moving non rigid object captured over time <eos> useful vision research such scans need registered aligned common topology <eos> consequently extending mesh registration method important <eos> unfortunately no ground truth datasets available quantitative evaluation comparison registration method <eos> address create novel dataset high resolution scans human subjects motion captured fps <eos> propose new mesh registration method uses both three dimensional geometry texture information register all scans sequence common reference topology <eos> approach exploits consistency texture over both short long time intervals deals temporal offsets between shape texture capture <eos> show how using geometry alone result significant errors alignment when motions fast non rigid <eos> evaluate accuracy registration provide dataset raw aligned meshes <eos> dynamic faust extends popular faust dataset dynamic data available research purposes dfaust <eos> <eop> cnn slam real time dense monocular slam learned depth prediction <eos> given recent advances depth prediction convolutional neural network cnn paper investigates how predicted depth maps deep neural network deployed goal accurate dense monocular reconstruction <eos> propose method cnn predicted dense depth maps naturally fused together depth measurements obtained direct monocular slam based scheme privileges depth prediction image locations monocular slam approaches tend fail <eos> along low textured region vice versa <eos> demonstrate use depth prediction estimate absolute scale reconstruction hence overcoming one major limitations monocular slam <eos> finally propose framework efficiently fuse semantic labels obtained single frame dense slam so yield semantically coherent scene reconstruction single view <eos> evaluation result two benchmark datasets show robustness accuracy approach <eos> <eop> alternating direction graph matching <eos> paper introduce graph matching method account constraints arbitrary order arbitrary potential functions <eos> unlike previous decomposition approaches rely graph structures introduce decomposition matching constraints <eos> graph matching then reformulated non convex non separable optimization problem split into smaller much easier solve subproblems means alternating direction method multipliers <eos> proposed framework modular scalable instantiated into different variants <eos> two instantiations studied exploring pairwise higher order constraints <eos> experimental result widely adopted benchmarks involving synthetic real examples demonstrate proposed solutions outperform existing pairwise graph matching method competitive state art higher order settings <eos> <eop> dust dual union spatio temporal subspaces monocular multiple object three dimensional reconstruction <eos> present approach reconstruct three dimensional shape multiple deforming object incomplete trajectories acquired single camera <eos> additionally simultaneously provide spatial segmentation <eos> identify each object every frame temporal clustering <eos> split sequence into primitive actions <eos> advances existing work only tackled problem one single object non occluded tracks <eos> order handle several object time partial observations model point trajectories union spatial temporal subspaces optimize parameters both modalities non observed point tracks three dimensional shape via augmented lagrange multipliers <eos> algorithm fully unsupervised result formulation need initialization <eos> thoroughly validate method challenging scenarios several human subjects performing different activities involve complex motions close interaction <eos> show approach achieves state art three dimensional reconstruction result while also provides spatial temporal segmentation <eos> <eop> unsupervised part learning visual recognition <eos> part based image classification aims representing categories small set learned discriminative parts upon image representation built <eos> considered promising avenue decade ago direction neglected since advent deep neural network <eos> context paper brings two contributions first work proceeds one step further compared recent part based models pbm focusing how learn parts without using any labeled data <eos> instead learning set parts per class generally performed pbm literature proposed approach both constructs partition given set image into visually similar groups subsequently learns set discriminative parts per group fully unsupervised fashion <eos> strategy opens door use pbm new applications labeled data typically available such instance based image retrieval <eos> second paper shows despite recent success end end models explicit part learning still boost classification performance <eos> experimentally show learned parts help building efficient image representations outperform state art deep convolutional neural network dcnn both classification retrieval tasks <eos> <eop> parsing image overlapping organisms deep singling out network <eos> work motivated mostly unsolved task parsing biological image multiple overlapping articulated model organisms such worms larvae <eos> present general approach separates two main challenges associated such data individual object shape estimation object groups disentangling <eos> core approach deep feed forward singling out network son trained map each local patch vectorial descriptor sensitive characteristics <eos> shape central object while being invariant variability all other surrounding elements <eos> given son local image patch matched gallery isolated elements using their son descriptors thus producing hypothesis about shape central element patch <eos> image level optimization based integer programming then pick subset hypotheses explain parse whole image disentangle groups organisms <eos> while sharing many similarities existing analysis synthesis approaches method avoids need stochastic search high dimensional configuration space numerous rendering operations test time <eos> show approach parse microscopy image three popular model organisms <eos> elegans roundworms drosophila larvae <eos> coli bacteria even under significant crowding overlaps between organisms <eos> speculate overall approach applicable wider class image parsing problems concerned crowded articulated object rendering training image possible <eos> <eop> deep multitask architecture integrated three dimensional human sensing <eos> propose deep multitask architecture fully automatic human sensing dmhs including recognition reconstruction monocular image <eos> system computes figure ground segmentation semantically identifies human body parts pixel level estimates pose person <eos> model supports joint training all components means multi task losses early processing stages recursively feed into advanced ones increasingly complex calculations accuracy robustness <eos> design allows tie complete training protocol taking advantage multiple datasets would otherwise restrictively cover only some model components complex image data no body part labeling without associated ground truth complex data limited background variability <eos> detailed experiments based several challenging datasets lsp humaneva human <eos> evaluate sub structures model effect various types training data multitask loss demonstrate state art result achieved all processing levels <eos> also show wild monocular rgb architecture perceptually competitive state art commercial kinect system based rgb data <eos> <eop> quo vadis action recognition new model kinetics dataset <eos> paucity video current action classification datasets ucf hmdb made difficult identify good video architectures most method obtain similar performance existing small scale benchmarks <eos> paper re evaluates state art architectures light new kinetics human action video dataset <eos> kinetics two orders magnitude more data human action classes over clips per class collected realistic challenging youtube video <eos> provide analysis how current architectures fare task action classification dataset how much performance improves smaller benchmark datasets after pre training kinetics <eos> also introduce new two stream inflated three dimensional convnet based convnet inflation filters pooling kernels very deep image classification convnets expanded into three dimensional making possible learn seamless spatio temporal feature extractors video while leveraging successful imagenet architecture designs even their parameters <eos> show after pre training kinetics models considerably improve upon state art action classification reaching <eos> <eop> discriminative correlation filter channel spatial reliability <eos> short term tracking open challenging problem discriminative correlation filters dcf shown excellent performance <eos> introduce channel spatial reliability concepts dcf tracking provide novel learning algorithm its efficient seamless integration filter update tracking process <eos> spatial reliability map adjusts filter support part object suitable tracking <eos> allows tracking non rectangular object well extending search region <eos> channel reliability reflects quality learned filter used feature weighting coefficient localization <eos> experimentally only two simple standard feature hogs colornames novel csr dcf method dcf channel spatial reliability achieves state art result vot vot otb <eos> csr dcf runs real time cpu <eos> <eop> light field reconstruction using deep convolutional network epi <eos> paper take advantage clear texture structure epipolar plane image epi light field data model problem light field reconstruction sparse set views cnn based angular detail restoration epi <eos> indicate one main challenges sparsely sampled light field reconstruction information asymmetry between spatial angular domain detail portion angular domain damaged undersampling <eos> balance spatial angular information spatial high frequency components epi removed using epi blur before feeding network <eos> finally non blind deblur operation used recover spatial detail suppressed epi blur <eos> evaluate approach several datasets including synthetic scenes real world scenes challenging microscope light field data <eos> demonstrate high performance robustness proposed framework compared state arts algorithms <eos> also show further application depth enhancement using reconstructed light field <eos> <eop> noise robust depth focus using ring difference filter <eos> depth focus dff method estimating depth scene using information acquired through change focus camera <eos> within framework dff focus measure fm forms foundation accuracy output determined <eos> result fm role dff pipeline determine recalculate unreliable measurements while enhancing reliable <eos> paper propose new fm more accurately robustly measures focus call ring difference filter rdf <eos> fms usually categorized confident local method noise robust non local method <eos> rdf unique ring disk structure allows advantageous sides both local non local fms <eos> then describe efficient pipeline utilizes properties rdf brings <eos> method able reproduce result par even better than state art while spending less time computation <eos> <eop> improving ransac based segmentation through cnn encapsulation <eos> work present method improving random sample consensus ransac based image segmentation algorithm encapsulating within convolutional neural network cnn <eos> improvements gained gradient descent training set pre ransac filtering thresholding operations using novel ransac based loss function geared toward optimizing strength correct model relative most convincing false model <eos> thus said loss function trains network metrics directly dictate success failure final segmentation rather than metrics merely correlated success failure <eos> demonstrate successful application method ransac method identifying pupil boundary image casia irisv iris recognition data set expect method could successfully applied any ransac based segmentation algorithm <eos> <eop> bayesian supervised hashing <eos> among learning based hashing method supervised hashing seeks compact binary representation training data preserve semantic similarities <eos> recent years witnessed various problem formulations optimization method supervised hashing <eos> most them optimize form loss function regulization term viewed maximum posterior map estimation hashing codes <eos> however approaches prone overfitting unless hyperparameters tuned carefully <eos> address problem present novel fully bayesian treatment supervised hashing problem named bayesian supervised hashing bsh hyperparameters automatically tuned during optimization <eos> additionally utilizing automatic relevance determination ard figure out relative discriminating ability different hashing bits select most informative bits among them <eos> experimental result three real world image datasets semantic information show bsh achieve superior performance over state art method comparable training time <eos> <eop> mimicking very efficient network object detection <eos> current cnn based object detectors need initialization pre trained imagenet classification models usually time consuming <eos> paper present fully convolutional feature mimic framework train very efficient cnn based detectors need imagenet pre training achieve competitive performance large slow models <eos> add supervision high level feature large network training help small network better learn object representation <eos> more specifically conduct mimic method feature sampled entire feature map use transform layer map feature small network onto same dimension large network <eos> training small network optimize similarity between feature sampled same region feature maps both network <eos> extensive experiments conducted pedestrian common object detection tasks using vgg inception resnet <eos> both caltech pascal voc show modified <eos> accelerated inception network achieves competitive performance full inception network <eos> faster model runs fps large input only minor degradation performance caltech <eos> <eop> menagerie modeling three dimensional shape pose animals <eos> there significant work learning realistic articulated three dimensional models human body <eos> contrast there few such models animals despite many applications <eos> main challenge animals much less cooperative than humans <eos> best human body models learned thousands three dimensional scans people specific poses infeasible live animals <eos> consequently learn model small set three dimensional scans toy figurines arbitrary poses <eos> employ novel part based shape model compute initial registration scans <eos> then normalize their pose learn statistical shape model refine registrations model together <eos> way accurately align animal scans different quadruped families very different shapes poses <eos> registration common template learn shape space representing animals including lions cats dogs horses cows hippos <eos> animal shapes sampled model posed animated fit data <eos> demonstrate generalization fitting image real animals including species seen training <eos> <eop> training object class detectors click supervision <eos> training object class detectors typically requires large set image object annotated bounding boxes <eos> however manually drawing bounding boxes very time consuming <eos> paper greatly reduce annotation time proposing center click annotations ask annotators click center imaginary bounding box tightly encloses object instance <eos> then incorporate clicks into existing multiple instance learning techniques weakly supervised object localization jointly localize object bounding boxes over all training image <eos> extensive experiments pascal voc ms coco show scheme delivers high quality detectors performing substantially better than produced weakly supervised techniques modest extra annotation effort detectors fact perform range close trained manually drawn bounding boxes center click task very fast scheme reduces total annotation time <eos> <eop> light field superpixel segmentation <eos> superpixel segmentation image widely used many computer vision tasks <eos> however limited gaussian imaging principle there thorough segmentation solution ambiguity defocus occlusion boundary areas <eos> paper consider essential element image pixel <eos> rays light space propose light field superpixel lfsp segmentation eliminate ambiguity <eos> lfsp first defined mathematically then refocus invariant metric named lfsp self similarity proposed evaluate segmentation performance <eos> building clique system containing neighbors light field robust refocus invariant lfsp segmentation algorithm developed <eos> experimental result both synthetic real light field datasets demonstrate advantages over state arts terms traditional evaluation metrics <eos> additionally lfsp self similarity evaluation under different light field refocus levels shows refocus invariance proposed algorithm <eos> <eop> joint sequence learning cross modality convolution three dimensional biomedical segmentation <eos> deep learning models such convolutional neural network widely used three dimensional biomedical segmentation achieve state art performance <eos> however most them often adapt single modality stack multiple modalities different input channels ignores correlations among them <eos> leverage multi modalities propose deep convolution encoder decoder structure fusion layer incorporate different modalities mri data <eos> addition exploit convolutional lstm convlstm model sequence slices jointly learn multi modalities convlstm end end manner <eos> avoid converging certain labels adopt re weighting scheme two phase training handle label imbalance <eos> experimental result brats show method outperforms state art biomedical segmentation approaches <eos> <eop> multi task clustering human actions sharing information <eos> sharing information between multiple tasks enhance accuracy human action recognition systems <eos> however using shared information improve multi task human action clustering never considered before cannot achieved using existing clustering method <eos> work present novel effective multi task information bottleneck mtib clustering method capable exploring shared information between multiple action clustering tasks improve performance individual task <eos> motivation different action collections always share many similar action patterns exploiting shared information lead improved performance <eos> specifically mtib generally formulates problem information loss minimization function <eos> function shared information quantified distributional correlation clusters different tasks based high level common vocabulary constructed through novel agglomerative information maximization method <eos> extensive experiments two kinds challenging data set including realistic action data set hmdb ucf olympic youtube cross view data set ixmas wvu show proposed approach compares favorably state art method <eos> <eop> geodesic distance descriptors <eos> gromov hausdorff gh distance traditionally used measuring distances between metric spaces <eos> was adapted non rigid shape comparison matching isometric surfaces defined minimal distortion embedding one surface into other while optimal correspondence described map minimizes distortion <eos> solving such minimization hard combinatorial problem requires precomputation storing all pairwise geodesic distances matched surfaces <eos> popular way compact representation functions surfaces projecting them into leading eigenfunctions laplace beltrami operator lbo <eos> when truncated basis lbo known optimal representing functions bounded gradient min max sense <eos> method such spectral gmds exploit idea simplify efficiently approximate minimization related gh distance operating truncated spectral domain obtain state art result matching nearly isometric shapes <eos> however when considering only specific set functions surface such geodesic distances optimized basis could considered even better alternative <eos> moreover current simplifications approximating gh distance introduce errors due low rank approximations relaxations permutation matrices <eos> here define geodesic distance basis optimal compact approximation geodesic distances terms frobenius norm <eos> use suggested basis extract geodesic distance descriptor gdd encodes geodesic distances information linear combination basis functions <eos> then show how ideas used efficiently accurately approximate metric spaces matching problem almost no loss information <eos> incorporate recent method efficient approximation proposed basis descriptor without actually computing storing all geodesic distances <eos> observations used construct very simple efficient procedure shape correspondence <eos> experimental result show gdd improves both accuracy efficiency state art shape matching procedures <eos> <eop> deeply aggregated alternating minimization image restoration <eos> regularization based image restoration remained active research topic image processing computer vision <eos> often leverages guidance signal captured different fields additional cue <eos> work present general framework image restoration called deeply aggregated alternating minimization deepam <eos> propose train deep neural network advance two steps conventional am algorithm proximal mapping continuation <eos> both steps learned large dataset end end manner <eos> proposed framework enables convolutional neural network cnn operate regularizer am algorithm <eos> show learned regularizer via deep aggregation outperforms recent data driven approaches well nonlocal based method <eos> flexibility effectiveness framework demonstrated several restoration tasks including single image denoising rgb nir restoration depth super resolution <eos> <eop> mdnet semantically visually interpretable medical image diagnosis network <eos> inability interpret model prediction semantically visually meaningful ways well known shortcoming most existing computer aided diagnosis method <eos> paper propose mdnet establish direct multimodal mapping between medical image diagnostic reports read image generate diagnostic reports retrieve image symptom descriptions visualize attention provide justifications network diagnosis process <eos> mdnet includes image model language model <eos> image model proposed enhance multi scale feature ensembles utilization efficiency <eos> language model integrated improved attention mechanism aims read explore discriminative image feature descriptions reports learn direct mapping sentence words image pixels <eos> overall network trained end end using developed optimization strategy <eos> based pathology bladder cancer image its diagnostic reports bcidr dataset conduct sufficient experiments demonstrate mdnet outperforms comparative baselines <eos> proposed image model obtains state art performance two cifar datasets well <eos> <eop> computational imaging electric grid <eos> night beats alternating current ac illumination <eos> passively sensing beat reveal new scene information includes type bulbs scene phases electric grid up city scale light transport matrix <eos> information yields unmixing reflections semi reflections nocturnal high dynamic range scene rendering bulbs observed during acquisition <eos> latter facilitated database bulb response functions range sources collected provide <eos> all built novel coded exposure high dynamic range imaging technique specifically designed operate grid ac lighting <eos> <eop> lip reading sentences wild <eos> goal work recognise phrases sentences being spoken talking face without audio <eos> unlike previous works focussed recognising limited number words phrases tackle lip reading open world problem unconstrained natural language sentences wild video <eos> key contributions watch listen attend spell wlas network learns transcribe video mouth motion characters curriculum learning strategy accelerate training reduce overfitting lip reading sentences lrs dataset visual speech recognition consisting over natural sentences british television <eos> wlas model trained lrs dataset surpasses performance all previous work standard lip reading benchmark datasets often significant margin <eos> lip reading performance beats professional lip reader video bbc television also demonstrate if audio available then visual information helps improve speech recognition performance <eos> <eop> arttrack articulated multi person tracking wild <eos> paper propose approach articulated tracking multiple people unconstrained video <eos> starting point model resembles existing architectures single frame pose estimation but substantially faster <eos> achieve two ways simplifying sparsifying body part relationship graph leveraging recent method faster inference offloading substantial share computation onto feed forward convolutional architecture able detect associate body joints same person even clutter <eos> use model generate proposals body joint locations formulate articulated tracking spatio temporal grouping such proposals <eos> allows jointly solve association problem all people scene propagating evidence strong detections through time enforcing constraints each proposal assigned one person only <eos> report result public mpii human pose benchmark new mpii video pose dataset image sequences multiple people <eos> demonstrate model achieves state art result while using only fraction time able leverage temporal information improve state art crowded scenes <eos> <eop> lstm self supervision detailed behavior analysis <eos> behavior analysis provides crucial non invasive easily accessible diagnostic tool biomedical research <eos> detailed analysis posture changes during skilled motor tasks reveal distinct functional deficits their restoration during recovery <eos> specific scenario based neuroscientific study rodents recovering large sensorimotor cortex stroke skilled forelimb grasping being recorded <eos> given large amounts unlabeled video recorded during such long term studies seek approach captures fine grained details posture its change during rehabilitation without costly manual supervision <eos> therefore utilize self supervision automatically learn accurate posture behavior representations analyzing motor function <eos> learning model depends following fundamental elements limb detection based fully convolutional network ini tialized solely using motion information ii novel self supervised training lstms using only temporal permu tation yields detailed representation behavior iii back propagation sequence representation also im proves description individual postures <eos> establish novel test dataset expert annotations evaluation fine grained behavior analysis <eos> moreover demonstrate generality approach successfully applying self supervised learning human posture two standard benchmark datasets <eos> <eop> making deg video watchable learning videography click free viewing <eos> deg video requires human viewers actively control look while watching video <eos> although provides more immersive experience visual content also introduces additional burden viewers awkward interfaces navigate video lead suboptimal viewing experiences <eos> virtual cinematography appealing direction remedy problems but conventional method limited virtual environments rely hand crafted heuristics <eos> propose new algorithm virtual cinematography automatically controls virtual camera within deg video <eos> compared state art algorithm allows more general camera control avoids redundant outputs extracts its output video substantially more efficiently <eos> experimental result over hours real wild video show generalized camera control crucial viewing deg video while proposed efficient algorithm essential making generalized control computationally tractable <eos> <eop> creativity generating diverse questions using variational autoencoders <eos> generating diverse questions given image important task computational education entertainment ai assistants <eos> different many conventional prediction techniques need algorithms generate diverse set plausible questions refer creativity <eos> paper propose creative algorithm visual question generation combines advantages variational autoencoders long short term memory network <eos> demonstrate framework able generate large set varying questions given single input image <eos> <eop> tracking natural language specification <eos> paper strives track target object video <eos> rather than specifying target first frame video bounding box propose track object based natural language specification target provides more natural human machine interaction well means improve tracking result <eos> define three variants tracking language specification one relying lingual target specification only one relying visual target specification based language one leveraging their joint capacity <eos> show potential tracking natural language specification extend two popular tracking datasets lingual descriptions report experiments <eos> finally also sketch new tracking scenarios surveillance other live video streams become feasible lingual specification target <eos> <eop> video captioning transferred semantic attributes <eos> automatically generating natural language descriptions video plays fundamental challenge computer vision community <eos> most recent progress problem achieved through employing convolutional neural network cnn encode video content recurrent neural network rnns decode sentence <eos> paper present long short term memory transferred semantic attributes lstm tsa novel deep architecture incorporates transferred semantic attributes learnt image video into cnn plus rnn framework training them end end manner <eos> design lstm tsa highly inspired facts semantic attributes play significant contribution captioning image video carry complementary semantics thus reinforce each other captioning <eos> boost video captioning propose novel transfer unit model mutually correlated attributes learnt image video <eos> extensive experiments conducted three public datasets <eos> msvd vad mpii md <eos> proposed lstm tsa achieves date best published performance sentence generation msvd <eos> terms bleu cider <eos> superior result also reported vad mpii md when compared state art method <eos> <eop> personalizing gesture recognition using hierarchical bayesian neural network <eos> building robust classifiers trained data susceptible group subject specific variations challenging pattern recognition problem <eos> develop hierarchical bayesian neural network capture subject specific variations share statistical strength across subjects <eos> leveraging recent work learning bayesian neural network build fast scalable algorithms inferring posterior distribution over all network weights hierarchy <eos> also develop method adapting model new subjects when small number subject specific personalization data available <eos> finally investigate active learning algorithms interactively labeling personalization data resource constrained scenarios <eos> focusing problem gesture recognition inter subject variations commonplace demonstrate effectiveness proposed techniques <eos> test framework three widely used gesture recognition datasets achieving personalization performance competitive state art <eos> <eop> flexible spatio temporal network video prediction <eos> describe modular framework video frame prediction <eos> refer flexible spatio temporal network fstn allows extrapolation video sequence well estimation synthetic frames lying between observed frames thus generation slow motion video <eos> devising customized objective function comprising decoding encoding adversarial losses able mitigate common problem blurry predictions managing retain high frequency information even relatively distant future predictions <eos> propose analyse different training strategies optimize model <eos> extensive experiments several challenging public datasets demonstrate both versatility validity model <eos> <eop> soft margin mixture regressions <eos> nonlinear regression common statistical tool solve many computer vision problems <eos> age estimation pose estimation <eos> existing approaches nonlinear regression fall into two main categories universal approach provides implicit explicit homogeneous feature mapping <eos> kernel ridge regression gaussian process regression neural network <eos> approaches may fail when data heterogeneous discontinuous <eos> divide conquer approaches partition heterogeneous input feature space learn multiple local regressors <eos> however existing divide conquer approaches fail deal discontinuities between partitions <eos> gaussian mixture regressions they cannot guarantee partitioned input space will homogeneously modeled local regressors <eos> address issues paper proposes soft margin mixture regressions smmr method directly learns homogeneous partitions input space able deal discontinuities <eos> smmr outperforms state art method three popular computer vision tasks age estimation crowd counting viewpoint estimation image <eos> <eop> network dissection quantifying interpretability deep visual representations <eos> propose general framework called network dissection quantifying interpretability latent representations cnn evaluating alignment between individual hidden units set semantic concepts <eos> given any cnn model proposed method draws data set concepts score semantics hidden units each intermediate convolutional layer <eos> units semantics labeled across broad range visual concepts including object parts scenes textures materials colors <eos> use proposed method test hypothesis interpretability axis independent property representation space then apply method compare latent representations various network when trained solve different classification problems <eos> further analyze effect training iterations compare network trained different initializations measure effect dropout batch normalization interpretability deep visual representations <eos> demonstrate proposed method shed light characteristics cnn models training method go beyond measurements their discriminative power <eop> straight shapes real time detection encoded shapes <eos> current object detection approaches predict bounding boxes provide little instance specific information beyond location scale aspect ratio <eos> work propose regress directly object shapes addition their bounding boxes categories <eos> crucial find appropriate shape representation compact decodable object compared higher order concepts such view similarity pose variation occlusion <eos> achieve use denoising convolutional auto encoder learn low dimensional shape embedding space <eos> place decoder network after fast end end deep convolutional network trained regress directly shape vectors provided auto encoder <eos> yields best knowledge first real time shape prediction network running fps high end desktop <eos> higher order shape reasoning well integrated into network pipeline network shows useful practical quality generalising unseen categories similar ones training set something most existing approaches fail handle <eos> <eop> fcss fully convolutional self similarity dense semantic correspondence <eos> present descriptor called fully convolutional self similarity fcss dense semantic correspondence <eos> robustly match point among different instances within same object class formulate fcss using local self similarity lss within fully convolutional network <eos> contrast existing cnn based descriptors fcss inherently insensitive intra class appearance variations because its lss based structure while maintaining precise localization ability deep neural network <eos> sampling patterns local structure self similarity measure jointly learned within proposed network end end multi scale manner <eos> training data semantic correspondence rather limited propose leverage object candidate priors provided existing image datasets also correspondence consistency between object pairs enable weakly supervised learning <eos> experiments demonstrate fcss outperforms conventional handcrafted descriptors cnn based descriptors various benchmarks <eos> <eop> variational bayesian multiple instance learning gaussian processes <eos> gaussian processes gps effective bayesian predictors <eos> here show first time instance labels gp classifier inferred multiple instance learning mil setting using variational bayes <eos> achieve via new construction bag likelihood assumes large value if instance predictions obey mil constraints small value otherwise <eos> construction lets derive update rules variational parameters analytically assuring both scalable learning fast convergence <eos> observe model improve state art instance label prediction bag level supervision newsgroups benchmark well barrett cancer tumor localization histopathology tissue microarray image <eos> furthermore introduce novel pipeline weakly supervised object detection naturally complemented model improves state art pascal voc data set <eos> last but least performance model further boosted up using mixed supervision combination weak bag strong instance labels <eos> <eop> incorporating copying mechanism image captioning learning novel object <eos> image captioning often requires large set training image sentence pairs <eos> practice however acquiring sufficient training pairs always expensive making recent captioning models limited their ability describe object outside training corpora <eos> paper present long short term memory copying mechanism lstm new architecture incorporates copying into convolutional neural network cnn plus recurrent neural network rnn image captioning framework describing novel object captions <eos> specifically freely available object recognition datasets leveraged develop classifiers novel object <eos> lstm then nicely integrates standard word word sentence generation decoder rnn copying mechanism may instead select words novel object proper places output sentence <eos> extensive experiments conducted both mscoco image captioning imagenet datasets demonstrating ability proposed lstm architecture describe novel object <eos> furthermore superior result reported when compared state art deep models <eos> <eop> beyond instance level image retrieval leveraging captions learn global visual representation semantic retrieval <eos> querying example image simple intuitive interface retrieve information visual database <eos> most research image retrieval focused task instance level image retrieval goal retrieve image contain same object instance query image <eos> work move beyond instance level retrieval consider task semantic image retrieval complex scenes goal retrieve image share same semantics query image <eos> show despite its subjective nature task semantically ranking visual scenes consistently implemented across pool human annotators <eos> also show similarity based human annotated region level captions highly correlated human ranking constitutes good computable surrogate <eos> following observation learn visual embedding image similarity visual space correlated their semantic similarity surrogate <eos> further extend model learn joint embedding visual textual cues allows one query database using text modifier addition query image adapting result modifier <eos> finally model ground ranking decisions showing region contributed most similarity between pairs image providing visual explanation similarity <eos> <eop> fast three dimensional reconstruction faces glasses <eos> present method fast three dimensional face reconstruction people wearing glasses <eos> method explicitly robustly models case face reconstructed partially occluded glasses <eos> propose simple generic model glasses copes wide variety different shapes colors styles without need any database learning <eos> algorithm simple fast requires only small amounts both memory runtime resources allowing fast interactive three dimensional reconstruction commodity mobile phones <eos> thorough evaluation approach synthetic real data demonstrates superior reconstruction result due explicit modeling glasses <eos> <eop> non local deep feature salient object detection <eos> saliency detection aims highlight most relevant object image <eos> method using conventional models struggle whenever salient object pictured top cluttered background while deep neural nets suffer excess complexity slow evaluation speeds <eos> paper propose simplified convolutional neural network combines local global information through multi resolution grid structure <eos> instead enforcing spacial coherence crf superpixels usually case implemented loss function inspired mumford shah functional penalizes errors boundary <eos> trained model msra dataset tested six different saliency benchmark datasets <eos> result show method par state art while reducing computation time factor times enabling near real time high performance saliency detection <eos> <eop> simultaneous feature aggregating hashing large scale image search <eos> most state art hashing based visual search systems local image descriptors image first aggregated single feature vector <eos> feature vector then subjected hashing function produces binary hash code <eos> previous work aggregating hashing processes designed independently <eos> paper propose novel framework feature aggregating hashing designed simultaneously optimized jointly <eos> specifically joint optimization produces aggregated representations better reconstructed some binary codes <eos> leads more discriminative binary hash codes improved retrieval accuracy <eos> addition also propose fast version recently proposed binary autoencoder used proposed framework <eos> perform extensive retrieval experiments several benchmark datasets both sift convolutional feature <eos> result suggest proposed framework achieves significant improvements over state art <eos> <eop> afraid dark nir vis face recognition via cross spectral hallucination low rank embedding <eos> surveillance cameras today often capture nir near infrared image low light environments <eos> however most face datasets accessible training verification only collected vis visible light spectrum <eos> remains challenging problem match nir vis face image due different light spectrum <eos> recently breakthroughs made vis face recognition applying deep learning huge amount labeled vis face sample <eos> same deep learning approach cannot simply applied nir face recognition two main reasons first much limited nir face image available training compared vis spectrum <eos> second face galleries matched mostly available only vis spectrum <eos> paper propose approach extend deep learning breakthrough vis face recognition nir spectrum without retraining underlying deep models see only vis faces <eos> approach consists two core components cross spectral hallucination low rank embedding optimize respectively input output vis deep model cross spectral face recognition <eos> cross spectral hallucination produces vis faces nir image through deep learning approach <eos> low rank embedding restores low rank structure faces deep feature across both nir vis spectrum <eos> observe often equally effective perform hallucination input nir image low rank embedding output deep feature vis deep model cross spectral recognition <eos> when hallucination low rank embedding deployed together observe significant further improvement obtain state art accuracy casia nir vis <eos> benchmark without need all re train recognition system <eos> <eop> eco efficient convolution operators tracking <eos> recent years discriminative correlation filter dcf based method significantly advanced state art tracking <eos> however pursuit ever increasing tracking performance their characteristic speed real time capability gradually faded <eos> further increasingly complex models massive number trainable parameters introduced risk severe over fitting <eos> work tackle key causes behind problems computational complexity over fitting aim simultaneously improving both speed performance <eos> revisit core dcf formulation introduce factorized convolution operator drastically reduces number parameters model ii compact generative model training sample distribution significantly reduces memory time complexity while providing better diversity sample iii conservative model update strategy improved robustness reduced complexity <eos> perform comprehensive experiments four benchmarks vot uav otb templecolor <eos> when using expensive deep feature tracker provides fold speedup achieves <eos> relative gain expected average overlap compared top ranked method vot challenge <eos> moreover fast variant using hand crafted feature operates hz single cpu while obtaining <eos> <eop> semi supervised deep learning monocular depth map prediction <eos> supervised deep learning often suffers lack sufficient training data <eos> specifically context monocular depth map prediction barely possible determine dense ground truth depth image realistic dynamic outdoor environments <eos> when using lidar sensors instance noise present distance measurements calibration between sensors cannot perfect measurements typically much sparser than camera image <eos> paper propose novel approach depth map prediction monocular image learns semi supervised way <eos> while use sparse ground truth depth supervised learning also enforce deep network produce photoconsistent dense depth maps stereo setup using direct image alignment loss <eos> experiments demonstrate superior performance depth map prediction single image compared state art method <eos> <eop> end end instance segmentation recurrent attention <eos> while convolutional neural network gained impressive success recently solving structured prediction problems such semantic segmentation remains challenge differentiate individual object instances scene <eos> instance segmentation very important variety applications such autonomous driving image captioning visual question answering <eos> techniques combine large graphical models low level vision proposed address problem however propose end end recurrent neural network rnn architecture attention mechanism model human like counting process produce detailed instance segmentations <eos> network jointly trained sequentially produce region interest well dominant object segmentation within each region <eos> proposed model achieves competitive result cvppp kitti cityscapes datasets <eos> <eop> multigrid neural architectures <eos> propose multigrid extension convolutional neural network cnn <eos> rather than manipulating representations living single spatial grid network layer operate across scale space pyramid grids <eos> they consume multigrid inputs produce multigrid outputs convolutional filters themselves both within scale cross scale extent <eos> aspect distinct simple multiscale designs only process input different scales <eos> viewed terms information flow multigrid network passes messages across spatial pyramid <eos> consequence receptive field size grows exponentially depth facilitating rapid integration context <eos> most critically multigrid structure enables network learn internal attention dynamic routing mechanisms use them accomplish tasks modern cnn fail <eos> experiments demonstrate wide ranging performance advantages multigrid <eos> cifar imagenet classification tasks flipping single grid multigrid within standard cnn paradigm improves accuracy while being compute parameter efficient <eos> multigrid independent other architectural choices show synergy combination residual connections <eos> multigrid yields dramatic improvement synthetic semantic segmentation dataset <eos> most strikingly relatively shallow multigrid network learn directly perform spatial transformation tasks contrast current cnn fail <eos> together result suggest continuous evolution feature multigrid pyramid more powerful alternative existing cnn designs flat grid <eos> <eop> fast boosting based detection using scale invariant multimodal multiresolution filtered feature <eos> paper propose novel boosting based sliding window solution object detection keep up precision state art deep learning approaches while being times faster <eos> solution takes advantage multisensorial perception exploits information color motion depth <eos> introduce multimodal multiresolution filtering signal intensity gradient magnitude orientation channels order capture structure multiple scales orientations <eos> achieve scale invariant classification feature analyze effect scale change feature different filter types propose correction scheme <eos> improve recognition incorporate three dimensional context generating spatial geometric symmetrical channels <eos> finally evaluate proposed solution multiple benchmarks detection pedestrians cars bicyclists <eos> achieve competitive result over frames per second <eos> <eop> dsac differentiable ransac camera localization <eos> ransac important algorithm robust optimization central building block many computer vision applications <eos> recent years traditionally hand crafted pipelines replaced deep learning pipelines trained end end fashion <eos> however ransac so far used part such deep learning pipelines because its hypothesis selection procedure non differentiable <eos> work present two different ways overcome limitation <eos> most promising approach inspired reinforcement learning namely replace deterministic hypothesis selection probabilistic selection derive expected loss <eos> all learnable parameters <eos> call approach dsac differentiable counterpart ransac <eos> apply dsac problem camera localization deep learning so far failed improve traditional approaches <eos> demonstrate directly minimizing expected loss output camera poses robustly estimated ransac achieve increase accuracy <eos> future any deep learning pipeline use dsac robust optimization component <eos> <eop> group wise point set registration based renyi second order entropy <eos> paper describe set robust algorithms group wise registration using both rigid non rigid transformations multiple unlabelled point set no bias toward given set <eos> method mitigate need establish correspondence among point set representing them probability density functions registration treated multiple distribution alignment <eos> holder jensen inequalities provide notion similarity distance among point set renyi second order entropy yields closed form solution cost function update equations <eos> also show method improved normalizing entropy scale factor <eos> provide simple fast accurate algorithms compute spatial transformation function needed register multiple point set <eos> algorithms compared against two well known method group wise point set registration <eos> result show improvement both accuracy computational complexity <eos> <eop> poseagent budget constrained object pose estimation via reinforcement learning <eos> state art computer vision algorithms often achieve efficiency making discrete choices about hypotheses explore next <eos> allows allocation computational resources promising candidates however such decisions non differentiable <eos> result algorithms hard train end end fashion <eos> work propose learn efficient algorithm task object pose estimation <eos> system optimizes parameters existing state art pose estimation system using reinforcement learning pose estimation system now becomes stochastic policy parametrized cnn <eos> additionally present efficient training algorithm dramatically reduces computation time <eos> show empirically learned pose estimation procedure makes better use limited resources improves upon state art challenging dataset <eos> approach enables differentiable end end training complex algorithmic pipelines learns make optimal use given computational budget <eos> <eop> mucale net multi categorical level network generate more discriminating feature <eos> transfer learning scheme intermediate layer pre trained cnn employed universal image representation tackle many visual classification problems <eos> current trend generate such representation learn cnn large set image labeled among most specific categories <eos> such processes ignore potential relations between categories well categorical levels used humans classify <eos> paper propose multi categorical level network mucale net include human categorization knowledge into cnn learning process <eos> mucale net separates generic categories each other while independently distinguishes specific ones <eos> thereby generates different feature intermediate layer complementary when combined together <eos> advantageously method require additive data nor annotation train network <eos> extensive experiments over four publicly available benchmarks image classification exhibit state art performances <eos> <eop> high resolution image inpainting using multi scale neural patch synthesis <eos> recent advances deep learning shown exciting promise filling large holes natural image semantically plausible context aware details impacting fundamental image manipulation tasks such object removal <eos> while learning based method significantly more effective capturing high level feature than prior techniques they only handle very low resolution inputs due memory limitations difficulty training <eos> even slightly larger image inpainted region would appear blurry unpleasant boundaries become visible <eos> propose multi scale neural patch synthesis approach based joint optimization image content texture constraints only preserves contextual structures but also produces high frequency details matching adapting patches most similar mid layer feature correlations deep classification network <eos> evaluate method imagenet paris streetview datasets achieved state art inpainting accuracy <eos> show approach produces sharper more coherent result than prior method especially high resolution image <eos> <eop> temporal attention gated model robust sequence classification <eos> typical techniques sequence classification designed well segmented sequences edited remove noisy irrelevant parts <eos> therefore such method cannot easily applied noisy sequences expected real world applications <eos> paper present temporal attention gated model tagm integrates ideas attention models gated recurrent network better deal noisy unsegmented sequences <eos> specifically extend concept attention model measure relevance each observation time step sequence <eos> then use novel gated recurrent network learn hidden representation final prediction <eos> important advantage approach interpretability since temporal attention weights provide meaningful value salience each time step sequence <eos> demonstrate merits tagm approach both prediction accuracy interpretability three different tasks spoken digit recognition text based sentiment analysis visual event recognition <eos> <eop> multiple scattering microphysics tomography <eos> scattering effects image including related haze fog appearance clouds fundamentally dictated microphysical characteristics scatterers <eos> work defines derives recovery characteristics three dimensional three dimensional heterogeneous medium <eos> recovery based novel tomography approach <eos> multi view multi angular multi spectral data linked underlying microphysics using three dimensional radiative transfer accounting multiple scattering <eos> despite nonlinearity tomography model inversion enabled using few approximations describe <eos> case study focus passive remote sensing atmosphere scatterer retrieval benefit modeling forecasting weather climate pollution <eos> <eop> why you should forget luminance conversion something better <eos> one most frequently applied low level operations computer vision conversion rgb camera image into its luminance representation <eos> also one most incorrectly applied operations <eos> even most trusted softwares matlab opencv perform luminance conversion correctly <eos> paper examine main factors make proper rgb luminance conversion difficult particular incorrect white balance incorrect gamma tone curve correction incorrect equations <eos> analysis shows errors up various colors uncommon <eos> result argue most computer vision problems there no need attempt luminance conversion instead there better alternatives depending task <eos> <eop> deep quantization encoding convolutional activations deep generative model <eos> deep convolutional neural network cnn proven highly effective visual recognition learning universal representation activations convolutional layer plays fundamental problem <eos> paper present fisher vector encoding variational auto encoder fv vae novel deep architecture quantizes local activations convolutional layer deep generative model training them end end manner <eos> incorporate fv encoding strategy into deep generative models introduce variational auto encoder model steers variational inference learning neural network straightforwardly optimized using standard stochastic gradient method <eos> different fv characterized conventional generative models <eos> gaussian mixture model parsimoniously fit discrete mixture model data distribution proposed fv vae more flexible represent natural property data better generalization <eos> extensive experiments conducted three public datasets <eos> ucf activitynet cub context video action recognition fine grained image classification respectively <eos> superior result reported when compared state art representations <eos> most remarkably proposed fv vae achieves date best published accuracy <eos> <eop> joint multi person pose estimation semantic part segmentation <eos> human pose estimation semantic part segmentation two complementary tasks computer vision <eos> paper propose solve two tasks jointly natural multi person image estimated pose provides object level shape prior regularize part segments while part level segments constrain variation pose locations <eos> specifically first train two fully convolutional neural network fcns namely pose fcn part fcn provide initial estimation pose joint potential semantic part potential <eos> then refine pose joint location two types potentials fused fully connected conditional random field fcrf novel segment joint smoothness term used encourage semantic spatial consistency between parts joints <eos> refine part segments refined pose original part potential integrated through part fcn skeleton feature pose serves additional regularization cues part segments <eos> finally reduce complexity fcrf induce human detection boxes infer graph inside each box making inference forty times faster <eos> since there no dataset contains both part segments pose labels extend pascal voc part dataset human pose joints perform extensive experiments compare method against several most recent strategies <eos> show algorithm surpasses competing method <eos> pose estimation much faster speed <eos> semantic part segmentation <eos> <eop> dope distributed optimization pairwise energies <eos> formulate alternating direction method multipliers admm systematically distributes computations any technique optimizing pairwise functions including non submodular potentials <eos> such discrete functions very useful segmentation breadth other vision problems <eos> method decomposes problem into large set small sub problems each involving sub region image domain solved parallel <eos> achieve consistency between sub problems through novel constraint used large class pairwise functions <eos> give iterative numerical solution alternates between solving sub problems updating consistency variables until convergence <eos> report comprehensive experiments demonstrate benefit general distributed solution case popular serial algorithm boykov kolmogorov bk algorithm also context non submodular functions <eos> <eop> reflectance adaptive filtering improves intrinsic image estimation <eos> separating image into reflectance shading layer poses challenge learning approaches because no large corpus precise realistic ground truth decompositions exists <eos> intrinsic image wild iiw dataset provides sparse set relative human reflectance judgments serves standard benchmark intrinsic image <eos> number method use iiw learn statistical dependencies between image their reflectance layer <eos> although learning plays important role high performance show standard signal processing technique achieves performance par current state art <eos> propose loss function cnn learning dense reflectance predictions <eos> result show simple pixel wise decision without any context prior knowledge sufficient provide strong baseline iiw <eos> set competitive baseline only two other approaches surpass <eos> then develop joint bilateral filtering method implements strong prior knowledge about reflectance constancy <eos> filtering operation applied any intrinsic image algorithm improve several previous result achieving new state art iiw <eos> findings suggest effect learning based approaches may over estimated so far <eos> explicit prior knowledge still least important obtain high performance intrinsic image decompositions <eos> <eop> densereg fully convolutional dense shape regression wild <eos> paper propose learn mapping image pixels into dense template grid through fully convolutional network <eos> formulate task regression problem train network leveraging upon manually annotated facial landmarks wild <eos> use such landmarks establish dense correspondence field between three dimensional object template input image then serves ground truth training regression system <eos> show combine ideas semantic segmentation regression network yielding highly accurate quantized regression architecture <eos> system called densereg allows estimate dense image template correspondences fully convolutional manner <eos> such network provide useful correspondence information stand alone system while when used initialization statistical deformable models obtain landmark localization result largely outperform current state art challenging benchmark <eos> thoroughly evaluate method host facial analysis tasks demonstrate its use other correspondence estimation tasks such human body human ear <eos> densereg code made available alpguler <eos> html along supplementary materials <eos> <eop> deep learning human mind automated visual classification <eos> if could effectively read mind transfer human visual capabilities computer vision method paper aim addressing question developing first visual object classifier driven human brain signals <eos> particular employ eeg data evoked visual object stimuli combined recurrent neural network rnn learn discriminative brain activity manifold visual categories reading mind effort <eos> afterward transfer learned capabilities machines training convolutional neural network cnn based regressor project image onto learned manifold thus allowing machines employ human brain based feature automated visual classification <eos> use channel eeg active electrodes record brain activity several subjects while looking image imagenet object classes <eos> proposed rnn based approach discriminating object classes using brain signals reaches average accuracy about greatly outperforms existing method attempting learn eeg visual object representations <eos> automated object categorization human brain driven approach obtains competitive performance comparable achieved powerful cnn models also able generalize over different visual datasets <eos> <eop> learning discriminative transformation covariant local feature detectors <eos> robust covariant local feature detectors important detecting local feature discriminative image content repeatably detected consistent locations when image undergoes diverse transformations <eos> such detectors critical applications such image search scene reconstruction <eos> many learning based local feature detectors address one two problems while overlooking other <eos> work propose novel learning based method simultaneously address both issues <eos> specifically extend covariant constraint proposed lenc vedaldi defining concepts standard patch canonical feature leverage train novel robust covariant detector <eos> show introduction concepts greatly simplifies learning stage covariant detector also makes detector much more robust <eos> extensive experiments show method outperforms previous hand crafted learning based detectors large margins terms repeatability <eos> <eop> temporal action co segmentation three dimensional motion capture data video <eos> given two action sequences interested spotting co segmenting all pairs sub sequences represent same action <eos> propose totally unsupervised solution problem <eos> no priori model actions assumed available <eos> number common sub sequences may unknown <eos> sub sequences located anywhere original sequences may differ duration corresponding actions may performed different person different style <eos> treat type temporal action co segmentation stochastic optimization problem solved employing particle swarm optimization pso <eos> objective function minimized pso capitalizes dynamic time warping dtw compare two action sub sequences <eos> due generic problem formulation solution proposed method applied motion capture <eos> three dimensional skeletal data conventional rgb video acquired wild <eos> present extensive quantitative experiments standard data set well data set introduced paper <eos> obtained result demonstrate proposed method achieves remarkable increase co segmentation quality compared all tested state art method <eos> <eop> learning diverse image colorization <eos> colorization ambiguous problem multiple viable colorizations single grey level image <eos> however previous method only produce single most probable colorization <eos> goal model diversity intrinsic problem colorization produce multiple colorizations display long scale spatial co ordination <eos> learn low dimensional embedding color fields using variational autoencoder vae <eos> construct loss terms vae decoder avoid blurry outputs take into account uneven distribution pixel colors <eos> finally build conditional model multi modal distribution between grey level image color field embeddings <eos> sample conditional model result diverse colorization <eos> demonstrate method obtains better diverse colorizations than standard conditional variational autoencoder cvae model well recently proposed conditional generative adversarial network cgan <eos> <eop> non uniform subset selection active learning structured data <eos> several works shown relationships between data point <eos> context structured data exploited obtain better recognition performance <eos> paper explore different but related problem how inter relationships used efficiently learn continuously update recognition model minimal human labeling effort <eos> towards goal propose active learning framework select optimal subset data point manual labeling exploiting relationships between them <eos> construct graph unlabeled data represent underlying structure such each node represents data point edges represent inter relationships between them <eos> thereafter considering flow beliefs graph choose sample labeling minimize joint entropy nodes graph <eos> result significant reduction manual labeling effort without compromising recognition performance <eos> method chooses non uniform number sample each batch streaming data depending its information content <eos> also submodular property objective function makes computationally efficient optimize <eos> proposed framework demonstrated various applications including document analysis scene object recognition activity recognition <eos> <eop> vidloc deep spatio temporal model dof video clip relocalization <eos> machine learning techniques namely convolutional neural network cnn regression forests recently shown great promise performing dof localization monocular image <eos> however most cases image sequences rather only single image readily available <eos> extent none proposed learning based approaches exploit valuable constraint temporal smoothness often leading situations per frame error larger than camera motion <eos> paper propose recurrent model performing dof localization video clips <eos> find even considering only short sequences frames pose estimates smoothed localization error drastically reduced <eos> finally consider means obtaining probabilistic pose estimates model <eos> evaluate method openly available real world autonomous driving indoor localization datasets <eos> <eop> hard mixtures experts large scale weakly supervised vision <eos> training convolutional network cnn fit single gpu minibatch stochastic gradient descent become effective practice <eos> however there still no effective method training large network fit memory few gpu cards parallelizing cnn training <eos> work show simple hard mixture experts model efficiently trained good effect large scale hashtag multilabel prediction tasks <eos> mixture experts models new but past researchers had devise sophisticated method deal data fragmentation <eos> show empirically modern weakly supervised data set large enough support naive partitioning schemes each data point assigned single expert <eos> because experts independent training them parallel easy evaluation cheap size model <eos> furthermore show use single decoding layer all experts allowing unified feature embedding space <eos> demonstrate feasible fact relatively painless train far larger models than could practically trained standard cnn architectures extra capacity well used current datasets <eos> <eop> colorization proxy task visual understanding <eos> investigate improve self supervision drop replacement imagenet pretraining focusing automatic colorization proxy task <eos> self supervised training shown more promising utilizing unlabeled data than other traditional unsupervised learning method <eos> build success evaluate ability self supervised network several contexts <eos> voc segmentation classification tasks present result state art among method using imagenet labels pretraining representations <eos> moreover present first depth analysis self supervision via colorization concluding formulation loss training details network architecture play important roles its effectiveness <eos> investigation further expanded revisiting imagenet pretraining paradigm asking questions such how much training data needed how many labels needed how much feature change when fine tuned relate questions back self supervision showing colorization provides similarly powerful supervisory signal various flavors imagenet pretraining <eos> <eop> dataset exploration models understanding video data through fill blank question answering <eos> while deep convolutional neural network frequently approach exceed human level performance benchmark tasks involving static image extending success moving image straightforward <eos> video understanding interest many applications including content recommendation prediction summarization event object detection understanding human visual perception <eos> however many domains lack sufficient data explore perfect video models <eos> order address need simple quantitative benchmark developing understanding video present moviefib fill blank question answering dataset over examples based descriptive video annotations visually impaired <eos> addition presenting statistics description dataset perform detailed analysis different models predictions compare human performance <eos> investigate relative importance language static visual feature moving three dimensional visual feature effects increasing dataset size number frames sampled vocabulary size <eos> illustrate task solvable language model alone model combining three dimensional visual information indeed provides best result all models perform significantly worse than human level <eos> provide human evaluation responses given different models find accuracy moviefib evaluation corresponds well human judgment <eos> suggest avenues improving video models hope moviefib challenge useful measuring encouraging progress very interesting field <eos> <eop> interspecies knowledge transfer facial keypoint detection <eos> present method localizing facial keypoints animals transferring knowledge gained human faces <eos> instead directly finetuning network trained detect keypoints human faces animal faces sub optimal since human animal faces look quite different propose first adapt animal image pre trained human detection network correcting differences animal human face shape <eos> first find nearest human neighbors each animal image using unsupervised shape matching method <eos> use matches train thin plate spline warping network warp each animal face look more human like <eos> warping network then jointly finetuned pre trained human facial keypoint detection network using animal dataset <eos> demonstrate state art result both horse sheep facial keypoint detection significant improvement over simple finetuning especially when training data scarce <eos> additionally present new dataset image horse face facial keypoint annotations <eos> <eop> making vqa matter elevating role image understanding visual question answering <eos> problems intersection vision language significant importance both challenging research questions rich set applications they enable <eos> however inherent structure world bias language tend simpler signal learning than visual modalities resulting models ignore visual information leading inflated sense their capability <eos> propose counter language priors task visual question answering vqa make vision vqa matter specifically balance popular vqa dataset antol <eos> iccv collecting complementary image such every question balanced dataset associated just single image but rather pair similar image result two different answers question <eos> dataset construction more balanced than original vqa dataset approximately twice number image question pairs <eos> complete balanced dataset publicly available visualqa <eos> org part nd iteration visual question answering dataset challenge vqa <eos> further benchmark number state art vqa models balanced dataset <eos> all models perform significantly worse balanced dataset suggesting models indeed learned exploit language priors <eos> finding provides first concrete empirical evidence seems qualitative sense among practitioners <eos> finally data collection protocol identifying complementary image enables develop novel interpretable model addition providing answer given image question pair also provides counter example based explanation <eos> specifically identifies image similar original image but believes different answer same question <eos> help building trust machines among their users <eos> <eop> deep semantic feature matching <eos> estimating dense visual correspondences between object intra class variation deformations background clutter remains challenging problem <eos> thanks breakthrough cnn there new powerful feature available <eos> despite their easy accessibility great success existing semantic flow method could significantly benefit without extensive additional training <eos> introduce novel method semantic matching pre trained cnn feature based convolutional feature pyramids activation guided feature selection <eos> final matching propose sparse graph matching framework each salient feature selects among small subset nearest neighbors target image <eos> improve method unconstrained setting without bounding box annotations introduce novel object proposal based matching constraints <eos> furthermore show sparse matching transformed into dense correspondence field <eos> extensive experimental evaluations benchmark datasets show method significantly outperforms existing semantic matching method <eos> <eop> improved texture network maximizing quality diversity feed forward stylization texture synthesis <eos> recent work gatys <eos> who characterized style image statistics convolutional neural network filters ignited renewed interest texture generation image stylization problems <eos> while their image generation technique uses slow optimization process recently several authors proposed learn generator neural network produce similar outputs one quick forward pass <eos> while generator network promising they still inferior visual quality diversity compared generation optimization <eos> work advance them two significant ways <eos> first introduce instance normalization module replace batch normalization significant improvements quality image stylization <eos> second improve diversity introducing new learning formulation encourages generators sample unbiasedly julesz texture ensemble equivalence class all image characterized certain filter responses <eos> together two improvements take feed forward texture synthesis image stylization much closer quality generation via optimization while retaining speed advantage <eos> <eop> reinforcement learning approach view planning problem <eos> present reinforcement learning rl solution view planning problem vpp generates sequence view point capable sensing all accessible area given object represented three dimensional model <eos> doing so goal minimize number view point making vpp class set covering optimization problem scop <eos> scop np hard inapproximability result tell greedy algorithm provides best approximation runs polynomial time <eos> order find solution better than greedy algorithm introduce novel score function exploiting geometry three dimensional model ii device intuitive approach vpp using score function iii cast vpp markovian decision process mdp solve mdp rl framework using well known rl algorithms <eos> particular use sarsa watkins td function approximation solve mdp <eos> compare result method baseline greedy algorithm extensive set test object show outperform baseline almost all cases <eos> <eop> improving facial attribute prediction using semantic segmentation <eos> attributes semantically meaningful characteristics whose applicability widely crosses category boundaries <eos> they particularly important describing recognizing concepts no explicit training example given <eos> zero shot learning <eos> additionally since attributes human describable they used efficient human computer interaction <eos> paper propose employ semantic segmentation improve facial attribute prediction <eos> core idea lies fact many facial attributes describe local properties <eos> other words probability attribute appear face image far being uniform spatial domain <eos> build facial attribute prediction model jointly deep semantic segmentation network <eos> harnesses localization cues learned semantic segmentation guide attention attribute prediction region different attributes naturally show up <eos> result approach addition recognition able localize attributes despite merely having access image level labels weak supervision during training <eos> evaluate proposed method celeba lfwa datasets achieve superior result prior arts <eos> furthermore show reverse problem semantic face parsing improves when facial attributes available <eos> reaffirms need jointly model two interconnected tasks <eos> <eop> deep network flow multi object tracking <eos> data association problems important component many computer vision applications multi object tracking being one most prominent examples <eos> typical approach data association involves finding graph matching network flow minimizes sum pairwise association costs often either hand crafted learned linear functions fixed feature <eos> work demonstrate possible learn feature network flow based data association via backpropagation expressing optimum smoothed network flow problem differentiable function pairwise association costs <eos> apply approach multi object tracking network flow formulation <eos> experiments demonstrate able successfully learn all cost functions association problem end end fashion outperform hand crafted costs all settings <eos> integration combination various sources inputs becomes easy cost functions learned entirely data alleviating tedious hand designing costs <eos> <eop> bidirectional beam search forward backward inference neural sequence models fill blank image captioning <eos> develop first approximate inference algorithm best best decoding bidirectional neural sequence models extending beam search bs reason about both forward backward time dependencies <eos> beam search bs widely used approximate inference algorithm decoding sequences unidirectional neural sequence models <eos> interestingly approximate inference bidirectional models remains open problem despite their significant advantage modeling information both past future <eos> enable use bidirectional models present bidirectional beam search bibs efficient algorithm approximate bidirectional inference <eos> evaluate method interesting problem its own right introduce novel fill blank image captioning task requires reasoning about both past future sentence structure reconstruct sensible image descriptions <eos> use task well visual madlibs dataset demonstrate effectiveness approach consistently outperforming all baseline method <eos> <eop> matting depth recovery thin structures using focal stack <eos> thin structures such fence grass vessels common photography scientific imaging <eos> they exhibit complex three dimensional structures sharp depth variations discontinuities mutual occlusions <eos> paper develop method estimate occlusion matte depths thin structures focal image stack obtained either varying focus aperture lens computed one shot light field image <eos> propose image formation model explicitly describes spatially varying optical blur mutual occlusions structures located different depths <eos> based model derive efficient mcmc inference algorithm enables direct analytical computations iterative update model image without re rendering image sampling process <eos> then depths thin structures recovered using gradient descent differential terms computed using image formation model <eos> apply proposed method scenes both macro micro scales <eos> macro scale evaluate method scenes complex three dimensional thin structures such tree branches grass <eos> micro scale apply method vivo microscopic image micro vessels diameters less than um <eos> knowledge proposed method first approach reconstruct three dimensional structures micro vessels non invasive vivo image measurements <eos> <eop> discovering causal signals image <eos> paper establishes existence observable footprints reveal causal dispositions object categories appearing collections image <eos> achieve goal two steps <eos> first take learning approach observational causal discovery build classifier achieves state art performance finding causal direction between pairs random variables given sample their joint distribution <eos> second use causal direction classifier effectively distinguish between feature object feature their contexts collections static image <eos> experiments demonstrate existence relation between direction causality difference between object their contexts same token existence observable signals reveal causal dispositions object <eos> <eop> harvesting multiple views marker less three dimensional human pose annotations <eos> recent advances convolutional network convnets shifted bottleneck many computer vision tasks annotated data collection <eos> paper present geometry driven approach automatically collect annotations human pose prediction tasks <eos> starting generic convnet human pose assuming multi view setup describe automatic way collect accurate three dimensional human pose annotations <eos> capitalize constraints offered three dimensional geometry camera setup three dimensional structure human body probabilistically combine per view convnet predictions into globally optimal three dimensional pose <eos> three dimensional pose used basis harvesting annotations <eos> benefit annotations produced automatically approach demonstrated two challenging settings fine tuning generic convnet based pose predictor capture discriminative aspects subject appearance <eos> personalization ii training convnet scratch single view three dimensional human pose prediction without leveraging three dimensional pose groundtruth <eos> proposed multi view pose estimator achieves state art result standard benchmarks demonstrating effectiveness method exploiting available multi view information <eos> <eop> shading annotations wild <eos> understanding shading effects image critical variety vision graphics problems including intrinsic image decomposition shadow removal image relighting inverse rendering <eos> case other vision tasks machine learning promising approach understanding shading but there little ground truth shading data available real world image <eos> introduce shading annotations wild saw new large scale public dataset shading annotations indoor scenes comprised multiple forms shading judgments obtained via crowdsourcing along shading annotations automatically generated rgb imagery <eos> use data train convolutional neural network predict per pixel shading information image <eos> demonstrate value data network application intrinsic image reduce decomposition artifacts produced existing algorithms <eos> database available opensurfaces <eos> <eop> self critical sequence training image captioning <eos> recently shown policy gradient method reinforcement learning utilized train deep end end systems directly non differentiable metrics task hand <eos> paper consider problem optimizing image captioning systems using reinforcement learning show carefully optimizing systems using test metrics mscoco task significant gains performance realized <eos> systems built using new optimization approach call self critical sequence training scst <eos> scst form popular reinforce algorithm rather than estimating baseline normalize rewards reduce variance utilizes output its own test time inference algorithm normalize rewards experiences <eos> using approach estimating reward signal actor critic method must estimating normalization reinforce algorithms typically avoided while same time harmonizing model respect its test time inference procedure <eos> empirically find directly optimizing cider metric scst greedy decoding test time highly effective <eos> result mscoco evaluation sever establish new state art task improving best result terms cider <eos> <eop> coarse fine volumetric prediction single image three dimensional human pose <eos> paper addresses challenge three dimensional human pose estimation single color image <eos> despite general success end end learning paradigm top performing approaches employ two step solution consisting convolutional network convnet joint localization subsequent optimization step recover three dimensional pose <eos> paper identify representation three dimensional pose critical issue current convnet approaches make two important contributions towards validating value end end learning task <eos> first propose fine discretization three dimensional space around subject train convnet predict per voxel likelihoods each joint <eos> creates natural representation three dimensional pose greatly improves performance over direct regression joint coordinates <eos> second further improve upon initial estimates employ coarse fine prediction scheme <eos> step addresses large dimensionality increase enables iterative refinement repeated processing image feature <eos> proposed approach outperforms all state art method standard benchmarks achieving relative error reduction greater than average <eos> additionally investigate using volumetric representation related architecture suboptimal compared end end approach but practical interest since enables training when no image corresponding three dimensional groundtruth available allows present compelling result wild image <eos> <eop> human pose estimation pose estimation matching <eos> explore three dimensional human pose estimation single rgb image <eos> while many approaches try directly predict three dimensional pose image measurements explore simple architecture reasons through intermediate pose predictions <eos> approach based two key observations deep neural nets revolutionized pose estimation producing accurate predictions even poses self occlusions big data set three dimensional mocap data now readily available making tempting lift predicted poses three dimensional through simple memorization <eos> resulting architecture straightforward implement off shelf pose estimation systems three dimensional mocap libraries <eos> importantly demonstratethatsuchmethodsoutperformalmostallstate theart three dimensional pose estimation systems most directly try regress three dimensional pose measurements <eos> <eop> level playing field million scale face recognition <eos> face recognition perception solved problem however when tested million scale exhibits dramatic variation accuracies across different algorithms <eos> algorithms very different access good big training data their secret weapon should face recognition improve address questions created benchmark mf requires all algorithms trained same data tested million scale <eos> mf public large scale set identities <eos> photos created goal level playing field large scale face recognition <eos> contrast result findings other two large scale benchmarks megaface challenge ms celebs groups were allowed train any private public big small set <eos> some key discoveries algorithms trained mf were able achieve state art comparable result algorithms trained massive private set some outperformed themselves once trained mf invariance aging suffers low accuracies megaface identifying need larger age variations possibly within identities adjustment algorithms future testing <eos> <eop> unsupervised adaptive re identification open world dynamic camera network <eos> person re identification open challenging problem computer vision <eos> existing approaches concentrated either designing best feature representation learning optimal matching metrics static setting number cameras fixed network <eos> most approaches neglected dynamic open world nature re identification problem new camera may temporarily inserted into existing system get additional information <eos> address such novel very practical problem propose unsupervised adaptation scheme re identification models dynamic camera network <eos> first formulate domain perceptive re identification method based geodesic flow kernel effectively find best source camera already installed adapt newly introduced target camera without requiring very expensive training phase <eos> second introduce transitive inference algorithm re identification exploit information best source camera improve accuracy across other camera pairs network multiple cameras <eos> extensive experiments four benchmark datasets demonstrate proposed approach significantly outperforms state art unsupervised learning based alternatives whilst being extremely efficient compute <eos> <eop> deep feature interpolation image content changes <eos> propose deep feature interpolation dfi new data driven baseline automatic high resolution image transformation <eos> name suggests dfi relies only simple linear interpolation deep convolutional feature pre trained convnets <eos> show despite its simplicity dfi perform high level semantic transformations like make older younger make bespectacled add smile among others surprisingly well sometimes even matching outperforming state art <eos> particularly unexpected dfi requires no specialized network architecture even any deep network trained tasks <eos> dfi therefore used new baseline evaluate more complex algorithms provides practical answer question image transformation tasks still challenging after advent deep learning <eos> <eop> bounding box estimation using deep learning geometry <eos> present method three dimensional object detection pose estimation single image <eos> contrast current techniques only regress three dimensional orientation object method first regresses relatively stable three dimensional object properties using deep convolutional neural network then combines estimates geometric constraints provided object bounding box produce complete three dimensional bounding box <eos> first network output estimates three dimensional object orientation using novel hybrid discrete continuous loss significantly outperforms loss <eos> second output regresses three dimensional object dimensions relatively little variance compared alternatives often predicted many object types <eos> estimates combined geometric constraints translation imposed bounding box enable recover stable accurate three dimensional object pose <eos> evaluate method challenging kitti object detection benchmark both official metric three dimensional orientation estimation also accuracy obtained three dimensional bounding boxes <eos> although conceptually simple method outperforms more complex computationally expensive approaches leverage semantic segmentation instance level segmentation flat ground priors sub category detection <eos> discrete continuous loss also produces state art result three dimensional viewpoint estimation pascal three dimensional dataset <eos> <eop> collaborative summarization topic related video <eos> large collections video grouped into clusters topic keyword such eiffel tower surfing many important visual concepts repeating across them <eos> such topically close set video mutual influence each other could used summarize one them exploiting information others set <eos> build intuition develop novel approach extract summary simultaneously captures both important particularities arising given video well generalities identified set video <eos> topic related video provide visual context identify important parts video being summarized <eos> achieve developing collaborative sparse optimization method efficiently solved half quadratic minimization algorithm <eos> work builds upon idea collaborative techniques information retrieval natural language processing typically use attributes other similar object predict attribute given object <eos> experiments two challenging diverse datasets well demonstrate efficacy approach over state art method <eos> <eop> synthesizing dynamic patterns spatial temporal generative convnet <eos> video sequences contain rich dynamic patterns such dynamic texture patterns exhibit stationarity temporal domain action patterns non stationary either spatial temporal domain <eos> show spatial temporal generative convnet used model synthesize dynamic patterns <eos> model defines probability distribution video sequence log probability defined spatial temporal convnet consists multiple layer spatial temporal filters capture spatial temporal patterns different scales <eos> model learned training video sequences analysis synthesis learning algorithm iterates following two steps <eos> step synthesizes video sequences currently learned model <eos> step then updates model parameters based difference between synthesized video sequences observed training sequences <eos> show learning algorithm synthesize realistic dynamic patterns <eos> <eop> comprehension guided referring expressions <eos> consider generation comprehension natural language referring expression object image <eos> unlike generic image captioning lacks natural standard evaluation criteria quality referring expression may measured receiver ability correctly infer object being described <eos> following intuition propose two approaches utilize models trained comprehension task generate better expressions <eos> first use comprehension module trained human generated expressions critic referring expression generator <eos> comprehension module serves differentiable proxy human evaluation providing training signal generation module <eos> second use comprehension model generate rerank pipeline chooses candidate expressions generated model according their performance comprehension task <eos> show both approaches lead improved referring expression generation multiple benchmark datasets <eos> <eop> zero shot learning via multi scale manifold regularization <eos> address zero shot learning using new manifold alignment framework based localized multi scale transform graphs <eos> inference approach includes smoothness criterion function mapping nodes graph visual representation onto linear space semantic representation optimize using multi scale graph wavelets <eos> robustness ensuing scheme allows operate automatically generated semantic annotations resulting algorithm entirely free manual supervision yet improves state art measured benchmark datasets <eos> <eop> lcnn lookup based convolutional neural network <eos> porting state art deep learning algorithms resource constrained compute platforms <eos> vr ar wearables extremely challenging <eos> propose fast compact accurate model convolutional neural network enables efficient learning inference <eos> introduce lcnn lookup based convolutional neural network encodes convolutions few lookups dictionary trained cover space weights cnn <eos> training lcnn involves jointly learning dictionary small set linear combinations <eos> size dictionary naturally traces spectrum trade offs between efficiency accuracy <eos> experimental result imagenet challenge show lcnn offer <eos> speedup while achieving <eos> top accuracy using alexnet architecture <eos> fastest lcnn offers <eos> speed up over alexnet while maintaining <eos> lcnn only offers dramatic speed ups inference but also enables efficient training <eos> paper show benefits lcnn few shot learning few iteration learning two crucial aspects device training deep learning models <eos> <eop> deep unsupervised similarity learning using partially ordered set <eos> unsupervised learning visual similarities paramount importance computer vision particularly due lacking training data fine grained similarities <eos> deep learning similarities often based relationships between pairs triplets sample <eos> many relations unreliable mutually contradicting implying inconsistencies when trained without supervision information relates different tuples triplets each other <eos> overcome problem use local estimates reliable dis similarities initially group sample into compact surrogate classes use local partial orders sample classes link classes each other <eos> similarity learning then formulated partial ordering task soft correspondences all sample classes <eos> adopting strategy self supervision cnn trained optimally represent sample mutually consistent manner while updating classes <eos> similarity learning grouping procedure integrated single model optimized jointly <eos> proposed unsupervised approach shows competitive performance detailed pose estimation object classification <eos> <eop> zero shot classification discriminative semantic representation learning <eos> zero shot learning special case unsupervised domain adaptation source target domains disjoint label spaces become increasingly popular computer vision community <eos> paper propose novel zero shot learning method based discriminative sparse non negative matrix factorization <eos> proposed approach aims identify set common high level semantic components across two domains via non negative sparse matrix factorization while enforcing representation vectors image common component based space discriminatively aligned attribute based label representation vectors <eos> fully exploit aligned semantic information contained learned representation vectors instances develop label propagation based testing procedure classify unlabeled instances unseen classes target domain <eos> conduct experiments four standard zero shot learning image datasets comparing proposed approach state art zero shot learning method <eos> empirical result demonstrate efficacy proposed approach <eos> <eop> learning detection diverse proposals <eos> predict set diverse informative proposals enriched representations paper introduces differentiable determinantal point process dpp layer able augment object detection architectures <eos> most modern object detection architectures such faster cnn learn localize object minimizing deviations ground truth but ignore correlation between multiple proposals object categories <eos> non maximum suppression nms widely used proposal pruning scheme ignores label instance level relations between object candidates resulting multi labeled detections <eos> multi class case nms selects boxes largest prediction scores ignoring semantic relation between categories potential election <eos> contrast trainable dpp layer allowing learning detection diverse proposals lddp considers both label level contextual information spatial layout relationships between proposals without increasing number parameters network thus improves location category specifications final detected bounding boxes substantially during both training inference schemes <eos> furthermore show lddp keeps superiority over faster cnn even if number proposals generated ldpp only many faster cnn <eos> <eop> learning random walk label propagation weakly supervised semantic segmentation <eos> large scale training semantic segmentation challenging due expense obtaining training data task relative other vision tasks <eos> propose novel training approach address difficulty <eos> given cheaply obtained sparse image labelings propagate sparse labels produce guessed dense labelings <eos> standard cnn based segmentation network trained mimic labelings <eos> label propagation process defined via random walk hitting probabilities leads differentiable parameterization uncertainty estimates incorporated into loss <eos> show learning label propagator jointly segmentation predictor able effectively learn semantic edges given no direct edge supervision <eos> experiments also show training segmentation network way outperforms naive approach <eos> <eop> adversarial discriminative domain adaptation <eos> adversarial learning method promising approach training robust deep network generate complex sample across diverse domains <eos> they also improve recognition despite presence domain shift dataset bias recent adversarial approaches unsupervised domain adaptation reduce difference between training test domain distributions thus improve generalization performance <eos> however while generative adversarial network gans show compelling visualizations they optimal discriminative tasks limited smaller shifts <eos> other hand discriminative approaches handle larger domain shifts but impose tied weights model exploit gan based loss <eos> work first outline novel generalized framework adversarial adaptation subsumes recent state art approaches special cases use generalized view better relate prior approaches <eos> then propose previously unexplored instance general framework combines discriminative modeling untied weight sharing gan loss call adversarial discriminative domain adaptation adda <eos> show adda more effective yet considerably simpler than competing domain adversarial method demonstrate promise approach exceeding state art unsupervised adaptation result standard domain adaptation tasks well difficult cross modality object classification task <eos> <eop> efficient background term three dimensional reconstruction tracking smooth surface models <eos> present novel strategy shrink constrain three dimensional model represented smooth spline like surface within visual hull object observed one multiple views <eos> new background silhouette term combines efficiency previous approaches based image plane distance transform accuracy formulations based raycasting ray potentials <eos> overall formulation solved alternating inner nonlinear minization raycasting joint optimization surface geometry camera poses data correspondences <eos> experiments three dimensional reconstruction object tracking show new formulation corrects several deficiencies existing approaches instance when modelling non convex shapes <eos> moreover proposal more robust against defects object segmentation inherently handles presence uncertainty measurements <eos> null depth values image provided rgb cameras <eos> <eop> amazing mysteries gutter drawing inferences between panels comic book narratives <eos> visual narrative often combination explicit information judicious omissions relying viewer supply missing details <eos> comics most movements time space hidden gutters between panels <eos> follow story readers logically connect panels together inferring unseen actions through process called closure <eos> while computers now describe content natural image paper examine whether they understand closure driven narratives conveyed stylized artwork dialogue comic book panels <eos> collect dataset comics consists over <eos> million panels gb paired automatic textbox transcriptions <eos> depth analysis comics demonstrates neither text nor image alone tell comic book story so computer must understand both modalities keep up plot <eos> introduce three cloze style tasks ask models predict narrative character centric aspects panel given preceding panels context <eos> various deep neural architectures underperform human baselines tasks suggesting comics contains fundamental challenges both vision language <eos> <eop> commonly uncommon semantic sparsity situation recognition <eos> semantic sparsity common challenge structured visual classification problems when output space complex vast majority possible predictions rarely if ever seen training set <eos> paper studies semantic sparsity situation recognition task producing structured summaries happening image including activities object roles object play within activity <eos> problem find empirically most substructures required prediction rare current state art model performance dramatically decreases if even one such rare substructure exists target output <eos> avoid many such errors introducing novel tensor composition function learns share examples across substructures more effectively se mantically augmenting training data automatically gathered examples rarely observed outputs using web data <eos> when integrated within complete crf based structured prediction model tensor based approach outperforms existing state art relative improvement <eos> top verb noun role accuracy respectively <eos> adding million image semantic aug mentation techniques gives further relative improvements <eos> top verb noun role accuracy <eos> <eop> top down visual saliency guided captions <eos> neural image video captioning models generate accurate descriptions but their internal process mapping region words black box therefore difficult explain <eos> top down neural saliency method find important region given high level semantic task such object classification but cannot use natural language sentence top down input task <eos> paper propose caption guided visual saliency expose region word mapping modern encoder decoder network demonstrate learned implicitly caption training data without any pixel level annotations <eos> approach produce spatial spatiotemporal heatmaps both predicted captions arbitrary query sentences <eos> recovers saliency without overhead introducing explicit attention layer used analyze variety existing model architectures improve their design <eos> evaluation large scale video image datasets demonstrates approach achieves comparable captioning performance existing method while providing more accurate saliency heatmaps <eos> code available visionlearninggroup <eos> io caption guided saliency <eop> geometry first returning photons non line sight imaging <eos> non line sight nlos imaging utilizes full light transient measurements reconstruct scenes beyond camera field view <eos> mathematically requires solving elliptical tomography problem unmixes shape albedo spatially multiplexed measurements nlos scene <eos> paper propose new approach nlos imaging studying properties first returning photons three bounce light paths <eos> show times flight first returning photons dependent only geometry nlos scene each observation almost always generated single nlos scene point <eos> exploiting properties derive space carving algorithm nlos scenes <eos> addition assuming local planarity derive algorithm localize nlos scene point three dimensional estimate their surface normals <eos> method require either full transient measurements solving hard elliptical tomography problem <eos> demonstrate effectiveness method through simulations well real data captured spad sensor <eos> <eop> efficient algebraic solution perspective three point problem <eos> work present algebraic solution classical perspective point problem determining position attitude camera observations three known reference point <eos> contrast previous approaches first directly determine camera attitude employing corresponding geometric constraints formulate system trigonometric equations <eos> then efficiently solved following algebraic approach determine unknown rotation matrix subsequently camera position <eos> compared recent alternatives method avoids computing unnecessary potentially numerically unstable intermediate result thus achieves higher numerical accuracy robustness lower computational cost <eos> benefits validated through extensive monte carlo simulations both nominal close singular geometric configurations <eos> <eop> wsisa making survival prediction whole slide histopathological image <eos> image based precision medicine techniques used better treat cancer patients <eos> however gigapixel resolution whole slide histopathological image wsis makes traditional survival models computationally impossible <eos> models usually adopt manually labeled discriminative patches region interests rois unable directly learn discriminative patches wsis <eos> argue only small set patches cannot fully represent patients survival status due heterogeneity tumor <eos> another challenge survival prediction usually comes insufficient training patient sample <eos> paper propose effective whole slide histopathological image survival analysis framework wsisa overcome above challenges <eos> exploit survival discriminative patterns wsis first extract hundreds patches each wsi adaptive sampling then group image into different clusters <eos> then propose train aggregation model make patient level predictions based cluster level deep convolutional survival deepconvsurv prediction result <eos> different existing state arts image based survival models extract feature using some patches small region wsis proposed framework efficiently exploit utilize all discriminative patterns wsis predict patients survival status <eos> best knowledge shown before <eos> apply method survival predictions glioma non small cell lung cancer using three datasets <eos> result demonstrate proposed framework significantly improve prediction performance compared existing state arts survival method <eos> <eop> low power fully event based gesture recognition system <eos> present first gesture recognition system implemented end end event based hardware using truenorth neurosynaptic processor recognize hand gestures real time low power events streamed live dynamic vision sensor dvs <eos> biologically inspired dvs transmits data only when pixel detects change unlike traditional frame based cameras sample every pixel fixed frame rate <eos> sparse asynchronous data representation lets event based cameras operate much lower power than frame based cameras <eos> however much energy efficiency lost if previous work event stream interpreted conventional synchronous processors <eos> here first time process live dvs event stream using truenorth natively event based processor million spiking neurons <eos> configured here convolutional neural network cnn truenorth chip identifies onset gesture latency ms while consuming less than mw <eos> out sample accuracy newly collected dvs dataset dvsgesture comprising hand gesture categories subjects under illumination conditions <eos> <eop> modeling sub event dynamics first person action recognition <eos> first person video unique characteristics such heavy egocentric motion strong preceding events salient transitional activities post event impacts <eos> action recognition method designed third person video may optimally represent actions captured first person video <eos> propose method represent high level dynamics sub events first person video dynamically pooling feature sub intervals time series using temporal feature pooling function <eos> sub event dynamics then temporally aligned make new series <eos> keep track how sub event dynamics evolve over time recursively employ fast fourier transform pyramidal temporal structure <eos> fourier coefficients segment define overall video representation <eos> perform experiments two existing benchmark first person video datasets captured controlled environment <eos> addressing gap introduce new dataset collected youtube larger number classes greater diversity capture conditions thereby more closely depicting real world challenges first person video analysis <eos> compare method state art first person generic video recognition algorithms <eos> method consistently outperforms nearest competitors <eos> respectively three datasets <eos> <eop> yolo better faster stronger <eos> introduce yolo state art real time object detection system detect over object categories <eos> first propose various improvements yolo detection method both novel drawn prior work <eos> improved model yolov state art standard detection tasks like pascal voc coco <eos> using novel multi scale training method same yolov model run varying sizes offering easy tradeoff between speed accuracy <eos> fps yolov gets <eos> fps yolov gets <eos> map outperforming state art method like faster rcnn resnet ssd while still running significantly faster <eos> finally propose method jointly train object detection classification <eos> using method train yolo simultaneously coco detection dataset imagenet classification dataset <eos> joint training allows yolo predict detections object classes don labelled detection data <eos> validate approach imagenet detection task <eos> map imagenet detection validation set despite only having detection data classes <eos> classes coco yolo gets <eos> yolo predicts detections more than different object categories all real time <eos> <eop> skeleton key image captioning skeleton attribute decomposition <eos> recently there lot interest automatically generating descriptions image <eos> most existing language model based approaches task learn generate image description word word its original word order <eos> however humans more natural locate object their relationships first then elaborate each object describing notable attributes <eos> present coarse fine method decomposes original image description into skeleton sentence its attributes generates skeleton sentence attribute phrases separately <eos> decomposition method generate more accurate novel descriptions than previous state art <eos> experimental result ms coco larger scale stock datasets show algorithm yields consistent improvements across different evaluation metrics especially spice metric much higher correlation human ratings than conventional metrics <eos> furthermore algorithm generate descriptions varied length benefiting separate control skeleton attributes <eos> enables image description generation better accommodates user preferences <eos> <eop> joint speaker listener reinforcer model referring expressions <eos> referring expressions natural language constructions used identify particular object within scene <eos> paper propose unified framework tasks referring expression comprehension generation <eos> model composed three modules speaker listener reinforcer <eos> speaker generates referring expressions listener comprehends referring expressions reinforcer introduces reward function guide sampling more discriminative expressions <eos> listener speaker modules trained jointly end end learning framework allowing modules aware one another during learning while also benefiting discriminative reinforcer feedback <eos> demonstrate unified framework training achieves state art result both comprehension generation three referring expression datasets <eos> project demo page vision <eos> edu refer <eop> realtime multi person pose estimation using part affinity fields <eos> present approach efficiently detect pose multiple people image <eos> approach uses nonparametric representation refer part affinity fields pafs learn associate body parts individuals image <eos> architecture encodes global context allowing greedy bottom up parsing step maintains high accuracy while achieving realtime performance irrespective number people image <eos> architecture designed jointly learn part locations their association via two branches same sequential prediction process <eos> method placed first inaugural coco keypoints challenge significantly exceeds previous state art result mpii multi person benchmark both performance efficiency <eos> <eop> newton type method inference higher order markov random fields <eos> linear programming relaxations central map ference discrete markov random fields <eos> ability properly solve lagrangian dual critical component such method <eos> paper study benefit ing newton type method solve lagrangian dual smooth version problem <eos> investigate their abil ity achieve superior convergence behavior bet ter handle ill conditioned nature formulation compared first order method <eos> show indeed possible efficiently apply trust region newton method broad range map inference problems <eos> pa per propose provably globally efficient framework includes excellent compromise between computational complexity precision concerning hessian matrix construction ii damping strategy aids efficient opti mization iii truncation strategy coupled generic pre conditioner conjugate gradients iv efficient sum product computation sparse clique potentials <eos> result higher order markov random fields demonstrate potential approach <eos> <eop> speed accuracy trade offs modern convolutional object detectors <eos> goal paper serve guide selecting detection architecture achieves right speed memory accuracy balance given application platform <eos> end investigate various ways trade accuracy speed memory usage modern convolutional object detection systems <eos> number successful systems proposed recent years but apples apples comparisons difficult due different base feature extractors <eos> vgg residual network different default image resolutions well different hardware software platforms <eos> present unified implementation faster cnn ren <eos> systems view meta architectures trace out speed accuracy trade off curve created using alternative feature extractors varying other critical parameters such image size within each meta architectures <eos> one extreme end spectrum speed memory critical present detector achieves real time speeds deployed mobile device <eos> opposite end accuracy critical present detector achieves state art performance measured coco detection task <eos> <eop> deep outdoor illumination estimation <eos> present cnn based technique estimate high dynamic range outdoor illumination single low dynamic range image <eos> train cnn leverage large dataset outdoor panoramas <eos> fit low dimensional physically based outdoor illumination model skies panoramas giving compact set parameters including sun position atmospheric conditions camera parameters <eos> extract limited field view image panoramas train cnn large set input image output lighting parameter pairs <eos> given test image network used infer illumination parameters turn used reconstruct outdoor illumination environment map <eos> demonstrate approach allows recovery plausible illumination conditions enables photorealistic virtual object insertion single image <eos> extensive evaluation both panorama dataset captured hdr environment maps shows technique significantly outperforms previous solutions problem <eos> <eop> weakly supervised semantic segmentation using web crawled video <eos> propose novel algorithm weakly supervised semantic segmentation based image level class labels only <eos> weakly supervised setting commonly observed trained model overly focuses discriminative parts rather than entire object area <eos> goal overcome limitation no additional human intervention retrieving video relevant target class labels web repository generating segmentation labels retrieved video simulate strong supervision semantic segmentation <eos> during process take advantage image classification discriminative localization technique reject false alarms retrieved video identify relevant spatio temporal volumes within retrieved video <eos> although entire procedure require any additional supervision segmentation annotations obtained video sufficiently strong learn model semantic segmentation <eos> proposed algorithm substantially outperforms existing method based same level supervision even competitive approaches relying extra annotations <eos> <eop> global optimality neural network training <eos> past few years seen dramatic increase performance recognition systems thanks introduction deep network representation learning <eos> however mathematical reasons success remain elusive <eos> key issue neural network training problem nonconvex hence optimization algorithms may return global minima <eos> paper provides sufficient conditions guarantee local minima globally optimal local descent strategy reach global minima any initialization <eos> conditions require both network output regularization positively homogeneous functions network parameters regularization being designed control network size <eos> result apply network one hidden layer size measured number neurons hidden layer multiple deep subnetworks connected parallel size measured number subnetworks <eos> <eop> fine tuning convolutional neural network biomedical image analysis actively incrementally <eos> intense interest applying convolutional neural network cnn biomedical image analysis wide spread but its success impeded lack large annotated datasets biomedical imaging <eos> annotating biomedical image only tedious time consuming but also demanding costly specialty oriented knowledge skills easily accessible <eos> dramatically reduce annotation cost paper presents novel method called aift active incremental fine tuning naturally integrate active learning transfer learning into single framework <eos> aift starts directly pre trained cnn seek worthy sample unannotated annotation fine tuned cnn further fine tuned continuously incorporating newly annotated sample each iteration enhance cnn performance incrementally <eos> evaluated method three different biomedical imaging applications demonstrating cost annotation cut least half <eos> performance attributed several advantages derived advanced active incremental capability aift method <eos> <eop> fason first second order information fusion network texture recognition <eos> deep network shown impressive performance many computer vision tasks <eos> recently deep convolutional neural network cnn used learn discriminative texture representations <eos> one most successful approaches bilinear cnn model explicitly captures second order statistics within deep feature <eos> however network cut off first order information flow deep network make gradient back propagation difficult <eos> propose effective fusion architecture fason combines second order information flow first order information flow <eos> method allows gradients back propagate through both flows freely trained effectively <eos> then build multi level deep architecture exploit first second order information within different convolutional layer <eos> experiments show method achieves improvements over state art method several benchmark datasets <eos> <eop> recurrent convolutional neural network continuous sign language recognition staged optimization <eos> work presents weakly supervised framework deep neural network vision based continuous sign language recognition ordered gloss labels but no exact temporal locations available video sign sentence amount labeled sentences training limited <eos> approach addresses mapping video segments glosses introducing recurrent convolutional neural network spatio temporal feature extraction sequence learning <eos> design three stage optimization process architecture <eos> first develop end end sequence learning scheme employ connectionist temporal classification ctc objective function alignment proposal <eos> second take alignment proposal stronger supervision tune feature extractor <eos> finally optimize sequence learning model improved feature representations design weakly supervised detection network regularization <eos> apply proposed approach real world continuous sign language recognition benchmark method no extra supervision achieves result comparable state art <eos> <eop> compressing deep models low rank sparse decomposition <eos> deep compression refers removing redundancy parameters feature maps deep learning models <eos> low rank approximation pruning sparse structures play vital role many compression works <eos> however weight filters tend both low rank sparse <eos> neglecting either part structure information previous method result iteratively retraining compromising accuracy low compression rates <eos> here propose unified framework integrating low rank sparse decomposition weight matrices feature map reconstructions <eos> model includes method like pruning connections special cases optimized fast svd free algorithm <eos> theoretically proven small sample due its generalizability model well reconstruct feature maps both training test data result less compromising accuracy prior subsequent retraining <eos> such warm start retrain compression method always possesses several merits higher compression rates little loss accuracy fewer rounds compress deep models <eos> experimental result several popular models such alexnet vgg googlenet show model significantly reduce parameters both convolutional fully connected layer <eos> result model reduces size vgg better than other recent compression method use single strategy <eos> <eop> cross modality binary code learning via fusion similarity hashing <eos> binary code learning emerging topic large scale cross modality retrieval recently <eos> aims map feature multiple modalities into common hamming space cross modality similarity approximated efficiently via hamming distance <eos> end most existing works learn binary codes directly data instances multiple modalities preserve both intra inter modal similarities respectively <eos> few method consider preserve fusion similarity among multi modal instances instead explicitly capture their heterogeneous correlation cross modality retrieval <eos> paper propose hashing scheme termed fusion similarity hashing fsh explicitly embeds graph based fusion similarity across modalities into common hamming space <eos> inspired fusion diffusion core idea construct undirected asymmetric graph model fusion similarity among different modalities upon graph hashing scheme alternating optimization introduced learn binary codes embeds such fusion similarity <eos> quantitative evaluations three widely used benchmarks <eos> uci handwritten digit mir flickr nus wide demonstrate proposed fsh approach achieve superior performance over state art method <eos> <eop> adaptive relaxed admm convergence theory practical implementation <eos> many modern computer vision machine learning applications rely solving difficult optimization problems involve non differentiable objective functions constraints <eos> alternating direction method multipliers admm widely used approach solve such problems <eos> relaxed admm generalization admm often achieves better performance but its efficiency depends strongly algorithm parameters must chosen expert user <eos> propose adaptive method automatically tunes key algorithm parameters achieve optimal performance without user oversight <eos> inspired recent work adaptivity proposed adaptive relaxed admm aradmm derived assuming barzilai borwein style linear gradient <eos> detailed convergence analysis aradmm provided numerical result several applications demonstrate fast practical convergence <eos> <eop> generative hierarchical learning sparse frame models <eos> paper proposes method generative learning hierarchical random field models <eos> resulting model call hierarchical sparse frame filters random field maximum entropy model generalization original sparse frame model decomposing into multiple parts allowed shift their locations scales rotations so resulting model becomes hierarchical deformable template <eos> model trained em type algorithm alternates following two steps inference given current model match each training image inferring unknown locations scales rotations object its parts recursive sum max maps re learning given inferred geometric configurations object their parts re learn model parameters maximum likelihood estimation via stochastic gradient algorithm <eos> experiments show proposed method capable learning meaningful interpretable templates used object detection classification clustering <eos> <eop> exploiting symmetry manhattan properties three dimensional object structure estimation single multiple image <eos> many man made object intrinsic symmetries manhattan structure <eos> assuming orthographic projection model paper addresses estimation three dimensional structures camera projection using symmetry manhattan structure cues occur when input single multiple image same category <eos> multiple different cars <eos> specifically analysis single image case implies manhattan alone sufficient recover camera projection then three dimensional structure reconstructed uniquely exploiting symmetry <eos> however manhattan structure difficult observe single image due occlusion <eos> end extend multiple image case also exploit symmetry but require manhattan axes <eos> propose novel rigid structure motion method exploiting symmetry using multiple image same category input <eos> experimental result pascal dataset show method significantly outperforms baseline method <eos> <eop> fast haze removal nighttime image using maximum reflectance prior <eos> paper address haze removal problem single nighttime image even presence varicolored non uniform illumination <eos> core idea lies novel maximum reflectance prior <eos> first introduce nighttime hazy imaging model includes local ambient illumination item both direct attenuation term scattering term <eos> then propose simple but effective image prior maximum reflectance prior estimate varying ambient illumination <eos> maximum reflectance prior based key observation most daytime haze free image patches each color channel very high intensity some pixels <eos> nighttime haze image local maximum intensities each color channel mainly contributed ambient illumination <eos> therefore directly estimate ambient illumination transmission map consequently restore high quality haze free image <eos> experimental result various nighttime hazy image demonstrate effectiveness proposed approach <eos> particular approach advantage computational efficiency times faster than state art method <eos> <eop> adaptive class preserving representation image classification <eos> linear representation based image classification unlabeled sample represented entire training set <eos> obtain stable discriminative solution regularization vector representation coefficients necessary <eos> example representation sparse representation based classification src uses norm penalty regularization equal lasso <eos> however lasso overemphasizes role sparseness while ignoring inherent structure among sample belonging same class <eos> many recent developed representation classifications adopted lasso type regressions improve performance <eos> paper propose adaptive class preserving representation classification acprc <eos> method related group lasso based classification but different two key point when training sample class uncorrelated acprc turns into src when sample class highly correlated obtains similar result group lasso <eos> superiority acprc over other state art regularization techniques including lasso group lasso sparse group lasso etc <eos> evaluated extensive experiments <eos> <eop> dataset benchmarking image based localization <eos> novel dataset benchmarking image based localization presented <eos> increasing research interests visual place recognition localization several datasets published past few years <eos> one evident limitations existing datasets precise ground truth camera poses query image available meaningful three dimensional metric system <eos> part due underlying three dimensional models datasets reconstructed structure motion method <eos> so far little attention paid metric evaluations localization accuracy <eos> paper address problem whether state art visual localization techniques applied tasks demanding accuracy requirements <eos> acquired training data large indoor environment cameras lidar scanner <eos> addition collected over query image cell phone cameras <eos> using lidar point clouds reference employed semi automatic approach estimate degrees freedom camera poses precisely world coordinate system <eos> proposed dataset enables quantitatively assess performance various algorithms using fair intuitive metric <eos> <eop> low rank sparse subspace representation robust regression <eos> learning robust regression model high dimensional corrupted data essential difficult problem many practical applications <eos> state art method studied low rank regression models robust against typical noises like gaussian noise out sample sparse noise outliers such regression model learned clean data lying underlying subspaces <eos> however few existing low rank regression method handle outliers noise lying sparsely corrupted disjoint subspaces <eos> address issue propose low rank sparse subspace representation robust regression hereafter referred lrs rr paper <eos> main contribution include following unlike most existing regression method propose approach two phases low rank sparse subspace recovery regression optimization being carried out simultaneously also apply linearized alternating direction method adaptive penalty solved formulated lrs rr problem prove convergence algorithm analyze its complexity demonstrate efficiency method high dimensional corrupted data both synthetic data two benchmark datasets against several state art robust method <eos> <eop> aga attribute guided augmentation <eos> consider problem data augmentation <eos> generating artificial sample extend given corpus training data <eos> specifically propose attributed guided augmentation aga learns mapping allows synthesize data such attribute synthesized sample desired value strength <eos> particularly interesting situations little data no attribute annotation available learning but access large external corpus heavily annotated sample <eos> while prior works primarily augment space image propose perform augmentation feature space instead <eos> implement approach deep encoder decoder architecture learns synthesis function end end manner <eos> demonstrate utility approach problems one shot object recognition transfer learning setting no prior knowledge new classes well object based one shot scene recognition <eos> external data leverage three dimensional depth pose information sun rgb dataset <eos> experiments show attribute guided augmentation high level cnn feature considerably improves one shot recognition performance both problems <eos> <eop> awesome typography statistics based text effects transfer <eos> work explore problem generating fantastic special effects typography <eos> quite challenging due model diversities illustrate varied text effects different characters <eos> address issue key idea exploit analytics high regularity spatial distribution text effects guide synthesis process <eos> specifically characterize stylized patches their normalized positions optimal scales depict their style elements <eos> method first estimates two feature derives their correlation statistically <eos> they then converted into soft constraints texture transfer accomplish adaptive multi scale texture synthesis make style element distribution uniform <eos> allows algorithm produce artistic typography fits both local texture patterns global spatial distribution example <eos> experimental result demonstrate superiority method various text effects over conventional style transfer method <eos> addition validate effectiveness algorithm extensive artistic typography library generation <eos> <eop> lean crowdsourcing combining humans machines online system <eos> introduce method greatly reduce amount redundant annotations required when crowdsourcing annotations such bounding boxes parts class labels <eos> example if two mechanical turkers happen click same pixel location when annotating part given image event very unlikely occur random chance strong indication location correct <eos> similar type confidence obtained if single turker happened agree computer vision estimate <eos> thus incrementally collect variable number worker annotations per image based online estimates confidence <eos> done using sequential estimation risk over probabilistic model combines worker skill image difficulty incrementally trained computer vision model <eos> develop specialized models algorithms binary annotation part keypoint annotation set bounding box annotations <eos> show method reduce annotation time factor binary filtering websearch result annotation boxes pedestrians image while many cases also reducing annotation error <eos> will make end end version system publicly available <eos> <eop> 