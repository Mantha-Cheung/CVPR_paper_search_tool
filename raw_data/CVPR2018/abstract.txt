embodied question answering <eos> present new ai task embodied question answering embodiedqa agent spawned random location three dimensional environment asked question color car <eos> order answer agent must first intelligently navigate explore environment gather necessary visual information through first person egocentric vision then answer question orange <eos> embodiedqa requires range ai skills language understanding visual recognition active perception goal driven navigation commonsense reasoning long term memory grounding language into actions <eos> work develop dataset questions answers house environments evaluation metrics hierarchical model trained imitation reinforcement learning <eos> <eop> learning asking questions <eos> introduce interactive learning framework development testing intelligent visual systems called learning asking lba <eos> explore lba context visual question answering vqa task <eos> lba differs standard vqa training most questions observed during training time learner must ask questions wants answers <eos> thus lba more closely mimics natural learning potential more data efficient than traditional vqa setting <eos> present model performs lba clevr dataset show automatically discovers easy hard curriculum when learning interactively oracle <eos> lba generated data consistently matches outperforms clevr train data more sample efficient <eos> also show model asks questions generalize state art vqa models novel test time distributions <eos> <eop> finding tiny faces wild generative adversarial network <eos> face detection techniques developed decades one remaining open challenges detecting small faces unconstrained conditions <eos> reason tiny faces often lacking detailed information blurring <eos> paper proposed algorithm directly generate clear high resolution face blurry small one adopting generative adversarial network gan <eos> toward end basic gan formulation achieves super resolving refining sequentially <eos> sr gan cycle gan <eos> however design novel network address problem super resolving refining jointly <eos> also introduce new training losses guide generator network recover fine details promote discriminator network distinguish real vs <eos> fake face vs <eos> non face simultaneously <eos> extensive experiments challenging dataset wider face demonstrate effectiveness proposed method restoring clear high resolution face blurry small one show detection performance outperforms other state art method <eos> <eop> learning face age progression pyramid architecture gans <eos> two underlying requirements face age progression <eos> aging accuracy identity permanence well studied literature <eos> paper present novel generative adversarial network based approach <eos> separately models constraints intrinsic subject specific characteristics age specific facial changes respect elapsed time ensuring generated faces present desired aging effects while simultaneously keeping personalized properties stable <eos> further generate more lifelike facial details high level age specific feature conveyed synthesized face estimated pyramidal adversarial discriminator multiple scales simulates aging effects finer manner <eos> proposed method applicable diverse face sample presence variations pose expression makeup etc <eos> remarkably vivid aging effects achieved <eos> both visual fidelity quantitative evaluations show approach advances state art <eos> <eop> pairedcyclegan asymmetric style transfer applying removing makeup <eos> paper introduces automatic method editing portrait photo so subject appears wearing makeup style another person reference photo <eos> unsupervised learning approach relies new framework cycle consistent generative adversarial network <eos> different image domain transfer problem style transfer problem involves two asymmetric functions forward function encodes example based style transfer whereas backward function removes style <eos> construct two coupled network implement functions one transfers makeup style second remove makeup such output their successive application input photo will match input <eos> learned style network then quickly apply arbitrary makeup style arbitrary photo <eos> demonstrate effectiveness broad range portraits styles <eos> <eop> ganerated hands real time three dimensional hand tracking monocular rgb <eos> address highly challenging problem real time three dimensional hand tracking based monocular rgb only sequence <eos> tracking method combines convolutional neural network kinematic three dimensional hand model such generalizes well unseen data robust occlusions varying camera viewpoints leads anatomically plausible well temporally smooth hand motions <eos> training cnn propose novel approach synthetic generation training data based geometrically consistent image image translation network <eos> more specific use neural network translates synthetic image real image such so generated image follow same statistical distribution real world hand image <eos> training translation network combine adversarial loss cycle consistency loss geometric consistency loss order preserve geometric properties such hand pose during translation <eos> demonstrate hand tracking system outperforms current state art challenging rgb only footage <eos> <eop> learning pose specific representations predicting different views <eos> labeled data required learn pose estimation articulated object difficult provide desired quantity realism density accuracy <eos> address issue develop method learn representations very specific articulated poses without need labeled training data <eos> exploit observation object pose known object predictive appearance any known view <eos> given only pose shape parameters hand hand appearance any viewpoint approximated <eos> exploit observation train model given input one view estimates latent representation trained predictive appearance object when captured another viewpoint <eos> thus only necessary supervision second view <eos> training process model reveals implicit pose representation latent space <eos> importantly test time pose representation inferred using only single view <eos> qualitative quantitative experiments show learned representations capture detailed pose information <eos> moreover when training proposed method jointly labeled unlabeled data consistently surpasses performance its fully supervised counterpart while reducing amount needed labeled sample least one order magnitude <eos> <eop> weakly semi supervised human body part parsing via pose guided knowledge transfer <eos> human body part parsing human semantic part segmentation fundamental many computer vision tasks <eos> conventional semantic segmentation method ground truth segmentations provided fully convolutional network fcn trained end end scheme <eos> although method demonstrated impressive result their performance highly depends quantity quality training data <eos> paper present novel method generate synthetic human part segmentation data using easily obtained human keypoint annotations <eos> key idea exploit anatomical similarity among human transfer parsing result person another person similar pose <eos> using estimated result additional training data semi supervised model outperforms its strong supervised counterpart miou pascal person part dataset achieve state art human parsing result <eos> approach general readily extended other object animal parsing task assuming their anatomical similarity annotated keypoints <eos> proposed model accompanying source code will made publicly available <eos> <eop> person transfer gan bridge domain gap person re identification <eos> although performance person re identification reid significantly boosted many challenging issues real scenarios fully investigated <eos> complex scenes lighting variations viewpoint pose changes large number identities camera network <eos> facilitate research towards conquering issues paper contributes new dataset called msmt many important feature <eos> raw video taken camera network deployed both indoor outdoor scenes video cover long period time present complex lighting variations contains currently largest number annotated identities <eos> identities bounding boxes <eos> also observe domain gap commonly exists between datasets essentially causes severe performance drop when training testing different datasets <eos> result available training data cannot effectively leveraged new testing domains <eos> relieve expensive costs annotating new training sample propose person transfer generative adversarial network ptgan bridge domain gap <eos> comprehensive experiments show domain gap could substantially narrowed down ptgan <eos> <eop> cross modal deep variational hand pose estimation <eos> human hand moves complex high dimensional ways making estimation three dimensional hand pose configurations image alone challenging task <eos> work propose method learn statistical hand model represented cross modal trained latent space via generative deep neural network <eos> derive objective function variational lower bound vae framework jointly optimize resulting cross modal kl divergence posterior reconstruction objective naturally admitting training regime leads coherent latent space across multiple modalities such rgb image keypoint detections three dimensional hand configurations <eos> additionally grants straightforward way using semi supervision <eos> latent space directly used estimate three dimensional hand poses rgb image outperforming state art different settings <eos> furthermore show proposed method used without changes depth image performs comparably specialized method <eos> finally model fully generative synthesize consistent pairs hand configurations across modalities <eos> evaluate method both rgb depth datasets analyze latent space qualitatively <eos> <eop> disentangled person image generation <eos> generating novel yet realistic image persons challenging task due complex interplay between different image factors such foreground background pose information <eos> work aim generating such image based novel two stage reconstruction pipeline learns disentangled representation aforementioned image factors generates novel person image same time <eos> first multi branched reconstruction network proposed disentangle encode three factors into embedding feature then combined re compose input image itself <eos> second three corresponding mapping functions learned adversarial manner order map gaussian noise learned embedding feature space each factor respectively <eos> using proposed framework manipulate foreground background pose input image also sample new embedding feature generate such targeted manipulations provide more control over generation process <eos> experiments market deepfashion datasets show model only generate realistic person image new foregrounds backgrounds poses but also manipulates generated factors interpolates between states <eos> another set experiments market shows model also beneficial person re identification task <eos> <eop> super fan integrated facial landmark localization super resolution real world low resolution faces arbitrary poses gans <eos> paper addresses challenging tasks improving quality low resolution facial image accurately locating facial landmarks such poor resolution image <eos> end make following contributions propose super fan very first end end system addresses both tasks simultaneously <eos> both improves face resolution detects facial landmarks <eos> novelty super fan lies incorporating structural information gan based super resolution algorithm via integrating sub network face alignment through heatmap regression optimizing novel heatmap loss <eos> illustrate benefit training two network jointly reporting good result only frontal image prior work but whole spectrum facial poses only synthetic low resolution image prior work but also real world image <eos> improve upon state art face super resolution proposing new residual based architecture <eos> quantitatively show large improvement over state art both face super resolution alignment <eos> qualitatively show first time good result real world low resolution image <eos> <eop> multistage adversarial losses pose based human image synthesis <eos> human image synthesis extensive practical applications <eos> person re identification data augmentation human pose estimation <eos> however much more challenging than rigid object synthesis <eos> cars chairs due variability human posture <eos> paper propose pose based human image synthesis method keep human posture unchanged novel viewpoints <eos> furthermore adopt multistage adversarial losses separately foreground background generation fully exploits multi modal characteristics generative loss generate more realistic looking image <eos> perform extensive experiments human <eos> dataset verify effectiveness each stage method <eos> generated human image only keep same pose input image but also clear detailed foreground background <eos> quantitative comparison result illustrate approach achieves much better result than several state art method <eos> <eop> rotation averaging strong duality <eos> paper explore role duality principles within problem rotation averaging fundamental task wide range computer vision applications <eos> its conventional form rotation averaging stated minimization over multiple rotation constraints <eos> constraints non convex problem generally considered challenging solve globally <eos> show how circumvent difficulty through use lagrangian duality <eos> while such approach well known normally guaranteed provide tight relaxation <eos> based spectral graph theory analytically prove many cases there no duality gap unless noise levels severe <eos> allows obtain certifiably global solutions class important non convex problems polynomial time <eos> also propose efficient scalable algorithm out performs general purpose numerical solvers able handle large problem instances commonly occurring structure motion settings <eos> potential proposed method demonstrated number different problems consisting both synthetic real world data <eos> <eop> hybrid camera pose estimation <eos> paper aim solve pose estimation problem calibrated pinhole generalized cameras <eos> structure motion sfm model leveraging both three dimensional correspondences well correspondences <eos> traditional approaches either focus use three dimensional matches known structure based pose estimation solely matches structure less pose estimation <eos> absolute pose approaches limited their performance quality three dimensional point triangulations well completeness three dimensional model <eos> relative pose approaches other hand while being more accurate also tend far more computationally costly often return dozens possible solutions <eos> work aims bridge gap between two paradigms <eos> propose new ransac based approach automatically chooses best type solver use each iteration data driven way <eos> solvers chosen ransac range pure structure based structure less solvers any possible combination hybrid solvers <eos> using both types matches between <eos> number new hybrid minimal solvers also presented paper <eos> both synthetic real data experiments show approach accurate structure less approaches while staying close efficiency structure based method <eos> <eop> certifiably globally optimal solution non minimal relative pose problem <eos> finding relative pose between two calibrated views ranks among most fundamental geometric vision problems <eos> therefore appears somewhat surprise globally optimal solver minimizes properly defined energy over non minimal correspondence set original space relative transformations yet discovered <eos> notably contribution present paper <eos> formulate problem quadratically constrained quadratic program qcqp converted into semidefinite program sdp using shor convex relaxation <eos> while theoretical proof tightness relaxation remains open prove through exhaustive validation both simulated real experiments approach always finds certifies posteriori global optimum cost function <eos> <eop> single view stereo matching <eos> previous monocular depth estimation method take single view directly regress expected result <eos> though recent advances made applying geometrically inspired loss functions during training inference procedure explicitly impose any geometrical constraint <eos> therefore models purely rely quality data effectiveness learning generalize <eos> either leads suboptimal result demand huge amount expensive ground truth labelled data generate reasonable result <eos> paper show first time monocular depth estimation problem reformulated two sub problems view synthesis procedure followed stereo matching two intriguing properties namely geometrical constraints explicitly imposed during inference ii demand labelled depth data greatly alleviated <eos> show whole pipeline still trained end end fashion new formulation plays critical role advancing performance <eos> resulting model outperforms all previous monocular depth estimation method well stereo block matching method challenging kitti dataset only using small number real training data <eos> model also generalizes well other monocular depth estimation benchmarks <eos> also discuss implications advantages solving monocular depth estimation using stereo method <eos> <eop> fight ill posedness ill posedness single shot variational depth super resolution shading <eos> put forward principled variational approach up sampling single depth map resolution companion color image provided rgb sensor <eos> combine heterogeneous depth color data order jointly solve ill posed depth super resolution shape shading problems <eos> low frequency geometric information necessary disambiguate shape shading extracted low resolution depth measurements symmetrically high resolution photometric clues rgb image provide high frequency information required disambiguate depth super resolution <eos> <eop> deep depth completion single rgb image <eos> goal work complete depth channel rgb image <eos> commodity grade depth cameras often fail sense depth shiny bright transparent distant surfaces <eos> address problem train deep network takes rgb image input predicts dense surface normals occlusion boundaries <eos> predictions then combined raw depth observations provided rgb camera solve depths all pixels including missing original observation <eos> method was chosen over others <eos> inpainting depths directly result extensive experiments new depth completion benchmark dataset holes filled training data through rendering surface reconstructions created multiview rgb scans <eos> experiments different network inputs depth representations loss functions optimization method inpainting method deep depth estimation network show proposed approach provides better depth completions than alternatives <eos> <eop> multi view harmonized bilinear network three dimensional object recognition <eos> view based method achieved considerable success object recognition tasks <eos> different existing view based method pooling view wise feature tackle problem perspective patches patches similarity measurement <eos> exploiting relationship between polynomial kernel bilinear pooling obtain effective object representation aggregating local convolutional feature through bilinear pooling <eos> meanwhile harmonize different components inherited pooled bilinear feature obtain more discriminative representation object <eos> achieve end end trainable framework incorporate harmonized bilinear pooling operation layer network constituting proposed multi view harmonized bilinear network mhbn <eos> systematic experiments conducted two public benchmark datasets demonstrate efficacy proposed method object recognition <eos> <eop> ppfnet global context aware local feature robust three dimensional point matching <eos> present ppfnet point pair feature network deeply learning globally informed three dimensional local feature descriptor find correspondences unorganized point clouds <eos> ppfnet learns local descriptors pure geometry highly aware global context important cue deep learning <eos> three dimensional representation computed collection point pair feature combined point normals within local vicinity <eos> permutation invariant network design inspired pointnet set ppfnet ordering free <eos> opposed voxelization method able consume raw point clouds exploit full sparsity <eos> ppfnet uses novel tuple loss architecture injecting global information naturally into local descriptor <eos> shows context awareness also boosts local feature representation <eos> qualitative quantitative evaluations network suggest increased recall improved robustness invariance well vital step three dimensional descriptor extraction performance <eos> <eop> foldingnet point cloud auto encoder via deep grid deformation <eos> recent deep network directly handle point point set <eos> pointnet state art supervised learning tasks point clouds such classification segmentation <eos> work novel end end deep auto encoder proposed address unsupervised learning challenges point clouds <eos> encoder side graph based enhancement enforced promote local structures top pointnet <eos> then novel folding based decoder deforms canonical grid onto underlying three dimensional object surface point cloud achieving low reconstruction errors even object delicate structures <eos> proposed decoder only uses about parameters decoder fully connected neural network yet leads more discriminative representation achieves higher linear svm classification accuracy than benchmark <eos> addition proposed decoder structure shown theory generic architecture able reconstruct arbitrary point cloud grid <eos> code available www <eos> com research license foldingnet <eop> lego learning edge geometry all once watching video <eos> learning estimate three dimensional geometry single image watching unlabeled video via deep convolutional network attracting significant attention <eos> paper introduce three dimensional smooth possible three dimensional asap prior inside pipeline enables joint estimation edges three dimensional scene yielding result significant improvement accuracy fine detailed structures <eos> specifically define three dimensional asap prior requiring any two point recovered three dimensional image should lie existing planar surface if no other cues provided <eos> design unsupervised framework learns edges geometry depth normal all once lego <eos> predicted edges embedded into depth surface normal smoothness terms pixels without edges between constrained satisfy prior <eos> framework predicted depths normals edges forced consistent all time <eos> conduct experiments kitti evaluate estimated geometry cityscapes perform edge evaluation <eos> show all tasks <eos> depth normal edge algorithm vastly outperforms other state art sota algorithms demonstrating benefits approach <eos> <eop> five point fundamental matrix estimation uncalibrated cameras <eos> aim estimating fundamental matrix two views five correspondences rotation invariant feature obtained <eos> proposed minimal solver first estimates homography three correspondences assuming they co planar exploiting their rotational components <eos> then fundamental matrix obtained homography two additional point pairs general position <eos> proposed approach combined robust estimators like graph cut ransac superior other state art algorithms both terms accuracy number iterations required <eos> validated synthesized data real image pairs <eos> moreover tests show requiring three point plane too restrictive urban environment locally optimized robust estimators lead accurate estimates even if point entirely co planar <eos> potential application show using proposed method makes two view multi motion estimation more accurate <eos> <eop> pointfusion deep sensor fusion three dimensional bounding box estimation <eos> present pointfusion generic three dimensional object detection method leverages both image three dimensional point cloud information <eos> unlike existing method either use multi stage pipelines hold sensor dataset specific assumptions pointfusion conceptually simple application agnostic <eos> image data raw point cloud data independently processed cnn pointnet architecture respectively <eos> resulting outputs then combined novel fusion network predicts multiple three dimensional box hypotheses their confidences using input three dimensional point spatial anchors <eos> evaluate pointfusion two distinctive datasets kitti dataset feature driving scenes captured lidar camera setup sun rgbd dataset captures indoor environments rgb cameras <eos> model first one able perform par better than state art diverse datasets without any dataset specific model tuning <eos> <eop> scalable dense non rigid structure motion grassmannian perspective <eos> paper addresses task dense non rigid structure motion nrsfm using multiple image <eos> state art method problem often hurdled scalability expensive computations noisy measurements <eos> further recent method nrsfm usually either assume small number sparse feature point ignore local non linearities shape deformations thus cannot reliably model complex non rigid deformations <eos> address issues paper propose new approach dense nrsfm modeling problem grassmann manifold <eos> specifically assume complex non rigid deformations lie union local linear subspaces both spatially temporally <eos> naturally allows compact representation complex non rigid deformation over frames <eos> provide experimental result several synthetic real benchmark datasets <eos> procured result clearly demonstrate method apart being scalable more accurate than state art method also more robust noise generalizes highly non linear deformations <eos> <eop> gvcnn group view convolutional neural network three dimensional shape recognition <eos> three dimensional shape recognition attracted much attention recently <eos> its recent advances advocate usage deep feature achieve state art performance <eos> however existing deep feature three dimensional shape recognition restricted view shape setting learns shape descriptor view level feature directly <eos> despite exciting progress view based three dimensional shape description intrinsic hierarchical correlation discriminability among views well exploited important three dimensional shape representation <eos> tackle issue paper propose group view convolutional neural network gvcnn framework hierarchical correlation modeling towards discriminative three dimensional shape description <eos> proposed gvcnn framework composed hierarchical view group shape architecture <eos> view level group level shape level organized using grouping strategy <eos> concretely first use expanded cnn extract view level descriptor <eos> then grouping module introduced estimate content discrimination each view based all views splitted into different groups according their discriminative level <eos> group level description further generated pooling view descriptors <eos> finally all group level descriptors combined into shape level descriptor according their discriminative weights <eos> experimental result comparison state art method show proposed gvcnn method achieve significant performance gain both three dimensional shape classification retrieval tasks <eos> <eop> depth transient imaging compressive spad array cameras <eos> time flight depth imaging transient imaging two imaging modalities recently received lot interest <eos> despite much research existing hardware systems limited either terms temporal resolution prohibitively expensive <eos> arrays single photon avalanche diodes spads promise fill gap providing higher temporal resolution affordable cost <eos> unfortunately spad arrays date only available relatively small resolutions <eos> work aim overcome spatial resolution limit spad arrays employing compressive sensing camera design <eos> using dmd custom optics achieve image resolution up spad arrays resolution <eos> using new data fitting model time histograms suppress noise while abstracting phase amplitude information so realize temporal resolution few tens picoseconds <eos> <eop> geonet geometric neural network joint depth surface normal estimation <eos> paper propose geometric neural network geonet jointly predict depth surface normal maps single image <eos> building top two stream cnn geonet incorporates geometric relation between depth surface normal via new depth normal normal depth network <eos> depth normal network exploits least square solution surface normal depth im proves its quality residual module <eos> normal depth network contrarily refines depth map based con straints surface normal through kernel regression module no parameter learn <eos> two net works enforce underlying model efficiently predict depth surface normal high consistency corre sponding accuracy <eos> experiments nyu dataset verify geonet able predict geometrically con sistent depth normal maps <eos> achieves top performance surface normal estimation par state art depth estimation method <eos> <eop> real time seamless single shot object pose prediction <eos> propose single shot approach simultaneously detecting object rgb image predicting its pose without requiring multiple stages having examine multiple hypotheses <eos> unlike recently proposed single shot technique task kehl <eos> only predicts approximate pose must then refined ours accurate enough require additional post processing <eos> result much faster fps titan pascal gpu more suitable real time processing <eos> key component method new cnn architecture inspired redmon <eos> redmon farhadi directly predicts image locations projected vertices object three dimensional bounding box <eos> object pose then estimated using pnp algorithm <eos> single object multiple object pose estimation linemod occlusion datasets approach substantially outperforms other recent cnn based approaches kehl <eos> rad lepetit when they all used without post processing <eos> during post processing pose refinement step used boost accuracy two method but fps less they much slower than method <eos> <eop> factoring shape pose layout image three dimensional scene <eos> goal paper take single image scene recover three dimensional structure terms small set factors layout representing enclosing surfaces well set object represented terms shape pose <eos> propose convolutional neural network based approach predict representation benchmark large dataset indoor scenes <eos> experiments evaluate number practical design questions demonstrate infer representation quantitatively qualitatively demonstrate its merits compared alternate representations <eos> <eop> monocular relative depth perception web stereo data supervision <eos> paper study problem monocular relative depth perception wild <eos> introduce simple yet effective method automatically generate dense relative depth annotations web stereo image propose new dataset consists diverse image well corresponding dense relative depth maps <eos> further improved ranking loss introduced deal imbalanced ordinal relations enforcing network focus set hard pairs <eos> experimental result demonstrate proposed approach only achieves state art accuracy relative depth perception wild but also benefits other dense per pixel prediction tasks <eos> metric depth estimation semantic segmentation <eos> <eop> spline error weighting robust visual inertial fusion <eos> paper derive test probability based weighting balance residuals different types spline fitting <eos> contrast previous formulations proposed spline error weighting scheme also incorporates prediction approximation error spline fit <eos> demonstrate effectiveness prediction synthetic experiment apply visual inertial fusion rolling shutter cameras <eos> result method estimate three dimensional structure metric scale generic first person video <eos> also propose quality measure spline fitting used automatically select knot spacing <eos> experiments verify obtained trajectory quality corresponds well requested quality <eos> finally linearly scaling weights show proposed spline error weighting minimizes estimation errors real sequences terms scale end point errors <eos> <eop> single image depth estimation based fourier domain analysis <eos> propose deep learning algorithm single image depth estimation based fourier frequency domain analysis <eos> first develop convolutional neural network structure propose new loss function called depth balanced euclidean loss train network reliably wide range depths <eos> then generate multiple depth map candidates cropping input image various cropping ratios <eos> general cropped image small ratio yields depth details more faithfully while large ratio provides overall depth distribution more reliably <eos> take advantage complementary properties combine multiple candidates frequency domain <eos> experimental result demonstrate proposed algorithm provides state art performance <eos> furthermore through frequency domain analysis validate efficacy proposed algorithm most frequency bands <eos> <eop> unsupervised learning monocular depth estimation visual odometry deep feature reconstruction <eos> despite learning based method showing promising result single view depth estimation visual odometry most existing approaches treat tasks supervised manner <eos> recent approaches single view depth estimation explore possibility learning without full supervision via minimizing photometric error <eos> paper explore use stereo sequences learning depth visual odometry <eos> use stereo sequences enables use both spatial between left right pairs temporal forward backward photometric warp error constrains scene depth camera motion common real world scale <eos> test time framework able estimate single view depth two view odometry monocular sequence <eos> also show how improve standard photometric warp loss considering warp deep feature <eos> show through extensive experiments jointly training single view depth visual odometry improves depth prediction because additional constraint imposed depths achieves competitive result visual odometry ii deep feature based warping loss improves upon simple photometric warp loss both single view depth estimation visual odometry <eos> method outperforms existing learning based method kitti driving dataset both tasks <eos> source code available github <eos> com huangying zhan depth vo feat <eos> <eop> detect track efficient pose estimation video <eos> paper addresses problem estimating tracking human body keypoints complex multi person video <eos> propose extremely lightweight yet highly effective approach builds upon latest advancements human detection video understanding <eos> method operates two stages keypoint estimation frames short clips followed lightweight tracking generate keypoint predictions linked over entire video <eos> frame level pose estimation experiment mask cnn well own proposed three dimensional extension model leverages temporal information over small clips generate more robust frame predictions <eos> conduct extensive ablative experiments newly released multi person video pose estimation benchmark posetrack validate various design choices model <eos> approach achieves accuracy <eos> test set using multi object tracking accuracy mota metric achieves state art performance iccv posetrack keypoint tracking challenge <eos> <eop> supervision registration unsupervised approach improve precision facial landmark detectors <eos> paper present supervision registration unsupervised approach improve precision facial landmark detectors both image video <eos> key observation detections same landmark adjacent frames should coherent registration <eos> interestingly coherency optical flow source supervision require manual labeling leveraged during detector training <eos> example enforce training loss function detected landmark frame followed optical flow tracking frame frame should coincide location detection frame <eos> essentially supervision registration augments training loss function registration loss thus training detector output only close annotations labeled image but also consistent registration large amounts unlabeled video <eos> end end training registration loss made possible differentiable lucas kanade operation computes optical flow registration forward pass back propagates gradients encourage temporal coherency detector <eos> output method more precise image based facial landmark detector applied single image video <eos> supervision registration demonstrate improvements facial landmark detection both image alfw video vw youtube celebrities significant reduction jittering video detections <eos> <eop> diversity regularized spatiotemporal attention video based person re identification <eos> video based person re identification matches video clips people across non overlapping cameras <eos> most existing method tackle problem encoding each video frame its entirety computing aggregate representation across all frames <eos> practice people often partially occluded corrupt extracted feature <eos> instead propose new spatiotemporal attention model automatically discovers diverse set distinctive body parts <eos> allows useful information extracted all frames without succumbing occlusions misalignments <eos> network learns multiple spatial attention models employs diversity regularization term ensure multiple models discover same body part <eos> feature extracted local image region organized spatial attention model combined using temporal attention <eos> result network learns latent representations face torso other body parts using best available image patches entire video sequence <eos> extensive evaluations three datasets show framework outperforms state art approaches large margins multiple metrics <eos> <eop> style aggregated network facial landmark detection <eos> recent advances facial landmark detection achieve success learning discriminative feature rich deformation face shapes poses <eos> besides variance faces themselves intrinsic variance image styles <eos> color image light vs <eos> dark intense vs <eos> dull so constantly overlooked <eos> issue becomes inevitable increasing web image collected various sources training neural network <eos> work propose style aggregated approach deal large intrinsic variance image styles facial landmark detection <eos> method transforms original face image style aggregated image generative adversarial module <eos> proposed scheme uses style aggregated image maintain face image more robust environmental changes <eos> then original face image accompanying style aggregated ones play duet train landmark detector complementary each other <eos> way each face method takes two image input <eos> one its original style other aggregated style <eos> experiments observe large variance image styles would degenerate performance facial landmark detectors <eos> moreover show robustness method large variance image styles comparing variant approach generative adversarial module removed no style aggregated image used <eos> approach demonstrated perform well when compared state art algorithms benchmark datasets aflw <eos> code publicly available github github <eos> com san <eop> learning deep models face anti spoofing binary auxiliary supervision <eos> face anti spoofing crucial prevent face recognition systems security breach <eos> previous deep learning approaches formulate face anti spoofing binary classification problem <eos> many them struggle grasp adequate spoofing cues generalize poorly <eos> paper argue importance auxiliary supervision guide learning toward discriminative generalizable cues <eos> cnn rnn model learned estimate face depth pixel wise supervision estimate rppg signals sequence wise supervision <eos> estimated depth rppg fused distinguish live vs <eos> further introduce new face anti spoofing database covers large range illumination subject pose variations <eos> experiments show model achieves state art result both intra cross database testing <eos> <eop> deep cost sensitive order preserving feature learning cross population age estimation <eos> facial age estimation face image important yet very challenging task computer vision since humans different races genders exhibit quite different patterns their facial aging processes <eos> deal influence race gender previous method perform age estimation within each population separately <eos> practice however often very difficult collect label sufficient data each population <eos> therefore would helpful exploit existing large labeled dataset one source population improve age estimation performance another target population only small labeled dataset available <eos> work propose deep cross population dcp age estimation model achieve goal <eos> particular dcp model develops two stage training strategy <eos> first novel cost sensitive multi task loss function designed learn transferable aging feature training source population <eos> second novel order preserving pair wise loss function designed align aging feature two populations <eos> doing so dcp model transfer knowledge encoded source population target population <eos> extensive experiments two largest benchmark datasets show dcp model outperforms several strong baseline method many state art method <eos> <eop> first person hand action benchmark rgb video three dimensional hand pose annotations <eos> work study use three dimensional hand poses recognize first person dynamic hand actions interacting three dimensional object <eos> towards goal collected rgb video sequences comprised more than frames daily hand action categories involving different object several hand configurations <eos> obtain hand pose annotations used own mo cap system automatically infers three dimensional location each joints hand model via magnetic sensors inverse kinematics <eos> additionally recorded object poses provide three dimensional object models subset hand object interaction sequences <eos> best knowledge first benchmark enables study first person hand actions use three dimensional hand poses <eos> present extensive experimental evaluation rgb pose based action recognition baselines state art approaches <eos> impact using appearance feature poses their combinations measured different training testing protocols evaluated <eos> finally assess how ready three dimensional hand pose estimation field when hands severely occluded object egocentric views its influence action recognition <eos> result see clear benefits using hand pose cue action recognition compared other data modalities <eos> dataset experiments interest communities three dimensional hand pose estimation object pose robotics well action recognition <eos> <eop> pose sensitive embedding person re identification expanded cross neighborhood re ranking <eos> person re identification challenging retrieval task requires matching person acquired image across non overlapping camera views <eos> paper propose effective approach incorporates both fine coarse pose information person learn discrim inative embedding <eos> contrast recent direction explicitly modeling body parts correcting misalignment based show rather straightforward inclusion acquired camera view detected joint locations into convolutional neural network helps learn very effective representation <eos> increase retrieval performance re ranking techniques based computed distances recently gained much attention <eos> propose new unsupervised automatic re ranking framework achieves state art re ranking performance <eos> show contrast current state art re ranking method approach require compute new rank lists each image pair <eos> based reciprocal neighbors performs well using simple direct rank list based comparison even just using already computed euclidean distances between image <eos> show both learned representation re ranking method achieve state art performance number challenging surveillance image video datasets <eos> <eop> disentangling three dimensional pose dendritic cnn unconstrained face alignment <eos> heatmap regression used landmark localization quite while now <eos> most method use very deep stack bottleneck modules heatmap classification stage followed heatmap regression extract keypoints <eos> paper present single dendritic cnn termed pose conditioned dendritic convolution neural network pcd cnn classification network followed second modular classification network trained end end fashion obtain accurate landmark point <eos> following bayesian formulation disentangle three dimensional pose face image explicitly conditioning landmark estimation pose making different multi tasking approaches <eos> extensive experimentation shows conditioning pose reduces localization error making agnostic face pose <eos> proposed model extended yield variable number landmark point hence broadening its applicability other datasets <eos> instead increasing depth width network train cnn efficiently mask softmax loss hard sample mining achieve upto reduction error compared state art method extreme medium pose face image challenging datasets including aflw afw cofw ibug <eos> <eop> hierarchical generative model eye image synthesis eye gaze estimation <eos> work introduce hierarchical generative model hgm enable realistic forward eye image synthe sis well effective backward eye gaze estimation <eos> proposed hgm consists hierarchical generative shape model hgsm conditional bidirectional generative adversarial network bigan <eos> hgsm encodes eye ge ometry knowledge relates eye gaze eye shape while bigan leverages big data captures dependency between eye shape eye appearance <eos> intermedi ate component eye shape connects knowledge based model hgsm data driven model bigan enables bidirectional inference <eos> through top down inference hgm synthesize eye image consistent given eye gaze <eos> through bottom up inference hgm infer eye gaze effectively given eye image <eos> qualitative quantitative evaluations benchmark datasets demonstrate model effectiveness both eye image synthesis eye gaze estimation <eos> addition proposed model restricted eye image only <eos> adapted face image any shape appearance related fields <eos> <eop> mict mixed three dimensional convolutional tube human action recognition <eos> human actions video three dimensional three dimensional signals <eos> recent attempts use three dimensional convolutional neural network cnn explore spatio temporal information human action recognition <eos> though promising three dimensional cnn achieved high performanceon task respect their well established two dimensional counterparts visual recognition still image <eos> argue high training complexity spatio temporal fusion huge memory cost three dimensional convolution hinder current three dimensional cnn stack three dimensional convolutions layer layer outputting deeper feature maps crucial high level tasks <eos> thus propose mixed convolutional tube mict integrates cnn three dimensional convolution module generate deeper more informative feature maps while reducing training complexity each round spatio temporal fusion <eos> new end end trainable deep three dimensional network mict net also proposed based mict better explore spatio temporal information human actions <eos> evaluations three well known benchmark datasets ucf sport hmdb show proposed mict net significantly outperforms original three dimensional cnn <eos> compared state art approaches action recognition ucf hmdb mict net yields best performance <eos> <eop> learning estimate three dimensional human pose shape single color image <eos> work addresses problem estimating full body three dimensional human pose shape single color image <eos> task iterative optimization based solutions typically prevailed while convolutional network convnets suffered because lack training data their low resolution three dimensional predictions <eos> work aims bridge gap proposes efficient effective direct prediction method based convnets <eos> central part approach incorporation parametric statistical body shape model smpl within end end framework <eos> allows get very detailed three dimensional mesh result while requiring estimation only small number parameters making friendly direct network prediction <eos> interestingly demonstrate parameters predicted reliably only keypoints masks <eos> typical outputs generic human analysis convnets allowing relax massive requirement image three dimensional shape ground truth available training <eos> simultaneously maintaining differentiability training time generate three dimensional mesh estimated parameters optimize explicitly surface using three dimensional per vertex loss <eos> finally differentiable renderer employed project three dimensional mesh image enables further refinement network optimizing consistency projection annotations <eos> proposed approach outperforms previous baselines task offers attractive solution direct prediction three dimensional shape single color image <eos> <eop> glimpse clouds human activity recognition unstructured feature point <eos> propose method human activity recognition rgb data rely any pose information during test time explicitly calculate pose information internally <eos> instead visual attention module learns predict glimpse sequences each frame <eos> glimpses correspond interest point scene relevant classified activities <eos> no spatial coherence forced glimpse locations gives attention module liberty explore different point each frame better optimize process scrutinizing visual information <eos> tracking sequentially integrating kind unstructured data challenge address sep arating set glimpses set recurrent tracking recognition workers <eos> workers receive glimpses jointly performing subsequent motion tracking activity prediction <eos> glimpses soft assigned workers optimizing coherence assignments space time feature space using external memory module <eos> no hard decisions taken <eos> each glimpse point assigned all existing workers albeit different importance <eos> method outperform state art largest human activity recognition dataset available date ntu rgb northwestern ucla multiview action three dimensional dataset <eos> <eop> context aware deep feature compression high speed visual tracking <eos> propose new context aware correlation filter based tracking framework achieve both high computational speed state art performance among real time trackers <eos> major contribution high computational speed lies proposed deep feature compression achieved context aware scheme utilizing multiple expert auto encoders context framework refers coarse category tracking target according appearance patterns <eos> pre training phase one expert auto encoder trained per category <eos> tracking phase best expert auto encoder selected given target only auto encoder used <eos> achieve high tracking performance compressed feature map introduce extrinsic denoising processes new orthogonality loss term pre training fine tuning expert auto encoders <eos> validate proposed context aware framework through number experiments method achieves comparable performance state art trackers cannot run real time while running significantly fast speed over fps <eos> <eop> correlation tracking via joint discrimination reliability learning <eos> visual tracking ideal filter learned correlation filter cf method should take both discrimination reliability information <eos> however existing attempts usually focus former one while pay less attention reliability learning <eos> may make learned filter dominated unexpected salient region feature map thereby resulting model degradation <eos> address issue propose novel cf based optimization problem jointly model discrimination reliability information <eos> first treat filter element wise product base filter reliability term <eos> base filter aimed learn discrimination information between target backgrounds reliability term encourages final filter focus more reliable region <eos> second introduce local response consistency regular term emphasize equal contributions different region avoid tracker being dominated unreliable region <eos> proposed optimization problem solved using alternating direction method speeded up fourier domain <eos> conduct extensive experiments otb otb vot datasets evaluate proposed tracker <eos> experimental result show tracker performs favorably against other state art trackers <eos> <eop> phasenet video frame interpolation <eos> most approaches video frame interpolation require accurate dense correspondences synthesize between frame <eos> therefore they perform well challenging scenarios <eos> lighting changes motion blur <eos> recent deep learning approaches rely kernels represent motion only alleviate problems some extent <eos> cases method use per pixel phase based motion representation shown work well <eos> however they only applicable limited amount motion <eos> propose new approach phasenet designed robustly handle challenging scenarios while also coping larger motion <eos> approach consists neural network decoder directly estimates phase decomposition intermediate frame <eos> show superior hand crafted heuristics previously used phase based method also compares favorably recent deep learning based approaches video frame interpolation challenging datasets <eos> <eop> best both worlds combining cnn geometric constraints hierarchical motion segmentation <eos> traditional method motion segmentation use powerful geometric constraints understand motion but fail leverage semantics high level image understanding <eos> modern cnn method motion analysis other hand excel identifying well known structures but may precisely characterize well known geometric constraints <eos> work build new statistical model rigid motion flow based classical perspective projection constraints <eos> then combine piecewise rigid motions into complex deformable articulated object guided semantic segmentation cnn second object level statistical model <eos> combination classical geometric knowledge combined pattern recognition abilities cnn yields excellent performance wide range motion segmentation benchmarks complex geometric scenes camouflaged animals <eos> <eop> hyperparameter optimization tracking continuous deep learning <eos> hyperparameters numerical presets whose values assigned prior commencement learning process <eos> selecting appropriate hyperparameters critical accuracy tracking algorithms yet difficult determine their optimal values particular adaptive ones each specific video sequence <eos> most hyperparameter optimization algorithms depend searching generic range they imposed blindly all sequences <eos> here propose novel hyperparameter optimization method find optimal hyperparameters given sequence using action prediction network leveraged continuous deep learning <eos> since common state spaces object tracking tasks significantly more complex than ones traditional control problems existing continuous deep learning algorithms cannot directly applied <eos> overcome challenge introduce efficient heuristic accelerate convergence behavior <eos> evaluate method several tracking benchmarks demonstrate its superior performance <eos> <eop> scale transferrable object detection <eos> scale problem lies heart object detection <eos> work develop novel scale transferrable detection network stdn detecting multi scale object image <eos> contrast previous method simply combine object predictions multiple feature maps different network depths proposed network equipped embedded super resolution layer named scale transfer layer module work explicitly explore inter scale consistency nature across multiple detection scales <eos> scale transfer module naturally fits base network little computational cost <eos> module further integrated dense convolutional network densenet yield one stage object detector <eos> evaluate proposed architecture pascal voc ms coco benchmark tasks stdn obtains significant improvements over comparable state art detection models <eos> <eop> prior less method multi face tracking unconstrained video <eos> paper presents prior less method tracking clustering unknown number human faces maintaining their individual identities unconstrained video <eos> key challenge accurately track faces partial occlusion drastic appearance changes multiple shots resulting significant variations makeup facial expression head pose illumination <eos> address challenge propose new multi face tracking re identification algorithm provides high accuracy face association entire video automatic cluster number generation robust outliers <eos> develop co occurrence model multiple body parts seamlessly create face tracklets recursively link tracklets construct graph extracting clusters <eos> gaussian process model introduced compensate deep feature insufficiency further used refine linking result <eos> advantages proposed algorithm demonstrated using variety challenging music video newly introduced body worn camera video <eos> proposed method obtains significant improvements over state art while relying less handling video specific prior information achieve high performance <eos> <eop> end end flow correlation tracking spatial temporal attention <eos> discriminative correlation filters dcf deep convolutional feature achieved favorable performance recent tracking benchmarks <eos> however most existing dcf trackers only consider appearance feature current frame hardly benefit motion inter frame information <eos> lack temporal information degrades tracking performance during challenges such partial occlusion deformation <eos> paper propose flowtrack focuses making use rich flow information consecutive frames improve feature representation tracking accuracy <eos> flowtrack formulates individual components including optical flow estimation feature extraction aggregation correlation filters tracking special layer network <eos> best knowledge first work jointly train flow tracking task deep learning framework <eos> then historical feature maps predefined intervals warped aggregated current ones guiding flow <eos> adaptive aggregation propose novel spatial temporal attention mechanism <eos> experiments proposed method achieves leading performance otb otb vot vot <eos> <eop> deep texture manifold ground terrain recognition <eos> present texture network called deep encoding pooling network dep task ground terrain recognition <eos> recognition ground terrain important task establishing robot vehicular control parameters well localization within outdoor environment <eos> architecture dep integrates orderless texture details local spatial information performance dep surpasses state art method task <eos> gtos database comprised over image classes ground terrain outdoor scenes enables supervised recognition <eos> evaluation under realistic conditions use test image existing gtos dataset but instead hand held mobile phone video similar terrain <eos> new evaluation dataset gtos mobile consists video classes ground terrain such grass gravel asphalt sand <eos> resultant network shows excellent performance only gtos mobile but also more general databases minc dtd <eos> leveraging discriminant feature learned network build new texture manifold called dep manifold <eos> learn parametric distribution feature space fully supervised manner gives distance relationship among classes provides means implicitly represent ambiguous class boundaries <eos> source code database publicly available <eos> <eop> learning superpixels segmentation aware affinity loss <eos> superpixel segmentation widely used many computer vision tasks <eos> existing superpixel algorithms mainly based hand crafted feature often fail preserve weak object boundaries <eos> work leverage deep neural network facilitate extracting superpixels image <eos> show simple integration deep feature existing superpixel algorithms result better performance feature model segmentation <eos> instead propose segmentation aware affinity learning approach superpixel segmentation <eos> specifically propose new loss function takes segmentation error into account affinity learning <eos> also develop pixel affinity net affinity prediction <eos> extensive experimental result show proposed algorithm based learned segmentation aware loss performs favorably against state art method <eos> also demonstrate use learned superpixels numerous vision applications consistent improvements <eos> <eop> interactive image segmentation latent diversity <eos> interactive image segmentation characterized multimodality <eos> when user clicks door they intend select door whole house present end end learning approach interactive image segmentation tackles ambiguity <eos> architecture couples two convolutional network <eos> first trained synthesize diverse set plausible segmentations conform user input <eos> second trained select among <eos> selecting single solution approach retains compatibility existing interactive segmentation interfaces <eos> synthesizing multiple diverse solutions before selecting one architecture given representational power explore multimodal solution space <eos> show proposed approach outperforms existing method interactive image segmentation including prior work applied convolutional network problem while being much faster <eos> <eop> unreasonable effectiveness deep feature perceptual metric <eos> while nearly effortless humans quickly assess perceptual similarity between two image underlying processes thought quite complex <eos> despite most widely used perceptual metrics today such psnr ssim simple shallow functions fail account many nuances human perception <eos> recently deep learning community found feature vgg network trained imagenet classification remarkably useful training loss image synthesis <eos> but how perceptual so called perceptual losses elements critical their success answer questions introduce new dataset human perceptual similarity judgments <eos> systematically evaluate deep feature across different architectures tasks compare them classic metrics <eos> find deep feature outperform all previous metrics large margins dataset <eos> more surprisingly result restricted imagenet trained vgg feature but holds across different deep architectures levels supervision supervised self supervised even unsupervised <eos> result suggest perceptual similarity emergent property shared across deep visual representations <eos> <eop> local descriptors optimized average precision <eos> extraction local feature descriptors vital stage solution pipelines numerous computer vision tasks <eos> learning based approaches improve performance certain tasks but still cannot replace handcrafted feature general <eos> paper improve learning local feature descriptors optimizing performance descriptor matching common stage follows descriptor extraction local feature based pipelines formulated nearest neighbor retrieval <eos> specifically directly optimize ranking based retrieval performance metric average precision using deep neural network <eos> general purpose solution also viewed listwise learning rank approach advantageous compared recent local ranking approaches <eos> standard benchmarks descriptors learned formulation achieve state art result patch verification patch retrieval image matching <eos> <eop> recovering realistic texture image super resolution deep spatial feature transform <eos> despite convolutional neural network cnn recently demonstrated high quality reconstruction single image super resolution sr recovering natural realistic texture remains challenging problem <eos> paper show possible recover textures faithful semantic classes <eos> particular only need modulate feature few intermediate layer single network conditioned semantic segmentation probability maps <eos> made possible through novel spatial feature transform sft layer generates affine transformation parameters spatial wise feature modulation <eos> sft layer trained end end together sr network using same loss function <eos> during testing accepts input image arbitrary size generates high resolution image just single forward pass conditioned categorical priors <eos> final result show sr network equipped sft generate more realistic visually pleasing textures comparison state art srgan enhancenet <eos> <eop> deep extreme cut extreme point object segmentation <eos> paper explores use extreme point object left most right most top bottom pixels input obtain precise object segmentation image video <eos> so adding extra channel image input convolutional neural network cnn contains gaussian centered each extreme point <eos> cnn learns transform information into segmentation object matches extreme point <eos> demonstrate usefulness approach guided segmentation grabcut style interactive segmentation video object segmentation dense segmentation annotation <eos> show obtain most precise result date also less user input extensive varied selection benchmarks datasets <eos> all models code publicly available www <eos> ch cvlsegmentation dextr <eos> <eop> learning parse wireframes image man made environments <eos> paper propose learning based approach task automatically extracting wireframe representation image cluttered man made environments <eos> wireframe contains all salient straight lines their junctions scene encode efficiently accurately large scale geometry object shapes <eos> end built very large new dataset over image wireframes thoroughly labelled humans <eos> proposed two convolutional neural network suitable extracting junctions lines large spatial support respectively <eos> network trained dataset achieved significantly better performance than state art method junction detection line segment detection respectively <eos> conducted extensive experiments evaluate quantitatively qualitatively wireframes obtained method convincingly shown effectively efficiently parsing wireframes image man made environments feasible goal within reach <eos> such wireframes could benefit many important visual tasks such feature correspondence three dimensional reconstruction vision based mapping localization navigation <eos> <eop> occlusion aware rolling shutter rectification three dimensional scenes <eos> vast majority contemporary cameras employ rolling shutter rs mechanism capture image <eos> due sequential mechanism image acquired moving camera subjected rolling shutter effect manifests geometric distortions <eos> work consider specific scenario fast moving camera wherein rolling shutter distortions only predominant but also become depth dependent turn result intra frame occlusions <eos> end develop first its kind pipeline recover latent image three dimensional scene set such rs distorted image <eos> proposed approach sequentially recovers both camera motion scene structure while accounting rs occlusion effects <eos> subsequently perform depth occlusion aware rectification rs image yield desired latent image <eos> experiments synthetic real image sequences reveal proposed approach achieves state art result <eos> <eop> content sensitive supervoxels via uniform tessellations video manifolds <eos> supervoxels perceptually meaningful atomic region video obtained grouping voxels exhibit coherence both appearance motion <eos> paper propose content sensitive supervoxels css regularly shaped three dimensional primitive volumes possess following characteristic they typically larger longer content sparse region <eos> homogeneous appearance motion smaller shorter content dense region <eos> high variation appearance motion <eos> compute css map video dimensional manifold embedded whose volume elements give good measure content density <eos> propose efficient lloyd like method splitting merging scheme compute uniform tessellation induces css <eos> theoretically method good competitive ratio <eos> also present simple extension css stream css processing long video cannot loaded into main memory once <eos> evaluate css stream css seven representative supervoxel method four video datasets <eos> result show method outperforms existing supervoxel method <eos> <eop> intrinsic image transformation via scale space decomposition <eos> introduce new network structure decomposing image into its intrinsic albedo shading <eos> treat image image transformation problem explore scale space input output <eos> expanding output image albedo shading into their laplacian pyramid components develop multi channel network structure learns image image transformation function successive frequency bands parallel within each channel fully convolutional neural network skip connections <eos> network structure general extensible demonstrated excellent performance intrinsic image decomposition problem <eos> evaluate network two benchmark datasets mpi sintel dataset mit intrinsic image dataset <eos> both quantitative qualitative result show model delivers clear progression over state art <eos> <eop> learned shape tailored descriptors segmentation <eos> address problem texture segmentation grouping dense pixel wise descriptors <eos> introduce construct learned shape tailored descriptors aggregate image statistics only within region interest avoid mixing statistics different textures invariant complex nuisances <eos> illumination perspective deformations <eos> accomplished training neural network discriminate base shape tailored descriptors oriented gradients various scales <eos> descriptors defined through partial differential equations obtain data various scales arbitrarily shaped region <eos> formulate optimize joint optimization problem segmentation descriptors discriminate base descriptors using learned metric equivalent grouping learned descriptors <eos> test method datasets illustrate effect both shape tailored learned properties descriptors <eos> experiments show descriptors learned small dataset segmented image generalize well unseen textures other datasets showing generic nature descriptors <eos> show stateof art result texture segmentation benchmarks <eos> <eop> pad net multi tasks guided prediction distillation network simultaneous depth estimation scene parsing <eos> depth estimation scene parsing two particularly important tasks visual scene understanding <eos> paper tackle problem simultaneous depth estimation scene parsing joint cnn <eos> task typically treated deep multi task learning problem <eos> different previous method directly optimizing multiple tasks given input training data paper proposes novel multi task guided prediction distillation network pad net first predicts set intermediate auxiliary tasks ranging low level high level then predictions intermediate auxiliary tasks utilized multi modal input via proposed multi modal distillation modules final tasks <eos> during joint learning intermediate tasks only act supervision learning more robust deep representations but also pro vide rich multi modal information improving final tasks <eos> extensive experiments conducted two challenging datasets <eos> nyud cityscapes both depth estimation scene parsing tasks demonstrating effectiveness proposed approach <eos> <eop> multi image semantic matching mining consistent feature <eos> work proposes multi image matching method estimate semantic correspondences across multiple image <eos> contrast previous method optimize all pairwise correspondences proposed method identifies matches only sparse set reliable feature image collection <eos> way proposed method able prune nonrepeatable feature also highly scalable handle thousands image <eos> additionally propose low rank constraint ensure geometric consistency feature correspondences over whole image collection <eos> besides competitive performance multi graph matching semantic flow benchmarks also demonstrate applicability proposed method reconstructing object class models discovering object class landmarks image without using any annotation <eos> <eop> density aware single image de raining using multi stream dense network <eos> single image rain streak removal extremely challenging problem due presence non uniform rain densities image <eos> present novel density aware multi stream densely connected convolutional neural network based algorithm called did mdn joint rain density estimation de raining <eos> proposed method enables network itself automatically determine rain density information then efficiently remove corresponding rain streaks guided estimated rain density label <eos> better characterize rain streaks different scales shapes multi stream densely connected de raining network proposed efficiently leverages feature different scales <eos> furthermore new dataset containing image rain density labels created used train proposed density aware network <eos> extensive experiments synthetic real datasets demonstrate proposed method achieves significant improvements over recent state art method <eos> addition ablation study performed demonstrate improvements obtained different modules proposed method <eos> <eop> joint cuts matching partitions one graph <eos> two fundamental problems graph cuts graph matching intensively investigated over decades resulting vast literature two topics respectively <eos> however way jointly applying solving graph cuts matching receives few attention <eos> paper first formalize problem simultaneously cutting graph into two partitions <eos> graph cuts establishing their correspondence <eos> then develop optimization algorithm updating matching cutting alternatively provided theoretical analysis <eos> efficacy algorithm verified both synthetic dataset real world image containing similar region structures <eos> <eop> progressive attention guided recurrent network salient object detection <eos> effective convolutional feature play important role saliency estimation but how learn powerful feature saliency still challenging task <eos> fcn based method directly apply multi level convolutional feature without distinction leads sub optimal result due distraction redundant details <eos> paper propose novel attention guided network selectively integrates multi level contextual information progressive manner <eos> attentive feature generated network alleviate distraction background thus achieve better performance <eos> other hand observed most existing algorithms conduct salient object detection exploiting side output feature backbone feature extraction network <eos> however shallower layer backbone network lack ability obtain global semantic information limits effective feature learning <eos> address problem introduce multi path recurrent feedback enhance proposed progressive attention driven framework <eos> through multi path recurrent connections global semantic information top convolutional layer transferred shallower layer intrinsically refines entire network <eos> experimental result six benchmark datasets demonstrate algorithm performs favorably against state art approaches <eos> <eop> fast accurate single image super resolution via information distillation network <eos> recently deep convolutional neural network cnn demonstrated remarkable progress single image super resolution <eos> however depth width network increase cnn based super resolution method faced challenges computational complexity memory consumption practice <eos> order solve above questions propose deep but compact convolutional network directly reconstruct high resolution image original low resolution image <eos> general proposed model consists three parts feature extraction block stacked information distillation blocks reconstruction block respectively <eos> combining enhancement unit compression unit into distillation block local long short path feature effectively extracted <eos> specifically proposed enhancement unit mixes together two different types feature compression unit distills more useful information sequential blocks <eos> addition proposed network advantage fast execution due comparatively few number filters per layer use group convolution <eos> experimental result demonstrate proposed method superior state art method especially terms time performance <eos> <eop> hallucinated iqa no reference image quality assessment via adversarial learning <eos> no reference image quality assessment nr iqa fundamental yet challenging task low level computer vision community <eos> difficulty particularly pronounced limited information corresponding reference comparison typically absent <eos> although various feature extraction mechanisms leveraged natural scene statistics deep neural network previous method performance bottleneck still exists <eos> work propose hallucination guided quality regression network address issue <eos> firstly generate hallucinated reference constrained distorted image compensate absence true reference <eos> then pair information hallucinated reference distorted image forward them regressor learn perceptual discrepancy guidance implicit ranking relationship within generator therefore produce precise quality prediction <eos> demonstrate effectiveness approach comprehensive experiments evaluated four popular image quality assessment benchmarks <eos> method significantly outperforms all previous state art method large margins <eos> code model publicly available project page kwanyeelin <eos> io projects hiqa hiqa <eos> html <eop> nag network adversary generation <eos> adversarial perturbations pose serious threat deploying machine learning systems <eos> recent works shown existence image agnostic perturbations fool classifiers over most natural image <eos> existing method present optimization approaches solve fooling objective imperceptibility constraint craft perturbations <eos> however given classifier they generate one perturbation time single instance manifold adversarial perturbations <eos> also order build robust models essential explore manifold adversarial perturbations <eos> paper propose first time generative approach model distribution adversarial perturbations <eos> architecture proposed model inspired gans trained using fooling diversity objectives <eos> trained generator network attempts capture distribution adversarial perturbations given classifier readily generates wide variety such perturbations <eos> experimental evaluation demonstrates perturbations crafted model achieve state art fooling rates ii exhibit wide variety iii deliver excellent cross model generalizability <eos> work deemed important step process inferring about complex manifolds adversarial perturbations <eos> <eop> dynamic structured semantic propagation network <eos> semantic concept hierarchy yet under explored semantic segmentation due inefficiency complicated optimization incorporating structural inference into dense prediction <eos> lack modeling dependencies among concepts severely limits generalization capability segmentation models open set large scale vocabularies <eos> prior works thus must tune highly specified models each task due label discrepancy across datasets <eos> paper propose dynamic structured semantic propagation network dsspn builds semantic neuron graph explicitly incorporate concept hierarchy into dynamic network construction leading interpretable reasoning process <eos> each neuron one super class eg food represents instantiated module recognizing among fine grained child concepts eg editable fruit pizza then its learned feature flow into child neurons eg distinguishing between orange apple hierarchical categorization finer levels <eos> dense semantic enhanced neural block propagates learned knowledge all ancestral neurons into each fine grained child neuron progressive feature evolving <eos> during training dsspn performs dynamic structured neuron computational graph only activating sub graph neurons each image <eos> another merit such semantic explainable structure ability learn unified model concurrently diverse datasets selectively activating different neuron sub graphs each annotation each step <eos> extensive experiments four public semantic segmentation datasets <eos> ade coco stuff cityscape mapillary demonstrate superiority dsspn universal segmentation model jointly trained diverse datasets surpass common fine tuning scheme exploiting multi domain knowledge <eos> <eop> cross domain self supervised multi task feature learning using synthetic imagery <eos> human learning common use multiple sources information jointly <eos> however most existing feature learning approaches learn only single task <eos> paper propose novel multi task deep network learn generalizable high level visual representations <eos> since multi task learning requires annotations multiple properties same training instance look synthetic image train network <eos> overcome domain difference between real synthetic data employ unsupervised feature space domain adaptation method based adversarial learning <eos> given input synthetic rgb image network simultaneously predicts its surface normal depth instance contour while also minimizing feature space domain differences between real synthetic data <eos> through extensive experiments demonstrate network learns more transferable representations compared single task baselines <eos> learned representation produces state art transfer learning result pascal voc classification detection <eos> <eop> two step disentanglement method <eos> address problem disentanglement factors generate given data into correlated labeling <eos> solution simpler than previous solutions employs adversarial training <eos> first part data correlated labels extracted training classifier <eos> then other part extracted such enables reconstruction original data but contain label information <eos> utility new method demonstrated visual datasets well financial data <eos> code available github <eos> com naamahadad two step disentanglement method <eos> <eop> robust facial landmark detection via fully convolutional local global context network <eos> while fully convolutional neural network very strong modeling local feature they fail aggregate global context due their constrained receptive field <eos> modern method typically address lack global context introducing cascades pooling fitting statistical model <eos> work propose new approach introduces global context into fully convolutional neural network directly <eos> key concept implicit kernel convolution within network <eos> kernel convolution blurs output local context subnet then refined global context subnet using dilated convolutions <eos> kernel convolution crucial convergence network because smoothens gradients reduces overfitting <eos> postprocessing step simple pca based shape model fitted network output order filter outliers <eos> experiments demonstrate effectiveness approach outperforming several state art method facial landmark detection <eos> <eop> decorrelated batch normalization <eos> batch normalization bn capable accelerating training deep models centering scaling activations within mini batches <eos> work propose decorrelated batch normalization dbn just centers scales activations but whitens them <eos> explore multiple whitening techniques find pca whitening causes problem call stochastic axis swapping detrimental learning <eos> show zca whitening suffer problem permitting successful learning <eos> dbn retains desirable qualities bn further improves bn optimization efficiency generalization ability <eos> design comprehensive experiments show dbn improve performance bn multilayer perceptrons convolutional neural network <eos> furthermore consistently improve accuracy residual network cifar cifar imagenet <eos> <eop> learning sketch shortcut cycle consistency <eos> see sketch free hand sketching naturally builds ties between human machine vision <eos> paper present novel approach translating object photo sketch mimicking human sketching process <eos> extremely challenging task because photo sketch domains differ significantly <eos> furthermore human sketches exhibit various levels sophistication abstraction even when depicting same object instance reference photo <eos> means even if photo sketch pairs available they only provide weak supervision signal learn translation model <eos> compared existing supervised approaches solve problem photo sketch denote encoder decoder respectively take advantage inverse problem <eos> sketch photo combine unsupervised learning tasks within domain reconstruction all within multi task learning framework <eos> compared existing unsupervised approaches based cycle consistency <eos> photo photo introduce shortcut consistency enforced encoder bottleneck <eos> photo photo exploit additional self supervision <eos> both qualitative quantitative result show proposed model superior number state art alternatives <eos> also show synthetic sketches used train better fine grained sketch based image retrieval fg sbir model effectively alleviating problem sketch data scarcity <eos> <eop> towards mathematical understanding difficulty learning feedforward neural network <eos> training deep neural network solving machine learning problems one great challenge field mainly due its associated optimisation problem being highly non convex <eos> recent developments suggested many training algorithms suffer undesired local minima under certain scenario consequently led great efforts pursuing mathematical explanations such observations <eos> work provides alternative mathematical understanding challenge smooth optimisation perspective <eos> assuming exact learning finite sample sufficient conditions identified via critical point analysis ensure any local minimum globally minimal well <eos> furthermore state art algorithm known generalised gauss newton ggn algorithm rigorously revisited approximate newton algorithm shares property being locally quadratically convergent global minimum under condition exact learning <eos> <eop> faceid gan learning symmetry three player gan identity preserving face synthesis <eos> face synthesis achieved advanced development using generative adversarial network gans <eos> existing method typically formulate gan two player game discriminator distinguishes face image real synthesized domains while generator reduces its discriminativeness synthesizing face photo realistic quality <eos> their competition converges when discriminator unable differentiate two domains <eos> unlike two player gans work generates identity preserving faces proposing faceid gan treats classifier face identity third player competing generator distinguishing identities real synthesized faces see fig <eos> stationary point reached when generator produces faces high quality well preserve identity <eos> instead simply modeling identity classifier additional discriminator faceid gan formulated satisfying information symmetry ensures real synthesized image projected into same feature space <eos> other words identity classifier used extract identity feature both input real output synthesized face image generator substantially alleviating training difficulty gan <eos> extensive experiments show faceid gan able generate faces arbitrary viewpoint while preserve identity outperforming recent advanced approaches <eos> <eop> constrained deep neural network ordinal regression <eos> ordinal regression supervised learning problem aiming classify instances into ordinal categories <eos> challenging automatically extract high level feature representing intraclass information interclass ordinal relationship simultaneously <eos> paper proposes constrained optimization formulation ordinal regression problem minimizes negative loglikelihood multiple categories constrained order relationship between instances <eos> mathematically equivalent unconstrained formulation pairwise regularizer <eos> implementation based cnn framework proposed solve problem such high level feature extracted automatically optimal solution learned through traditional back propagation method <eos> proposed pairwise constraints make algorithm work even small datasets proposed efficient implementation make scalable large datasets <eos> experimental result four real world benchmarks demonstrate proposed algorithm outperforms traditional deep learning approaches other state art approaches based hand crafted feature <eos> <eop> modulated convolutional network <eos> despite great effectiveness very deep wide convolutional neural network cnn various computer vision tasks significant cost terms storage requirement such network impedes deployment computationally limited devices <eos> paper propose new modulated convolutional network mcns improve portability cnn via binarized filters <eos> mcns propose new loss function considers filter loss center loss softmax loss end end framework <eos> first introduce modulation filters filters recover unbinarized filters leads new architecture calculate network model <eos> convolution operation further approximated considering intra class compactness loss function <eos> result mcns reduce size required storage space convolutional filters factor contrast full precision model while achieving much better performances than state art binarized models <eos> most importantly mcns achieve comparable performance full precision resnets wide resnets <eos> code will available publicly soon <eos> <eop> learning steerable filters rotation equivariant cnn <eos> many machine learning tasks desirable model prediction transforms equivariant way under transformations its input <eos> convolutional neural network cnn implement translational equivariance construction other transformations however they compelled learn proper mapping <eos> work develop steerable filter cnn sfcnns achieve joint equivariance under translations rotations design <eos> proposed architecture employs steerable filters efficiently compute orientation dependent responses many orientations without suffering interpolation artifacts filter rotation <eos> utilize group convolutions guarantee equivariant mapping <eos> addition generalize he weight initialization scheme filters defined linear combination system atomic filters <eos> numerical experiments show substantial enhancement sample complexity growing number sampled filter orientations confirm network generalizes learned patterns over orientations <eos> proposed approach achieves state art rotated mnist benchmark isbi em segmentation challenge <eos> <eop> efficient interactive annotation segmentation datasets polygon rnn <eos> manually labeling datasets object masks extremely time consuming <eos> work follow idea polygon rnn produce polygonal annotations object interactively using humans loop <eos> introduce several important improvements model design new cnn encoder architecture show how effectively train model reinforcement learning significantly increase output resolution using graph neural network allowing model accurately annotate high resolution object image <eos> extensive evaluation cityscapes dataset shows model refer polygon rnn significantly outperforms original model both automatic absolute relative improvement mean iou interactive modes requiring fewer clicks annotators <eos> further analyze cross domain scenario model trained one dataset used out box datasets varying domains <eos> result show polygon rnn exhibits powerful generalization capabilities achieving significant improvements over existing pixel wise method <eos> using simple online fine tuning further achieve high reduction annotation time new datasets moving step closer towards interactive annotation tool used practice <eos> <eop> splinecnn fast geometric deep learning continuous spline kernels <eos> present spline based convolutional neural network splinecnns variant deep neural network irregular structured geometric input <eos> main contribution novel convolution operator based splines makes computation time independent kernel size due local support property spline basis functions <eos> result obtain generalization traditional cnn convolution operator using continuous kernel functions parametrized fixed number trainable weights <eos> contrast related approaches filter spectral domain proposed method aggregates feature purely spatial domain <eos> addition splinecnn allows entire end end training deep architectures using only geometric structure input instead handcrafted feature descriptors <eos> validation apply method tasks fields image graph classification shape correspondence graph node classification show outperforms pars state art approaches while being significantly faster having favorable properties like domain independence <eos> source code available github <eos> <eop> gagan geometry aware generative adversarial network <eos> deep generative models learned through adversarial training become increasingly popular their ability generate naturalistic image textures <eos> however aside their texture visual appearance object significantly influenced their shape geometry information taken into account existing generative models <eos> paper introduces geometry aware generative adversarial network gagan incorporating geometric information into image generation process <eos> specifically gagan generator sample latent variables probability space statistical shape model <eos> mapping output generator canonical coordinate frame through differentiable geometric transformation enforce geometry object add implicit connection prior generated object <eos> experimental result face generation indicate gagan generate realistic image faces arbitrary facial attributes such facial expression pose morphology better quality than current gan based method <eos> method used augment any existing gan architecture improve quality image generated <eos> <eop> robustness semantic segmentation models adversarial attacks <eos> deep neural network dnns demonstrated perform exceptionally well most recognition tasks such image classification segmentation <eos> however they also shown vulnerable adversarial examples <eos> phenomenon recently attracted lot attention but extensively studied multiple large scale datasets complex tasks such semantic segmentation often require more specialised network additional components such crfs dilated convolutions skip connections multiscale processing <eos> paper present knowledge first rigorous evaluation adversarial attacks modern semantic segmentation models using two large scale datasets <eos> analyse effect different network architectures model capacity multiscale processing show many observations made task classification always transfer more complex task <eos> furthermore show how mean field inference deep structured models multiscale processing naturally implement recently proposed adversarial defenses <eos> observations will aid future efforts understanding defending against adversarial examples <eos> moreover shorter term show segmentation models should currently preferred safety critical applications due their inherent robustness <eos> <eop> feedback prop convolutional neural network inference under partial evidence <eos> propose inference procedure deep convolutional neural network cnn when partial evidence available <eos> method consists general feedback based propagation approach feedback prop boosts prediction accuracy arbitrary set unknown target labels when values non overlapping arbitrary set target labels known <eos> show existing models trained multi label multi task setting readily take advantage feedback prop without any retraining fine tuning <eos> feedback prop inference procedure general simple reliable works different challenging visual recognition tasks <eos> present two variants feedback prop based layer wise residual iterative updates <eos> experiment using several multi task models show feedback prop effective all them <eos> result unveil previously unreported but interesting dynamic property deep cnn <eos> also present associated technical approach takes advantage property inference under partial evidence general visual recognition tasks <eos> <eop> super resolving very low resolution face image supplementary attributes <eos> given tiny face image conventional face hallucination method aim super resolve its high resolution hr counterpart learning mapping exemplar dataset <eos> since low resolution lr input patch may correspond many hr candidate patches ambiguity may lead erroneous hr facial details thus distorts final result such gender reversal <eos> lr input contains low frequency facial components its hr version while its residual face image defined difference between hr ground truth interpolated lr image contains missing high frequency facial details <eos> demonstrate supplementing residual image feature maps facial attribute information significantly reduce ambiguity face super resolution <eos> explore idea develop attribute embedded upsampling network consists upsampling network discriminative network <eos> upsampling network composed autoencoder skip connections incorporates facial attribute vectors into residual feature lr inputs bottleneck autoencoder deconvolutional layer used upsampling <eos> discriminative network designed examine whether super resolved faces contain desired attributes then its loss used updating upsampling network <eos> manner super resolve tiny unaligned imes pixels face image large upscaling factor imes while reducing uncertainty one many mappings significantly <eos> conducting extensive evaluations large scale dataset demonstrate method achieves superior face hallucination result outperforms state art <eos> <eop> frustum pointnets three dimensional object detection rgb data <eos> work study three dimensional object detection rgb data both indoor outdoor scenes <eos> while previous method focus image three dimensional voxels often obscuring natural three dimensional patterns invariances three dimensional data directly operate raw point clouds popping up rgb scans <eos> however key challenge approach how efficiently localize object point clouds large scale scenes region proposal <eos> instead solely relying three dimensional proposals method leverages both mature object detectors advanced three dimensional deep learning object localization achieving efficiency well high recall even small object <eos> benefited learning directly raw point clouds method also able precisely estimate three dimensional bounding boxes even under strong occlusion very sparse point <eos> evaluated kitti sun rgb three dimensional detection benchmarks method outperforms state art remarkable margins while having real time capability <eos> <eop> weakly supervised fully supervised framework object detection <eos> weakly supervised object detection attracted much attention lately since require bounding box annotations training <eos> although significant progress also made there still large gap performance between weakly supervised fully supervised object detection <eos> recently some works use pseudo ground truths generated weakly supervised detector train supervised detector <eos> such approaches incline find most representative parts object only seek one ground truth box per class even though many same class instances exist <eos> overcome issues propose weakly supervised fully supervised framework weakly supervised detector implemented using multiple instance learning <eos> then propose pseudo ground truth excavation pge algorithm find pseudo ground truth each instance image <eos> moreover pseudo ground truth adaptation pga algorithm designed further refine pseudo ground truths pge <eos> finally use pseudo ground truths train fully supervised detector <eos> extensive experiments challenging pascal voc benchmarks strongly demonstrate effectiveness framework <eos> map voc voc respectively significant improvement over previous state art method <eos> <eop> object detection latent support surfaces <eos> develop three dimensional object detection algorithm uses latent support surfaces capture contextual relationships indoor scenes <eos> existing three dimensional representations rgb image capture local shape appearance object categories but limited power represent object different visual styles <eos> detection small object also challenging because search space very large three dimensional scenes <eos> however observe much shape variation within three dimensional object categories explained location latent support surface smaller object often supported larger object <eos> therefore explicitly use latent support surfaces better represent three dimensional appearance large object provide contextual cues improve detection small object <eos> evaluate model object categories sun rgb database demonstrate state art performance <eos> <eop> towards faster training global covariance pooling network iterative matrix square root normalization <eos> global covariance pooling convolutional neural network achieved impressive improvement over classical first order pooling <eos> recent works shown matrix square root normalization plays central role achieving state art performance <eos> however existing method depend heavily eigendecomposition eig singular value decomposition svd suffering inefficient training due limited support eig svd gpu <eos> towards addressing problem propose iterative matrix square root normalization method fast end end training global covariance pooling network <eos> core method meta layer designed loop embedded directed graph structure <eos> meta layer consists three consecutive nonlinear structured layer perform pre normalization coupled matrix iteration post compensation respectively <eos> method much faster than eig svd based ones since involves only matrix multiplications suitable parallel implementation gpu <eos> moreover proposed network resnet architecture converge much less epochs further accelerating network training <eos> large scale imagenet achieve competitive performance superior existing counterparts <eos> finetuning models pre trained imagenet establish state art result three challenging fine grained benchmarks <eos> source code network models will available www <eos> org isqrt cov <eos> <eop> recurrent scene parsing perspective understanding loop <eos> object may appear arbitrary scales perspective image scene posing challenge recognition systems process image fixed resolution <eos> propose depth aware gating module adaptively selects pooling field size convolutional network architecture according object scale inversely proportional depth so small details preserved distant object while larger receptive fields used nearby <eos> depth gating signal provided stereo disparity estimated directly monocular input <eos> integrate depth aware gating into recurrent convolutional neural network perform semantic segmentation <eos> recurrent module iteratively efines segmentation result leveraging depth semantic predictions previous iterations <eos> through extensive experiments four popular large scale datasets demonstrate approach achieves competitive semantic segmentation performance model substantially more compact <eos> carry out extensive analysis architecture including variants operate monocular rgb but use depth side information during training unsupervised gating generic attentional mechanism multi resolution gating <eos> find gated pooling joint semantic segmentation depth yields state art result quantitative monocular depth estimation <eos> <eop> improving occlusion hard negative handling single stage pedestrian detectors <eos> propose method addressing two critical issues pedestrian detection occlusion target object false negative failure ii confusion hard negative examples like vertical structures false positive failure <eos> solutions two problems general flexible enough applicable any single stage detection models <eos> implement method into four state art single stage models including squeezedet yolov ssd dssd <eos> empirically validate approach indeed improves performance four models caltech pedestrian citypersons dataset <eos> moreover some heavy occlusion settings approach achieves best reported performance <eos> specifically two solutions follows <eos> better occlusion handling update output tensors single stage models so they include prediction part confidence scores compute final occlusion aware detection score <eos> reducing confusion hard negative examples introduce average grid classifiers post refinement classifiers trainable end end fashion little memory time overhead <eos> increase mb memory ms inference time <eos> <eop> learning act properly predicting explaining affordances image <eos> address problem affordance reasoning diverse scenes appear real world <eos> affordances relate agent actions their effects when taken surrounding object <eos> work take egocentric view scene aim reason about action object affordances respect both physical world well social norms imposed society <eos> also aim teach artificial agents why some actions should taken certain situations would likely happen if actions would taken <eos> collect new dataset builds upon ade referred ade affordance containing annotations enabling such rich visual reasoning <eos> propose model exploits graph neural network propagate contextual information scene order perform detailed affordance reasoning about each object <eos> model showcased through various ablation studies pointing successes challenges complex task <eos> <eop> pointwise convolutional neural network <eos> deep learning three dimensional data such reconstructed point clouds cad models received great research interests recently <eos> however capability using point clouds convolutional neural network so far fully explored <eos> paper present convolutional neural network semantic segmentation object recognition three dimensional point clouds <eos> core network pointwise convolution new convolution operator applied each point point cloud <eos> fully convolutional network design while being surprisingly simple implement yield competitive accuracy both semantic segmentation object recognition task <eos> <eop> image image domain adaptation preserved self similarity domain dissimilarity person re identification <eos> person re identification re id models trained one domain often fail generalize well another <eos> attempt present learning via translation framework <eos> baseline translate labeled image source target domain unsupervised manner <eos> then train re id models translated image supervised method <eos> yet being essential part framework unsupervised image image translation suffers information loss source domain labels during translation <eos> motivation two fold <eos> first each image discriminative cues contained its id label should maintained after translation <eos> second given fact two domains entirely different persons translated image should dissimilar any target ids <eos> end propose preserve two types unsupervised similarities self similarity image before after translation domain dissimilarity translated source image target image <eos> both constraints implemented similarity preserving generative adversarial network spgan consists siamese network cyclegan <eos> through domain adaptation experiment show image generated spgan more suitable domain adaptation yield consistent competitive re id accuracy two large scale datasets <eos> <eop> generative adversarial approach zero shot learning noisy texts <eos> most existing zero shot learning method consider problem visual semantic embedding one <eos> given demonstrated capability generative adversarial network gans generate image instead leverage gans imagine unseen categories text descriptions hence recognize novel classes no examples being seen <eos> specifically propose simple yet effective generative model takes input noisy text descriptions about unseen class <eos> wikipedia articles generates synthesized visual feature class <eos> added pseudo data zero shot learning naturally converted traditional classification problem <eos> additionally preserve inter class discrimination generated feature visual pivot regularization proposed explicit supervision <eos> unlike previous method using complex engineered regularizers approach suppress noise well without additional regularization <eos> empirically show method consistently outperforms state art largest available benchmarks text based zero shot learning <eos> <eop> tensorize factorize regularize robust visual relationship learning <eos> visual relationships provide higher level information object their relations image enables semantic understanding scene helps downstream applications <eos> given set localized object some training data visual relationship detection seeks detect most likely relationship between object given image <eos> while specific object may well represented training data their relationships may still infrequent <eos> empirical distribution obtained seeing relationships dataset model underlying distribution well serious issue most learning method <eos> work start simple multi relational learning model principle offers rich formalization deriving strong prior learning visual relationships <eos> while inference problem deriving regularizer challenging main technical contribution show how adapting recent result numerical linear algebra lead efficient algorithms factorization scheme yields highly informative priors <eos> factorization provides sample size bounds inference under mild conditions underlying object predicate object relationship learning task its own surprisingly outperforms some cases existing method even without utilizing visual feature <eos> then when integrated end end architecture visual relationship detection leveraging image data substantially improve state art <eos> <eop> transductive unbiased embedding zero shot learning <eos> most existing zero shot learning zsl method strong bias problem instances unseen target classes tend categorized one seen source classes <eos> so they yield poor performance after being deployed generalized zsl settings <eos> paper propose straightforward yet effective method named quasi fully supervised learning qfsl alleviate bias problem <eos> method follows way transductive learning assumes both labeled source image unlabeled target image available training <eos> semantic embedding space labeled source image mapped several fixed point specified source categories unlabeled target image forced mapped other point specified target categories <eos> experiments conducted awa cub sun datasets demonstrate method outperforms existing state art approaches huge margin <eos> following generalized zsl settings large margin <eos> following conventional zsl settings <eos> <eop> hierarchical novelty detection visual object recognition <eos> deep neural network achieved impressive success large scale visual object recognition tasks predefined set classes <eos> however recognizing object novel classes unseen during training still remains challenging <eos> problem detecting such novel classes addressed literature but most prior works focused providing simple binary regressive decisions <eos> output would known novel corresponding confidence intervals <eos> paper study more informative novelty detection schemes based hierarchical classification framework <eos> object novel class aim finding its closest super class hierarchical taxonomy known classes <eos> end propose two different approaches termed top down flatten method their combination well <eos> essential ingredients method confidence calibrated classifiers data relabeling leave one out strategy modeling novel classes under hierarchical taxonomy <eos> furthermore method generate hierarchical embedding leads improved generalized zero shot learning performance combination other commonly used semantic embeddings <eos> <eop> zero shot visual recognition using semantics preserving adversarial embedding network <eos> propose novel framework called semantics preserving adversarial embedding network sp aen zero shot visual recognition zsl test image their classes both unseen during training <eos> sp aen aims tackle inherent problem semantic loss prevailing family embedding based zsl some semantics would discarded during training if they non discriminative training classes but could become critical recognizing test classes <eos> specifically sp aen prevents semantic loss introducing independent visual semantic space embedder disentangles semantic space into two subspaces two arguably conflicting objectives classification reconstruction <eos> through adversarial learning two subspaces sp aen transfer semantics reconstructive subspace discriminative one accomplishing improved zero shot recognition unseen classes <eos> comparing prior works sp aen only improve classification but also generate photo realistic image demonstrating effectiveness semantic preservation <eos> four popular benchmarks cub awa sun apy sp aen considerably outperforms other state art method absolute performance difference <eos> terms harmonic mean values <eos> <eop> learning rich feature image manipulation detection <eos> image manipulation detection different traditional semantic object detection because pays more attention tampering artifacts than image content suggests richer feature need learned <eos> propose two stream faster cnn network train end end detect tampered region given manipulated image <eos> one two streams rgb stream whose purpose extract feature rgb image input find tampering artifacts like strong contrast difference unnatural tampered boundaries so <eos> other noise stream leverages noise feature extracted steganalysis rich model filter layer discover noise inconsistency between authentic tampered region <eos> then fuse feature two streams through bilinear pooling layer further incorporate spatial co occurrence two modalities <eos> experiments four standard image manipulation datasets demonstrate two stream framework outperforms each individual stream also achieves state art performance compared alternative method robustness resizing compression <eos> <eop> human semantic parsing person re identification <eos> person re identification challenging task mainly due factors such background clutter pose illumination camera point view variations <eos> elements hinder process extracting robust discriminative representations hence preventing different identities being successfully distinguished <eos> improve representation learning usually local feature human body parts extracted <eos> however common practice such process based bounding box part detection <eos> paper propose adopt human semantic parsing due its pixel level accuracy capability modeling arbitrary contours naturally better alternative <eos> proposed spreid integrates human semantic parsing person re identification only considerably outperforms its counter baseline but achieves state art performance <eos> also show employing simple yet effective training strategy standard popular deep convolutional architectures such inception resnet no modification while operating solely full image dramatically outperform current state art <eos> proposed method improve state art person re identification market map rank cuhk rank dukemtmc reid map rank <eos> <eop> stacked latent attention multimodal reasoning <eos> attention shown pivotal development deep learning used multitude multimodal learning tasks such visual question answering image captioning <eos> work pinpoint potential limitations design traditional attention model <eos> identify current attention mechanisms discard latent information intermediate reasoning losing positional information already captured attention heatmaps stacked attention common way improve spatial reasoning may suboptimal performance because vanishing gradient problem <eos> introduce novel attention architecture address problems all spatial configuration information contained intermediate reasoning process retained pathway convolutional layer <eos> show new attention leads substantial improvements multiple multimodal reasoning tasks including achieving single model performance without using external knowledge comparable state art vqa dataset well clear gains image captioning task <eos> <eop> fcn fps decoupling detection classification <eos> propose modular approach towards large scale real time object detection decoupling objectness detection classification <eos> exploit fact many object classes visually similar share parts <eos> thus universal objectness detector learned class agnostic object detection followed fine grained classification using non linear classifier <eos> approach modification fcn architecture learn shared filters performing localization across different object classes <eos> trained detector object classes called fcn obtains map <eos> imagenet detection dataset <eos> outperforms yolo while processing image per second <eos> also show objectness learned fcn generalizes novel classes performance increases number training object classes supporting hypothesis possible learn universal objectness detector <eos> <eop> csrnet dilated convolutional neural network understanding highly congested scenes <eos> propose network congested scene recognition called csrnet provide data driven deep learning method understand highly congested scenes perform accurate count estimation well present high quality density maps <eos> proposed csrnet composed two major components convolutional neural network cnn front end feature extraction dilated cnn back end uses dilated kernels deliver larger reception fields replace pooling operations <eos> csrnet easy trained model because its pure convolutional structure <eos> demonstrate csrnet four datasets shanghaitech dataset ucf cc dataset worldexpo dataset ucsd dataset deliver state art performance <eos> shanghaitech part dataset csrnet achieves <eos> lower mean absolute error mae than previous state art method <eos> extend targeted applications counting other object such vehicle trancos dataset <eos> result show csrnet significantly improves output quality <eos> lower mae than previous state art approach <eos> <eop> revisiting knowledge transfer training object class detectors <eos> propose revisit knowledge transfer training object detectors target classes weakly supervised training image helped set source classes bounding box annotations <eos> present unified knowledge transfer framework based training single neural network multi class object detector over all source classes organized semantic hierarchy <eos> generates proposals scores multiple levels hierarchy use explore knowledge transfer over broad range generality ranging class specific bycicle motorbike class generic objectness any class <eos> experiments object classes ilsvrc detection dataset show technique leads much better performance target classes <eos> map than weakly supervised baseline uses manually engineered objectness <eos> delivers target object detectors reaching map their fully supervised counterparts <eos> outperforms best reported transfer learning result dataset corloc map over <eos> moreover also carry out several across dataset knowledge transfer experiments find technique outperforms weakly supervised baseline all dataset pairs <eos> establishing its general applicability <eos> <eop> deep sparse coding invariant multimodal halle berry neurons <eos> deep feed forward convolutional neural network cnn become ubiquitous virtually all machine learning computer vision challenges however advancements cnn arguably reached engineering saturation point incremental novelty result minor performance gains <eos> although there evidence object classification reached human levels narrowly defined tasks general applications biological visual system far superior any computer <eos> research reveals there numerous missing components feed forward deep neural network critical mammalian vision <eos> brain work solely feed forward fashion but rather all neurons competition each other neurons integrating information bottom up top down fashion incorporating expectation feedback modeling process <eos> furthermore visual cortex working tandem parietal lobe integrating sensory information various modalities <eos> work sought improve upon standard feed forward deep learning model augmenting them biologically inspired concepts sparsity top down feedback lateral inhibition <eos> define model sparse coding problem using hierarchical layer <eos> solve sparse coding problem additional top down feedback error driving dynamics neural network <eos> while building observing behavior model were fascinated multimodal invariant neurons naturally emerged mimicked halle berry neurons found human brain <eos> neurons trained sparse model learned respond high level concepts multiple modalities case standard feed forward autoencoder <eos> furthermore sparse representation multimodal signals demonstrates qualitative quantitative superiority standard feed forward joint embedding common vision machine learning tasks <eos> <eop> convergence patchmatch its variants <eos> many problems image video processing computer vision require computation dense nearest neighbor field nnf between two image <eos> each patch query image nnf determines positions most similar patches database image <eos> introduction patchmatch algorithm barnes <eos> demonstrated large search problem approximated efficiently collaborative search method exploit local coherency image patches <eos> after its introduction several variants original patchmatch algorithm proposed some them reducing computational time two orders magnitude <eos> work propose theoretical framework analysis patchmatch its variants apply derive bounds their covergence rate <eos> consider generic patchmatch algorithm most specific instances found literature derived particular cases <eos> also derive more specific bounds two particular cases original patchmatch coherency sensitive hashing <eos> proposed bounds validated contrasting them convergence observed practice <eos> <eop> rethinking faster cnn architecture temporal action localization <eos> propose tal net improved approach temporal action localization video inspired faster cnn object detection framework <eos> tal net addresses three key shortcomings existing approaches improve receptive field alignment using multi scale architecture accommodate extreme variation action durations better exploit temporal context actions both proposal generation action classification appropriately extending receptive fields explicitly consider multi stream feature fusion demonstrate fusing motion late important <eos> achieve state art performance both action proposal localization thumos detection benchmark competitive performance activitynet challenge <eos> <eop> monet deep motion exploitation video object segmentation <eos> paper propose novel monet model deeply exploit motion cues boosting video object segmentation performance two aspects <eos> frame representation learning segmentation refinement <eos> concretely monet exploits computed motion cue <eos> optical flow reinforce representation target frame aligning integrating representations its neighbors <eos> new representation provides valuable temporal contexts segmentation improves robustness various common contaminating factors <eos> motion blur appearance variation deformation video object <eos> moreover monet exploits motion inconsistency transforms such motion cue into foreground background prior eliminate distraction confusing instances noisy region <eos> introducing distance transform layer monet effectively separate motion inconstant instances region thoroughly refine segmentation result <eos> integrating proposed two motion exploitation components standard segmentation network monet provides new state art performance three competitive benchmark datasets <eos> <eop> video representation learning using discriminative pooling <eos> popular deep models action recognition video generate independent predictions short clips then pooled heuristically assign action label full video segment <eos> all frames may characterize underlying action indeed many common across multiple actions pooling schemes impose equal importance all frames might unfavorable <eos> attempt tackle problem propose discriminative pooling based notion among deep feature generated all short clips there least one characterizes action <eos> end learn nonlinear hyperplane separates unknown yet discriminative feature rest <eos> applying multiple instance learning large margin setup use parameters separating hyperplane descriptor full video segment <eos> since parameters directly related support vectors max margin framework they serve robust representations pooling feature <eos> formulate joint objective efficient solver learns hyperplanes per video corresponding action classifiers over hyperplanes <eos> pooling scheme end end trainable within deep framework <eos> report result experiments three benchmark datasets spanning variety challenges demonstrate state art performance across tasks <eos> <eop> recognizing human actions evolution pose estimation maps <eos> most video based action recognition approaches choose extract feature whole video recognize actions <eos> cluttered background non action motions limit performances method since they lack explicit modeling human body movements <eos> recent advances human pose estimation work presents novel method recognize human action evolution pose estimation maps <eos> instead relying inaccurate human poses estimated video observe pose estimation maps byproduct pose estimation preserve richer cues human body benefit action recognition <eos> specifically evolution pose estimation maps decomposed evolution heatmaps <eos> probabilistic maps evolution estimated human poses denote changes body shape body pose respectively <eos> considering sparse property heatmap develop spatial rank pooling aggregate evolution heatmaps body shape evolution image <eos> body shape evolution image differentiate body parts design body guided sampling aggregate evolution poses body pose evolution image <eos> complementary properties between both types image explored deep convolutional neural network predict action label <eos> experiments ntu rgb utd mhad pennaction datasets verify effectiveness method outperforms most state art method <eos> <eop> video person re identification competitive snippet similarity aggregation co attentive snippet embedding <eos> paper address video based person re identification competitive snippet similarity aggregation co attentive snippet embedding <eos> approach divides long person sequences into multiple short video snippets aggregates top ranked snippet similarities sequence similarity estimation <eos> strategy intra person visual variation each sample could minimized similarity estimation while diverse appearance temporal information maintained <eos> snippet similarities estimated deep neural network novel temporal co attention snippet embedding <eos> attention weights obtained based query feature learned whole probe snippet lstm network making resulting embeddings less affected noisy frames <eos> gallery snippet shares same query feature probe snippet <eos> thus embedding gallery snippet present more relevant feature compare probe snippet yielding more accurate snippet similarity <eos> extensive ablation studies verify effectiveness competitive snippet similarity aggregation well temporal co attentive embedding <eos> method significantly outperforms current state art approaches multiple datasets <eos> <eop> mask guided contrastive attention model person re identification <eos> person re identification reid important yet challenging task computer vision <eos> due diverse background clutters variations viewpoints body poses far solved <eos> how extract discriminative robust feature invariant background clutters core problem <eos> paper first introduce binary segmentation masks construct synthetic rgb mask pairs inputs then design mask guided contrastive attention model mgcam learn feature separately body background region <eos> moreover propose novel region level triplet loss restrain feature learnt different region <eos> pulling feature full image body region close whereas pushing feature backgrounds away <eos> may first one successfully introduce binary mask into person reid task first one propose region level contrastive learning <eos> evaluate proposed method three public datasets including mars market cuhk <eos> extensive experimental result show proposed method effective achieves state art result <eos> mask code will released upon request <eos> <eop> blazingly fast video object segmentation pixel wise metric learning <eos> paper tackles problem video object segmentation given some user annotation indicates object interest <eos> problem formulated pixel wise retrieval learned embedding space embed pixels same object instance into vicinity each other using fully convolutional network trained modified triplet loss embedding model <eos> then annotated pixels set reference rest pixels classified using nearest neighbor approach <eos> proposed method supports different kinds user input such segmentation mask first frame semi supervised scenario sparse set clicked point interactive scenario <eos> semi supervised scenario achieve result competitive state art but fraction computation cost milliseconds per frame <eos> interactive scenario user able refine their input iteratively proposed method provides instant response each input reaches comparable quality competing method much less interaction <eos> <eop> learning compare relation network few shot learning <eos> present conceptually simple flexible general framework few shot learning classifier must learn recognise new classes given only few examples each <eos> method called relation network rn trained end end scratch <eos> during meta learning learns learn deep distance metric compare small number image within episodes each designed simulate few shot setting <eos> once trained rn able classify image new classes computing relation scores between query image few examples each new class without further updating network <eos> besides providing improved performance few shot learning framework easily extended zero shot learning <eos> extensive experiments five benchmarks demonstrate simple approach provides unified effective approach both two tasks <eos> <eop> coco stuff thing stuff classes context <eos> semantic classes either things object well defined shape <eos> car person stuff amorphous background region <eos> while lots classification detection works focus thing classes less attention given stuff classes <eos> nonetheless stuff classes important they allow explain important aspects image including scene type thing classes likely present their location through contextual reasoning physical attributes material types geometric properties scene <eos> understand stuff things context introduce coco stuff augments all image coco dataset pixel wise annotations stuff classes <eos> introduce efficient stuff annotation protocol based superpixels leverages original thing annotations <eos> quantify speed versus quality trade off protocol explore relation between annotation time boundary complexity <eos> furthermore use coco stuff analyze importance stuff thing classes terms their surface cover how frequently they mentioned image captions spatial relations between stuff things highlighting rich contextual relations make dataset unique performance modern semantic segmentation method stuff thing classes whether stuff easier segment than things <eos> <eop> image generation scene graphs <eos> truly understand visual world models should able only recognize image but also generate them <eos> end there exciting recent progress gen erating image natural language descriptions <eos> method give stunning result limited domains such descriptions birds flowers but struggle faithfully reproduce complex sentences many object rela tionships <eos> overcome limitation propose method generating image scene graphs enabling explic itly reasoning about object their relationships <eos> model uses graph convolution process input graphs com putes scene layout predicting bounding boxes seg mentation masks object converts layout image cascaded refinement network <eos> network trained adversarially against pair discriminators en sure realistic outputs <eos> validate approach visual genome coco stuff qualitative result abla tions user studies demonstrate method ability generate complex image multiple object <eos> <eop> deep cauchy hashing hamming space retrieval <eos> due its computation efficiency retrieval quality hashing widely applied approximate nearest neighbor search large scale image retrieval while deep hashing further improves retrieval quality end end representation learning hash coding <eos> compact hash codes hamming space retrieval enables most efficient constant time search returns data point within given hamming radius each query hash table lookups instead linear scan <eos> however subject weak capability concentrating relevant image within small hamming ball due mis specified loss functions existing deep hashing method may underperform hamming space retrieval <eos> work presents deep cauchy hashing dch novel deep hashing model generates compact concentrated binary hash codes enable efficient effective hamming space retrieval <eos> main idea design pairwise cross entropy loss based cauchy distribution penalizes significantly similar image pairs hamming distance larger than given hamming radius threshold <eos> comprehensive experiments demonstrate dch generate highly concentrated hash codes yield state art hamming space retrieval performance three datasets nus wide cifar ms coco <eos> <eop> learning look around intelligently exploring unseen environments unknown tasks <eos> common implicitly assume access intelligently captured inputs <eos> photos human photographer yet autonomously capturing good observations itself major challenge <eos> address problem learning look around if agent ability voluntarily acquire new views observe its environment how learn efficient exploratory behaviors acquire informative visual observations propose reinforcement learning solution agent rewarded actions reduce its uncertainty about unobserved portions its environment <eos> based principle develop recurrent neural network based approach perform active completion panoramic natural scenes three dimensional object shapes <eos> crucially learned policies tied any recognition task nor particular semantic content seen during training <eos> result learned look around behavior relevant even new tasks unseen environments training data acquisition involves no manual labeling <eos> through tests diverse settings demonstrate approach learns useful generic policies transfer new unseen tasks environments <eos> <eop> multi scale location aware kernel representation object detection <eos> although faster cnn its variants shown promising performance object detection they only exploit simple first order representation object proposals final classification regression <eos> recent classification method demonstrate integration high order statistics into deep convolutional neural network achieve impressive improvement but their goal model whole image discarding location information so they cannot directly adopted object detection <eos> paper make attempt exploit high order statistics object detection aiming generating more discriminative representations proposals enhance performance detectors <eos> end propose novel multi scale location aware kernel representation mlkp capture high order statistics deep feature proposals <eos> mlkp efficiently computed modified multi scale feature map using low dimensional polynomial kernel approximation <eos> moreover different existing orderless global representations based high order statistics proposed mlkp location retentive sensitive so flexibly adopted object detection <eos> through integrating into faster cnn schema proposed mlkp achieves very competitive performance state art method improves faster cnn <eos> pascal voc voc ms coco benchmarks respectively <eos> code available github <eos> com hwang mlkp <eos> <eop> clinical skin lesion diagnosis using representations inspired dermatologist criteria <eos> skin largest organ human body <eos> around individuals worldwide skin related health problems whom effective efficient diagnosis necessary <eos> recently computer aided diagnosis cad systems successfully applied recognition skin cancers dermatoscopic image <eos> however little work concentrated commonly encountered skin diseases clinical image captured easily accessed cameras mobile phones <eos> meanwhile cad system representations skin lesions required understandable dermatologists so predictions convincing <eos> address problem present effective representations inspired accepted dermatological criteria diagnosing clinical skin lesions <eos> demonstrate dermatological criteria highly correlated measurable visual components <eos> accordingly design six medical representations considering different criteria recognition skin lesions construct diagnosis system clinical skin disease image <eos> experimental result show proposed medical representations only capture manifestations skin lesions effectively consistently dermatological criteria but also improve prediction performance respect state art method based uninterpretable feature <eos> <eop> compare contrast learning prominent visual differences <eos> relative attribute models compare image terms all detected properties attributes exhaustively predicting image fancier more natural so without any regard ordering <eos> however when humans compare image certain differences will naturally stick out come mind first <eos> most noticeable differences prominent differences likely described first <eos> addition many differences although present may mentioned all <eos> work introduce model prominent differences rich new functionality comparing image <eos> collect instance level annotations most noticeable differences build model trained relative attribute feature predicts prominent differences unseen pairs <eos> test model challenging ut zap shoes lfw faces datasets outperform array baseline method <eos> then demonstrate how prominence model improves two vision tasks image search description generation enabling more natural communication between people vision systems <eos> <eop> multi evidence filtering fusion multi label classification object detection semantic segmentation based weakly supervised learning <eos> supervised object detection semantic segmentation require object even pixel level annotations <eos> when there exist image level labels only challenging weakly supervised algorithms achieve accurate predictions <eos> accuracy achieved top weakly supervised algorithms still significantly lower than their fully supervised counterparts <eos> paper propose novel weakly supervised curriculum learning pipeline multi label object recognition detection semantic segmentation <eos> pipeline first obtain intermediate object localization pixel labeling result training image then use such result train task specific deep network fully supervised manner <eos> entire process consists four stages including object localization training image filtering fusing object instances pixel labeling training image task specific network training <eos> obtain clean object instances training image propose novel algorithm filtering fusing classifying object instances collected multiple solution mechanisms <eos> algorithm incorporate both metric learning density based clustering filter detected object instances <eos> experiments show weakly supervised pipeline achieves state art result multi label image classification well weakly supervised object detection very competitive result weakly supervised semantic segmentation ms coco pascal voc pascal voc <eos> <eop> hashgan deep learning hash pair conditional wasserstein gan <eos> deep learning hash improves image retrieval performance end end representation learning hash coding training data pairwise similarity information <eos> subject scarcity similarity information often expensive collect many application domains existing deep learning hash method may overfit training data result substantial loss retrieval quality <eos> paper presents hashgan novel architecture deep learning hash learns compact binary hash codes both real image diverse image synthesized generative models <eos> main idea augment training data nearly real image synthesized new pair conditional wasserstein gan pc wgan conditioned pairwise similarity information <eos> extensive experiments demonstrate hashgan generate high quality binary hash codes yield state art image retrieval performance three benchmarks nus wide cifar ms coco <eos> <eop> min entropy latent model weakly supervised object detection <eos> weakly supervised object detection challenging task when provided image category supervision but required learn same time object locations object detectors <eos> inconsistency between weak supervision learning objectives introduces randomness object locations ambiguity detectors <eos> paper min entropy latent model melm proposed weakly supervised object detection <eos> min entropy used metric measure randomness object localization during learning well serving model learn object locations <eos> aims principally reduce variance positive instances alleviate ambiguity detectors <eos> melm deployed two sub models respectively discovers localizes object minimizing global local entropy <eos> melm unified feature learning optimized recurrent learning algorithm progressively transfers weak supervision object locations <eos> experiments demonstrate melm significantly improves performance weakly supervised detection weakly supervised localization image classification against state art approaches <eos> <eop> mattnet modular attention network referring expression comprehension <eos> paper address referring expression comprehension localizing image region described natural language expression <eos> while most recent work treats expressions single unit propose decompose them into three modular components related subject appearance location relationship other object <eos> allows flexibly adapt expressions containing different types information end end framework <eos> model call modular attention network mattnet two types attention utilized language based attention learns module weights well word phrase attention each module should focus visual attention allows subject relationship modules focus relevant image components <eos> module weights combine scores all three modules dynamically output overall score <eos> experiments show mattnet outperforms previous state art method large margin both bounding box level pixel level comprehension tasks <eos> <eop> attngan fine grained text image generation attentional generative adversarial network <eos> paper propose attentional generative adversarial network attngan allows attention driven multi stage refinement fine grained text image generation <eos> novel attentional generative network attngan synthesize fine grained details different sub region image paying attentions relevant words natural language description <eos> addition deep attentional multimodal similarity model proposed compute fine grained image text matching loss training generator <eos> proposed attngan significantly outperforms previous state art boosting best reported inception score <eos> more challenging coco dataset <eos> detailed analysis also performed visualizing attention layer attngan <eos> first time shows layered attentional gan able automatically select condition word level generating different parts image <eos> <eop> adversarial complementary learning weakly supervised object localization <eos> work propose adversarial complementary learning acol automatically localize integral object semantic interest weak supervision <eos> first mathematically prove class localization maps obtained directly selecting class specific feature maps last convolutional layer paves simple way identify object region <eos> then present simple network architecture including two parallel classifiers object localization <eos> specifically leverage one classification branch dynamically localize some discriminative object region during forward pass <eos> although usually responsive sparse parts target object classifier drive counterpart classifier discover new complementary object region erasing its discovered region feature maps <eos> such adversarial learning two parallel classifiers forced leverage complementary object region classification finally generate integral object localization together <eos> merits acol mainly two fold trained end end manner dynamically erasing enables counterpart classifier discover complementary object region more effectively <eos> demonstrate superiority acol approach variety experiments <eos> particular top localization error rate ilsvrc dataset <eos> new state art <eos> <eop> conditional generative adversarial network structured domain adaptation <eos> recent years deep neural nets triumphed over many computer vision problems including semantic segmentation critical task emerging autonomous driving medical image diagnostics applications <eos> general training deep neural nets requires humongous amount labeled data laborious costly collect annotate <eos> recent advances computer graphics shed light utilizing photo realistic synthetic data computer generated annotations train neural nets <eos> nevertheless domain mismatch between real image synthetic ones major challenge against harnessing generated data labels <eos> paper propose principled way conduct structured domain adaption semantic segmentation <eos> integrating gan into fcn framework mitigate gap between source target domains <eos> specifically learn conditional generator transform feature synthetic image real image like feature discriminator distinguish them <eos> each training batch conditional generator discriminator compete against each other so generator learns produce real image like feature fool discriminator afterwards fcn parameters updated accommodate changes gan <eos> experiments without using labels real image data method significantly outperforms baselines well state art method mean iou cityscapes dataset <eos> <eop> groupcap group based image captioning structured relevance diversity constraints <eos> most image captioning models focus one line single image captioning correlations like relevance diversity among group image <eos> within same album event simply neglected resulting less accurate diverse captions <eos> recent works mainly consider imposing diversity during online inference only neglect correlation among visual structures offline training <eos> paper propose novel group based image captioning scheme termed groupcap jointly models structured relevance diversity among visual contents group image towards optimal collaborative captioning <eos> particular first propose visual tree parser vp tree construct structured semantic correlations within individual image <eos> then relevance diversity among image well modeled exploiting correlations among their tree structures <eos> finally such correlations modeled constraints sent into lstm based captioning generator <eos> offline optimization adopt end end formulation jointly trains visual tree parser structured relevance diversity constraints well lstm based captioning model <eos> facilitate quantitative evaluation further release two group captioning datasets derived ms coco benchmark serving first their kind <eos> quantitative result show proposed groupcap model outperforms state art alternative approaches generate much more accurate discriminative captions under various evaluation metrics <eos> <eop> weakly supervised semantic segmentation iteratively mining common object feature <eos> weakly supervised semantic segmentation under image tags supervision challenging task directly associates high level semantic low level appearance <eos> bridge gap paper propose iterative bottom up top down framework alternatively expands object region optimizes segmentation network <eos> start initial localization produced classification network <eos> while classification network only responsive small coarse discriminative object region argue region contain significant common feature about object <eos> so bottom up step mine common object feature initial localization expand object region mined feature <eos> supplement non discriminative region saliency maps then considered under bayesian framework refine object region <eos> then top down step refined object region used supervision train segmentation network predict object masks <eos> object masks provide more accurate localization contain more region object <eos> further take object masks initial localization mine common object feature them <eos> processes conducted iteratively progressively produce fine object masks optimize segmentation network <eos> experimental result pascal voc dataset demonstrate proposed method outperforms previous state art method large margin <eos> <eop> bootstrapping performance webly supervised semantic segmentation <eos> fully supervised method semantic segmentation require pixel level class masks train creation expensive terms manual labour time <eos> work focus weak supervision developing method training high quality pixel level classifier semantic segmentation using only image level class labels provided ground truth <eos> method formulated two stage approach first aim create accurate pixel level masks training image via bootstrapping process then use now accurately segmented image proxy ground truth more standard supervised setting <eos> key driver work target dataset typically reliable ground truth image level labels while data crawled web may unreliable labels but filtered comprise only easy image segment therefore having reliable boundaries <eos> two forms information complementary use observation build novel bi directional transfer learning <eos> framework transfers knowledge between two domains target domain web domain bootstrapping performance weakly supervised semantic segmentation <eos> conducting experiments popular benchmark dataset pascal voc based both vgg network resnet reach state art performance scores <eos> <eop> deepvoting robust explainable deep network semantic part detection under partial occlusion <eos> paper study task detecting semantic parts object <eos> wheel car under partial occlusion <eos> propose all models should trained without seeing occlusions while being able transfer learned knowledge deal occlusions <eos> setting alleviates difficulty collecting exponentially large dataset cover occlusion patterns more essential <eos> scenario proposal based deep network like rcnn series often produce unsatisfactory result because both proposal extraction classification stages may confused irrelevant occluders <eos> address proposed voting mechanism combines multiple local visual cues detect semantic parts <eos> semantic parts still detected even though some visual cues missing due occlusions <eos> however method manually designed thus hard optimized end end manner <eos> paper present deepvoting incorporates robustness shown into deep network so whole pipeline jointly optimized <eos> specifically adds two layer after intermediate feature deep network <eos> pool layer vggnet <eos> first layer extracts evidence local visual cues second layer performs voting mechanism utilizing spatial relationship between visual cues semantic parts <eos> also propose improved version deepvoting learning visual cues context outside object <eos> experiments deepvoting achieves significantly better performance than several baseline method including faster rcnn semantic part detection under occlusion <eos> addition deepvoting enjoys explainability detection result diagnosed via looking up voting cues <eos> <eop> geometry aware scene text detection instance transformation network <eos> localizing text wild challenging situations complicated geometric layout targets like random orientation large aspect ratio <eos> paper propose geometry aware modeling approach tailored scene text representation end end learning scheme <eos> approach novel instance transformation network itn presented learn geometry aware representation encoding unique geometric configurations scene text instances network transformation embedding resulting robust elegant framework detect words text lines one pass <eos> end end multi task learning strategy transformation regression text non text classification coordinate regression adopted itn <eos> experiments benchmark datasets demonstrate effectiveness proposed approach detecting scene text various geometric configurations <eos> <eop> optical flow guided feature fast robust motion representation video action recognition <eos> motion representation plays vital role human action recognition video <eos> study introduce novel compact motion representation video action recognition named optical flow guided feature off enables network distill temporal information through fast robust approach <eos> off derived definition optical flow orthogonal optical flow <eos> derivation also provides theoretical support using difference between two frames <eos> directly calculating pixel wise spatio temporal gradients deep feature maps off could embedded any existing cnn based video action recognition framework only slight additional cost <eos> enables cnn extract spatio temporal information especially temporal information between frames simultaneously <eos> simple but powerful idea validated experimental result <eos> network off fed only rgb inputs achieves competitive accuracy <eos> ucf comparable result obtained two streams rgb optical flow but times faster speed <eos> experimental result also show off complementary other motion modalities such optical flow <eos> when proposed method plugged into state art video action recognition framework <eos> accuracy ucf hmdb respectively <eos> code project available github <eos> com kevin ssy optical flow guided feature <eop> motion guided cascaded refinement network video object segmentation <eos> deep cnn achieved superior performance many tasks computer vision image understanding <eos> however still difficult effectively apply deep cnn video object segmentation vos since treating video frames separate static will lose information hidden motion <eos> tackle problem propose motion guided cascaded refinement network vos <eos> assuming object motion normally different background motion video frame first apply active contour model optical flow coarsely segment object interest <eos> then proposed cascaded refinement network crn takes coarse segmentation guidance generate accurate segmentation full resolution <eos> way motion information deep cnn well complement each other accurately segment object video frames <eos> furthermore crn introduce single channel residual attention module incorporate coarse segmentation map attention making network effective efficient both training testing <eos> perform experiments popular benchmarks result show method achieves state art performance much faster speed <eos> <eop> memory network approach story based temporal summarization video <eos> address problem story based temporal summarization long video <eos> propose novel memory network model named past future memory network pfmn first compute scores normal field view nfov region proposals cropped input video then recover latent collective summary using network two external memories store embeddings previously selected subshots future candidate subshots <eos> major contributions two fold <eos> first work first address story based temporal summarization video <eos> second model first attempt leverage memory network video summarization tasks <eos> evaluation perform three set experiments <eos> first investigate view selection capability model pano vid dataset <eos> second evaluate temporal summarization newly collected video dataset <eos> finally experiment model performance another domain image based storytelling vist dataset <eos> verify model achieves state art performance all tasks <eos> <eop> cube padding weakly supervised saliency prediction video <eos> automatic saliency prediction video critical viewpoint guidance applications <eos> propose spatial temporal network unsupervisedly trained tailor made viewing sphere <eos> note most existing method less scalable since they rely annotated saliency map training <eos> most importantly they convert sphere image <eos> single equirectangular image multiple separate normal field view nfov image introduces distortion image boundaries <eos> contrast propose simple effective cube padding cp technique follows <eos> firstly render view six faces cube using perspective projection <eos> thus introduces very little distortion <eos> then concatenate all six faces while utilizing connectivity between faces cube image padding <eos> cube padding convolution pooling convolutional lstm layer <eos> way pc introduces no image boundary while being applicable almost all convolutional neural network cnn structures <eos> evaluate method propose wild new video saliency dataset containing challenging video saliency heatmap annotations <eos> experiments method outperforms all baseline method both speed quality <eos> <eop> appearance relation network video classification <eos> spatiotemporal feature learning video fundamental problem computer vision <eos> paper presents new architecture termed appearance relation network artnet learn video representation end end manner <eos> artnets constructed stacking multiple generic building blocks called smart whose goal simultaneously model appearance relation rgb input separate explicit manner <eos> specifically smart blocks decouple spatiotemporal learning module into appearance branch spatial modeling relation branch temporal modeling <eos> appearance branch implemented based linear combination pixels filter responses each frame while relation branch designed based multiplicative interactions between pixels filter responses across multiple frames <eos> perform experiments three action recognition benchmarks kinetics ucf hmdb demonstrating smart blocks obtain evident improvement over three dimensional convolutions spatiotemporal feature learning <eos> under same training setting artnets achieve superior performance three datasets existing state art method <eos> <eop> excitation backprop rnns <eos> deep models state art many vision tasks including video action recognition video captioning <eos> models trained caption classify activity video but little known about evidence used make such decisions <eos> grounding decisions made deep network studied spatial visual content giving more insight into model predictions image <eos> however such studies relatively lacking models spatiotemporal visual content video <eos> work devise formulation simultaneously grounds evidence space time single pass using top down saliency <eos> visualize spatiotemporal cues contribute deep model classification captioning output using model internal representation <eos> based spatiotemporal cues able localize segments within video correspond specific action phrase caption without explicitly optimizing training tasks <eos> <eop> one shot action localization learning sequence matching network <eos> learning based temporal action localization method require vast amounts training data <eos> however such large scale video datasets expected capture dynamics every action category only very expensive acquire but also practical simply because there exists uncountable number action classes <eos> poses critical restriction current method when training sample few rare <eos> when target action classes present current publicly available datasets <eos> address challenge conceptualize new example based action detection problem only few examples provided goal find occurrences examples untrimmed video sequence <eos> towards objective introduce novel one shot action localization method alleviates need large amounts training sample <eos> solution adopts one shot learning technique matching network utilizes correlations mine localize actions previously unseen classes <eos> evaluate one shot action localization method thumos activitynet datasets modified configuration fit one shot problem setup <eos> <eop> structure preserving video prediction <eos> despite recent emergence adversarial based method video prediction existing algorithms often produce unsatisfied result image region rich structural information <eos> object boundary detailed motion <eos> articulated body movement <eos> end present structure preserving video prediction framework explicitly address above issues enhance video prediction quality <eos> one hand framework contains two stream generation architecture deals high frequency video content <eos> detailed object articulated motion structure low frequency video content <eos> location moving directions two separate streams <eos> other hand propose rnn structure video prediction employs temporal adaptive convolutional kernels capture time varying motion patterns well tiny object within scene <eos> extensive experiments diverse scene ranging human motion semantic layout prediction demonstrate effectiveness proposed video prediction approach <eos> <eop> person re identification cascaded pairwise convolutions <eos> paper novel deep architecture named braidnet proposed person re identification <eos> braidnet specially designed wconv layer cascaded wconv structure learns extract comparison feature two image robust misalignments color differences across cameras <eos> furthermore channel scaling layer designed optimize scaling factor each input channel helps mitigate zero gradient problem training phase <eos> solve problem imbalanced volume negative positive training sample sample rate learning strategy proposed adaptively update ratio between positive negative sample each batch <eos> experiments conducted cuhk detected cuhk labeled cuhk market dukemtmc reid datasets demonstrate method achieves competitive performance when compared state art method <eos> <eop> importance label quality semantic segmentation <eos> convolutional network convnets become dominant approach semantic image segmentation <eos> producing accurate pixel level labels required task tedious time consuming process however producing approximate coarse labels could take only fraction time effort <eos> investigate relationship between quality labels performance convnets semantic segmentation <eos> create very large synthetic dataset perfectly labeled street view scenes <eos> perfect labels synthetically coarsen labels different qualities estimate human hours required producing them <eos> perform series experiments training convnets varying number training image label quality <eos> found performance convnets mostly depends time spent creating training labels <eos> larger coarsely annotated dataset yield same performance smaller finely annotated one <eos> furthermore fine tuning coarsely pre trained convnets few finely annotated labels yield comparable superior performance training large amount finely annotated labels alone fraction labeling cost <eos> demonstrate result also valid different network architectures various object classes urban scene <eos> <eop> scalable effective deep cca via soft decorrelation <eos> recently widely used multi view learning model canonical correlation analysis cca generalised non linear setting via deep neural network <eos> existing deep cca models typically first decorrelate feature dimensions each view before different views maximally correlated common latent space <eos> feature decorrelation achieved enforcing exact decorrelation constraint models thus computationally expensive due matrix inversion svd operations required exact decorrelation each training iteration <eos> furthermore decorrelation step often separated gradient descent based optimisation resulting sub optimal solutions <eos> propose novel deep cca model soft cca overcome problems <eos> specifically exact decorrelation replaced soft decorrelation via mini batch based stochastic decorrelation loss sdl optimised jointly other training objectives <eos> extensive experiments show proposed soft cca more effective efficient than existing deep cca models <eos> addition sdl loss applied other deep models beyond multi view learning obtains superior performance compared existing decorrelation losses <eos> <eop> duplex generative adversarial network unsupervised domain adaptation <eos> domain adaptation attempts transfer knowledge obtained source domain target domain <eos> domain testing data <eos> main challenge lies distribution discrepancy between source target domain <eos> most existing works endeavor learn domain invariant representation usually minimizing distribution distance <eos> mmd discriminator recently proposed generative adversarial network gan <eos> following similar idea gan work proposes novel gan architecture duplex adversarial discriminators referred dupgan achieve domain invariant representation domain transformation <eos> specifically proposed network consists three parts encoder generator two discriminators <eos> encoder embeds sample both domains into latent representation generator decodes latent representation both source target domains respectively conditioned domain code <eos> achieves domain transformation <eos> generator pitted against duplex discriminators one source domain other target ensure reality domain transformation latent representation domain invariant category information preserved well <eos> proposed work achieves state art performance unsupervised domain adaptation digit classification object recognition <eos> <eop> edit probability scene text recognition <eos> consider scene text recognition problem under attention based encoder decoder framework state art <eos> existing method usually employ frame wise maximal likelihood loss optimize models <eos> when train model misalignment between ground truth strings attention output sequences probability distribution caused missing superfluous characters will confuse mislead training process consequently make training costly degrade recognition accuracy <eos> handle problem propose novel method called edit probability ep scene text recognition <eos> ep tries effectively estimate probability generating string output sequence probability distribution conditioned input image while considering possible occurrences missing superfluous characters <eos> advantage lies training process focus missing superfluous unrecognized characters thus impact misalignment problem alleviated even overcome <eos> conduct extensive experiments standard benchmarks including iiit street view text icdar datasets <eos> experimental result show ep substantially boost scene text recognition performance <eos> <eop> global versus localized generative adversarial nets <eos> paper present novel localized generative adversarial net gan learn manifold real data <eos> compared classic gan em globally parameterizes manifold localized gan lgan uses local coordinate charts parameterize distinct local geometry how data point transform different locations manifold <eos> specifically around each point there exists em local generator produce data following diverse patterns transformations manifold <eos> locality nature lgan enables local generators adapt directly access local geometry without need invert generator global gan <eos> furthermore prevent manifold being locally collapsed dimensionally deficient tangent subspace imposing orthonormality prior between tangents <eos> provides geometric approach alleviating mode collapse least locally manifold imposing independence between data transformations different tangent directions <eos> will also demonstrate lgan applied train robust classifier prefers locally consistent classification decisions manifold resultant regularizer closely related laplace beltrami operator <eos> experiments show proposed lgans only produce diverse image transformations but also deliver superior classification performances <eos> <eop> mocogan decomposing motion content video generation <eos> visual signals video divided into content motion <eos> while content specifies object video motion describes their dynamics <eos> based prior propose motion content decomposed generative adversarial network mocogan framework video generation <eos> proposed framework generates video mapping sequence random vectors sequence video frames <eos> each random vector consists content part motion part <eos> while content part kept fixed motion part realized stochastic process <eos> learn motion content decomposition unsupervised manner introduce novel adversarial learning scheme utilizing both image video discriminators <eos> extensive experimental result several challenging datasets qualitative quantitative comparison state art approaches verify effectiveness proposed framework <eos> addition show mocogan allows one generate video same content but different motion well video different content same motion <eos> code available github <eos> com sergeytulyakov mocogan <eos> <eop> recurrent residual module fast inference video <eos> deep convolutional neural network cnn made impressive progress many video recognition tasks such video pose estimation video object detection <eos> however running cnn inference video requires numerous computation usually slow <eos> work propose framework called recurrent residual module rrm accelerate cnn inference video recognition tasks <eos> framework novel design using similarity intermediate feature maps two consecutive frames largely reduce redundant computation <eos> one unique property proposed method compared previous work feature maps each frame precisely computed <eos> experiments show while maintaining similar recognition performance rrm yields averagely acceleration commonly used cnn such alexnet resnet deep compression model thus faster than original dense models ef cient inference engine impressively acceleration some binary network such xnor nets thus faster than original model <eos> further verify effectiveness rrm speeding cnn video pose estimation video object detection <eos> <eop> improving landmark localization semi supervised learning <eos> present two techniques improve landmark localization image partially annotated datasets <eos> primary goal leverage common situation precise landmark locations only provided small data subset but class labels classification regression tasks related landmarks more abundantly available <eos> first propose framework sequential multitasking explore here through architecture landmark localization training class labels acts auxiliary signal guide landmark localization unlabeled data <eos> key aspect approach errors backpropagated through complete landmark localization model <eos> second propose explore unsupervised learning technique landmark localization based having model predict equivariant landmarks respect transformations applied image <eos> show techniques improve landmark prediction considerably learn effective detectors even when only small fraction dataset landmark labels <eos> present result two toy datasets four real datasets hands faces report new state art two datasets wild <eos> only labeled image outperform previous state art trained aflw dataset <eos> <eop> adversarial data programming using gans relax bottleneck curated labeled data <eos> paucity large curated hand labeled training data forms major bottleneck deployment machine learning models computer vision other fields <eos> recent work data programming shown how distant supervision signals form labeling functions used obtain labels given data near constant time <eos> work present adversarial data programming adp presents adversarial methodology generate data well curated aggregated label given set weak labeling functions <eos> validated method mnist fashion mnist cifar svhn datasets outperformed many state art models <eos> conducted extensive experiments study its usefulness well showed how proposed adp framework used transfer learning well multitask learning data two domains generated simultaneously using framework along label information <eos> future work will involve understanding theoretical implications new framework game theoretic perspective well explore performance method more complex datasets <eos> <eop> stochastic variational inference gradient linearization <eos> variational inference experienced recent surge popularity owing stochastic approaches yielded practical tools wide range model classes <eos> key benefit stochastic variational inference obviates tedious process deriving analytical expressions closed form variable updates <eos> instead one simply needs derive gradient log posterior often much easier <eos> yet certain model classes log posterior itself difficult optimize using standard gradient techniques <eos> one such example random field models optimization based gradient linearization proven popular since speeds up convergence significantly avoid poor local optima <eos> paper propose stochastic variational inference gradient linearization svigl <eos> similarly convenient standard stochastic variational inference all required local linearization energy gradient <eos> its benefit over stochastic variational inference conventional gradient method clear improvement convergence speed while yielding comparable even better variational approximations terms kl divergence <eos> demonstrate benefits svigl three applications optical flow estimation poisson gaussian denoising three dimensional surface reconstruction <eos> <eop> multi label zero shot learning structured knowledge graphs <eos> paper propose novel deep learning architecture multi label zero shot learning ml zsl able predict multiple unseen class labels each input instance <eos> inspired way humans utilize semantic knowledge between object interests propose framework incorporates knowledge graphs describing relationships between multiple labels <eos> model learns information propagation mechanism semantic label space applied model interdependencies between seen unseen class labels <eos> such investigation structured knowledge graphs visual reasoning show model applied solving multi label classification ml zsl tasks <eos> compared state art approaches comparable improved performances achieved method <eos> <eop> morphnet fast simple resource constrained structure learning deep network <eos> present morphnet approach automate design neural network structures <eos> morphnet iteratively shrinks expands network shrinking via resource weighted sparsifying regularizer activations expanding via uniform multiplicative factor all layer <eos> contrast previous approaches method scalable large network adaptable specific resource constraints <eos> number floating point operations per inference capable increasing network performance <eos> when applied standard network architectures wide variety datasets approach discovers novel structures each domain obtaining higher performance while respecting resource constraint <eos> <eop> deep adversarial subspace clustering <eos> most existing subspace clustering method hinge self expression handcrafted representations unaware potential clustering errors <eos> thus they perform unsatisfactorily real data complex underlying subspaces <eos> solve issue propose novel deep adversarial subspace clustering dasc model learns more favorable sample representations deep learning subspace clustering more importantly introduces adversarial learning supervise sample representation learning subspace clustering <eos> specifically dasc consists subspace clustering generator quality verifying discriminator learn against each other <eos> generator produces subspace estimation sample clustering <eos> discriminator evaluates current clustering performance inspecting whether re sampled data estimated subspaces consistent subspace properties supervises generator progressively improve subspace clustering <eos> experimental result handwritten recognition face object clustering tasks demonstrate advantages dasc over shallow few deep subspace clustering models <eos> moreover best knowledge first successful application gan alike model unsupervised subspace clustering also paves way deep learning solve other unsupervised learning problems <eos> <eop> towards human machine cooperation self supervised sample mining object detection <eos> though quite challenging leveraging large scale unlabeled partially labeled image cost effective way increasingly attracted interests its great importance computer vision <eos> tackle problem many active learning method developed <eos> however method mainly define their sample selection criteria within single image context leading suboptimal robustness impractical solution large scale object detection <eos> paper aiming remedy drawbacks existing method present principled self supervised sample mining ssm process accounting real challenges object detection <eos> specifically ssm process concentrates automatically discovering pseudo labeling reliable region proposals enhancing object detector via introduced cross image validation <eos> pasting proposals into different labeled image comprehensively measure their values under different image contexts <eos> resorting ssm process propose new framework gradually incorporating unlabeled partially labeled data into model learning while minimizing annotating effort users <eos> extensive experiments two public benchmarks clearly demonstrate proposed framework achieve comparable performance state art method significantly fewer annotations <eos> <eop> discrete continuous admm transductive inference higher order mrfs <eos> paper introduces novel algorithm transductive inference higher order mrfs unary energies parameterized variable classifier <eos> considered task posed joint optimization problem continuous classifier parameters discrete label variables <eos> contrast prior approaches such convex relaxations propose advantageous decoupling objective function into discrete continuous subproblems novel efficient optimization method related admm <eos> approach preserves integrality discrete label variables guarantees global convergence critical point <eos> demonstrate advantages approach several experiments including video object segmentation davis data set interactive image segmentation <eos> <eop> robust physical world attacks deep learning visual classification <eos> recent studies show state art deep neural network dnns vulnerable adversarial examples resulting small magnitude perturbations added input <eos> given emerging physical systems using dnns safety critical situations adversarial examples could mislead systems cause dangerous situations <eos> therefore understanding adversarial examples physical world important step towards developing resilient learning algorithms <eos> propose general attack algorithm robust physical perturbations rp generate robust visual adversarial perturbations under different physical conditions <eos> using real world case road sign classification show adversarial examples generated using rp achieve high targeted misclassification rates against standard architecture road sign classifiers physical world under various environmental conditions including viewpoints <eos> due current lack standardized testing method propose two stage evaluation methodology robust physical adversarial examples consisting lab field tests <eos> using methodology evaluate efficacy physical adversarial manipulations real object <eos> perturbation form only black white stickers attack real stop sign causing targeted misclassification image obtained lab settings <eos> captured video frames obtained moving vehicle field test target classifier <eos> <eop> generating fusion image one identity another shape <eos> generating novel image manipulating two input image interesting research problem study generative adversarial network gans <eos> propose new gan based network generates fusion image identity input image shape input image <eos> network simultaneously train more than two image datasets unsupervised manner <eos> define identity loss li catch identity image shape loss ls get shape <eos> addition propose novel training method called min patch training focus generator crucial parts image rather than its entirety <eos> show qualitative result vgg youtube pose dataset eye dataset mpiigaze unityeyes photo sketch cartoon dataset <eos> <eop> learning promote saliency detectors <eos> categories appearance salient object vary image image therefore saliency detection image specific task <eos> due lack large scale saliency training data using deep neural network dnns pre training difficult precisely capture image specific saliency cues <eos> solve issue formulate zero shot learning problem promote existing saliency detectors <eos> concretely dnn trained embedding function map pixels attributes salient background region image into same metric space image specific classifier learned classify pixels <eos> since image specific task performed classifier dnn embedding effectively plays role general feature extractor <eos> compared transferring learning new recognition task using limited data formulation makes dnn learn more effectively small data <eos> extensive experiments five data set show method significantly improves accuracy existing method compares favorably against state art approaches <eos> <eop> image super resolution via dual state recurrent network <eos> advances image super resolution sr recently benefited significantly rapid developments deep neural network <eos> inspired recent discoveries note many state art deep sr architectures reformulated single state recurrent neural network rnn finite unfoldings <eos> paper explore new structures sr based compact rnn view leading dual state design dual state recurrent network dsrn <eos> compared its single state counterparts op erate fixed spatial resolution dsrn exploits both low resolution lr high resolution hr signals jointly <eos> recurrent signals exchanged between states both directions both lr hr hr lr via de layed feedback <eos> extensive quantitative qualitative eval uations benchmark datasets recent challenge demonstrate proposed dsrn performs favorably against state art algorithms terms both mem ory consumption predictive accuracy <eos> <eop> deep back projection network super resolution <eos> feed forward architectures recently proposed deep super resolution network learn representations low resolution inputs non linear mapping high resolution output <eos> however approach fully address mutual dependencies low high resolution image <eos> propose deep back projection network dbpn exploit iterative up down sampling layer providing error feedback mechanism projection errors each stage <eos> construct mutually connected up down sampling stages each represents different types image degradation high resolution components <eos> show extending idea allow concatenation feature across up down sampling stages dense dbpn allows reconstruct further improve super resolution yielding superior result particular establishing new state art result large scaling factors such across multiple data set <eos> <eop> focus manipulation detection via photometric histogram analysis <eos> rise misinformation spread via social media channels enabled increasing automation realism image manipulation tools image forensics increasingly relevant problem <eos> classic image forensic method leverage low level cues such metadata sensor noise fingerprints others easily fooled when image re encoded upon upload facebook etc <eos> necessitates use higher level physical semantic cues once hard estimate reliably wild become more effective due increasing power computer vision <eos> particular detect manipulations introduced artificial blurring image creates inconsistent photometric relationships between image intensity various cues <eos> achieve accuracy most challenging cases new dataset blur manipulations blur geometrically correct consistent scene physical arrangement <eos> such manipulations now easily generated instance smartphone cameras having hardware measure depth <eos> portrait mode iphone plus <eos> also demonstrate good performance challenge dataset evaluating wider range manipulations imagery representing wild conditions <eos> <eop> compassionately conservative balanced cuts image segmentation <eos> normalized cut ncut objective function widely used data clustering image segmentation quantifies cost graph partitioning way biases clusters segments balanced towards having lower values than unbalanced partitionings <eos> however bias so strong avoids any singleton partitions even when vertices very weakly connected rest graph <eos> motivated buehler hein family balanced cut costs propose family compassionately conservative balanced ccb cut costs indexed parameter used strike compromise between desire avoid too many singleton partitions notion all partitions should balanced <eos> show ccb cut minimization relaxed into orthogonally constrained ell au minimization problem coincides problem computing piecewise flat embeddings pfe one particular index value present algorithm solving relaxed problem iteratively minimizing sequence reweighted rayleigh quotients irrq <eos> using image bsds database show image segmentation based ccb cut minimization provides better accuracy respect ground truth greater variability region size than ncut based image segmentation <eos> <eop> high quality denoising dataset smartphone cameras <eos> last decade seen astronomical shift imaging dslr point shoot cameras imaging smartphone cameras <eos> due small aperture sensor size smartphone image notably more noise than their dslr counterparts <eos> while denoising smartphone image active research area research community currently lacks denoising image dataset representative real noisy image smartphone cameras high quality ground truth <eos> address issue paper following contributions <eos> propose systematic procedure estimating ground truth noisy image used benchmark denoising performance smartphone cameras <eos> using procedure captured dataset smartphone image denoising dataset sidd noisy image scenes under different lighting conditions using five representative smartphone cameras generated their ground truth image <eos> used dataset benchmark number denoising algorithms <eos> show cnn based method perform better when trained high quality dataset than when trained using alternative strategies such low iso image used proxy ground truth data <eos> <eop> context aware synthesis video frame interpolation <eos> video frame interpolation algorithms typically estimate optical flow its variations then use guide synthesis intermediate frame between two consecutive original frames <eos> handle challenges like occlusion bidirectional flow between two input frames often estimated used warp blend input frames <eos> however how effectively blend two warped frames still remains challenging problem <eos> paper presents context aware synthesis approach warps only input frames but also their pixel wise contextual information uses them interpolate high quality intermediate frame <eos> specifically first use pre trained neural network extract per pixel contextual information input frames <eos> then employ state art optical flow algorithm estimate bidirectional flow between them pre warp both input frames their context maps <eos> finally unlike common approaches blend pre warped frames method feeds them their context maps video frame synthesis neural network produce interpolated frame context aware fashion <eos> neural network fully convolutional trained end end <eos> experiments show method handle challenging scenarios such occlusion large motion outperforms representative state art approaches <eos> <eop> salient object detection driven fixation prediction <eos> research visual saliency focused two major types models namely fixation prediction salient object detection <eos> relationship between two however less explored <eos> paper propose employ former model type identify segment salient object scenes <eos> build novel neural network called attentive saliency network asnet learns detect salient object fixation maps <eos> fixation map derived upper network layer captures high level understanding scene <eos> salient object detection then viewed fine grained object level saliency segmentation progressively optimized guidance fixation map top down manner <eos> asnet based hierarchy convolutional lstms convlstms offers efficient recurrent mechanism sequential refinement segmentation map <eos> several loss functions introduced boosting performance asnet <eos> extensive experimental evaluation shows proposed asnet capable generating accurate segmentation maps help computed fixation map <eos> work offers deeper insight into mechanisms attention narrows gap between salient object detection fixation prediction <eos> <eop> enhancing spatial resolution stereo image using parallax prior <eos> present novel method enhance spatial resolution stereo image using parallax prior <eos> while traditional stereo imaging focused estimating depth stereo image method utilizes stereo image enhance spatial resolution instead estimating disparity <eos> critical challenge enhancing spatial resolution stereo image how register corresponding pixels subpixel accuracy <eos> since disparity traditional stereo imaging calculated per pixel directly inappropriate enhancing spatial resolution <eos> therefore learn parallax prior stereo image datasets jointly training two stage network <eos> first network learns how enhance spatial resolution stereo image luminance second network learns how reconstruct high resolution color image high resolution luminance chrominance input image <eos> two stage joint network enhances spatial resolution stereo image significantly more than single image super resolution method <eos> proposed method directly applicable any stereo depth imaging method enabling enhance spatial resolution stereo image <eos> <eop> hats histograms averaged time surfaces robust event based object classification <eos> event based cameras recently drawn attention computer vision community thanks their advantages terms high temporal resolution low power consumption high dynamic range compared traditional frame based cameras <eos> properties make event based cameras ideal choice autonomous vehicles robot navigation uav vision among others <eos> however accuracy event based object classification algorithms crucial importance any reliable system working real world conditions still far behind their frame based counterparts <eos> two main reasons performance gap <eos> lack effective low level representations architectures event based object classification <eos> absence large real world event based datasets <eos> paper address both problems <eos> first introduce novel event based feature representation together new machine learning architecture <eos> compared previous approaches use local memory units efficiently leverage past temporal information build robust event based representation <eos> second release first large real world event based dataset object classification <eos> compare method state art extensive experiments showing better classification performance real time computation <eos> <eop> bi directional message passing model salient object detection <eos> recent progress salient object detection beneficial fully convolutional neural network fcn <eos> saliency cues contained multi level convolutional feature complementary detecting salient object <eos> how integrate multi level feature becomes open problem saliency detection <eos> paper propose novel bi directional message passing model integrate multi level feature salient object detection <eos> first adopt multi scale context aware feature extraction module mcfem multi level feature maps capture rich context information <eos> then bi directional structure designed pass messages between multi level feature gate function exploited control message passing rate <eos> use feature after message passing simultaneously encode semantic information spatial details predict saliency maps <eos> finally predicted result efficiently combined generate final saliency map <eos> quantitative qualitative experiments five benchmark datasets demonstrate proposed model performs favorably against state art method under different evaluation metrics <eos> <eop> matching pixels using co occurrence statistics <eos> propose new error measure matching pixels based co occurrence statistics <eos> measure relies co occurrence matrix counts number times pairs pixel values co occur within window <eos> error incurred matching pair pixels inverse proportional probability their values co occur together their color difference <eos> measure also works feature other than color <eos> show improves state art performance template matching standard benchmarks <eos> then propose embedding scheme maps input image embedded image such euclidean distance between pixel values embedded space resembles co occurrence statistics original space <eos> lets run existing vision algorithms embedded image enjoy power co occurrence statistics free <eos> demonstrate two algorithms lucas kanade image registration kernelized correlation filter kcf tracker <eos> experiments show performance each algorithm improves about <eos> <eop> seednet automatic seed generation deep reinforcement learning robust interactive segmentation <eos> paper propose automatic seed generation technique deep reinforcement learning solve interactive segmentation problem <eos> one main issues interactive segmentation problem robust consistent object extraction less human effort <eos> most existing algorithms highly depend distribution inputs differs one user another hence need sequential user interactions achieve adequate performance <eos> system when user first specifies point desired object point background sequence artificial user input automatically generated precisely segmenting desired object <eos> proposed system allows user reduce number input significantly <eos> problem difficult cast supervised learning problem because possible define globally optimal user input some stage interactive segmentation task <eos> hence formulate automatic seed generation problem markov decision process mdp then optimize reinforcement learning deep network dqn <eos> train network msra dataset show network achieves notable performance improvement inaccurate initial segmentation both seen unseen datasets <eos> <eop> jerk aware video acceleration magnification <eos> video magnification reveals subtle changes invisible naked eye but such tiny yet meaningful changes often hidden under large motions small deformation muscles doing sports tiny vibrations strings ukulele playing <eos> magnifying subtle changes under large motions video acceleration magnification method recently proposed <eos> method magnifies subtle acceleration changes ignores slow large motions <eos> however quick large motions severely distort method <eos> paper present novel use jerk make acceleration method robust quick large motions <eos> jerk used assess smoothness time series data neuroscience mechanical engineering fields <eos> basis observation subtle changes smoother than quick large motions temporal scale used jerk based smoothness design jerk aware filter passes subtle changes only under quick large motions <eos> applying filter acceleration method obtain impressive magnification result better than obtained state art <eos> <eop> defense against adversarial attacks using high level representation guided denoiser <eos> neural network vulnerable adversarial examples poses threat their application security sensitive systems <eos> propose high level representation guided denoiser hgd defense image classification <eos> standard denoiser suffers error amplification effect small residual adversarial noise progressively amplified leads wrong classifications <eos> hgd overcomes problem using loss function defined difference between target model outputs activated clean image denoised image <eos> compared ensemble adversarial training state art defending method large image hgd three advantages <eos> first hgd defense target model more robust either white box black box adversarial attacks <eos> second hgd trained small subset image generalizes well other image unseen classes <eos> third hgd transferred defend models other than one guiding <eos> nips competition defense against adversarial attacks hgd solution won first place outperformed other models large margin <eos> footnote code url github <eos> com lfz guided denoise <eos> <eop> stacked conditional generative adversarial network jointly learning shadow detection shadow removal <eos> understanding shadows single image consists two types task previous studies containing shadow detection shadow removal <eos> paper present multi task perspective embraced any existing work jointly learn both detection removal end end fashion aims enjoying mutually improved benefits each other <eos> framework based novel stacked conditional generative adversarial network st cgan composed two stacked cgans each generator discriminator <eos> specifically shadow image fed into first generator produces shadow detection mask <eos> shadow image concatenated its predicted mask goes through second generator order recover its shadow free image consequently <eos> addition two corresponding discriminators very likely model higher level relationships global scene characteristics detected shadow region reconstruction via removing shadows respectively <eos> more importantly multi task learning design stacked paradigm provides novel view notably different commonly used one multi branch version <eos> fully evaluate performance proposed framework construct first large scale benchmark image triplets shadow image shadow mask image shadow free image under scenes <eos> extensive experimental result consistently show advantages st cgan over several representative state art method two large scale publicly available datasets newly released one <eos> <eop> image correction via deep reciprocating hdr transformation <eos> image correction aims adjust input image into visually pleasing one detail under over exposed region recovered <eos> however existing image correction method mainly based image pixel operations attempting recover lost detail under over exposed region challenging <eos> therefore revisit image formation procedure notice detail contained high dynamic range hdr light intensities perceived human eyes but lost during nonlinear imaging process camera low dynamic range ldr domain <eos> inspired observation formulate image correction problem deep reciprocating hdr transformation drht process propose novel approach first reconstruct lost detail hdr domain then transfer them back ldr image output image recovered detail preserved <eos> end propose end end drht model contains two cnn one hdr detail reconstruction other ldr detail correction <eos> experiments standard benchmarks demonstrate effectiveness proposed method compared state art image correction method <eos> <eop> pieapp perceptual image error assessment through pairwise preference <eos> ability estimate perceptual error between image important problem computer vision many applications <eos> although studied extensively however no method currently exists robustly predict visual differences like humans <eos> some previous approaches used hand coded models but they fail model complexity human visual system <eos> others used machine learning train models human labeled datasets but creating large high quality datasets difficult because people unable assign consistent error labels distorted image <eos> paper present new learning based method first predict perceptual image error like human observers <eos> since much easier people compare two given image identify one more similar reference than assign quality scores each propose new large scale dataset labeled probability humans will prefer one image over another <eos> then train deep learning model using novel pairwise learning framework predict preference one distorted image over other <eos> key observation trained network then used separately only one distorted image reference predict its perceptual error without ever being trained explicit human perceptual error labels <eos> perceptual error estimated new metric pieapp well correlated human opinion <eos> furthermore significantly outperforms existing algorithms beating state art almost test set terms binary error rate while also generalizing new kinds distortions unlike previous learning based method <eos> <eop> normalized cut loss weakly supervised cnn segmentation <eos> most recent semantic segmentation method train deep convolutional neural network fully annotated masks requiring pixel accuracy good quality training <eos> common weakly supervised approaches generate full masks partial input <eos> scribbles seeds using standard interactive segmentation method preprocessing <eos> but errors such masks result poorer training since standard loss functions <eos> cross entropy distinguish seeds potentially mislabeled other pixels <eos> inspired general ideas semi supervised learning address problems via new principled loss function evaluating network output criteria standard shallow segmentation <eos> unlike prior work cross entropy part loss evaluates only seeds labels known while normalized cut softly evaluates consistency all pixels <eos> focus normalized cut loss dense gaussian kernel efficiently implemented linear time fast bilateral filtering <eos> normalized cut loss approach segmentation brings quality weakly supervised training significantly closer fully supervised method <eos> <eop> ista net interpretable optimization inspired deep network image compressive sensing <eos> aim developing fast yet accurate algorithm compressive sensing cs reconstruction natural image combine paper merits two existing categories cs method structure insights traditional optimization based method performance speed recent network based ones <eos> specifically propose novel structured deep network dubbed ista net inspired iterative shrinkage thresholding algorithm ista optimizing general norm cs reconstruction model <eos> cast ista into deep network form develop effective strategy solve proximal mapping associated sparsity inducing regularizer using nonlinear transforms <eos> all parameters ista net <eos> nonlinear transforms shrinkage thresholds step sizes etc <eos> learned end end rather than being hand crafted <eos> moreover considering residuals natural image more compressible enhanced version ista net residual domain dubbed ista net derived further improve cs reconstruction <eos> extensive cs experiments demonstrate proposed ista nets outperform existing state art optimization based network based cs method large margins while maintaining fast computational speed <eos> <eop> fast end end trainable guided filter <eos> image processing pixel wise dense prediction advanced harnessing capabilities deep learning <eos> one central issue deep learning limited capacity handle joint upsampling <eos> present deep learning building block joint upsampling namely guided filtering layer <eos> layer aims efficiently generating high resolution output given corresponding low resolution one high resolution guidance map <eos> proposed layer composed guided filter reformulated fully differentiable block <eos> end show guided filter expressed group spatial varying linear transformation matrices <eos> layer could integrated convolutional neural network cnn jointly optimized through end end training <eos> further take advantage end end training plug trainable transformation function generates task specific guidance maps <eos> integrating cnn proposed layer form deep guided filtering network <eos> proposed network evaluated five advanced image processing tasks <eos> experiments mit adobe fivek dataset demonstrate proposed approach runs times faster achieves state art performance <eos> also show proposed guided filtering layer helps improve performance multiple pixel wise dense prediction tasks <eos> <eop> disentangling structure aesthetics style aware image completion <eos> content aware image completion painting fundamental tool correction defects removal object image <eos> propose non parametric painting algorithm enforces both structural aesthetic style consistency within resulting image <eos> contributions two fold explicitly disentangle image structure style during patch search selection ensure visually consistent look feel within target image perform adaptive stylization patches conform aesthetics selected patches target image so harmonising integration selected patches into final composition <eos> show explicit consideration visual style during painting delivers excellent qualitative quantitative result across varied image styles content over places photographic dataset challenging new painting dataset artwork derived bam <eop> learning discriminative feature network semantic segmentation <eos> most existing method semantic segmentation still suffer two aspects challenges intra class inconsistency inter class indistinction <eos> tackle two problems propose discriminative feature network dfn contains two sub network smooth network border network <eos> specifically handle intra class inconsistency problem specially design smooth network channel attention block global average pooling select more discriminative feature <eos> furthermore propose border network make bilateral feature boundary distinguishable deep semantic boundary supervision <eos> based proposed dfn achieve state art performance <eos> mean iou pascal voc <eos> mean iou cityscapes dataset <eos> <eop> kernelized subspace pooling deep local descriptors <eos> representing local image patches invariant discriminative manner active research topic computer vision <eos> recently demonstrated local feature learning based deep convolutional neural network cnn significantly improve matching performance <eos> previous works learning such descriptors focused developing various loss functions regularizations data mining strategies learn discriminative cnn representations <eos> such method however little analysis how increase geometric invariance their generated descriptors <eos> paper propose descriptor both highly invariant discriminative power <eos> abilities come novel pooling method dubbed subspace pooling sp invariant range geometric deformations <eos> further increase discriminative power descriptor propose simple distance kernel integrated marginal triplet loss helps focus hard examples cnn training <eos> finally show combining sp projection distance metric generated feature descriptor equivalent bilinear cnn model but outperforms latter much lower memory computation consumptions <eos> proposed method simple easy understand achieves good performance <eos> experimental result several patch matching benchmarks show method outperforms state arts significantly <eos> <eop> pose pseudo object space error initialization free bundle adjustment <eos> bundle adjustment nonlinear refinement method camera poses three dimensional structure requiring sufficiently good initialization <eos> recent years was experimentally observed useful minima reached even arbitrary initialization affine bundle adjustment problems fixed rank matrix factorization instances general <eos> key success factor lies use variable projection varpro method known wide basin convergence such problems <eos> paper propose pseudo object space error pose objective cameras represented hybrid between affine projective models <eos> formulation allows obtain three dimensional reconstructions close true projective reconstructions while retaining bilinear problem structure suitable varpro method <eos> experimental result show using pose high success rate yield faithful three dimensional reconstructions random initializations taking one step towards initialization free structure motion <eos> <eop> deformable shape completion graph convolutional autoencoders <eos> availability affordable portable depth sensors made scanning object people simpler than ever <eos> however dealing occlusions missing parts still significant challenge <eos> problem reconstructing possibly non rigidly moving three dimensional object single multiple partial scans received increasing attention recent years <eos> work propose novel learningbased method completion partial shapes <eos> unlike majority existing approaches method focuses object undergo non rigid deformations <eos> core method variational autoencoder graph convolutional operations learns latent space complete realistic shapes <eos> inference optimize find representation latent space best fits generated shape known partial input <eos> completed shape exhibits realistic appearance unknown part <eos> show promising result towards completion synthetic real scans human body face meshes exhibiting different styles articulation partiality <eos> <eop> learning millions three dimensional scans large scale three dimensional face recognition <eos> deep network trained millions facial image believed closely approaching human level performance face recognition <eos> however open world face recognition still remains challenge <eos> although three dimensional face recognition inherent edge over its counterpart benefited recent developments deep learning due unavailability large training well large test datasets <eos> recognition accuracies already saturated existing three dimensional face datasets due their small gallery sizes <eos> unlike photographs three dimensional facial scans cannot sourced web causing bottleneck development deep three dimensional face recognition network datasets <eos> backdrop propose method generating large corpus labeled three dimensional face identities their multiple instances training protocol merging most challenging existing three dimensional datasets testing <eos> also propose first deep cnn model designed specifically three dimensional face recognition trained <eos> million three dimensional facial scans identities <eos> test dataset comprises identities single three dimensional scan gallery another scans probes several orders magnitude larger than existing ones <eos> without fine tuning dataset network already outperforms state art face recognition over <eos> fine tune network gallery set perform end end large scale three dimensional face recognition further improves accuracy <eos> finally show efficacy method open world face recognition problem <eos> <eop> carfusion combining point tracking part detection dynamic three dimensional reconstruction vehicles <eos> despite significant research area reconstruction multiple dynamic rigid object eg <eos> vehicles observed wide baseline uncalibrated unsynchronized cameras remains hard <eos> one hand feature tracking works well within each view but hard correspond across multiple cameras limited overlap fields view due occlusions <eos> other hand advances deep learning resulted strong detectors work across different viewpoints but still precise enough triangulation based reconstruction <eos> work develop framework fuse both single view feature tracks multi view detected part locations significantly improve detection localization reconstruction moving vehicles even presence strong occlusions <eos> demonstrate framework busy traffic intersection reconstructing over vehicles passing within minute window <eos> evaluate different components within framework compare alternate approaches such reconstruction using tracking detection <eos> <eop> deep material aware cross spectral stereo matching <eos> cross spectral imaging provides strong benefits recognition detection tasks <eos> often multiple cameras used cross spectral imaging thus requiring image alignment disparity estimation stereo setting <eos> increasingly multi camera cross spectral systems embedded active rgbd devices <eos> rgb nir cameras kinect iphone <eos> hence stereo matching also provides opportunity obtain depth without active projector source <eos> however matching image different spectral bands challenging because large appearance variations <eos> develop novel deep learning framework simultaneously transform image across spectral bands estimate disparity <eos> material aware loss function incorporated within disparity prediction network handle region unreliable matching such light sources glass windshields glossy surfaces <eos> no depth supervision required method <eos> evaluate method used vehicle mounted rgb nir stereo system collect <eos> hours video data across range areas around city <eos> experiments show method achieves strong performance reaches real time speed <eos> <eop> augmenting crowd sourced three dimensional reconstructions using semantic detections <eos> image based three dimensional reconstruction internet photo collections become robust technology produce impressive virtual representations real world scenes <eos> however several fundamental challenges remain structure motion sfm pipelines namely placement reconstruction transient object only observed single views estimating absolute scale scene suprisingly often recovering ground surfaces scene <eos> propose method jointly address remaining open problems sfm <eos> particular focus detecting people individual image accurately placing them into existing three dimensional model <eos> part placement method also estimates absolute scale scene object semantics case constitutes height distribution population <eos> further obtain smooth approximation ground surface recover gravity vector scene directly individual person detections <eos> demonstrate result approach number unordered internet photo collections quantitatively evaluate obtained absolute scene scales <eos> <eop> matryoshka network predicting three dimensional geometry via nested shape layer <eos> paper develop novel efficient encodings three dimensional geometry enable reconstructing full three dimensional shapes single image high resolution <eos> key idea pose three dimensional shape reconstruction prediction problem <eos> end first develop simple baseline network predicts entire voxel tubes each pixel reference view <eos> leveraging well proven architectures pixel prediction tasks attain state art result clearly outperforming purely voxel based approaches <eos> scale baseline higher resolutions proposing memory efficient shape encoding recursively decomposes three dimensional shape into nested shape layer similar pieces matryoshka doll <eos> allows reconstructing highly detailed shapes complex topology demonstrated extensive experiments clearly outperform previous octree based approaches despite having much simpler architecture using standard network components <eos> matryoshka network further enable reconstructing shapes ids shape similarity well shape sampling <eos> <eop> triplet center loss multi view three dimensional object retrieval <eos> most existing three dimensional object recognition algorithms focus leveraging strong discriminative power deep learning models softmax loss classification three dimensional data while learning discriminative feature deep metric learning three dimensional object retrieval more less neglected <eos> paper study variants deep metric learning losses three dimensional object retrieval did receive enough attention area <eos> first two kinds representative losses triplet loss center loss introduced could learn more discriminative feature than traditional classification loss <eos> then propose novel loss named triplet center loss further enhance discriminative power feature <eos> proposed triplet center loss learns center each class requires distances between sample centers same class closer than different classes <eos> extensive experimental result two popular three dimensional object retrieval benchmarks two widely adopted sketch based three dimensional shape retrieval benchmarks consistently demonstrate effectiveness proposed loss significant improvements achieved compared state arts <eos> <eop> learning three dimensional shape completion laser scan data weak supervision <eos> three dimensional shape completion partial point clouds fundamental problem computer vision computer graphics <eos> recent approaches characterized either data driven learning based <eos> data driven approaches rely shape model whose parameters optimized fit observations <eos> learning based approaches contrast avoid expensive optimization step instead directly predict complete shape incomplete observations using deep neural network <eos> however full supervision required often available practice <eos> work propose weakly supervised learning based approach three dimensional shape completion neither requires slow optimization nor direct supervision <eos> while also learn shape prior synthetic data amortize <eos> learn maximum likelihood fitting using deep neural network resulting efficient shape completion without sacrificing accuracy <eos> tackling three dimensional shape completion cars shapenet kitti demonstrate proposed amortized maximum likelihood approach able compete fully supervised baseline state art data driven approach while being significantly faster <eos> modelnet additionally show approach able generalize other object categories well <eos> <eop> end end learning keypoint detector descriptor pose invariant three dimensional matching <eos> finding correspondences between image three dimensional scans heart many computer vision image retrieval applications often enabled matching local keypoint descriptors <eos> various learning approaches applied past different stages matching pipeline considering detection description metric learning objectives <eos> objectives were typically addressed separately most previous work focused image data <eos> paper proposes end end learning framework keypoint detection its representation descriptor three dimensional depth maps three dimensional scans two jointly optimized towards task specific objectives without need separate annotations <eos> employ siamese architecture augmented sampling layer novel score loss function turn affects selection region proposals <eos> positive negative examples obtained automatically sampling corresponding region proposals based their consistency known three dimensional pose labels <eos> matching experiments depth data multiple benchmark datasets demonstrate efficacy proposed approach showing significant improvements over state art method <eos> <eop> ice ba incremental consistent efficient bundle adjustment visual inertial slam <eos> modern visual inertial slam vi slam achieves higher accuracy robustness than pure visual slam thanks complementariness visual feature inertial measurements <eos> however jointly using visual inertial measurements optimize slam objective functions problem high computational complexity <eos> many vi slam applications conventional optimization solvers only use very limited number recent measurements real time pose estimation cost suboptimal localization accuracy <eos> work renovate numerical solver vi slam <eos> compared conventional solvers proposal provides exact solution significantly higher computational efficiency <eos> solver allows use remarkably larger number measurements achieve higher accuracy robustness <eos> furthermore method resolves global consistency problem unaddressed many state art slam systems guarantee minimization re projection function inertial constraint function during loop closure <eos> experiments demonstrate novel formulation renders lower localization error more than speedup compared alternatives <eos> release source code implementation benefit community <eos> <eop> geonet unsupervised learning dense depth optical flow camera pose <eos> propose geonet jointly unsupervised learning framework monocular depth optical flow ego motion estimation video <eos> three components coupled nature three dimensional scene geometry jointly learned framework end end manner <eos> specifically geometric relationships extracted over predictions individual modules then combined image reconstruction loss reasoning about static dynamic scene parts separately <eos> furthermore propose adaptive geometric consistency loss increase robustness towards outliers non lambertian region resolves occlusions texture ambiguities effectively <eos> experimentation kitti driving dataset reveals scheme achieves state art result all three tasks performing better than previously unsupervised method comparably supervised ones <eos> <eop> radially distorted conjugate translations <eos> paper introduces first minimal solvers jointly solve affine rectification radial lens distortion coplanar repeated patterns <eos> even imagery moderately distorted lenses plane rectification using pinhole camera model inaccurate invalid <eos> proposed solvers incorporate lens distortion into camera model extend accurate rectification wide angle imagery now common consumer cameras <eos> solvers derived constraints induced conjugate translations imaged scene plane integrated division model radial lens distortion <eos> hidden variable trick ideal saturation used reformulate constraints so solvers generated gr bner basis method stable small fast <eos> proposed solvers used ransac based estimator <eos> rectification lens distortion recovered either one conjugately translated affine covariant feature two independently translated similarity covariant feature <eos> experiments confirm ransac accurately estimates rectification radial distortion very few iterations <eos> proposed solvers evaluated against state art affine rectification radial distortion estimation <eos> <eop> deep ordinal regression network monocular depth estimation <eos> monocular depth estimation plays crucial role understanding three dimensional scene geometry ill posed prob lem <eos> recent method gained significant improvement exploring image level information hierarchical feature deep convolutional neural network dcnns <eos> method model depth estimation regression problem train regression network minimizing mean squared error suffers slow convergence unsatisfactory local solutions <eos> besides existing depth estimation network employ repeated spatial pooling operations resulting undesirable low resolution feature maps <eos> obtain high resolution depth maps skip connections multi layer deconvolution network required complicates network training consumes much more computations <eos> eliminate least largely reduce problems introduce spacing increasing discretization sid strategy discretize depth recast depth network learning ordinal regression problem <eos> training network using ordinary regression loss method achieves much higher accuracy faster convergence synch <eos> furthermore adopt multi scale network structure avoids unnecessary spatial pooling captures multi scale information parallel <eos> proposed deep ordinal regression network dorn achieves state art result three challenging benchmarks <eos> kitti make nyu depth outperforms existing method large margin <eos> <eop> analytical modeling vanishing point curves catadioptric cameras <eos> vanishing point vanishing lines classical geometrical concepts perspective cameras lineage dating back centuries <eos> vanishing point point image space parallel lines three dimensional space appear converge whereas vanishing line passes through more vanishing point <eos> while such concepts simple intuitive perspective cameras their counterparts catadioptric cameras obtained using mirrors lenses more involved <eos> example lines three dimensional space map higher degree curves catadioptric cameras <eos> projection set three dimensional parallel lines converges single point perspective image whereas they converge more than one point catadioptric cameras <eos> best knowledge aware any systematic development analytical models vanishing point vanishing curves different types catadioptric cameras <eos> paper derive parametric equations vanishing point vanishing curves using calibration parameters mirror shape coefficients direction vectors parallel lines three dimensional space <eos> show compelling experimental result vanishing point estimation absolute pose estimation wide variety catadioptric cameras both simulations real experiments <eos> <eop> learning depth monocular video using direct method <eos> ability predict depth single image using recent advances cnn increasing interest vision community <eos> unsupervised strategies learning particularly appealing they utilize much larger varied monocular video datasets during learning without need ground truth depth stereo <eos> previous works separate pose depth cnn predictors had determined such their joint outputs minimized photometric error <eos> inspired recent advances direct visual odometry dvo argue depth cnn predictor learned without pose cnn predictor <eos> further demonstrate empirically incorporation differentiable implementation dvo along novel depth normalization strategy substantially improves performance over state art use monocular video training <eos> <eop> salience guided depth calibration perceptually optimized compressive light field three dimensional display <eos> multi layer light field displays type computational three dimensional three dimensional display recently gained increasing interest its holographic like effect natural compatibility displays <eos> however major shortcoming depth limitation still cannot overcome traditional light field modeling reconstruction based multi layer liquid crystal displays lcds <eos> considering disadvantage paper incorporates salience guided depth optimization over limited display range calibrate displayed depth present maximum area salience region multi layer light field display <eos> different previously reported cascaded light field displays use fixed initialization plane depth center display content method automatically calibrates depth initialization based salience result derived proposed contrast enhanced salience detection method <eos> experiments demonstrate proposed method provides promising advantage visual perception compressive light field displays both software simulation prototype demonstration <eos> <eop> megadepth learning single view depth prediction internet photos <eos> single view depth prediction fundamental problem computer vision <eos> recently deep learning method led significant progress but such method limited available training data <eos> current datasets based three dimensional sensors key limitations including indoor only image nyu small numbers training examples make sparse sampling kitti <eos> propose use multi view internet photo collections virtually unlimited data source generate training data via modern structure motion multi view stereo mvs method present large depth dataset called megadepth based idea <eos> data derived mvs comes its own challenges including noise unreconstructable object <eos> address challenges new data cleaning method well automatically augmenting data ordinal depth relations generated using semantic segmentation <eos> validate use large amounts internet data showing models trained megadepth exhibit strong generalization only novel scenes but also other diverse datasets including make kitti diw even when no image datasets seen during training <eos> <eop> layoutnet reconstructing three dimensional room layout single rgb image <eos> propose algorithm predict room layout single image generalizes across panoramas perspective image cuboid layouts more general layouts <eos> method operates directly panoramic image rather than decomposing into perspective image recent works <eos> network architecture similar roomnet but show improvements due aligning image based vanishing point predicting multiple layout elements corners boundaries size translation fitting constrained manhattan layout resulting predictions <eos> method compares well speed accuracy other existing work panoramas achieves among best accuracy perspective image handle both cuboid shaped more general manhattan layouts <eos> <eop> cbmv coalesced bidirectional matching volume disparity estimation <eos> recently there paradigm shift stereo matching learning based method achieving best result all popular benchmarks <eos> success method due availability training data ground truth training learning based systems datasets allowed them surpass accuracy conventional approaches based heuristics assumptions <eos> many assumptions however had validated extensively hold majority possible inputs <eos> paper generate matching volume leveraging both data ground truth conventional wisdom <eos> accomplish coalescing diverse evidence bidirectional matching process via random forest classifiers <eos> show resulting matching volume estimation method achieves similar accuracy purely data driven alternatives benchmarks generalizes unseen data much better <eos> fact result submitted kitti benchmarks were generated using classifier trained middlebury dataset <eos> <eop> zoom learn generalizing deep stereo matching novel domains <eos> despite recent success stereo matching convolutional neural network cnn remains arduous generalize pre trained deep stereo model novel domain <eos> major difficulty collect accurate ground truth disparities stereo pairs target domain <eos> work propose self adaptation approach cnn training utilizing both synthetic training data ground truth disparities stereo pairs new domain without ground truths <eos> method driven two empirical observations <eos> feeding real stereo pairs different domains stereo models pre trained synthetic data see pre trained model generalize well new domain producing artifacts boundaries ill posed region however ii feeding up sampled stereo pair leads disparity map extra details <eos> avoid while exploiting ii formulate iterative optimization problem graph laplacian regularization <eos> each iteration cnn adapts itself better new domain let cnn learn its own higher resolution output meanwhile graph laplacian regularization imposed discriminatively keep desired edges while smoothing out artifacts <eos> demonstrate effectiveness method two domains daily scenes collected smartphone cameras street views captured driving car <eos> <eop> exploring disentangled feature representation beyond face identification <eos> paper proposes learning disentangled but complementary face feature minimal supervision face identification <eos> specifically construct identity distilling dispelling auto encoder ae framework adversarially learns identity distilled feature identity verification identity dispelled feature fool verification system <eos> thanks design two stream cues learned disentangled feature represent only identity attribute but complete input image <eos> comprehensive evaluations further demonstrate proposed feature only preserve state art identity verification performance lfw but also acquire comparable discriminative power face attribute recognition celeba lfwa <eos> moreover proposed system ready semantically control face generation editing based various identities attributes unsupervised manner <eos> <eop> learning facial action units web image scalable weakly supervised clustering <eos> present scalable weakly supervised clustering approach learn facial action units aus large freely available web image <eos> unlike most existing method <eos> cnn rely fully annotated data method exploits web image inaccurate annotations <eos> specifically derive weakly supervised spectral algorithm learns embedding space couple image appearance semantics <eos> algorithm efficient gradient update scales up large quantities image stochastic extension <eos> learned embedding space adopt rank order clustering identify groups visually semantically similar image re annotate groups training au classifiers <eos> evaluation millon emotionet dataset demonstrates effectiveness approach learned annotations reach average <eos> agreement human annotations common aus classifiers trained re annotated image perform comparably sometimes even better than its supervised cnn based counterpart method offers intuitive outlier noise pruning instead forcing one annotation every image <eos> <eop> human pose estimation parsing induced learner <eos> human pose estimation still faces various difficulties challenging scenarios <eos> human parsing closely related task provide valuable cues better pose estimation however fully exploited <eos> paper propose novel parsing induced learner exploit parsing information effectively assist pose estimation learning fast adapt base pose estimation model <eos> proposed parsing induced learner composed parsing encoder pose model parameter adapter together learn predict dynamic parameters pose model extract complementary useful feature more accurate pose estimation <eos> comprehensive experiments benchmarks lip extended pascal person part show proposed parsing induced learner improve performance both single multi person pose estimation new state art <eos> cross dataset experiments also show proposed parsing induced learner lip dataset accelerate learning human pose estimation model mpii benchmark addition achieving outperforming performance <eos> <eop> multi level factorisation net person re identification <eos> key effective person re identification re id modelling discriminative view invariant factors person appearance both high low semantic levels <eos> recently developed deep re id models either learn holistic single semantic level feature representation require laborious human annotation factors attributes <eos> propose multi level factorisation net mlfn novel network architecture factorises visual appearance person into latent discriminative factors multiple semantic levels without manual annotation <eos> mlfn composed multiple stacked blocks <eos> each block contains multiple factor modules model latent factors specific level factor selection modules dynamically select factor modules interpret content each input image <eos> outputs factor selection modules also provide compact latent factor descriptor complementary conventional deeply learned feature <eos> mlfn achieves state art result three re id datasets well compelling result general object categorisation cifar dataset <eos> <eop> attention aware compositional network person re identification <eos> person re identification reid identify pedestrians observed different camera views based visual appearance <eos> challenging task due large pose variations complex background clutters severe occlusions <eos> recently human pose estimation predicting joint locations was largely improved accuracy <eos> reasonable use pose estimation result handling pose variations background clutters such attempts obtained great improvement reid performance <eos> however argue pose information was well utilized hasn yet fully exploited person reid <eos> work introduce novel framework called attention aware compositional network aacn person reid <eos> aacn consists two main components pose guided part attention ppa attention aware feature composition afc <eos> ppa learned applied mask out undesirable background feature pedestrian feature maps <eos> furthermore pose guided visibility scores estimated body parts deal part occlusion proposed afc module <eos> extensive experiments ablation analysis show effectiveness method state art result achieved several public datasets including market cuhk cuhk sensereid cuhk np dukemtmc reid <eos> <eop> look boundary boundary aware face alignment algorithm <eos> present novel boundary aware face alignment algorithm utilising boundary lines geometric structure human face help facial landmark localisation <eos> unlike conventional heatmap based method regression based method approach derives face landmarks boundary lines remove ambiguities landmark definition <eos> three questions explored answered work <eos> why using boundary <eos> how use boundary <eos> relationship between boundary estimation landmarks localisation boundary aware face alignment algorithm achieves <eos> mean error fullset outperforms state art method large margin <eos> method also easily integrate information other datasets <eos> utilising boundary information dataset method achieves <eos> failure rate cofw dataset <eos> mean error aflw full dataset <eos> moreover propose new dataset wflw unify training testing across different factors including poses expressions illuminations makeups occlusions blurriness <eos> dataset model publicly available wywu <eos> io projects lab lab <eos> html <eop> demo vec reasoning object affordances online video <eos> watching expert demonstrations important way humans robots reason about affordances unseen object <eos> paper consider problem reasoning object affordances through feature embedding demonstration video <eos> design demo vec model learns extract embedded vectors demonstration video predicts interaction region action label target image same object <eos> introduce online product review dataset affordance opra collecting labeling diverse youtube product review video <eos> demo vec model outperforms various recurrent neural network baselines collected dataset <eos> <eop> monocular three dimensional pose shape estimation multiple people natural scenes importance multiple scene constraints <eos> human sensing greatly benefited recent advances deep learning parametric human modeling large scale datasets <eos> however existing models make strong assumptions about scene considering either single person per image full views person simple background many cameras <eos> paper leverage state art deep multi task neural network parametric human scene modeling towards fully automatic monocular visual sensing system multiple interacting people infers pose shape multiple people single image relying detailed semantic representations both model image level guide combined optimization feedforward feedback components ii automatically integrates scene constraints including ground plane support simultaneous volume occupancy multiple people iii extends single image model video optimally solving temporal person assignment problem imposing coherent temporal pose motion reconstructions while preserving image alignment fidelity <eos> perform experiments both single multi person datasets systematically evaluate each component model showing improved performance extensive multiple human sensing capability <eos> also apply method image multiple people severe occlusions diverse backgrounds captured challenging natural scenes obtain result good perceptual quality <eos> <eop> human sensing action emotion recognition robot assisted therapy children autism <eos> introduce new fine grained action emotion recognition tasks defined non staged video recorded during robot assisted therapy sessions children autism <eos> tasks present several challenges large dataset long video large number highly variable actions children only partially visible different ages may show unpredictable behaviour well non standard camera viewpoints <eos> investigate how state art human pose reconstruction method perform newly introduced tasks propose extensions adapt them deal challenges <eos> also analyze multiple approaches action emotion recognition human pose data establish several baselines discuss result their implications context child robot interaction <eos> <eop> facial expression recognition de expression residue learning <eos> facial expression combination expressive component neutral component person <eos> paper propose recognize facial expressions extracting information expressive component through de expression learning procedure called de expression residue learning derl <eos> first generative model trained cgan <eos> model generates corresponding neutral face image any input face image <eos> call procedure de expression because expressive information filtered out generative model however expressive information still recorded intermediate layer <eos> given neutral face image unlike previous works using pixel level feature level difference facial expression classification new method learns deposition residue remains intermediate layer generative model <eos> such residue essential contains expressive component deposited generative model any input facial expression image <eos> seven public facial expression databases employed experiments <eos> two databases bu dfe bp spontaneous pre training derl method evaluated five databases ck oulu casia mmi bu dfe bp <eos> experimental result demonstrate superior performance proposed method <eos> <eop> causal graph model visibility fluent reasoning tracking interacting object <eos> tracking humans interacting other subjects environment remains unsolved visual tracking because visibility human interests video unknown might vary over time <eos> particular still difficult state art human trackers recover complete human trajectories crowded scenes frequent human interactions <eos> work consider visibility status subject fluent variable whose change mostly attributed subject interaction surrounding <eos> crossing behind another object entering building getting into vehicle etc <eos> introduce causal graph aog represent causal effect relations between object visibility fluent its activities develop probabilistic graph model jointly reason visibility fluent change <eos> visible invisible track humans video <eos> formulate joint task iterative search feasible causal graph structure enables fast search algorithm <eos> dynamic programming method <eos> apply proposed method challenging video sequences evaluate its capabilities estimating visibility fluent changes subjects tracking subjects interests over time <eos> result comparisons demonstrate method outperforms alternative trackers recover complete trajectories humans complicated scenarios frequent human interactions <eos> <eop> weakly supervised facial action unit recognition through adversarial training <eos> current works facial action unit au recognition typically require fully au annotated facial image supervised au classifier training <eos> au annotation time consuming expensive error prone process <eos> while aus hard annotate facial expression relatively easy label <eos> furthermore there exist strong probabilistic dependencies between expressions aus well dependencies among aus <eos> such dependencies referred domain knowledge <eos> paper propose novel au recognition method learns au classifiers domain knowledge expression annotated facial image through adversarial training <eos> specifically first generate pseudo au labels according probabilistic dependencies between expressions aus well correlations among aus summarized domain knowledge <eos> then propose weakly supervised au recognition method via adversarial process simultaneously train two models recognition model learns au classifiers discrimination model estimates probability au labels generated domain knowledge rather than recognized au labels <eos> training procedure maximizes probability making mistake <eos> leveraging adversarial mechanism distribution recognized aus closed au prior distribution domain knowledge <eos> furthermore proposed weakly supervised au recognition extended semi supervised learning scenarios partially au annotated image <eos> experimental result three benchmark databases demonstrate proposed method successfully leverages summarized domain knowledge weakly supervised au classifier learning through adversarial process thus achieves state art performance <eos> <eop> non linear temporal subspace representations activity recognition <eos> representations compactly effectively capture temporal evolution semantic content important computer vision machine learning algorithms operate multi variate time series data <eos> investigate such representations motivated task human action recognition <eos> here each data instance encoded multivariate feature such via deep cnn action dynamics characterized their variations time <eos> feature often non linear propose novel pooling method kernelized rank pooling represents given sequence compactly pre image parameters hyperplane reproducing kernel hilbert space projections data onto captures their temporal order <eos> develop idea further show such pooling scheme cast order constrained kernelized pca objective <eos> then propose use parameters kernelized low rank feature subspace representation sequences <eos> cast formulation optimization problem generalized grassmann manifolds then solve efficiently using riemannian optimization techniques <eos> present experiments several action recognition datasets using diverse feature modalities demonstrate state art result <eos> <eop> towards pose invariant face recognition wild <eos> pose variation one key challenge face recognition <eos> opposed current techniques pose invariant face recognition either directly extract pose invariant feature recognition first normalize profile face image frontal pose before feature extraction argue more desirable perform both tasks jointly allow them benefit each other <eos> end propose pose invariant model pim face recognition wild three distinct novelties <eos> first pim novel unified deep architecture containing face frontalization sub net ffn discriminative learning sub net dln jointly learned end end <eos> second ffn well designed dual path generative adversarial network gan simultaneously perceives global structures local details incorporated unsupervised cross domain adversarial training learning learn strategy high fidelity identity preserving frontal view synthesis <eos> third dln generic convolutional neural network cnn face recognition enforced cross entropy optimization strategy learning discriminative yet generalized feature representation <eos> qualitative quantitative experiments both controlled wild benchmarks demonstrate superiority proposed model over state arts <eos> <eop> unifying identification context learning person recognition <eos> despite great success face recognition techniques recognizing persons under unconstrained settings remains challenging <eos> issues like profile views unfavorable lighting occlusions cause substantial difficulties <eos> previous works attempted tackle problem exploiting context <eos> clothes social relations <eos> while showing promising improvement they usually limited two important aspects relying simple heuristics combine different cues separating construction context people identities <eos> work aim move beyond such limitations propose new framework leverage context person recognition <eos> particular propose region attention network learned adaptively combine visual cues instance dependent weights <eos> also develop unified formulation social contexts learned along reasoning people identities <eos> models substantially improve robustness when working complex contextual relations unconstrained environments <eos> two large datasets pipa cast movies cim new dataset proposed work method consistently achieves state art performance under multiple evaluation policies <eos> <eop> jointly optimize data augmentation network training adversarial data augmentation human pose estimation <eos> random data augmentation critical technique avoid overfitting training deep models <eos> yet data augmentation network training often two isolated processes most settings yielding suboptimal training <eos> why jointly optimize two propose adversarial data augmentation address limitation <eos> key idea design generator <eos> augmentation network competes against discriminator <eos> target network generating hard examples online <eos> generator explores weaknesses discriminator while discriminator learns hard augmentations achieve better performance <eos> reward penalty strategy also proposed efficient joint training <eos> investigate human pose estimation carry out comprehensive ablation studies validate method <eos> result prove method effectively improve state art models without additional data effort <eos> <eop> wing loss robust facial landmark localisation convolutional neural network <eos> present new loss function namely wing loss robust facial landmark localisation convolutional neural network cnn <eos> first compare analyse different loss functions including smooth <eos> analysis loss functions suggests training cnn based localisation model more attention should paid small medium range errors <eos> end design piece wise loss function <eos> new loss amplifies impact errors interval switching loss modified logarithm function <eos> address problem under representation sample large out plane head rotations training set propose simple but effective boosting strategy referred pose based data balancing <eos> particular deal data imbalance problem duplicating minority training sample perturbing them injecting random image rotation bounding box translation other data augmentation approaches <eos> last proposed approach extended create two stage framework robust facial landmark localisation <eos> experimental result obtained aflw demonstrate merits wing loss function prove superiority proposed method over state art approaches <eos> <eop> multiple granularity group interaction prediction <eos> most human activity analysis works <eos> recognition prediction only focus single granularity <eos> either modelling global motion based coarse level movement such human trajectories forecasting future detailed action based body parts movement such skeleton motion <eos> contrast work propose multi granularity interaction prediction network integrates both global motion detailed local action <eos> built bi directional lstm network proposed method possesses between granularities links encourage feature sharing well cross feature consistency between both global local granularity <eos> trajectory local action turn predict long term global location local dynamics each individual <eos> validate method several public datasets promising performance <eos> <eop> social gan socially acceptable trajectories generative adversarial network <eos> understanding human motion behavior critical autonomous moving platforms like self driving cars social robots if they navigate human centric environments <eos> challenging because human motion inherently multimodal given history human motion paths there many socially plausible ways people could move future <eos> tackle problem combining tools sequence prediction generative adversarial network recurrent sequence sequence model observes motion histories predicts future behavior using novel pooling mechanism aggregate information across people <eos> predict socially plausible futures training adversarially against recurrent discriminator encourage diverse predictions novel variety loss <eos> through experiments several datasets demonstrate approach outperforms prior work terms accuracy variety collision avoidance computational complexity <eos> <eop> deep group shuffling random walk person re identification <eos> person re identification aims finding person interest image gallery comparing probe image person all gallery image <eos> generally treated retrieval problem affinities between probe image gallery image affinities used rank retrieved gallery image <eos> however most existing method only consider affinities but ignore affinities between all gallery image affinity <eos> some frameworks incorporated affinities into testing process end end trainable deep neural network <eos> paper propose novel group shuffling random walk network fully utilizing affinity information between gallery image both training testing processes <eos> proposed approach aims end end refining affinities based affinity information simple yet effective matrix operation integrated into deep neural network <eos> feature grouping group shuffle also proposed apply rich supervisions learning better person feature <eos> proposed approach outperforms state art method market cuhk dukemtmc datasets large margins demonstrate effectiveness approach <eos> <eop> transferable joint attribute identity deep learning unsupervised person re identification <eos> most existing person re identification re id method require supervised model learning separate large set pairwise labelled training data every single camera pair <eos> significantly limits their scalability usability real world large scale deployments need performing re id across many camera views <eos> address scalability problem develop novel deep learning method transferring labelled information existing dataset new unseen unlabelled target domain person re id without any supervised learning target domain <eos> specifically introduce transferable joint attribute identity deep learning tj aidl simultaneously learning attribute semantic identitydiscriminative feature representation space transferrable any new unseen target domain re id tasks without need collecting new labelled training data target domain <eos> unsupervised learning target domain <eos> extensive comparative evaluations validate superiority new tj aidl model unsupervised person re id over wide range state art method four challenging benchmarks including viper prid market dukemtmc reid <eos> <eop> harmonious attention network person re identification <eos> existing person re identi cation re id method either assume availability well aligned person bounding box image model input rely constrained attention selection mechanisms calibrate misaligned image <eos> they therefore sub optimal re id matching arbitrarily aligned person image potentially large human pose variations unconstrained auto detection errors <eos> work show advantages jointly learning attention selection feature representation convolutional neural network cnn maximising complementary information different levels visual attention subject re id discriminative learning constraints <eos> speci cally formulate novel harmonious attention cnn ha cnn model joint learning soft pixel attention hard regional attention along simultaneous optimisation feature representations dedicated optimise person re id uncontrolled misaligned image <eos> extensive comparative evaluations validate superiority new hacnn model person re id over wide variety state art method three large scale benchmarks including cuhk market dukemtmc reid <eos> <eop> real time rotation invariant face detection progressive calibration network <eos> rotation invariant face detection <eos> detecting faces arbitrary rotation plane rip angles widely required unconstrained applications but still remains challenging task due large variations face appearances <eos> most existing method compromise speed accuracy handle large rip variations <eos> address problem more efficiently propose progressive calibration network pcn perform rotation invariant face detection coarse fine manner <eos> pcn consists three stages each only distinguishes faces non faces but also calibrates rip orientation each face candidate upright progressively <eos> dividing calibration process into several progressive steps only predicting coarse orientations early stages pcn achieve precise fast calibration <eos> performing binary classification face vs <eos> non face gradually decreasing rip ranges pcn accurately detect faces full circ rip angles <eos> such designs lead real time rotation invariant face detector <eos> experiments multi oriented fddb challenging subset wider face containing rotated faces wild show pcn achieves quite promising performance <eos> <eop> deep regression forests age estimation <eos> age estimation facial image typically cast nonlinear regression problem <eos> main challenge problem facial feature space <eos> ages inhomogeneous due large variation facial appearance across different persons same age non stationary property aging patterns <eos> paper propose deep regression forests drfs end end model age estimation <eos> drfs connect split nodes fully connected layer convolutional neural network cnn deal inhomogeneous data jointly learning input dependant data partitions split nodes data abstractions leaf nodes <eos> joint learning follows alternating strategy first fixing leaf nodes split nodes well cnn parameters optimized back propagation then fixing split nodes leaf nodes optimized iterating step size free update rule derived variational bounding <eos> verify proposed drfs three standard age estimation benchmarks achieve state art result all them <eos> <eop> weakly supervised deep convolutional neural network learning facial action unit intensity estimation <eos> facial action unit au intensity estimation plays important role affective computing human computer interaction <eos> recent works introduced deep neural network au intensity estimation but they require large amount intensity annotations <eos> au annotation needs strong domain expertise expensive construct large database learn deep models <eos> propose novel knowledge based semi supervised deep convolutional neural network au intensity estimation extremely limited au annotations <eos> only intensity annotations peak valley frames training sequences needed <eos> provide additional supervision model learning exploit naturally existing constraints aus including relative appearance similarity temporal intensity ordering facial symmetry contrastive appearance difference <eos> experimental evaluations performed two public benchmark databases <eos> around intensity annotations fera around disfa training method achieve comparable even better performance than state art method use intensity annotations training set <eos> <eop> memory based online learning deep representations video streams <eos> present novel online unsupervised method face identity learning video streams <eos> method exploits deep face descriptors together memory based learning mechanism takes advantage temporal coherence visual data <eos> specifically introduce discriminative descriptor matching solution based reverse nearest neighbour forgetting strategy detect redundant descriptors discard them appropriately while time progresses <eos> shown proposed learning procedure asymptotically stable effectively used relevant applications like multiple face identification tracking unconstrained video streams <eos> experimental result show proposed method achieves comparable result task multiple face tracking better performance face identification offline approaches exploiting future information <eos> code will publicly available <eos> <eop> efficient deep person re identification using multi level similarity <eos> person re identification reid requires comparing two image person captured under different conditions <eos> existing work based neural network often computes similarity feature maps one single convolutional layer <eos> work propose efficient end end fully convolutional siamese network computes similarities multiple levels <eos> demonstrate multi level similarity improve accuracy considerably using low complexity network structures reid problem <eos> specifically first use several convolutional layer extract feature two input image <eos> then propose convolution similarity network compute similarity score maps inputs <eos> use spatial transformer network stns determine spatial attention <eos> propose apply efficient depth wise convolution compute similarity <eos> proposed convolution similarity network inserted into different convolutional layer extract visual similarities different levels <eos> furthermore use improved ranking loss further improve performance <eos> work first propose compute visual similarities low middle high levels reid <eos> extensive experiments analysis demonstrate system compact yet effective achieve competitive result much smaller model size computational complexity <eos> <eop> multi level fusion based three dimensional object detection monocular image <eos> paper present end end deep learning based framework three dimensional object detection single monocular image <eos> deep convolutional neural network introduced simultaneous three dimensional object detection <eos> first region proposals generated through region proposal network <eos> then shared feature learned within proposals predict class probability bounding box orientation dimension three dimensional location <eos> adopt stand alone module predict disparity extract feature computed point cloud <eos> thus feature original image point cloud will fused different levels accurate three dimensional localization <eos> estimated disparity also used front view feature encoding enhance input image regarded input fusionprocess <eos> proposed algorithm directly output both three dimensional object detection result end end fashion only single rgb image input <eos> experimental result challenging kitti benchmark demonstrate algorithm signi cantly outperforms state art method only monocular image <eos> <eop> perceptual measure deep single image camera calibration <eos> most current single image camera calibration method rely specific image feature user input cannot applied natural image captured uncontrolled settings <eos> propose inferring directly camera calibration parameters single image using deep convolutional neural network <eos> network trained using automatically generated sample large scale panorama dataset considerably outperforms other method including recent deep learning based approaches terms standard error <eos> however argue many cases more important consider how humans perceive errors camera estimation <eos> end conduct large scale human perception study ask users judge realism three dimensional object composited without ground truth camera calibration <eos> based study develop new perceptual measure camera calibration demonstrate deep calibration network outperforms other method measure <eos> finally demonstrate use calibration network number applications including virtual object insertion image retrieval compositing <eos> <eop> learning generate time lapse video using multi stage dynamic generative adversarial network <eos> taking photo outside predict immediate future <eos> how would cloud move sky address problem presenting generative adversarial network gan based two stage approach generating realistic time lapse video high resolution <eos> given first frame model learns generate long term future frames <eos> first stage generates video realistic contents each frame <eos> second stage refines generated video first stage enforcing closer real video regard motion dynamics <eos> further encourage vivid motion final generated video gram matrix employed model motion more precisely <eos> build large scale time lapse dataset test approach new dataset <eos> using model able generate realistic video up imes resolution frames <eos> quantitative qualitative experiment result demonstrated superiority model over state art models <eos> <eop> document enhancement using visibility detection <eos> paper re visits classical problems document enhancement <eos> rather than proposing new algorithm specific problem introduce novel general approach <eos> key idea modify any state art algorithm providing new information input improving its own result <eos> interestingly information based solution seemingly unrelated problem visibility detection <eos> show simple representation image three dimensional point cloud gives visibility detection cloud new interpretation <eos> mean point visible although question widely studied within computer vision always assumed point set sampling real scene <eos> show answer question context reveals unique useful information about image <eos> demonstrate benefit idea document binarization unshadowing <eos> <eop> weighted sparse sampling smoothing frame transition approach semantic fast forward first person video <eos> thanks advances technology low cost digital cameras popularity self recording culture amount visual data internet going opposite side available time patience users <eos> thus most uploaded video doomed forgotten unwatched computer folder website <eos> work address problem creating smooth fast forward video without losing relevant content <eos> present new adaptive frame selection formulated weighted minimum reconstruction problem combined smoothing frame transition method accelerates first person video emphasizing relevant segments avoids visual discontinuities <eos> experiments show method able fast forward video retain much relevant information smoothness state art techniques less time <eos> also present new hour multimodal rgb imu gps dataset first person video annotations recorder profile frame scene activities interaction attention <eos> <eop> context contrasted feature gated multi scale aggregation scene segmentation <eos> scene segmentation challenging task need label every pixel image <eos> crucial exploit discriminative context aggregate multi scale feature achieve better segmentation <eos> paper first propose novel context contrasted local feature only leverages informative context but also spotlights local information contrast context <eos> proposed context contrasted local feature greatly improves parsing performance especially inconspicuous object background stuff <eos> furthermore propose scheme gated sum selectively aggregate multi scale feature each spatial position <eos> gates scheme control information flow different scale feature <eos> their values generated testing image proposed network learnt training data so they adaptive only training data but also specific testing image <eos> without bells whistles proposed approach achieves state arts consistently three popular scene segmentation datasets pascal context sun rgbd coco stuff <eos> <eop> deep layer aggregation <eos> visual recognition requires rich representations span levels low high scales small large resolutions fine coarse <eos> even depth feature convolutional network layer isolation enough compounding aggregating representations improves inference <eos> architectural efforts exploring many dimensions network backbones designing deeper wider architectures but how best aggregate layer blocks across network deserves further attention <eos> although skip connections incorporated combine layer connections shallow themselves only fuse simple one step operations <eos> augment standard architectures deeper aggregation better fuse information across layer <eos> deep layer aggregation structures iteratively hierarchically merge feature hierarchy make network better accuracy fewer parameters <eos> experiments across architectures tasks show deep layer aggregation improves recognition resolution compared existing branching merging schemes <eos> <eop> convolutional neural network alternately updated clique <eos> improving information flow deep network helps ease training difficulties utilize parameters more efficiently <eos> here propose new convolutional neural network architecture alternately updated clique cliquenet <eos> contrast prior network there both forward backward connections between any two layer same block <eos> layer constructed loop updated alternately <eos> cliquenet some unique properties <eos> each layer both input output any other layer same block so information flow among layer maximized <eos> during propagation newly updated layer concatenated re update previously updated layer parameters reused multiple times <eos> recurrent feedback structure able bring higher level visual information back refine low level filters achieve spatial attention <eos> analyze feature generated different stages observe using refined feature leads better result <eos> adopt multi scale feature strategy effectively avoids progressive growth parameters <eos> experiments image recognition datasets including cifar cifar svhn imagenet show proposed models achieve state art performance fewer parameters <eos> <eop> practical block wise neural network architecture generation <eos> convolutional neural network gained remarkable success computer vision <eos> however most usable network architectures hand crafted usually require expertise elaborate design <eos> paper provide block wise network generation pipeline called blockqnn automatically builds high performance network using learning paradigm epsilon greedy exploration strategy <eos> optimal network block constructed learning agent trained sequentially choose component layer <eos> stack block construct whole auto generated network <eos> accelerate generation process also propose distributed asynchronous framework early stop strategy <eos> block wise generation brings unique advantages performs competitive result comparison hand crafted state art network image classification additionally best network generated blockqnn achieves <eos> top error rate cifar beats all existing auto generate network <eos> meanwhile offers tremendous reduction search space designing network only spends days gpus moreover strong generalizability network built cifar also performs well larger scale imagenet dataset <eos> <eop> xunit learning spatial activation function efficient image restoration <eos> recent years deep neural network dnns achieved unprecedented performance many low level vision tasks <eos> however state art result typically achieved very deep network reach tens layer tens millions parameters <eos> make dnns implementable platforms limited resources necessary weaken tradeoff between performance efficiency <eos> paper propose new activation unit particularly suitable image restoration problems <eos> contrast widespread per pixel activation units like relus sigmoids unit implements learnable nonlinear function spatial connections <eos> enables net capture much more complex feature thus requiring significantly smaller number layer order reach same performance <eos> illustrate effectiveness units through experiments state art nets denoising de raining super resolution already considered very small <eos> approach able further reduce models nearly without incurring any degradation performance <eos> <eop> crafting toolchain image restoration deep reinforcement learning <eos> investigate novel approach image restoration reinforcement learning <eos> unlike existing studies mostly train single large network specialized task prepare toolbox consisting small scale convolutional network different complexities specialized different tasks <eos> method rl restore then learns policy select appropriate tools toolbox progressively restore quality corrupted image <eos> formulate step wise reward function proportional how well image restored each step learn action policy <eos> also devise joint learning scheme train agent tools better performance handling uncertainty <eos> comparison conventional human designed network rl restore capable restoring image corrupted complex unknown distortions more parameter efficient manner using dynamically formed toolchain <eos> <eop> deformation aware image compression <eos> lossy compression algorithms aim compactly encode image way enables restore them minimal error <eos> show key limitation existing algorithms they rely error measures extremely sensitive geometric deformations <eos> force encoder invest many bits describing exact geometry every fine detail image obviously wasteful because human visual system indifferent small local translations <eos> motivated observation propose deformation insensitive error measure easily incorporated into any existing compression scheme <eos> show optimal compression under criterion involves slightly deforming input image such becomes more compressible <eos> surprisingly while small deformations barely noticeable they enable codec preserve details otherwise completely lost <eos> technique uses codec black box thus allowing simple integration arbitrary compression method <eos> extensive experiments including user studies confirm approach significantly improves visual quality many codecs <eos> include jpeg jpeg webp bpg recent deep net method <eos> <eop> distributable consistent multi object matching <eos> paper propose optimization based framework multiple object matching <eos> framework takes maps computed between pairs object input outputs maps consistent among all pairs object <eos> central idea approach divide input object collection into overlapping sub collections enforce map consistency among each sub collection <eos> leads distributed formulation scalable large scale datasets <eos> also present equivalence condition between decoupled scheme original scheme <eos> experiments both synthetic real world datasets show framework competitive against state art multi object matching techniques <eos> <eop> residual dense network image super resolution <eos> paper propose dense feature fusion dff image super resolution sr <eos> same content different natural image often various scales angles view jointly leaning hierarchical feature essential image sr <eos> other hand very deep convolutional neural network cnn recently achieved great success image sr offered hierarchical feature well <eos> however most deep cnn based sr models neglect jointly make full use hierarchical feature <eos> addition dense connected layer would allow network deeper efficient train more powerful <eos> embrace observations proposed dff model fully exploit all meaningful convolutional feature local global manners <eos> specifically use dense connected convolutional layer extract abundant local feature <eos> use local feature fusion adaptively learn more efficient feature preceding current local feature <eos> after fully obtaining dense local feature use global feature fusion jointly adaptively learn global hierarchical feature holistic way <eos> extensive experiments benchmark datasets show dff achieves favorable performance against state art method quantitatively visually <eos> <eop> attentive generative adversarial network raindrop removal single image <eos> raindrops adhered glass window camera lens severely hamper visibility background scene degrade image considerably <eos> paper address problem visually removing raindrops thus transforming raindrop degraded image into clean one <eos> problem intractable since first region occluded raindrops given <eos> second information about background scene occluded region completely lost most part <eos> resolve problem apply attentive generative network using adversarial training <eos> main idea inject visual attention into both generative discriminative network <eos> during training visual attention learns about raindrop region their surroundings <eos> hence injecting information generative network will pay more attention raindrop region surrounding structures discriminative network will able assess local consistency restored region <eos> injection visual attention both generative discriminative network main contribution paper <eos> experiments show effectiveness approach outperforms state art method quantitatively qualitatively <eos> <eop> fsrnet end end learning face super resolution facial priors <eos> face super resolution sr domain specific superresolution problem <eos> facial prior knowledge leveraged better super resolve face image <eos> present novel deep end end trainable face super resolution network fsrnet makes use geometry prior <eos> facial landmark heatmaps parsing maps superresolve very low resolution lr face image without wellaligned requirement <eos> specifically first construct coarse sr network recover coarse high resolution hr image <eos> then coarse hr image sent two branches fine sr encoder prior information estimation network extracts image feature estimates landmark heatmaps parsing maps respectively <eos> both image feature prior information sent fine sr decoder recover hr image <eos> generate realistic faces also propose face super resolution generative adversarial network fsrgan incorporate adversarial loss into fsrnet <eos> further introduce two related tasks face alignment parsing new evaluation metrics face sr address inconsistency classic metrics <eos> extensive experiments show fsrnet fsrgan significantly outperforms state arts very lr face sr both quantitatively qualitatively <eos> <eop> burst denoising kernel prediction network <eos> present technique jointly denoising bursts image taken handheld camera <eos> particular propose convolutional neural network architecture predicting spatially varying kernels both align denoise frames synthetic data generation approach based realistic noise formation model optimization guided annealed loss function avoid undesirable local minima <eos> model matches outperforms state art across wide range noise levels both real synthetic data <eos> <eop> unsupervised sparse dirichlet net hyperspectral image super resolution <eos> many computer vision applications obtaining image high resolution both spatial spectral domains equally important <eos> however due hardware limitations one only expect acquire image high resolution either spatial spectral domains <eos> paper focuses hyperspectral image super resolution hsi sr hyperspectral image hsi low spatial resolution lr but high spectral resolution fused multispectral image msi high spatial resolution hr but low spectral resolution obtain hr hsi <eos> existing deep learning based solutions all supervised would need large training set availability hr hsi unrealistic <eos> here make first attempt solving hsi sr problem using unsupervised encoder decoder architecture carries following uniquenesses <eos> first composed two encoder decoder network coupled through shared decoder order preserve rich spectral information hsi network <eos> second network encourages representations both modalities follow sparse dirichlet distribution naturally incorporates two physical constraints hsi msi <eos> third angular difference between representations minimized order reduce spectral distortion <eos> refer proposed architecture unsupervised sparse dirichlet net usdn <eos> extensive experimental result demonstrate superior performance usdn compared state art <eos> <eop> dynamic scene deblurring using spatially variant recurrent neural network <eos> due spatially variant blur caused camera shake object motions under different scene depths deblurring image captured dynamic scenes challenging <eos> although recent works based deep neural network shown great progress problem their models usually large computationally expensive <eos> paper propose novel spatially variant neural network address problem <eos> proposed network composed three deep convolutional neural network cnn recurrent neural network rnn <eos> rnn used deconvolution operator performed feature maps extracted input image one cnn <eos> another cnn used learn weights rnn every location <eos> result rnn spatially variant could implicitly model deblurring process spatially variant kernels <eos> third cnn used reconstruct final deblurred feature maps into restored image <eos> whole network end end trainable <eos> analysis shows proposed network large receptive field even small model size <eos> quantitative qualitative evaluations public datasets demonstrate proposed method performs favorably against state art algorithms terms accuracy speed model size <eos> <eop> splatnet sparse lattice network point cloud processing <eos> present network architecture processing point clouds directly operates collection point represented sparse set sample high dimensional lattice <eos> naively applying convolutions lattice scales poorly both terms memory computational cost size lattice increases <eos> instead network uses sparse bilateral convolutional layer building blocks <eos> layer maintain efficiency using indexing structures apply convolutions only occupied parts lattice allow flexible specifications lattice structure enabling hierarchical spatially aware feature learning well joint three dimensional reasoning <eos> both point based image based representations easily incorporated network such layer resulting model trained end end manner <eos> present result three dimensional segmentation tasks approach outperforms existing state art techniques <eos> <eop> surface network <eos> study data driven representations three dimensional triangle meshes one prevalent object used represent three dimensional geometry <eos> recent works developed models exploit intrinsic geometry manifolds graphs namely graph neural network gnns its spectral variants learn local metric tensor via laplacian operator <eos> despite offering excellent sample complexity built invariances intrinsic geometry alone invariant isometric deformations making unsuitable many applications <eos> overcome limitation propose several upgrades gnns leverage extrinsic differential geometry properties three dimensional surfaces increasing its modeling power <eos> particular propose exploit dirac operator whose spectrum detects principal curvature directions stark contrast classical laplace operator directly measures mean curvature <eos> coin resulting models emph surface network sn <eos> prove models define shape representations stable deformation discretization demonstrate efficiency versatility sns two challenging tasks temporal prediction mesh deformations under non linear dynamics generative models using variational autoencoder framework encoders decoders given sns <eos> <eop> self supervised multi level face model learning monocular reconstruction over hz <eos> reconstruction dense three dimensional models face geometry appearance single image highly challenging ill posed <eos> constrain problem many approaches rely strong priors such parametric face models learned limited three dimensional scan data <eos> however prior models restrict generalization true diversity facial geometry skin reflectance illumination <eos> alleviate problem present first approach jointly learns regressor face shape expression reflectance illumination basis concurrently learned parametric face model <eos> multi level face model combines advantage three dimensional morphable models regularization out space generalization learned corrective space <eos> train end end wild image without dense annotations fusing convolutional encoder differentiable expert designed renderer self supervised training loss both defined multiple detail levels <eos> approach compares favorably state art terms reconstruction quality better generalizes real world faces runs over hz <eos> <eop> codeslam learning compact optimisable representation dense visual slam <eos> representation geometry real time three dimensional perception systems continues critical research issue <eos> dense maps capture complete surface shape augmented semantic labels but their high dimensionality makes them computationally costly store process unsuitable rigorous probabilistic inference <eos> sparse feature based representations avoid problems but capture only partial scene information mainly useful localisation only <eos> present new compact but dense representation scene geometry conditioned intensity data single image generated code consisting small number parameters <eos> inspired work both learned depth image auto encoders <eos> approach suitable use keyframe based monocular dense slam system while each keyframe code produce depth map code optimised efficiently jointly pose variables together codes overlapping keyframes attain global consistency <eos> conditioning depth map image allows code only represent aspects local geometry cannot directly predicted image <eos> explain how learn code representation demonstrate its advantageous properties monocular slam <eos> <eop> sgpn similarity group proposal network three dimensional point cloud instance segmentation <eos> introduce similarity group proposal network sgpn simple intuitive deep learning framework three dimensional object instance segmentation point clouds <eos> sgpn uses single network predict point grouping proposals corresponding semantic class each proposal directly extract instance segmentation result <eos> important effectiveness sgpn its novel representation three dimensional instance segmentation result form similarity matrix indicates similarity between each pair point embedded feature space thus producing accurate grouping proposal each point <eos> best knowledge sgpn first framework learn three dimensional instance aware semantic segmentation point clouds <eos> experimental result various three dimensional scenes show effectiveness method three dimensional instance segmentation also evaluate capability sgpn improve three dimensional object detection semantic segmentation result <eos> also demonstrate its flexibility seamlessly incorporating cnn feature into framework boost performance <eos> <eop> planenet piece wise planar reconstruction single rgb image <eos> paper proposes deep neural network dnn piece wise planar depthmap reconstruction single rgb image <eos> while dnns brought remarkable progress single image pixel wise depth prediction piece wise planar depthmap reconstruction requires structured geometry representation difficult task master even dnns <eos> proposed end end dnn learns directly infer set plane parameters corresponding plane segmentation masks single rgb image <eos> generated more than piece wise planar depth maps training testing scannet large scale indoor capture database <eos> qualitative quantitative evaluations demonstrate proposed approach outperforms baseline method terms both plane segmentation depth estimation accuracy <eos> best knowledge paper presents first end end neural architecture piece wise planar reconstruction single rgb image <eos> <eop> deep parametric continuous convolutional neural network <eos> standard convolutional neural network assume grid structured input available exploit discrete convolutions their fundamental building blocks <eos> limits their applicability many real world applications <eos> paper propose parametric continuous convolution new learnable operator operates over non grid structured data <eos> key idea exploit parameterized kernel functions span full continuous vector space <eos> generalization allows learn over arbitrary data structures long their support relationship computable <eos> experiments show significant improvement over state art point cloud segmentation indoor outdoor scenes lidar motion estimation driving scenes <eos> <eop> feastnet feature steered graph convolutions three dimensional shape analysis <eos> convolutional neural network cnn massively impacted visual recognition image now ubiquitous state art approaches <eos> cnn easily extend however data represented regular grids such three dimensional shape meshes other graph structured data traditional local convolution operators directly apply <eos> address problem propose novel graph convolution operator establish correspondences between filter weights graph neighborhoods arbitrary connectivity <eos> key novelty approach correspondences dynamically computed feature learned network rather than relying predefined static coordinates over graph previous work <eos> obtain excellent experimental result significantly improve over previous state art shape correspondence result <eos> shows approach learn effective shape representations raw input coordinates without relying shape descriptors <eos> <eop> image collection pop up three dimensional reconstruction clustering rigid non rigid categories <eos> paper introduces approach simultaneously estimate three dimensional shape camera pose object type deformation clustering partial annotations multi instance collection image <eos> furthermore indistinctly process rigid non rigid categories <eos> advances existing work only addresses problem one single object if multiple object considered they assumed clustered priori <eos> handle broader version problem model object deformation using formulation based multiple unions subspaces able span small rigid motion complex deformations <eos> parameters model learned via augmented lagrange multipliers completely unsupervised manner require any training data all <eos> extensive validation provided wide variety synthetic real scenarios including rigid non rigid categories small large deformations <eos> all cases approach outperforms state art terms three dimensional reconstruction accuracy while also providing clustering result allow segmenting image into object instances their associated type deformation action object performing <eos> <eop> geometry aware learning maps camera localization <eos> maps key component image based camera localization visual slam systems they used establish geometric constraints between image correct drift relative pose estimation relocalize cameras after lost tracking <eos> exact definitions maps however often application specific hand crafted different scenarios <eos> three dimensional landmarks lines planes bags visual words <eos> propose represent maps deep neural net called mapnet enables learning data driven map representation <eos> unlike prior work learning maps mapnet exploits cheap ubiquitous sensory inputs like visual odometry gps addition image fuses them together camera localization <eos> geometric constraints expressed inputs traditionally used bundle adjustment pose graph optimization formulated loss terms mapnet training also used during inference <eos> addition directly improving localization accuracy allows update mapnet <eos> maps self supervised manner using additional unlabeled video sequences scene <eos> also propose novel parameterization camera rotation better suited deep learning based camera pose regression <eos> experimental result both indoor scenes outdoor oxford robotcar datasets show significant improvement over prior work <eos> mapnet project webpage goo <eos> gl mrb au <eos> <eop> recurrent slice network three dimensional segmentation point clouds <eos> point clouds efficient data format three dimensional data <eos> however existing three dimensional segmentation method point clouds either model local dependencies require added computations <eos> work presents novel three dimensional segmentation framework rsnet efficiently model local structures point clouds <eos> key component rsnet lightweight local dependency module <eos> combination novel slice pooling layer recurrent neural network rnn layer slice unpooling layer <eos> slice pooling layer designed project feature unordered point onto ordered sequence feature vectors so traditional end end learning algorithms rnns applied <eos> performance rsnet validated comprehensive experiments dis scannet shapenet datasets <eos> its simplest form rsnets surpass all previous state art method benchmarks <eos> comparisons against previous state art method demonstrate efficiency rsnets <eos> <eop> depth based three dimensional hand pose estimation current achievements future goals <eos> paper strive answer two questions current state three dimensional hand pose estimation depth image next challenges need tackled following successful hands million challenge him investigate top state art method three tasks single frame three dimensional pose estimation three dimensional hand tracking hand pose estimation during object interaction <eos> analyze performance different cnn structures regard hand shape joint visibility view point articulation distributions <eos> findings include isolated three dimensional hand pose estimation achieves low mean errors mm view point range degrees but far being solved extreme view point three dimensional volumetric representations outperform cnn better capturing spatial structure depth data discriminative method still generalize poorly unseen hand shapes while joint occlusions pose challenge most method explicit modeling structure constraints significantly narrow gap between errors visible occluded joints <eos> <eop> sobolevfusion three dimensional reconstruction scenes undergoing free non rigid motion <eos> present system builds three dimensional models non rigidly moving surfaces scratch real time using single rgb stream <eos> solution based variational level set method thus copes arbitrary geometry including topological changes <eos> warps given truncated signed distance field tsdf target tsdf via gradient flow <eos> unlike previous approaches define gradient using inner product method relies gradient flow sobolev space <eos> its favourable regularity properties allow more straightforward energy formulation faster compute achieves higher geometric detail mitigating over smoothing effects introduced other regularization schemes <eos> addition coarse fine evolution behaviour flow able handle larger motions making few frames sufficient high fidelity reconstruction <eos> last but least pipeline determines voxel correspondences between partial shapes matching signatures low dimensional embedding their laplacian eigenfunctions thus able reliably colour output model <eos> variety quantitative qualitative evaluations demonstrate advantages technique <eos> <eop> learning find good correspondences <eos> develop deep architecture learn find good correspondences wide baseline stereo <eos> given set putative sparse matches camera intrinsics train network end end fashion label correspondences inliers outliers while simultaneously using them recover relative pose encoded essential matrix <eos> architecture based multi layer perceptron operating pixel coordinates rather than directly image thus simple small <eos> introduce novel normalization technique called context normalization allows process each data point separately while embedding global information also makes network invariant order correspondences <eos> experiments multiple challenging datasets demonstrate method able drastically improve state art little training data <eos> <eop> oatm occlusion aware template matching consensus set maximization <eos> present novel approach template matching efficient handle partial occlusions comes provable performance guarantees <eos> key component method reduction transforms problem searching nearest neighbor among high dimensional vectors searching neighbors among two set order sqrt vectors found efficiently using range search techniques <eos> allows quadratic improvement search complexity makes method scalable handling large search spaces <eos> second contribution hashing scheme based consensus set maximization allows handle occlusions <eos> resulting scheme seen randomized hypothesize test algorithm equipped guarantees regarding number iterations required obtaining optimal solution high probability <eos> predicted matching rates validated empirically algorithm shows significant improvement over state art both speed robustness occlusions <eos> <eop> deep learning graph matching <eos> problem graph matching under node pair wise constraints fundamental areas diverse combinatorial optimization machine learning computer vision representing both relations between nodes their neighborhood structure essential <eos> present end end model makes possible learn all parameters graph matching process including unary pairwise node neighborhoods represented deep feature extraction hierarchies <eos> challenge formulation different matrix computation layer model way enables consistent efficient propagation gradients complete pipeline loss function through combinatorial optimization layer solving matching problem feature extraction hierarchy <eos> computer vision experiments ablation studies challenging datasets like pascal voc keypoints sintel cub show matching models refined end end superior counterparts based feature hierarchies trained other problems <eos> <eop> unsupervised discovery object landmarks structural representations <eos> deep neural network model image rich latent representations but they cannot naturally conceptualize structures object categories human perceptible way <eos> paper addresses problem learning object structures image modeling process without supervision <eos> propose autoencoding formulation discover landmarks explicit structural representations <eos> encoding module outputs landmark coordinates whose validity ensured constraints reflect necessary properties landmarks <eos> decoding module takes landmarks part learnable input representations end end differentiable framework <eos> discovered landmarks semantically meaningful more predictive manually annotated landmarks than discovered previous method <eos> coordinates landmarks also complementary feature pretrained deep neuralnetwork representations recognizing visual attributes <eos> addition proposed method naturally creates unsupervised perceptible interface manipulate object shapes decode image controllable structures <eos> <eop> quantization training neural network efficient integer arithmetic only inference <eos> rising popularity intelligent mobile devices daunting computational cost deep learning based visual recognition models call efficient device inference schemes <eos> propose quantization scheme along co designed training procedure allowing inference carried out using integer only arithmetic while preserving end end model accuracy close floating point inference <eos> inference using integer only arithmetic performs better than floating point arithmetic typical arm cpus implemented integer arithmetic only hardware such mobile accelerators <eos> quantizing both activations weights bit integers obtain close memory footprint reduction compared bit floating point representations <eos> even mobilenets model family known runtime efficiency quantization approach result improved tradeoff between latency accuracy popular arm cpus imagenet classification coco detection <eos> <eop> lean multiclass crowdsourcing <eos> introduce method efficiently crowdsourcing multiclass annotations challenging real world image datasets <eos> method designed minimize number human annotations necessary achieve desired level confidence class labels <eos> based combining models worker behavior computer vision <eos> method general handle large number classes worker labels come taxonomy rather than flat list model dependence labels when workers see history previous annotations <eos> method may used drop replacement majority vote algorithms used online crowdsourcing services aggregate multiple human annotations into final consolidated label <eos> experiments conducted two real life applications find method reduce number required annotations much factor <eos> reduce residual annotation error up when compared majority voting <eos> furthermore online risk estimates models may used sort annotated collection minimize subsequent expert review effort <eos> <eop> partial transfer learning selective adversarial network <eos> adversarial learning successfully embedded into deep network learn transferable feature reduce distribution discrepancy between source target domains <eos> existing domain adversarial network assume fully shared label space across domains <eos> presence big data there strong motivation transferring both classification representation models existing large scale domains unknown small scale domains <eos> paper introduces partial transfer learning relaxes shared label space assumption target label space only subspace source label space <eos> previous method typically match whole source domain target domain prone negative transfer partial transfer problem <eos> present selective adversarial network san simultaneously circumvents negative transfer selecting out outlier source classes promotes positive transfer maximally matching data distributions shared label space <eos> experiments demonstrate models exceed state art result partial transfer learning tasks several benchmark datasets <eos> <eop> self supervised feature learning learning spot artifacts <eos> introduce novel self supervised learning method based adversarial training <eos> objective train discriminator network distinguish real image image synthetic artifacts then extract feature its intermediate layer transferred other data domains tasks <eos> generate image artifacts pre train high capacity autoencoder then use damage repair strategy first freeze autoencoder damage output encoder randomly dropping its entries <eos> second augment decoder repair network train adversarial manner against discriminator <eos> repair network helps generate more realistic image inpainting dropped feature entries <eos> make discriminator focus artifacts also make predict entries feature were dropped <eos> demonstrate experimentally feature learned creating spotting artifacts achieve state art performance several benchmarks <eos> <eop> ldmnet low dimensional manifold regularized neural network <eos> deep neural network proved very successful archetypal tasks large training set available but when training data scarce their performance suffers overfitting <eos> many existing method reducing overfitting data independent <eos> data dependent regularizations mostly motivated observation data interest lie close manifold typically hard parametrize explicitly <eos> method usually only focus geometry input data necessarily encourage network produce geometrically meaningful feature <eos> resolve propose low dimensional manifold regularized neural network ldmnet incorporates feature regularization method focuses geometry both input data output feature <eos> ldmnet regularize network encouraging combination input data output feature sample collection low dimensional manifolds searched efficiently without explicit parametrization <eos> achieve directly use manifold dimension regularization term variational functional <eos> resulting euler lagrange equation laplace beltrami equation over point cloud solved point integral method without increasing computational complexity <eos> experiments show ldmnet significantly outperforms widely used regularizers <eos> moreover ldmnet extract common feature object imaged via different modalities very useful real world applications such cross spectral face recognition <eos> <eop> condensenet efficient densenet using learned group convolutions <eos> deep neural network increasingly used mobile devices computational resources limited <eos> paper develop condensenet novel network architecture unprecedented efficiency <eos> combines dense connectivity novel module called learned group convolution <eos> dense connectivity facilitates feature re use network whereas learned group convolutions remove connections between layer feature re use superfluous <eos> test time model implemented using standard group convolutions allowing efficient computation practice <eos> experiments show condensenets far more efficient than state art compact convolutional network such mobilenets shufflenets <eos> <eop> learning deep descriptors scale aware triplet network <eos> research learning suitable feature descriptors computer vision recently shifted deep learning biggest challenge lies formulation appropriate loss functions especially since descriptors learned known training time <eos> while approaches such siamese triplet losses applied success still well understood makes good loss function <eos> spirit work demonstrates many commonly used losses suffer range problems <eos> based analysis introduce mixed context losses scale aware sampling two method when combined enable network learn consistently scaled descriptors first time <eos> <eop> decoupled network <eos> inner product based convolution central component convolutional neural network cnn key learning visual representations <eos> inspired observation cnn learned feature naturally decoupled norm feature corresponding intra class variation angle corresponding semantic difference propose generic decoupled learning framework models intra class variation semantic difference independently <eos> specifically first reparametrize inner product decoupled form then generalize decoupled convolution operator serves building block decoupled network <eos> present several effective instances decoupled convolution operator <eos> each decoupled operator well motivated intuitive geometric interpretation <eos> based decoupled operators further propose directly learn operator data <eos> extensive experiments show such decoupled reparameterization renders significant performance gain easier convergence stronger robustness <eos> <eop> deep adversarial metric learning <eos> learning effective distance metric between image pairs plays important role visual analysis training procedure largely relies hard negative sample <eos> however hard negatives training set usually account tiny minority may fail fully describe distribution negative sample close margin <eos> paper propose deep adversarial metric learning daml framework generate synthetic hard negatives observed negative sample widely applicable supervised deep metric learning method <eos> different existing metric learning approaches simply ignore numerous easy negatives proposed daml exploits them generate potential hard negatives adversary learned metric complements <eos> simultaneously train hard negative generator feature embedding adversarial manner so more precise distance metrics learned adequate targeted synthetic hard negatives <eos> extensive experimental result three benchmark datasets including cub cars stanford online products show daml effectively boosts performance existing deep metric learning approaches through adversarial learning <eos> <eop> pu net point cloud upsampling network <eos> learning analyzing three dimensional point clouds deep network challenging due sparseness irregularity data <eos> paper present data driven point cloud upsampling technique <eos> key idea learn multi level feature per point expand point set via multi branch convolution unit implicitly feature space <eos> expanded feature then split multitude feature then reconstructed upsampled point set <eos> network applied patch level joint loss function encourages upsampled point remain underlying surface uniform distribution <eos> conduct various experiments using synthesis scan data evaluate method demonstrate its superiority over some baseline method optimization based method <eos> result show upsampled point better uniformity located closer underlying surfaces <eos> <eop> real time monocular depth estimation using synthetic data domain adaptation via image style transfer <eos> monocular depth estimation using learning based approaches become promising recent years <eos> however most monocular depth estimators either need rely large quantities ground truth depth data extremely expensive difficult obtain predict disparity intermediary step using secondary supervisory signal leading blurring other artefacts <eos> training depth estimation model using pixel perfect synthetic data resolve most issues but introduces problem domain bias <eos> inability apply model trained synthetic data real world scenarios <eos> advances image style transfer its connections domain adaptation maximum mean discrepancy take advantage style transfer adversarial training predict pixel perfect depth single real world color image based training over large corpus synthetic environment data <eos> experimental result indicate efficacy approach compared contemporary state art techniques <eos> <eop> learning disparity estimation through feature constancy <eos> stereo matching algorithms usually consist four steps including matching cost calculation matching cost aggregation disparity calculation disparity refinement <eos> existing cnn based method only adopt cnn solve parts four steps use different network deal different steps making them difficult obtain overall optimal solution <eos> paper propose network architecture incorporate all steps stereo matching <eos> network consists three parts <eos> first part calculates multi scale shared feature <eos> second part performs matching cost calculation matching cost aggregation disparity calculation estimate initial disparity using shared feature <eos> initial disparity shared feature used calculate feature constancy measures correctness correspondence between two input image <eos> initial disparity feature constancy then fed sub network refine initial disparity <eos> proposed method evaluated scene flow kitti datasets <eos> achieves state art performance kitti kitti benchmarks while maintaining very fast running time <eos> source code available github <eos> com leonzfa iresnet <eos> <eop> deepmvs learning multi view stereopsis <eos> present deepmvs deep convolutional neural network convnet multi view stereo reconstruction <eos> taking arbitrary number posed image input first produce set plane sweep volumes use proposed deepmvs network predict high quality disparity maps <eos> key contributions enable result supervised pretraining photorealistic synthetic dataset effective method aggregating information across set unordered image integrating multi layer feature activations pre trained vgg network <eos> validate efficacy deepmvs using eth benchmark <eos> result show deepmvs compares favorably against state art conventional mvs algorithms other convnet based method particularly near textureless region thin structures <eos> <eop> self calibrating polarising radiometric calibration <eos> present self calibrating polarising radiometric calibration method <eos> set image taken single viewpoint under different unknown polarising angles recover inverse camera response function polarising angles relative first angle <eos> problem solved integrated manner recovering both unknowns simultaneously <eos> method exploits fact intensity polarised light should vary sinusoidally polarising filter rotated provided response linear <eos> offers first solution demonstrate possibility radiometric calibration through polarisation <eos> evaluate accuracy proposed method using synthetic data real world object captured using different cameras <eos> self calibrated result were found comparable multiple exposure sequence <eos> <eop> coding kendall shape trajectories three dimensional action recognition <eos> suitable shape representations well their temporal evolution termed trajectories often lie non linear manifolds <eos> puts additional constraint <eos> non linearity using conventional machine learning techniques purpose classification event detection prediction etc <eos> paper accommodates well known sparse coding dictionary learning kendall shape space illustrates effective coding three dimensional skeletal sequences action recognition <eos> grounding riemannian geometry shape space intrinsic sparse coding dictionary learning formulation proposed static skeletal shapes overcome inherent non linearity manifold <eos> main result initial trajectories give rise sparse code functions suitable computational properties including sparsity vector space representation <eos> achieve action recognition two different classification schemes were adopted <eos> bi directional lstm directly performed sparse code functions while linear svm applied after representing sparse code functions using fourier temporal pyramid <eos> experiments conducted three publicly available datasets show superiority proposed approach compared existing riemannian representations its competitiveness respect other recently proposed approaches <eos> when benefits invariance maintained kendall shape representation approach only overcomes problem non linearity but also yields discriminative sparse code functions <eos> <eop> efficient sparse representation manifold distance matrices classical scaling <eos> geodesic distance matrices reveal shape properties largely invariant non rigid deformations thus often used analyze represent shapes <eos> however matrices grow quadratically number point <eos> thus large point set common use low rank approximation distance matrix fits memory efficiently analyzed using method such multidimensional scaling mds <eos> paper present novel sparse method efficiently representing geodesic distance matrices using biharmonic interpolation <eos> method exploits knowledge data manifold learn sparse interpolation operator approximates distances using subset point <eos> show method faster uses less memory than current leading method solving mds large point set similar quality <eos> enables analyses large point set were previously infeasible <eos> <eop> motion segmentation exploiting complementary geometric models <eos> many real world sequences cannot conveniently categorized general degenerate such cases imposing false dichotomy using fundamental matrix homography model motion segmentation would lead difficulty <eos> even when confronted general scene motion fundamental matrix approach model motion segmentation still suffers several defects discuss paper <eos> full potential fundamental matrix approach could only realized if judiciously harness information simpler homography model <eos> considerations propose multi view spectral clustering framework synergistically combines multiple models together <eos> show performance substantially improved way <eos> perform extensive testing existing motion segmentation datasets achieving state art performance all them also put forth more realistic challenging dataset adapted kitti benchmark containing real world effects such strong perspectives strong forward translations seen traditional datasets <eos> <eop> estimation camera locations highly corrupted scenarios all about base no shape trouble <eos> propose strategy improving camera location estimation structure motion <eos> setting assumes highly corrupted pairwise directions <eos> normalized relative location vectors so there clear room improving current state art solutions problem <eos> strategy identifies severely corrupted pairwise directions using geometric consistency condition <eos> then selects cleaner set pairwise directions preprocessing step common solvers <eos> theoretically guarantee successful performance basic version strategy under synthetic corruption model <eos> numerical result artificial real data demonstrate significant improvement obtained strategy <eos> <eop> human body correspondences panoramic depth maps <eos> availability affordable three dimensional full body reconstruction systems given rise free viewpoint video fvv human avatars <eos> most existing solutions produce temporally uncorrelated point clouds meshes unknown point vertex correspondences <eos> individually compressing each frame ineffective still yields ultra large data sizes <eos> present end end deep learning scheme establish dense shape correspondences subsequently compress data <eos> approach uses sparse set panoramic depth maps pdms each emulating inward viewing concentric mosaics cm <eos> then develop learning based technique learn pixel wise feature descriptors pdms <eos> result fed into autoencoder based network compression <eos> comprehensive experiments demonstrate solution robust effective both public newly captured datasets <eos> <eop> reconstructing thin structures manifold surfaces integrating spatial curves <eos> manifold surface reconstruction multi view stereo often fails retaining thin structures due incomplete noisy reconstructed point clouds <eos> paper address problem leveraging spatial curves <eos> curve representation nature advantageous modeling thin elongated structures implying topology connectivity information underlying geometry exactly compensates weakness scattered point clouds <eos> present novel surface reconstruction method using both curves point clouds <eos> first propose three dimensional curve reconstruction algorithm based initialize optimize expand strategy <eos> then tetrahedra constructed point curves volumes thin structures robustly preserved curve conformed delaunay refinement <eos> finally mesh surface extracted tetrahedra graph optimization <eos> method intensively evaluated both synthetic real world datasets showing significant improvements over state art method <eos> <eop> multi view consistency supervisory signal learning shape pose prediction <eos> present framework learning single view shape pose prediction without using direct supervision either <eos> approach allows leveraging multi view observations unknown poses supervisory signal during training <eos> proposed training setup enforces geometric consistency between independently predicted shape pose two views same instance <eos> consequently learn predict shape emergent canonical view agnostic frame along corresponding pose predictor <eos> show empirical qualitative result using shapenet dataset observe encouragingly competitive performance previous techniques rely stronger forms supervision <eos> also demonstrate applicability framework realistic setting beyond scope existing techniques using training dataset comprised online product image underlying shape pose unknown <eos> <eop> probabilistic plant modeling via multi view image image translation <eos> paper describes method inferring three dimensional three dimensional plant branch structures hidden under leaves multi view observations <eos> unlike previous geometric approaches heavily rely visibility branches use parametric branching models method makes statistical inferences branch structures probabilistic framework <eos> inferring probability branch existence using bayesian extension image image translation applied each multi view image method generates probabilistic plant three dimensional model represents three dimensional branching pattern cannot directly observed <eos> experiments demonstrate usefulness proposed approach generating convincing branch structures comparison prior approaches <eos> <eop> deep marching cubes learning explicit surface representations <eos> existing learning based solutions three dimensional surface prediction cannot trained end end they operate intermediate representations <eos> tsdf three dimensional surface meshes must extracted post processing step <eos> via marching cubes algorithm <eos> paper investigate problem end end three dimensional surface prediction <eos> first demonstrate marching cubes algorithm differentiable propose alternative differentiable formulation insert final layer into three dimensional convolutional neural network <eos> further propose set loss functions allow training model sparse point supervision <eos> experiments demonstrate model allows predicting sub voxel accurate three dimensional shapes arbitrary topology <eos> additionally learns complete shapes separate object inside its outside even presence sparse incomplete ground truth <eos> investigate benefits approach task inferring shapes three dimensional point clouds <eos> model flexible combined variety shape encoder shape inference techniques <eos> <eop> tags parts discovering semantic region shape tags <eos> propose novel method discovering shape region strongly correlate user prescribed tags <eos> example given collection chairs tagged either armrest lacks armrest system correctly highlights armrest region main distinctive parts between two chair types <eos> obtain point wise predictions shape wise tags develop novel neural network architecture trained tag classification loss but designed rely segmentation predict tag <eos> network inspired net but replicate shallow structures several times new skip connections pooling layer call resulting architecture wu net <eos> test method segmentation benchmarks show even weak supervision whole shape tags method infer meaningful semantic region without ever observing shape segmentations <eos> further once trained model process shapes tag entirely unknown <eos> bonus architecture directly operational under full supervision performs strongly standard benchmarks <eos> validate method through experiments many variant architectures prior baselines demonstrate several applications <eos> <eop> uncalibrated photometric stereo under natural illumination <eos> paper presents photometric stereo method works unknown natural illuminations without any calibration object <eos> solve challenging problem propose use equivalent directional lighting model small surface patches consisting slowly varying normals solve each patch up arbitrary rotation ambiguity <eos> method connects resulting patches unifies local ambiguities global rotation one through angular distance propagation defined over whole surface <eos> after applying integrability constraint final solution contains only binary ambiguity could easily removed <eos> experiments using both synthetic real world datasets show method provides even comparable result calibrated method <eop> robust depth estimation auto bracketed image <eos> demand advanced photographic applications hand held devices grows electronics require capture high quality depth <eos> however under low light conditions most devices still suffer low imaging quality inaccurate depth acquisition <eos> address problem present robust depth estimation method short burst shot varied intensity <eos> auto bracketing strong noise <eos> introduce geometric transformation between flow depth tailored burst image enabling learning based multi view stereo matching performed effectively <eos> then describe depth estimation pipeline incorporates geometric transformation into residual flow network <eos> allows framework produce accurate depth map even bracketed image sequence <eos> demonstrate method outperforms state art method various datasets captured smartphone dslr camera <eos> moreover show estimated depth applicable image quality enhancement photographic editing <eos> <eop> free supervision video games <eos> deep network extremely hungry data <eos> they devour hundreds thousands labeled image learn robust semantically meaningful feature representations <eos> current network so data hungry collecting labeled data become important designing network themselves <eos> unfortunately manual data collection both expensive time consuming <eos> present alternative show how ground truth labels many vision tasks easily extracted video games real time play them <eos> interface popular microsoft directx rendering api inject specialized rendering code into game running <eos> code produces ground truth labels instance segmentation semantic labeling depth estimation optical flow intrinsic image decomposition instance tracking <eos> instead labeling image researcher now simply plays video games all day long <eos> method general works wide range video games <eos> collected dataset training image test image across video games evaluate state art optical flow depth estimation intrinsic image decomposition algorithms <eos> video game data visually closer real world image than other synthetic dataset <eos> <eop> planar shape detection structural scales <eos> interpreting three dimensional data such point clouds surface meshes depends heavily scale observation <eos> yet existing algorithms shape detection rely trial error parameter tunings output configurations representative structural scale <eos> present framework automatically extract set representations capture shape structure man made object different key abstraction levels <eos> shape collapsing process first generates fine coarse sequence shape representations exploiting local planarity <eos> sequence then analyzed identify significant geometric variations between successive representations through supervised energy minimization <eos> framework flexible enough learn how detect both existing structural formalisms such citygml levels details expert specified levels abstraction <eos> experiments different input data classes man made object well comparisons existing shape detection method illustrate strengths approach terms efficiency flexibility <eos> <eop> pix dataset method single image three dimensional shape modeling <eos> study three dimensional shape modeling single image make contributions three aspects <eos> first present pix large scale benchmark diverse image shape pairs pixel level three dimensional alignment <eos> pix wide applications shape related tasks including reconstruction retrieval viewpoint estimation etc <eos> building such large scale dataset however highly challenging existing datasets either contain only synthetic data lack precise alignment between image three dimensional shapes only small number image <eos> second calibrate evaluation criteria three dimensional shape reconstruction through behavioral studies use them objectively systematically benchmark cutting edge reconstruction algorithms pix <eos> third design novel model simultaneously performs three dimensional reconstruction pose estimation multi task learning approach achieves state art performance both tasks <eos> <eop> camera pose estimation unknown principal point <eos> estimate dof extrinsic pose pinhole camera partially unknown intrinsic parameters critical sub problem structure motion camera localization <eos> most existing camera pose estimation solvers principal point assumed image center <eos> unfortunately assumption always true especially asymmetrically cropped image <eos> paper develop first exactly minimal solver case unknown principal point focal length using four half point correspondences <eos> also present extremely fast solver case unknown aspect ratio pfuva <eos> new solvers outperform previous state art terms stability speed <eos> finally explore extremely challenging case both unknown principal point radial distortion develop first practical non minimal solver using seven point correspondences pfruv <eos> experimental result both simulated data real internet image demonstrate usefulness new solvers <eos> <eop> inverse composition discriminative optimization point cloud registration <eos> rigid point cloud registration pcreg refers problem finding rigid transformation between two set point clouds <eos> problem particularly important due advances new three dimensional sensing hardware challenging because neither correspondence nor transformation parameters known <eos> traditional local pcreg method <eos> icp rely local optimization algorithms get trapped bad local minima presence noise outliers bad initializations etc <eos> alleviate issues paper proposes inverse composition discriminative optimization icdo extension discriminative optimization learns sequence update steps synthetic training data search parameter space improved solution <eos> unlike icdo object independent generalizes even unseen shapes <eos> evaluated icdo both synthetic real data show icdo match speed outperform accuracy state art pcreg algorithms <eos> <eop> surfconv bridging three dimensional convolution rgbd image <eos> last few years seen approaches trying combine increasing popularity depth sensors success convolutional neural network <eos> using depth additional channel alongside rgb input scale variance problem present image convolution based approaches <eos> other hand three dimensional convolution wastes large amount memory mostly unoccupied three dimensional space consists only surface visible sensor <eos> instead propose surfconv slides compact filters along visible three dimensional surface <eos> surfconv formulated simple depth aware multi scale convolution through new data driven depth discretization scheme <eos> demonstrate effectiveness method indoor outdoor three dimensional semantic segmentation datasets <eos> method achieves state art performance while using less than parameters used three dimensional convolution based approaches <eos> <eop> fast resection intersection method known rotation problem <eos> known rotation problem refers special case structure motion absolute orientations cameras known <eos> when formulated minimax infty problem reprojection errors problem instance pseudo convex programming <eos> though theoretically tractable solving known rotation problem large scale data views scene point using existing method very time consuming <eos> paper devise fast algorithm known rotation problem <eos> approach alternates between pose estimation triangulation <eos> resection intersection break problem into multiple simpler instances pseudo convex programming <eos> key vastly superior performance method lies using novel minimum enclosing ball meb technique calculation updating steps obviates need convex optimisation routines greatly reduces memory footprint <eos> demonstrate practicality method large scale problem instances easily overwhelm current state art algorithms demo program available supplementary <eos> <eop> pose estimation three dimensional model retrieval object wild <eos> propose scalable efficient accurate approach retrieve three dimensional models object wild <eos> first present three dimensional pose estimation approach object categories significantly outperforms state art pascal <eos> second use estimated pose prior retrieve three dimensional models accurately represent geometry object rgb image <eos> purpose render depth image three dimensional models under predicted pose match learned image descriptors rgb image against rendered depth image using cnn based multi view metric learning approach <eos> way first report quantitative result three dimensional model retrieval pascal method chooses same models human annotators validation image average <eos> addition show method was trained purely pascal retrieves rich accurate three dimensional models shapenet given rgb image object wild <eos> <eop> structure recurrent motion rigidity recurrency <eos> paper proposes new method non rigidstructure motion nrsfm <eos> departing significantlyfrom traditional idea using linear low order shapemodel nrsfm method exploits property shaperecurrence <eos> many dynamic shapes tend repeat them selves time <eos> show recurrency fact agen eralized rigidity <eos> based show how reducenrsfm problems rigid ones provided recurrencecondition satisfied <eos> given such reduction standardrigid sfm techniques applied directly without anychange reconstruct non rigid dynamic shape <eos> im plement idea practical approach paper de velops efficient reliable algorithm automatic recur rence detection well new method camera viewsclustering via rigidity check <eos> experiments both syntheticsequences real data demonstrate effectiveness theproposed method <eos> since method provides novel perspec tive look structure motion hope will inspireother new researches field <eos> <eop> learning patch reconstructability accelerating multi view stereo <eos> present approach accelerate multi view stereo mvs prioritizing computation image patches likely produce accurate three dimensional surface reconstructions <eos> key insight accuracy surface reconstruction given image patch predicted significantly faster than performing actual stereo matching <eos> intuition non specular fronto parallel focus patches more likely produce accurate surface reconstructions than highly specular slanted blurry patches properties reliably predicted image itself <eos> prioritizing stereo matching subset patches highly reconstructable also cover three dimensional surface able accelerate mvs minimal reduction accuracy completeness <eos> predict reconstructability score image patch single view train image reconstructability neural network rnet <eos> reconstructability score enables efficiently identify image patches likely provide most accurate surface estimates before performing stereo matching <eos> demonstrate rnet when trained scannet dataset generalizes dtu tanks temples mvs datasets <eos> using rnet existing mvs implementation show method achieve more than speed up over baseline only minimal loss completeness <eos> <eop> progressively complementarity aware fusion network rgb salient object detection <eos> how incorporate cross modal complementarity sufficiently cornerstone question rgb salient object detection <eos> previous works mainly address issue simply concatenating multi modal feature combining unimodal predictions <eos> paper answer question two perspectives argue if complementary part modelled more explicitly cross modal complement likely better captured <eos> end design novel complementarity aware fusion ca fuse module when adopting convolutional neural network cnn <eos> introducing cross modal residual functions complementarity aware supervisions each ca fuse module problem learning complementary information paired modality explicitly posed asymptotically approximating residual function <eos> exploring complement across all levels <eos> cascading ca fuse module adding level wise supervision deep shallow densely cross level complement selected combined progressively <eos> proposed rgb fusion network disambiguates both cross modal cross level fusion processes enables more sufficient fusion result <eos> experiments public datasets show effectiveness proposed ca fuse module rgb salient object detection network <eos> <eop> pixels voxels views study shape representations single view three dimensional object shape prediction <eos> goal paper compare surface based volumetric three dimensional object shape representations well viewer centered object centered reference frames single view three dimensional shape prediction <eos> propose new algorithm predicting depth maps multiple viewpoints single depth rgb image input <eos> modifying network way models evaluated directly compare merits voxels vs <eos> surfaces viewer centered vs <eos> object centered familiar vs <eos> unfamiliar object predicted rgb depth image <eos> among findings show surface based method outperform voxel representations object novel classes produce higher resolution outputs <eos> also find using viewer centered coordinates advantageous novel object while object centered representations better more familiar object <eos> interestingly coordinate frame significantly affects shape representation learned object centered placing more importance implicitly recognizing object category viewer centered producing shape representations less dependence category recognition <eos> <eop> learning dual convolutional neural network low level vision <eos> paper propose general dual convolutional neural network dualcnn low level vision problems <eos> super resolution edge preserving filtering deraining dehazing <eos> problems usually involve estimation two components target signals structures details <eos> motivated proposed dualcnn consists two parallel branches respectively recovers structures details end end manner <eos> recovered structures details generate target signals according formation model each particular application <eos> dualcnn flexible framework low level vision tasks easily incorporated existing cnn <eos> experimental result show dualcnn effectively applied numerous low level vision tasks favorable performance against state art method <eos> <eop> defocus blur detection via multi stream bottom top bottom fully convolutional network <eos> defocus blur detection dbd separation infocus out focus region image <eos> process paid considerable attention because its remarkable potential applications <eos> accurate differentiation homogeneous region detection low contrast focal region well suppression background clutter challenges associated dbd <eos> address issues propose multi stream bottom top bottom fully convolutional network btbnet first attempt develop end end deep network dbd <eos> first develop fully convolutional btbnet integrate low level cues high level semantic information <eos> then considering degree defocus blur sensitive scales propose multi stream btbnets handle input image different scales improve performance dbd <eos> finally design fusion recursive reconstruction network recursively refine preceding blur detection maps <eos> promote further study evaluation dbd models construct new database challenging image their pixel wise defocus blur annotations <eos> experimental result existing new datasets demonstrate proposed method achieves significantly better performance than other state art algorithms <eos> <eop> picanet learning pixel wise contextual attention saliency detection <eos> contexts play important role saliency detection task <eos> however given context region all contextual information helpful final task <eos> paper propose novel pixel wise contextual attention network <eos> picanet learn selectively attend informative context locations each pixel <eos> specifically each pixel generate attention map each attention weight corresponds contextual relevance each context location <eos> attended contextual feature then constructed selectively aggregating contextual information <eos> formulate proposed picanet both global local forms attend global local contexts respectively <eos> both models fully differentiable embedded into cnn joint training <eos> also incorporate proposed models net architecture detect salient object <eos> extensive experiments show proposed picanets consistently improve saliency detection performance <eos> global local picanets facilitate learning global contrast homogeneousness respectively <eos> result saliency model detect salient object more accurately uniformly thus performing favorably against state art method <eos> <eop> curve reconstruction via global statistics natural curves <eos> reconstructing missing parts curve subject much computational research applications image inpainting object synthesis etc <eos> different approaches solving problem typically based processes seek visually pleasing perceptually plausible completions <eos> work focus reconstructing underlying physically likely shape utilizing global statistics natural curves <eos> more specifically develop reconstruction model seeks mean physical curve given inducer configuration <eos> simple model both straightforward compute receptive diverse additional information but requires enough sample all curve configurations practical requirement limits its effective utilization <eos> address practical issue explore exploit statistical geometrical properties natural curves particular show many cases mean curve scale invariant often times extensible <eos> turn allows boost number examples thus robustness statistics its applicability <eos> reconstruction result only more physically plausible but they also lead important insights reconstruction problem including elegant explanation why certain inducer configurations more likely yield consistent perceptual completions than others <eos> <eop> deep network like see <eos> propose novel way measure understand convolutional neural network quantifying amount input signal they let <eos> autoencoder ae was fine tuned gradients pre trained classifier fixed parameters <eos> compared reconstructed sample aes were fine tuned set image classifiers alexnet vgg resnet inception found substantial differences <eos> ae learns aspects input space preserve ones ignore based information encoded backpropagated gradients <eos> measuring changes accuracy when signal one classifier used second one relation total order emerges <eos> order depends directly each classifier input signal but correlate classification accuracy network size <eos> further evidence phenomenon provided measuring normalized mutual information between original image auto encoded reconstructions different fine tuned aes <eos> findings break new ground area neural network understanding opening new way reason debug interpret their result <eos> present four concrete examples literature observations now explained terms input signal model uses <eos> <eop> zero shot super resolution using deep internal learning <eos> deep learning led dramatic leap super resolution sr performance past few years <eos> however being supervised sr method restricted specific training data acquisition low resolution lr image their high resolution hr counterparts predetermined <eos> bicubic downscaling without any distracting artifacts <eos> sensor noise image compression non ideal psf etc <eos> real lr image however rarely obey restrictions resulting poor sr result sota state art method <eos> paper introduce zero shot sr exploits power deep learning but rely prior training <eos> exploit internal recurrence information inside single image train small image specific cnn test time examples extracted solely input image itself <eos> such adapt itself different settings per image <eos> allows perform sr real old photos noisy image biological data other image acquisition process unknown non ideal <eos> such image method outperforms sota cnn based sr method well previous unsupervised sr method <eos> best knowledge first unsupervised cnn based sr method <eos> <eop> detect globally refine locally novel approach saliency detection <eos> effective integration contextual information crucial salient object detection <eos> achieve most existing method based skip architecture mainly focus how integrate hierarchical feature convolutional neural network cnn <eos> they simply apply concatenation element wise operation incorporate high level semantic cues low level detailed information <eos> however degrade quality predictions because cluttered noisy information also passed through <eos> address problem proposes global recurrent localization network rln exploits contextual information weighted response map order localize salient object more accurately <eos> emphasize more useful ones <eos> particularly recurrent module employed progressively refine inner structure cnn over multiple time steps <eos> moreover effectively recover object boundaries propose local boundary refinement network brn adaptively learn local contextual information each spatial position <eos> learned propagation coefficients used optimally capture relations between each pixel its neighbors <eos> experiments five challenging datasets show approach performs favorably against all existing method terms popular evaluation metrics <eos> <eop> beyond pixel wise loss topology aware delineation <eos> delineation curvilinear structures important problem computer vision multiple practical applications <eos> advent deep learning many current approaches automatic delineation focused finding more powerful deep architectures but continued using habitual pixel wise losses such binary cross entropy <eos> paper claim pixel wise losses alone unsuitable problem because their inability reflect topological importance prediction errors <eos> instead propose new loss term aware higher order topological feature linear structures <eos> also introduce refinement pipeline iteratively applies same model over previous delineation refine predictions each step while keeping number parameters complexity model constant <eos> when combined standard pixel wise loss both new loss term iterative refinement boost quality predicted delineations some cases almost doubling accuracy compared same classifier trained only binary cross entropy <eos> show approach outperforms state art method wide range data microscopy aerial image <eos> <eop> kippi kinetic polygonal partitioning image <eos> recent works showed floating polygons interesting alternative traditional superpixels especially analyzing scenes strong geometric signatures man made environments <eos> existing algorithms produce homogeneously sized polygons fail capture thin geometric structures over partition large uniform areas <eos> propose kinetic approach brings more flexibility polygon shape size <eos> key idea consists progressively extending pre detected line segments until they meet each other <eos> experiments demonstrate output partitions both contain less polygons better capture geometric structures than delivered existing method <eos> also show applicative potential method when used preprocessing object contouring <eos> <eop> image blind denoising generative adversarial network based noise modeling <eos> paper consider typical image blind denoising problem remove unknown noise noisy image <eos> all know discriminative learning based method such dncnn achieve state art denoising result but they applicable problem due lack paired training data <eos> tackle barrier propose novel two step framework <eos> first generative adversarial network gan trained estimate noise distribution over input noisy image generate noise sample <eos> second noise patches sampled first step utilized construct paired training dataset used turn train deep convolutional neural network cnn denoising <eos> extensive experiments done demonstrate superiority approach image blind denoising <eos> <eop> multi scale weighted nuclear norm image restoration <eos> prominent property natural image groups similar patches within them tend lie low dimensional subspaces <eos> property previously used image denoising particularly notable success via weighted nuclear norm minimization wnnm <eos> paper extend wnnm method into general image restoration algorithm capable handling arbitrary degradations <eos> blur missing pixels etc <eos> approach based novel regularization term simultaneously penalizes high weighted nuclear norm values all patch groups image <eos> regularizer isolated data term thus enabling convenient treatment arbitrary degradations <eos> furthermore exploits fractal property natural image accounting patch similarities also across different scales image <eos> propose variable splitting method solving resulting optimization problem <eos> leads algorithm quite different plug play techniques solve image restoration problems using sequence denoising steps <eos> verify through extensive experiments algorithm achieves state art result deblurring inpainting outperforming even recent deep net based method <eos> <eop> monet moments embedding network <eos> bilinear pooling recently proposed feature encoding layer used after convolutional layer deep network improve performance multiple vision tasks <eos> different conventional global average pooling fully connected layer bilinear pooling gathers nd order information translation invariant fashion <eos> however serious drawback family pooling layer their dimensionality explosion <eos> approximate pooling method compact properties explored towards resolving weakness <eos> additionally recent result shown significant performance gains achieved adding st order information applying matrix normalization regularize unstable higher order information <eos> however combining compact pooling matrix normalization other order information explored until now <eos> paper unify bilinear pooling global gaussian embedding layer through empirical moment matrix <eos> addition propose novel sub matrix square root layer used normalize output convolution layer directly mitigate dimensionality problem off shelf compact pooling method <eos> experiments three widely used fine grained classification datasets illustrate proposed architecture monet achieve similar better performance than state art denet <eos> furthermore when combined compact pooling technique monet obtains comparable performance encoded feature less dimensions <eos> <eop> active fixation control predict saccade sequences <eos> visual attention field considerable history eye movement control prediction forming important subfield <eos> fixation modeling past decades largely dominated computationally number highly influential bottom up saliency models such itti koch niebur model <eos> accuracy such models dramatically increased recently due deep learning <eos> however static image emphasis models largely based non ordered prediction fixations through saliency map <eos> very few implemented models generate temporally ordered human like sequences saccades beyond initial fixation point <eos> towards addressing shortcomings present star fc novel multi saccade generator based integration central high level object based saliency peripheral lower level feature based saliency <eos> evaluated model using cat database successfully predicting human patterns fixation equivalent accuracy quality compared achieved using one human sequence predict another <eos> <eop> densely connected pyramid dehazing network <eos> propose new end end single image dehazing method called densely connected pyramid dehazing network dcpdn jointly learn transmission map atmospheric light dehazing all together <eos> end end learning achieved directly embedding atmospheric scattering model into network thereby ensuring proposed method strictly follows physics driven scattering model dehazing <eos> inspired dense network maximize information flow along feature different levels propose new edge preserving densely connected encoder decoder structure multi level pyramid pooling module estimating transmission map <eos> network optimized using newly introduced edge preserving loss function <eos> further incor propose new end end single image dehazing method called densely connected pyramid dehazing net work dcpdn jointly learn transmission map atmospheric light dehazing all together <eos> end end learning achieved directly embedding atmo spheric scattering model into network thereby ensuring proposed method strictly follows physics driven scattering model dehazing <eos> inspired dense net work maximize information flow along feature different levels propose new edge preserving densely connected encoder decoder structure multi level pyramid pooling module estimating transmis sion map <eos> network optimized using newly troduced edge preserving loss function <eos> further incor porate mutual structural information between esti mated transmission map dehazed result pro pose joint discriminator based generative adversar ial network framework decide whether correspond ing dehazed image estimated transmission map real fake <eos> ablation study conducted demon strate effectiveness each module evaluated both estimated transmission map dehazed result <eos> exten sive experiments demonstrate proposed method achieves significant improvements over state art method <eos> code dataset made available github <eos> com hezhangsprinter dcpdn <eop> universal denoising network novel cnn architecture image denoising <eos> design novel network architecture learning discriminative image models employed efficiently tackle problem grayscale color image denoising <eos> based proposed architecture introduce two different variants <eos> first network involves convolutional layer core component while second one relies instead non local filtering layer thus able exploit inherent non local self similarity property natural image <eos> opposed most existing deep network approaches require training specific model each considered noise level proposed models able handle wide range noise levels using single set learned parameters while they very robust when noise degrading latent image match statistics noise used during training <eos> latter argument supported result report publicly available image corrupted unknown noise compare against solutions obtained competing method <eos> same time introduced network achieve excellent result under additive white gaussian noise awgn comparable current state art network while they depend more shallow architecture number trained parameters being one order magnitude smaller <eos> properties make proposed network ideal candidates serve sub solvers restoration method deal general inverse imaging problems such deblurring demosaicking superresolution etc <eos> <eop> learning convolutional network content weighted image compression <eos> lossy image compression generally formulated joint rate distortion optimization problem learn encoder quantizer decoder <eos> due non differentiable quantizer discrete entropy estimation very challenging develop convolutional network cnn based image compression system <eos> paper motivated local information content spatially variant image suggest bit rate different parts image adapted local content ii content aware bit rate allocated under guidance content weighted importance map <eos> sum importance map thus serve continuous alternative discrete entropy estimation control compression rate <eos> binarizer adopted quantize output encoder proxy function introduced approximating binary operation backward propagation make differentiable <eos> encoder decoder binarizer importance map jointly optimized end end manner <eos> convolutional entropy encoder further presented lossless compression importance map binary codes <eos> low bit rate image compression experiments show system significantly outperforms jpeg jpeg structural similarity ssim index produce much better visual result sharp edges rich textures fewer artifacts <eos> <eop> deep video super resolution network using dynamic upsampling filters without explicit motion compensation <eos> video super resolution vsr become even more important recently provide high resolution hr contents ultra high definition displays <eos> while many deep learning based vsr method proposed most them rely heavily accuracy motion estimation compensation <eos> introduce fundamentally different framework vsr paper <eos> propose novel end end deep neural network generates dynamic upsampling filters residual image computed depending local spatio temporal neighborhood each pixel avoid explicit motion compensation <eos> approach hr image reconstructed directly input image using dynamic upsampling filters fine details added through computed residual <eos> network help new data augmentation technique generate much sharper hr video temporal consistency compared previous method <eos> also provide analysis network through extensive experiments show how network deals motions implicitly <eos> <eop> erase fill deep joint recurrent rain removal reconstruction video <eos> paper address problem video rain removal constructing deep recurrent convolutional network <eos> visit rain removal case considering rain occlusion region <eos> light transmittance rain streaks low <eos> different additive rain streaks such rain occlusion region details background image completely lost <eos> therefore propose hybrid rain model depict both rain streaks occlusions <eos> wealth temporal redundancy build joint recurrent rain removal reconstruction network net seamlessly integrates rain degradation classification spatial texture appearances based rain removal temporal coherence based background details reconstruction <eos> rain degradation classification provides binary map reveals whether location degraded linear additive streaks occlusions <eos> side information gate recurrent unit learns make trade off between rain streak removal background details reconstruction <eos> extensive experiments series synthetic real video rain streaks verify superiority proposed method over previous state art method <eos> <eop> flow guided recurrent neural encoder video salient object detection <eos> image saliency detection recently witnessed significant progress due deep convolutional neural network <eos> however extending state art saliency detectors image video challenging <eos> performance salient object detection suffers object camera motion dramatic change appearance contrast video <eos> paper present flow guided recurrent neural encoder fgrne accurate end end learning framework video salient object detection <eos> works enhancing temporal coherence per frame feature exploiting both motion information terms optical flow sequential feature evolution encoding terms lstm network <eos> considered universal framework extend any fcn based static saliency detector video salient object detection <eos> intensive experimental result verify effectiveness each part fgrne confirm proposed method significantly outperforms state art method public benchmarks davis fbms <eos> <eop> gated fusion network single image dehazing <eos> paper propose efficient algorithm directly restore clear image hazy input <eos> proposed algorithm hinges end end trainable neural network consists encoder decoder <eos> encoder exploited capture context derived input image while decoder employed estimate contribution each input final dehazed result using learned representations attributed encoder <eos> constructed network adopts novel fusion based strategy derives three inputs original hazy image applying white balance wb contrast enhancing ce gamma correction gc <eos> compute pixel wise confidence maps based appearance differences between different inputs blend information derived inputs preserve region pleasant visibility <eos> final dehazed image yielded gating important feature derived inputs <eos> train network introduce multi scale based approach so halo artifacts avoided <eos> extensive experimental result both synthetic real world image demonstrate proposed algorithm performs favorably against state art algorithms <eos> <eop> learning single convolutional super resolution network multiple degradations <eos> recent years witnessed unprecedented success deep convolutional neural network cnn single image super resolution sisr <eos> however existing cnn based sisr method mostly assume low resolution lr image bicubicly downsampled high resolution hr image thus inevitably giving rise poor performance when true degradation follow assumption <eos> moreover they lack scalability learning single model non blindly deal multiple degradations <eos> address issues propose general framework dimensionality stretching strategy enables single convolutional super resolution network take two key factors sisr degradation process <eos> blur kernel noise level input <eos> consequently super resolver handle multiple even spatially variant degradations significantly improves practicability <eos> extensive experimental result synthetic real lr image show proposed convolutional super resolution network only produce favorable result multiple degradations but also computationally efficient providing highly effective scalable solution practical sisr applications <eos> <eop> non blind deblurring handling kernel uncertainty cnn <eos> blind motion deblurring method primarily responsible recovering accurate estimate blur kernel <eos> non blind deblurring nbd method other hand attempt faithfully restore original image given blur estimate <eos> however nbd quite susceptible errors blur kernel <eos> work present convolutional neural network based approach handle kernel uncertainty non blind motion deblurring <eos> provide multiple latent image estimates corresponding different prior strengths obtained given blurry observation order exploit complementarity inputs improved learning <eos> generalize performance tackle arbitrary kernel noise train network large number real synthetic noisy blur kernels <eos> network mitigates effects kernel noise so yield detail preserving artifact free restoration <eos> quantitative qualitative evaluations benchmark datasets demonstrate proposed method delivers state art result <eos> further underscore benefits achieved network propose two adaptations method improve kernel estimates image deblurring quality respectively <eos> <eop> boundary flow siamese network predicts boundary motion without training motion <eos> using deep learning paper addresses problem joint object boundary detection boundary motion estimation video named boundary flow estimation <eos> boundary flow important mid level visual cue boundaries characterize object spatial extents flow indicates object motions interactions <eos> yet most prior work motion estimation focused dense object motion feature point may necessarily reside boundaries <eos> boundary flow estimation specify new fully convolutional siamese network fcsn jointly estimates object level boundaries two consecutive frames <eos> boundary correspondences two frames predicted same fcsn new unconventional deconvolution approach <eos> finally boundary flow estimate improved edgelet based filtering <eos> evaluation conducted three tasks boundary detection video boundary flow estimation optical flow estimation <eos> boundary detection achieve state art performance benchmark vsb dataset <eos> boundary flow estimation present first result sintel training dataset <eos> optical flow estimation run recent approach cpm flow but augmented input boundary flow matches achieve significant performance improvement sintel benchmark <eos> <eop> learning see dark <eos> imaging low light challenging due low photon count low snr <eos> short exposure image suffer noise while long exposure lead blurry image often impractical <eos> variety denoising deblurring enhancement techniques proposed but their effectiveness limited extreme conditions such video rate imaging night <eos> support development learning based pipelines low light image processing introduce dataset raw short exposure low light image corresponding long exposure reference image <eos> using presented dataset develop pipeline processing low light image based end end training fully convolutional network <eos> network operates directly raw sensor data replaces much traditional image processing pipeline tends perform poorly such data <eos> report promising result new dataset analyze factors affect performance highlight opportunities future work <eos> <eop> bpgrad towards global optimality deep learning via branch pruning <eos> understanding global optimality deep learning dl attracting more more attention recently <eos> conventional dl solvers however developed intentionally seek such global optimality <eos> paper propose novel approximation algorithm em bpgrad towards optimizing deep models globally via branch pruning <eos> bpgrad based assumption lipschitz continuity dl result adaptively determine step size current gradient given history previous updates wherein theoretically no smaller steps achieve global optimality <eos> prove repeating such branch pruning procedure locate global optimality within finite iterations <eos> empirically efficient solver based bpgrad dl proposed well outperforms conventional dl solvers such adagrad adadelta rmsprop adam tasks object recognition detection segmentation <eos> <eop> perturbative neural network <eos> convolutional neural network witnessing wide adoption computer vision systems numerous applications across range visual recognition tasks <eos> much progress fueled through advances convolutional neural network architectures learning algorithms even basic premise convolutional layer remained unchanged <eos> paper seek revisit convolutional layer workhorse state art visual recognition models <eos> introduce very simple yet effective module called perturbation layer alternative convolutional layer <eos> perturbation layer away convolution traditional sense instead computes its response weighted linear combination non linearly activated additive noise perturbed inputs <eos> demonstrate both analytically empirically perturbation layer effective replacement standard convolutional layer <eos> empirically deep neural network perturbation layer called perturbative neural network pnns lieu convolutional layer perform comparably standard cnn range visual datasets mnist cifar pascal voc imagenet fewer parameters <eos> <eop> unsupervised correlation analysis <eos> linking between two data sources basic building block numerous computer vision problems <eos> paper set answer fundamental cognitive question prior correspondences necessary linking between different domains one most popular method linking between domains canonical correlation analysis cca <eos> all current cca algorithms require correspondences between views <eos> introduce new method unsupervised correlation analysis uca requires no prior correspondences between two domains <eos> correlation maximization term cca replaced combination reconstruction term similar autoencoders full cycle loss orthogonality multiple domain confusion terms <eos> due lack supervision optimization leads multiple alternative solutions similar scores therefore introduce consensus based mechanism often able recover desired solution <eos> remarkably suffices order link remote domains such text image <eos> also present result well accepted cca benchmarks showing performance far exceeds other unsupervised baselines approaches supervised performance some cases <eos> <eop> biresolution spectral framework product quantization <eos> product quantization pq its variants effec tively used encode high dimensional data into compact codes many problems vision <eos> principle pq decomposes given data into number lower dimensional subspaces quantization proceeds independently each subspace <eos> while original pq approach explicitly optimize subspaces later proposals argued performance tends benefit significantly if such subspaces chosen optimal manner <eos> despite such consensus existing approaches literature diverge terms specific properties subspaces desirable how one should proceed solve optimize them <eos> nonetheless despite empirical support there less clarity regarding theoretical properties underlie experimental benefits quantization problems general <eos> paper study quantization problem setting subspaces orthogonal show problem intricately related specific type spectral decomposition data <eos> insight only opens door rich body work spectral analysis but also leads distinct computational benefits <eos> resultant biresolution spectral formulation captures both subspace projection error well quantization error within same framework <eos> after reformulation core steps algorithm involve simple eigen decomposition step solved efficiently <eos> show method performs very favorably against number state art method standard data set <eos> <eop> domain adaptive faster cnn object detection wild <eos> object detection typically assumes training test data drawn identical distribution however always hold practice <eos> such distribution mismatch will lead significant performance drop <eos> work aim improve cross domain robustness object detection <eos> tackle domain shift two levels image level shift such image style illumination etc instance level shift such object appearance size etc <eos> build approach based recent state art faster cnn model design two domain adaptation components image level instance level reduce domain discrepancy <eos> two domain adaptation components based divergence theory implemented learning domain classifier adversarial training manner <eos> domain classifiers different levels further reinforced consistency regularization learn domain invariant region proposal network rpn faster cnn model <eos> evaluate newly proposed approach using multiple datasets including cityscapes kitti sim etc <eos> result demonstrate effectiveness proposed approach robust object detection various domain shift scenarios <eos> <eop> low shot learning large scale diffusion <eos> paper considers problem inferring image labels image when only few annotated examples available training time <eos> setup often referred low shot learning standard approach re train last few layer convolutional neural network learned separate classes training examples abundant <eos> consider semi supervised setting based large collection image support label propagation <eos> possible leveraging recent advances large scale similarity graph construction <eos> show despite its conceptual simplicity scaling label propagation up hundred millions image leads state art accuracy low shot learning regime <eos> <eop> joint pose expression modeling facial expression recognition <eos> facial expression recognition fer challenging task due different expressions under arbitrary poses <eos> most conventional approaches either perform face frontalization non frontal facial image learn separate classifiers each pose <eos> different existing method paper propose end end deep learning model exploiting different poses expressions jointly simultaneous facial image synthesis pose invariant facial expression recognition <eos> proposed model based generative adversarial network gan enjoys several merits <eos> first encoder decoder structure generator learn generative discriminative identity representation face image <eos> second identity representation explicitly disentangled both expression pose variations through expression pose codes <eos> third model automatically generate face image different expressions under arbitrary poses enlarge enrich training set fer <eos> quantitative qualitative evaluations both controlled wild datasets demonstrate proposed algorithm performs favorably against state art method <eos> <eop> lightweight probabilistic deep network <eos> even though probabilistic treatments neural network long history they found widespread use practice <eos> sampling approaches often too slow already simple network <eos> size inputs depth typical cnn architectures computer vision only compound problem <eos> uncertainty neural network thus largely ignored practice despite fact may provide important information about reliability predictions inner workings network <eos> paper introduce two lightweight approaches making supervised learning probabilistic deep network practical first suggest probabilistic output layer classification regression require only minimal changes existing network <eos> second employ assumed density filtering show activation uncertainties propagated practical fashion through entire network again minor changes <eos> both probabilistic network retain predictive power deterministic counterpart but yield uncertainties correlate well empirical error induced their predictions <eos> moreover robustness adversarial examples significantly increased <eos> <eop> adversarially learned one class classifier novelty detection <eos> novelty detection process identifying observation differ some respect training observations target class <eos> reality novelty class often absent during training poorly sampled well defined <eos> therefore one class classifiers efficiently model such problems <eos> however due unavailability data novelty class training end end deep network cumbersome task <eos> paper inspired success generative adversarial network training deep models unsupervised semi supervised settings propose end end architecture one class classification <eos> architecture composed two deep network each trained competing each other while collaborating understand underlying concept target class then classify testing sample <eos> one network works novelty detector while other supports enhancing inlier sample distorting outliers <eos> intuition separability enhanced inliers distorted outliers much better than deciding original sample <eos> proposed framework applies different related applications anomaly outlier detection image video <eos> result mnist caltech image datasets along challenging ucsd ped dataset video anomaly detection illustrate proposed method learns target class effectively superior baseline state art method <eos> <eop> defense against universal adversarial perturbations <eos> recent advances deep learning show existence image agnostic quasi imperceptible perturbations when applied any image fool state art network classifier change its prediction about image label <eos> universal adversarial perturbations pose serious threat success deep learning practice <eos> present first dedicated framework effectively defend network against such perturbations <eos> approach learns perturbation rectifying network prn pre input layer targeted model such targeted model needs no modification <eos> prn learned real synthetic image agnostic perturbations efficient method compute latter also proposed <eos> perturbation detector separately trained discrete cosine transform input output difference prn <eos> query image first passed through prn verified detector <eos> if perturbation detected output prn used label prediction instead actual image <eos> rigorous evaluation shows framework defend network classifiers against unseen adversarial perturbations real world scenarios up <eos> prn also generalizes well sense training one targeted network defends another network comparable success rate <eos> <eop> disentangling factors variation mixing them <eos> propose approach learn image representations consist disentangled factors variation without exploiting any manual labeling data domain knowledge <eos> factor variation corresponds image attribute discerned consistently across set image such pose color object <eos> disentangled representation consists concatenation feature chunks each chunk representing factor variation <eos> supports applications such transferring attributes one image another simply mixing unmixing feature chunks classification retrieval based one several attributes considering user specified subset feature chunks <eos> learn representation without any labeling knowledge data domain using autoencoder architecture two novel training objectives first propose invariance objective encourage encoding each attribute decoding each chunk invariant changes other attributes chunks respectively second include classification objective ensures each chunk corresponds consistently discernible attribute represented image hence avoiding degenerate feature mappings some chunks completely ignored <eos> demonstrate effectiveness approach mnist sprites celeba datasets <eos> <eop> deformable gans pose based human image generation <eos> paper address problem generating person image conditioned given pose <eos> specifically given image person target pose synthesize new image person novel pose <eos> order deal pixel pixel misalignments caused pose differences introduce deformable skip connections generator generative adversarial network <eos> moreover nearest neighbour loss proposed instead common losses order match details generated image target image <eos> test approach using photos persons different poses compare method previous work area showing state art result two benchmarks <eos> method applied wider field deformable object generation provided pose articulated object extracted using keypoint detector <eos> <eop> hierarchical recurrent attention network structured online maps <eos> paper tackle problem online road network extraction sparse three dimensional point clouds <eos> method inspired how annotator builds lane graph first identifying how many lanes there then drawing each one turn <eos> develop hierarchical recurrent network attends initial region lane boundary traces them out completely outputting structured polyline <eos> also propose novel differentiable loss function measures deviation edges ground truth polylines their predictions <eos> more suitable than distances vertices there exists many ways draw equivalent polylines <eos> demonstrate effectiveness method km stretch highway show recover right topology time <eos> <eop> sliced wasserstein distance learning gaussian mixture models <eos> gaussian mixture models gmm powerful parametric tools many applications machine learning computer vision <eos> expectation maximization em most popular algorithm estimating gmm parameters <eos> however em guarantees only convergence stationary point log likelihood function could arbitrarily worse than optimal solution <eos> inspired relationship between negative log likelihood function kullback leibler kl divergence propose alternative formulation estimating gmm parameters using sliced wasserstein distance gives rise new algorithm <eos> specifically propose minimizing sliced wasserstein distance between mixture model data distribution respect gmm parameters <eos> contrast kl divergence energy landscape sliced wasserstein distance more well behaved therefore more suitable stochastic gradient descent scheme obtain optimal gmm parameters <eos> show formulation result parameter estimates more robust random initializations demonstrate estimate high dimensional data distributions more faithfully than em algorithm <eos> <eop> aligning infinite dimensional covariance matrices reproducing kernel hilbert spaces domain adaptation <eos> domain shift occurs when there mismatch between distributions training source testing target datasets usually result poor performance trained model target domain <eos> existing algorithms typically solve issue reducing distribution discrepancy input spaces <eos> however kernel based learning machines performance highly depends statistical properties data reproducing kernel hilbert spaces rkhs <eos> motivated considerations propose novel strategy matching distributions rkhs done aligning rkhs covariance matrices descriptors across domains <eos> strategy generalization correlation alignment problem euclidean spaces potentially infinite dimensional feature spaces <eos> paper provide two alignment approaches both obtain closed form expressions via kernel matrices <eos> furthermore approaches scalable large datasets since they naturally handle out sample instances <eos> conduct extensive experiments domain adaptation tasks evaluate approaches <eos> experiment result show approaches outperform other state art method both accuracy computationally efficiency <eos> <eop> clear cumulative learning one shot one class image recognition <eos> work addresses novel problem one shot one class classification <eos> goal estimate classification decision boundary novel class based single image example <eos> method exploits transfer learning model transformation representation input extracted convolutional neural network classification decision boundary <eos> use deep neural network learn transformation large labelled dataset image their associated class decision boundaries generated imagenet then apply learned decision boundary classify subsequent query image <eos> tested approach several benchmark datasets significantly outperformed baseline method <eos> <eop> local global optimization techniques graph based clustering <eos> goal graph based clustering divide dataset into disjoint subsets members similar each other affinity similarity matrix between data <eos> most popular method solving graph based clustering spectral clustering <eos> however spectral clustering drawbacks <eos> spectral clustering only applied macro average based cost functions tend generate undesirable small clusters <eos> study first introduces novel cost function based micro average <eos> propose local optimization method widely applicable graph based clustering cost functions <eos> also propose initial guess free algorithm avoid its initialization dependency <eos> moreover present two global optimization techniques <eos> experimental result exhibit significant clustering performances proposed method including clustering accuracy coil dataset <eos> <eop> multi task learning maximizing statistical dependence <eos> present new multi task learning mtl approach applied multiple heterogeneous task estimators <eos> motivation best task estimator could change depending task itself <eos> example may deep neural network first task gaussian process second task <eos> classical mtl approaches cannot handle case they require same model even same parameter types all tasks <eos> tackle considering task specific estimators random variables <eos> then task relationships discovered measuring statistical dependence between each pair random variables <eos> doing so model independent parametric nature each task even agnostic existence such parametric formulation <eos> compare algorithm existing mtl approaches challenging real world ranking regression datasets show approach achieves comparable better performance without knowing parametric form <eos> <eop> robust classification convolutional prototype learning <eos> convolutional neural network cnn widely used image classification <eos> despite its high accuracies cnn shown easily fooled some adversarial examples indicating cnn robust enough pattern classification <eos> paper argue lack robustness cnn caused softmax layer totally discriminative model based assumption closed world <eos> fixed number categories <eos> improve robustness propose novel learning framework called convolutional prototype learning cpl <eos> advantage using prototypes well handle open world recognition problem therefore improve robustness <eos> under framework cpl design multiple classification criteria train network <eos> moreover prototype loss pl proposed regularization improve intra class compactness feature representation viewed generative model based gaussian assumption different classes <eos> experiments several datasets demonstrate cpl achieve comparable even better result than traditional cnn robustness perspective cpl shows great advantages both rejection incremental category learning tasks <eos> <eop> generative modeling using sliced wasserstein distance <eos> generative adversarial nets gans very successful modeling distributions given sample even high dimensional case <eos> however their formulation also known hard optimize often stable <eos> while particularly true early gan formulations there significant empirically motivated theoretically founded progress improve stability instance using wasserstein distance rather than jenson shannon divergence <eos> here consider alternative formulation generative modeling based random projections its simplest form result single objective rather than saddle point formulation <eos> augmenting approach discriminator improve its accuracy <eos> found ap proach significantly more stable compared even improved wasserstein gan <eos> further unlike traditional gan loss loss formulated method good mea sure actual distance between distributions first time gan training able show estimates same <eos> <eop> learning time memory efficient deep architectures budgeted super network <eos> propose focus problem discovering neural network architectures efficient terms both prediction quality cost <eos> instance approach able solve following tasks learn neural network able predict well less than milliseconds learn efficient model fits mb memory <eos> contribution novel family models called budgeted super network bsn <eos> they learned using gradient descent techniques applied budgeted learning objective function integrates maximum authorized cost while making no assumption nature cost <eos> present set experiments computer vision problems analyze ability technique deal three different costs computation cost memory consumption cost distributed computation cost <eos> particularly show model discover neural network architectures better accuracy than resnet convolutional neural fabrics architectures cifar cifar lower cost <eos> <eop> cross view image synthesis using conditional gans <eos> learning generate natural scenes always challenging task computer vision <eos> even more painstaking when generation conditioned image drastically different views <eos> mainly because understanding corresponding transforming appearance semantic information across views trivial <eos> paper attempt solve novel problem cross view image synthesis aerial street view vice versa using conditional generative adversarial network cgan <eos> two new architectures called crossview fork fork crossview sequential seq proposed generate scenes resolutions pixels <eos> fork architecture single discriminator single generator <eos> generator hallucinates both image its semantic segmentation target view <eos> seq architecture utilizes two cgans <eos> first one generates target image subsequently fed second cgan generating its corresponding semantic segmentation map <eos> feedback second cgan helps first cgan generate sharper image <eos> both proposed architectures learn generate natural image well their semantic segmentation maps <eos> proposed method show they able capture maintain true semantics object source target views better than traditional image image translation method considers only visual appearance scene <eos> extensive qualitative quantitative evaluations support effectiveness frameworks compared two state art method natural scene generation across drastically different views <eos> <eop> sparse smart contours represent edit image <eos> study problem reconstructing image information stored contour locations <eos> show high quality reconstructions high fidelity source image obtained sparse input <eos> comprising less than image pixels <eos> significant improvement over existing contour based reconstruction method require much denser input capture subtle texture information ensure image quality <eos> model based generative adversarial network synthesizes texture details region no input information provided <eos> semantic knowledge encoded into model sparsity input allows use contours intuitive interface semantically aware image manipulation local edits contour domain translate long range coherent changes pixel space <eos> perform complex structural changes such changing facial expression simple edits contours <eos> experiments demonstrate humans well face recognition system mostly cannot distinguish between reconstructions source image <eos> <eop> anticipating traffic accidents adaptive loss large scale incident db <eos> paper propose novel approach traffic accident anticipation through adaptive loss early anticipation adalea ii large scale self annotated incident database <eos> proposed adalea allows gradually learn earlier anticipation training progresses <eos> loss function adaptively assigns penalty weights depending how early model anticipate traffic accident each epoch <eos> additionally new near miss incident database nidb contains enormous number traffic near miss incidents four classes cyclist pedestrian vehicle background class labeled discussed <eos> nidb provides joint estimations traffic incident anticipation risk factor categorization <eos> experimental result found proposal achieved highest scores anticipation <eos> mean average precision map <eos> sec anticipation average time collision attc values <eos> sec faster than previous work joint estimation <eos> sec anticipation attc values <eos> sec faster than previous work <eos> <eop> minimalist approach type agnostic detection quadrics point clouds <eos> paper proposes segmentation free automatic efficient procedure detect general geometric quadric forms point clouds clutter occlusions inevitable <eos> everyday world dominated man made object designed using three dimensional primitives such planes cones spheres cylinders etc <eos> object also omnipresent industrial environments <eos> gives rise possibility abstracting three dimensional scenes through primitives thereby positions geometric forms integral part perception high level three dimensional scene understanding <eos> opposed state art tailored algorithm treats each primitive type separately propose encapsulate all types single robust detection procedure <eos> center approach lies closed form three dimensional quadric fit operating both primal dual spaces requiring low oriented point <eos> around fit design novel local null space voting strategy reduce point case <eos> voting coupled famous ransac makes algorithm orders magnitude faster than its conventional counterparts <eos> first method capable performing generic cross type multi object primitive detection difficult scenes <eos> result synthetic real datasets support validity method <eos> <eop> facelet bank fast portrait manipulation <eos> digital face manipulation become popular fascinating way touch image prevalence smart phones social network <eos> wide variety user preferences facial expressions accessories general flexible model necessary accommodate different types facial editing <eos> paper propose model achieve goal based end end convolutional neural network supports fast inference edit effect control quick partial model update <eos> addition model learns unpaired image set different attributes <eos> experimental result show framework handle wide range expressions accessories makeup effects <eos> produces high resolution high quality result fast speed <eos> <eop> visual sound generating natural sound video wild <eos> two five traditional human senses sight hearing taste smell touch vision sound basic sources through humans understand world <eos> often correlated during natural events two modalities combine jointly affect human perception <eos> paper pose task generating sound given visual input <eos> such capabilities could help enable applications virtual reality generating sound virtual scenes automatically provide additional accessibility image video people visual impairments <eos> first step direction apply learning based method generate raw waveform sample given input video frames <eos> evaluate models dataset video containing variety sounds such ambient sounds sounds people animals <eos> experiments show generated sounds fairly realistic good temporal synchronization visual inputs <eos> <eop> rcnn instance level three dimensional object reconstruction via render compare <eos> present fast inverse graphics framework instance level three dimensional scene understanding <eos> train deep convolutional network learns map image region full three dimensional shape pose all object instances image <eos> method produces compact three dimensional representation scene readily used applications like autonomous driving <eos> many traditional vision outputs like instance segmentations depth maps obtained simply rendering output three dimensional scene model <eos> exploit class specific shape priors learning low dimensional shape space collections cad models <eos> present novel representations shape pose strive towards better three dimensional equivariance generalization <eos> order exploit rich supervisory signals form annotations like segmentation propose differentiable render compare loss allows three dimensional shape pose learned supervision <eos> evaluate method challenging real world datasets pascal kitti achieve state art result <eos> <eop> fast furious real time end end three dimensional detection tracking motion forecasting single convolutional net <eos> paper propose novel deep neural network able jointly reason about three dimensional detection tracking motion forecasting given data captured three dimensional sensor <eos> jointly reasoning about tasks holistic approach more robust occlusion well sparse data range <eos> approach performs three dimensional convolutions across space time over bird eye view representation three dimensional world very efficient terms both memory computation <eos> experiments new very large scale dataset captured several north american cities show outperform state art large margin <eos> importantly sharing computation perform all tasks little ms <eos> <eop> analysis scale invariance object detection snip <eos> analysis different techniques recognizing detecting object under extreme scale variation presented <eos> scale specific scale invariant design detectors compared training them different configurations input data <eos> evaluating performance different network architectures classifying small object imagenet show cnn robust changes scale <eos> based analysis propose train test detectors same scales image pyramid <eos> since small large object difficult recognize smaller larger scales respectively present novel training scheme called scale normalization image pyramids snip selectively back propagates gradients object instances different sizes function image scale <eos> coco dataset single model performance <eos> ensemble network obtains map <eos> use off shelf imagenet pre trained models only train bounding box supervision <eos> submission won best student entry coco challenge <eos> code will made available url bit <eos> <eop> relation network object detection <eos> although well believed years modeling relations between object would help object recognition there evidence idea working deep learning era <eos> all state art object detection systems still rely recognizing object instances extbf individually without exploiting their relations during learning <eos> work proposes object relation module <eos> processes set object extbf simultaneously through interaction between their appearance feature geometry thus allowing modeling their relations <eos> require additional supervision easy embed existing network <eos> shown effective improving object recognition duplicate removal steps modern object detection pipeline <eos> verifies efficacy modeling object relations cnn based detection <eos> gives rise extbf first fully end end object detector <eos> <eop> zero shot sketch image hashing <eos> recent studies show large scale sketch based image retrieval sbir efficiently tackled cross modal binary representation learning method hamming distance matching significantly speeds up process similarity search <eos> providing training test data subjected fixed set pre defined categories cutting edge sbir cross modal hashing works obtain acceptable retrieval performance <eos> however most existing method fail when categories query sketches never seen during training <eos> paper above problem briefed novel but realistic zero shot sbir hashing task <eos> elaborate challenges special task accordingly propose zero shot sketch image hashing zsih model <eos> end end three network architecture built two treated binary encoders <eos> third network mitigates sketch image heterogeneity enhances semantic relations among data utilizing kronecker fusion layer graph convolution respectively <eos> important part zsih formulate generative hashing scheme reconstructing semantic knowledge representations zero shot retrieval <eos> best knowledge zsih first zero shot hashing work suitable sbir cross modal search <eos> comprehensive experiments conducted two extended datasets <eos> sketchy tu berlin novel zero shot train test split <eos> proposed model remarkably outperforms related works <eos> <eop> vizwiz grand challenge answering visual questions blind people <eos> study algorithms automatically answer visual questions currently motivated visual question answering vqa datasets constructed artificial vqa settings <eos> propose vizwiz first goal oriented vqa dataset arising natural vqa setting <eos> vizwiz consists visual questions originating blind people who each took picture using mobile phone recorded spoken question about together crowdsourced answers per visual question <eos> vizwiz differs many existing vqa datasets because image captured blind photographers so often poor quality questions spoken so more conversational often visual questions cannot answered <eos> evaluation modern algorithms answering visual questions deciding if visual question answerable reveals vizwiz challenging dataset <eos> introduce dataset encourage larger community develop more generalized algorithms assist blind people <eos> <eop> divide grow capturing huge diversity crowd image incrementally growing cnn <eos> automated counting people crowd image challenging task <eos> major difficulty stems large diversity way people appear crowds <eos> fact feature available crowd discrimination largely depend crowd density extent people only seen blobs highly dense scene <eos> tackle problem growing cnn progressively increase its capacity account wide variability seen crowd scenes <eos> model starts base cnn density regressor trained equivalence all types crowd image <eos> order adapt huge diversity create two child regressors exact copies base cnn <eos> differential training procedure divides dataset into two clusters fine tunes child network their respective specialties <eos> consequently without any hand crafted criteria forming specialties child regressors become experts certain types crowds <eos> child network again split recursively creating two experts every division <eos> hierarchical training leads cnn tree child regressors more fine experts than any their parents <eos> leaf nodes taken final experts classifier network then trained predict correct specialty given test image patch <eos> proposed model achieves higher count accuracy major crowd datasets <eos> further analyse characteristics specialties mined automatically method <eos> <eop> structured set matching network one shot part labeling <eos> diagrams often depict complex phenomena serve good test bed visual textual reasoning <eos> however understanding diagrams using natural image understanding approaches requires large training datasets diagrams very hard obtain <eos> instead addressed matching problem either between labeled diagrams image both <eos> problem very challenging since absence significant color texture renders local cues ambiguous requires global reasoning <eos> consider problem one shot part labeling labeling multiple parts object target image given only single source image category <eos> set set matching problem introduce structured set matching network ssmn structured prediction model incorporates convolutional neural network <eos> ssmn trained using global normalization maximize local match scores between corresponding elements global consistency score among all matched elements while also enforcing matching constraint between two set <eos> ssmn significantly outperforms several strong baselines three label transfer scenarios diagram diagram evaluated new diagram dataset over categories image image evaluated dataset built top pascal part dataset image diagram evaluated transferring labels across datasets <eos> <eop> self supervised learning geometrically stable feature through probabilistic introspection <eos> self supervision dramatically cut back amount manually labelled data required train deep neural network <eos> while self supervision usually considered tasks such image classification paper aim extending geometry oriented tasks such semantic matching part detection <eos> so building several recent ideas unsupervised landmark detection <eos> approach learns dense distinctive visual descriptors unlabeled dataset image using synthetic image transformations <eos> so means robust probabilistic formulation introspectively determine image region likely result stable image matching <eos> show empirically network pre trained manner requires significantly less supervision learn semantic object parts compared numerous pre training alternatives <eos> also show pre trained representation excellent semantic object matching <eos> <eop> link code fast indexing graphs compact regression codes <eos> similarity search approaches based graph walks recently attained outstanding speed accuracy trade offs taking aside memory requirements <eos> paper revisit approaches considering additionally memory constraint required index billions image single server <eos> leads propose method based both graph traversal compact representations <eos> encode indexed vectors using quantization exploit graph structure refine similarity estimation <eos> essence method takes best two worlds search strategy based nested graphs thereby providing high precision relatively small set comparisons <eos> same time offers significant memory compression <eos> result approach outperforms state art operating point considering bytes per vector demonstrated result two billion scale public benchmarks <eos> <eop> textbook question answering under instructor guidance memory network <eos> textbook question answering tqa task choose most proper answers reading multi modal context abundant essays image <eos> tqa serves favorable test bed visual textual reasoning <eos> however most current method incapable reasoning over long contexts image <eos> address issue propose novel approach instructor guidance memory network igmn conducts tqa task finding contradictions between candidate answers their corresponding context <eos> build contradiction entity relationship graph cerg extend passage level multi modal contradictions essay level <eos> machine thus performs instructor extract essay level contradictions guidance <eos> afterwards exploit memory network capture information guidance use attention mechanisms jointly reason over global feature multi modal input <eos> extensive experiments demonstrate method outperforms state arts tqa dataset <eos> source code available github <eos> com freerailway igmn <eos> <eop> unsupervised deep generative adversarial hashing network <eos> unsupervised deep hash functions shown satisfactory improvements against shallow alternatives usually require supervised pretraining avoid getting stuck bad local minima <eos> paper propose deep unsupervised hashing function called hashgan outperforms unsupervised hashing models significant margins without any supervised pretraining <eos> hashgan consists three network generator discriminator encoder <eos> sharing parameters encoder discriminator benefit adversarial loss data dependent regularization training deep hash function <eos> moreover novel loss function introduced hashing real image resulting minimum entropy uniform frequency consistent independent hash bits <eos> furthermore train generator conditioning random binary inputs also use binary variables triplet ranking loss improving hash codes <eos> experiments hashgan outperforms previous unsupervised hash functions image retrieval achieves state art performance image clustering <eos> also provide ablation study showing contribution each component loss function <eos> <eop> vision language navigation interpreting visually grounded navigation instructions real environments <eos> robot carry out natural language instruction dream since before jetsons cartoon series imagined life leisure mediated fleet attentive robot helpers <eos> dream remains stubbornly distant <eos> however recent advances vision language method made incredible progress closely related areas <eos> significant because robot interpreting natural language navigation instruction basis sees carrying out vision language process similar visual question answering <eos> both tasks interpreted visually grounded sequence sequence translation problems many same method applicable <eos> enable encourage application vision language method problem interpreting visually grounded navigation instructions present matterport simulator large scale reinforcement learning environment based real imagery <eos> using simulator future support range embodied vision language tasks provide first benchmark dataset visually grounded natural language navigation real buildings room room dataset <eos> <eop> denseaspp semantic segmentation street scenes <eos> semantic image segmentation basic street scene understanding task autonomous driving each pixel high resolution image categorized into set semantic labels <eos> unlike other scenarios object autonomous driving scene exhibit very large scale changes poses great challenges high level feature representation sense multi scale information must correctly encoded <eos> remedy problem atrous convolutioncite deeplabv was introduced generate feature larger receptive fields without sacrificing spatial resolution <eos> built upon atrous convolution atrous spatial pyramid pooling aspp cite deeplabv was proposed concatenate multiple atrous convolved feature using different dilation rates into final feature representation <eos> although aspp able generate multi scale feature argue feature resolution scale axis dense enough autonomous driving scenario <eos> end propose densely connected atrous spatial pyramid pooling denseaspp connects set atrous convolutional layer dense way such generates multi scale feature only cover larger scale range but also cover scale range densely without significantly increasing model size <eos> evaluate denseaspp street scene benchmark cityscapescite cityscapes achieve state art performance <eos> <eop> efficient optimization rank based loss functions <eos> accuracy information retrieval systems often measured using complex loss functions such average precision ap normalized discounted cumulative gain ndcg <eos> given set positive negative sample parameters retrieval system estimated minimizing loss functions <eos> however non differentiability non decomposability loss functions allow simple gradient based optimization algorithms <eos> issue generally circumvented either optimizing structured hinge loss upper bound loss function using asymptotic method like direct loss minimization framework <eos> yet high computational complexity loss augmented inference necessary both frameworks prohibits its use large training data set <eos> alleviate deficiency present novel quicksort flavored algorithm large class non decomposable loss functions <eos> provide complete characterization loss functions amenable algorithm show includes both ap ndcg based loss functions <eos> furthermore prove no comparison based algorithm improve upon computational complexity approach asymptotically <eos> demonstrate effectiveness approach context optimizing structured hinge loss upper bound ap ndcg loss learning models variety vision tasks <eos> show approach provides significantly better result than simpler decomposable loss functions while requiring comparable training time <eos> <eop> wasserstein introspective neural network <eos> present wasserstein introspective neural network winn both generator discriminator within single model <eos> winn provides significant improvement over recent introspective neural network inn method enhancing inn generative modeling capability <eos> winn three interesting properties mathematical connection between formulation inn algorithm wasserstein generative adversarial network wgan made <eos> explicit adoption wasserstein distance into inn result large enhancement inn achieving compelling result even single classifier <eos> providing nearly times reduction model size over inn unsupervised generative modeling <eos> when applied supervised classification winn also gives rise improved robustness against adversarial examples terms error reduction <eos> experiments report encouraging result unsupervised learning problems including texture face object modeling well supervised classification task against adversarial attacks <eos> <eop> taskonomy disentangling task transfer learning <eos> visual tasks relationship they unrelated instance could having surface normals simplify estimating depth image intuition answers questions positively implying existence structure among visual tasks <eos> knowing structure notable uses concept underlying transfer learning example provide principled way reusing supervision among related tasks finding tasks transfer well arbitrary target task solving many tasks one system without piling up complexity <eos> paper proposes fully computational approach finding structure space visual tasks <eos> done via sampled dictionary twenty six <eos> three dimensional semantic tasks modeling their st higher order transfer dependencies latent space <eos> product viewed computational taxonomic map task transfer learning <eos> study consequences structure <eos> nontrivial emerged relationships exploit them reduce demand labeled data <eos> example show total number labeled datapoints needed solving set tasks reduced roughly while keeping performance nearly same <eos> users employ provided binary integer programming solver leverages taxonomy find efficient supervision policies their own use cases <eos> <eop> maximum classifier discrepancy unsupervised domain adaptation <eos> work present method unsupervised domain adaptation <eos> many adversarial learning method train domain classifier network distinguish feature either source target train feature generator network mimic discriminator <eos> two problems exist method <eos> first domain classifier only tries distinguish feature source target thus consider task specific decision boundaries between classes <eos> therefore trained generator generate ambiguous feature near class boundaries <eos> second method aim completely match feature distributions between different domains difficult because each domain characteristics <eos> solve problems introduce new approach attempts align distributions source target utilizing task specific decision boundaries <eos> propose maximize discrepancy between two classifiers outputs detect target sample far support source <eos> feature generator learns generate target feature near support minimize discrepancy <eos> method outperforms other method several datasets image classification semantic segmentation <eos> codes available url github <eos> com mil tokyo mcd da <eop> unsupervised feature learning via non parametric instance discrimination <eos> neural net classifiers trained data annotated class labels also capture apparent visual similarity among categories without being directed so <eos> study whether observation extended beyond conventional domain supervised learning learn good feature representation captures apparent similarity among instances instead classes merely asking feature discriminative individual instances formulate intuition non parametric classification problem instance level use noise contrastive estimation tackle computational challenges imposed large number instance classes <eos> experimental result demonstrate under unsu pervised learning settings method surpasses state art imagenet classification large margin <eos> method also remarkable consistently improving test performance more training data better network architectures <eos> fine tuning learned feature further obtain competitive result semi supervised learning object detection tasks <eos> non parametric model highly compact feature per image method requires only mb storage million image enabling fast nearest neighbour retrieval run time <eos> <eop> multi task adversarial network disentangled feature learning <eos> address problem image feature learning applications multiple factors exist image generation process only some factors interest <eos> present novel multi task adversarial network based encoder discriminator generator architecture <eos> encoder extracts disentangled feature representation factors interest <eos> discriminators classify each factors individual tasks <eos> encoder discriminators trained cooperatively factors interest but adversarial way factors distraction <eos> generator provides further regularization learned feature reconstructing image shared factors input image <eos> design new optimization scheme stabilize adversarial optimization process when multiple distributions need aligned <eos> experiments face recognition font recognition tasks show method outperforms state art method terms both recognizing factors interest generalization image unseen variations <eos> <eop> learning synthetic data addressing domain shift semantic segmentation <eos> visual domain adaptation problem immense importance computer vision <eos> previous approaches showcase inability even deep neural network learn informative representations across domain shift <eos> problem more severe tasks acquiring hand labeled data extremely hard tedious <eos> work focus adapting representations learned segmentation network across synthetic real domains <eos> contrary previous approaches use simple adversarial objective superpixel information aid process propose approach based generative adversarial network gans brings embeddings closer learned feature space <eos> showcase generality scalability approach show achieve state art result two challenging scenarios synthetic real domain adaptation <eos> additional exploratory experiments show approach generalizes unseen domains result improved alignment source target distributions <eos> <eop> empirical study topology geometry deep network <eos> goal paper analyze geometric properties deep neural network image classifiers input space <eos> specifically study topology classification region created deep network well their associated decision boundary <eos> through systematic empirical study show state art deep nets learn connected classification region decision boundary vicinity datapoints flat along most directions <eos> further draw essential connection between two seemingly unrelated properties deep network their sensitivity additive perturbations inputs curvature their decision boundary <eos> directions decision boundary curved fact characterize directions classifier most vulnerable <eos> finally leverage fundamental asymmetry curvature decision boundary deep nets propose method discriminate between original image image perturbed small adversarial examples <eos> show effectiveness purely geometric approach detecting small adversarial perturbations image recovering labels perturbed image <eos> <eop> boosting domain adaptation discovering latent domains <eos> current domain adaptation da method based deep architectures assume source sample arise single distribution <eos> however practice most datasets regarded mixtures multiple domains <eos> cases exploiting single source da method learning target classifiers may lead sub optimal if poor result <eos> addition many applications difficult manually provide domain labels all source data point <eos> latent domains should automatically discovered <eos> paper introduces novel convolutional neural network cnn architecture automatically discovers latent domains visual datasets ii exploits information learn robust target classifiers <eos> approach based introduction two main components embedded into any existing cnn architecture side branch automatically computes assignment source sample latent domain ii novel layer exploit domain membership information appropriately align distribution cnn internal feature representations reference distribution <eos> test approach publicly available datasets showing outperforms state art multi source da method large margin <eos> <eop> shape shading through shape evolution <eos> paper address shape shading problem training deep network synthetic image <eos> unlike conventional approaches combine deep learning synthetic imagery propose approach need any external shape dataset render synthetic image <eos> approach consists two synergistic processes evolution complex shapes simple primitives training deep network shape shading <eos> evolution generates better shapes guided network training while training improves using evolved shapes <eos> show approach achieves state art performance shape shading benchmark <eos> <eop> weakly supervised instance segmentation using class peak response <eos> weakly supervised instance segmentation image level labels instead expensive pixel level masks remains unexplored <eos> paper tackle challenging problem exploiting class peak responses enable classification network instance mask extraction <eos> image labels supervision only cnn classifiers fully convolutional manner produce class response maps specify classification confidence each image location <eos> observed local maximums <eos> peaks class response map typically correspond strong visual cues residing inside each instance <eos> motivated first design process stimulate peaks emerge class response map <eos> emerged peaks then back propagated effectively mapped highly informative region each object instance such instance boundaries <eos> refer above maps generated class peak responses peak response maps prms <eos> prms provide fine detailed instance level representation allows instance masks extracted even some off shelf method <eos> best knowledge first time report result challenging image level supervised instance segmentation task <eos> extensive experiments show method also boosts weakly supervised pointwise localization well semantic segmentation performance reports state art result popular benchmarks including pascal voc ms coco <eos> <eop> collaborative adversarial network unsupervised domain adaptation <eos> paper propose new unsupervised domain adaptation approach called collaborative adversarial network through domain collaborative domain adversarial training neural network <eos> use several domain classifiers multiple cnn feature extraction layer blocks each domain classifier connected hidden representations one block one loss function defined based hidden presentation domain labels <eos> design new loss function integrating losses all blocks order learn informative representations lower layer through collaborative learning learn uninformative representations higher layer through adversarial learning <eos> further extend method incremental ican iteratively select set pseudo labelled target sample based image classifier last domain classifier previous training epoch re train model using enlarged training set <eos> comprehensive experiments two benchmark datasets office imageclef da clearly demonstrate effectiveness newly proposed approaches ican unsupervised domain adaptation <eos> <eop> environment upgrade reinforcement learning non differentiable multi stage pipelines <eos> recent advances multi stage algorithms shown great promise but two important problems still remain <eos> first all inference time information feed back downstream upstream <eos> second training time end end training possible if overall pipeline involves non differentiable functions so different stages jointly optimized <eos> paper propose novel environment upgrade reinforcement learning framework solve feedback joint optimization problems <eos> framework re links downstream stage upstream stage reinforcement learning agent <eos> while training agent improve final performance refining upstream stage output also upgrade downstream stage environment according agent policy <eos> way agent policy environment jointly optimized <eos> propose training algorithm framework address different training demands agent environment <eos> experiments instance segmentation human pose estimation demonstrate effectiveness proposed framework <eos> <eop> teaching categories human learners visual explanations <eos> study problem computer assisted teaching explanations <eos> conventional approaches machine teaching typically only provide feedback instance level <eos> category label instance <eos> however intuitive clear explanations knowledgeable teacher significantly improve student ability learn new concept <eos> address existing limitations propose teaching framework provides interpretable explanations feedback models how learner incorporates additional information <eos> case image show automatically generate explanations highlight parts image responsible class label <eos> experiments human learners illustrate average participants achieve better test set performance challenging categorization tasks when taught interpretable approach compared existing method <eos> <eop> density adaptive point set registration <eos> probabilistic method point set registration demonstrated competitive result recent years <eos> techniques estimate probability distribution model point clouds <eos> while such representation shown promise highly sensitive variations density three dimensional point <eos> fundamental problem primarily caused changes sensor location across point set <eos> revisit foundations probabilistic registration paradigm <eos> contrary previous works model underlying structure scene latent probability distribution thereby induce invariance point set density changes <eos> both probabilistic model scene registration parameters inferred minimizing kullback leibler divergence expectation maximization based framework <eos> density adaptive registration successfully handles severe density variations commonly encountered terrestrial lidar applications <eos> perform extensive experiments several challenging real world lidar datasets <eos> result demonstrate approach outperforms state art probabilistic method multi view registration without need re sampling <eos> <eop> left right comparative recurrent model stereo matching <eos> leveraging disparity information both left right views crucial stereo disparity estimation <eos> left right consistency check effective way enhance disparity estimation referring information opposite view <eos> however conventional left right consistency check isolated post processing step heavily hand crafted <eos> paper proposes novel left right comparative recurrent model perform left right consistency checking jointly disparity estimation <eos> each recurrent step model produces disparity result both views then performs online left right comparison identify mismatched region may probably contain erroneously labeled pixels <eos> soft attention mechanism introduced employs learned error maps better guiding model selectively focus refining unreliable region next recurrent step <eos> way generated disparity maps progressively improved proposed recurrent model <eos> extensive evaluations kitti scene flow middlebury benchmarks validate effectiveness model demonstrating state art stereo disparity estimation result achieved new model <eos> <eop> im pano extrapolating structure semantics beyond field view <eos> present im pano convolutional neural network generates dense prediction three dimensional structure probability distribution semantic labels full panoramic view indoor scene when given only partial observation form rgb image <eos> make possible im pano leverages strong contextual priors learned large scale synthetic real world indoor scenes <eos> ease prediction three dimensional structure propose parameterize three dimensional surfaces their plane equations train model predict parameters directly <eos> provide meaningful training supervision make use multiple loss functions consider both pixel level accuracy global context consistency <eos> experiments demonstrate im pano able predict semantics three dimensional structure unobserved scene more than pixel accuracy less than <eos> average distance error significantly better than alternative approaches <eos> <eop> polarimetric dense monocular slam <eos> paper presents novel polarimetric dense monocular slam pdms algorithm based polarization camera <eos> algorithm exploits both photometric polarimetric light information produce more accurate complete geometry <eos> polarimetric information allows recover azimuth angle surface normals each video frame facilitate dense reconstruction especially textureless specular region <eos> there two challenges approach surface azimuth angles polarization camera very noisy need near real time solution slam <eos> previous successful method polarimetric multi view stereo offline require manually pre segmented object masks suppress effects erroneous angle information along boundaries <eos> fully automatic approach efficiently iterates azimuth based depth propagations two view depth consistency check depth optimization produce depthmap real time all algorithmic steps carefully designed enable gpu implementation <eos> knowledge paper first propose photometric polarimetric method dense slam <eos> qualitatively quantitatively evaluated algorithm against few competing method demonstrating superior performance various indoor outdoor scenes <eos> <eop> unifying contrast maximization framework event cameras applications motion depth optical flow estimation <eos> present unifying framework solve several computer vision problems event cameras motion depth optical flow estimation <eos> main idea framework find point trajectories image plane best aligned event data maximizing objective function contrast image warped events <eos> method implicitly handles data association between events therefore rely additional appearance information about scene <eos> addition accurately recovering motion parameters problem framework produces motion corrected edge like image high dynamic range used further scene analysis <eos> proposed method only simple but more importantly best knowledge first method successfully applied such diverse set important vision tasks event cameras <eos> <eop> modeling facial geometry using compositional vaes <eos> propose method learning non linear face geometry representations using deep generative models <eos> model variational autoencoder multiple levels hidden variables lower layer capture global geometry higher ones encode more local deformations <eos> based propose new parameterization facial geometry naturally decomposes structure human face into set semantically meaningful levels detail <eos> parameterization enables model fitting while capturing varying level detail under different types geometrical constraints <eos> <eop> tangent convolutions dense prediction <eos> present approach semantic scene analysis using deep convolutional network <eos> approach based tangent convolutions new construction convolutional network three dimensional data <eos> contrast volumetric approaches method operates directly surface geometry <eos> crucially construction applicable unstructured point clouds other noisy real world data <eos> show tangent convolutions evaluated efficiently large scale point clouds millions point <eos> using tangent convolutions design deep fully convolutional network semantic segmentation three dimensional point clouds apply challenging real world datasets indoor outdoor three dimensional environments <eos> experimental result show presented approach outperforms other recent deep network constructions detailed analysis large three dimensional scenes <eos> <eop> raynet learning volumetric three dimensional reconstruction ray potentials <eos> paper consider problem reconstructing dense three dimensional model using image captured different views <eos> recent method based convolutional neural network cnn allow learning entire task data <eos> however they incorporate physics image formation such perspective geometry occlusion <eos> instead classical approaches based markov random fields mrf ray potentials explicitly model physical processes but they cannot cope large surface appearance variations across different viewpoints <eos> paper propose raynet combines strengths both frameworks <eos> raynet integrates cnn learns view invariant feature representations mrf explicitly encodes physics perspective projection occlusion <eos> train raynet end end using empirical risk minimization <eos> thoroughly evaluate approach challenging real world datasets demonstrate its benefits over piece wise trained baseline hand crafted models well other learning based approaches <eos> <eop> neural three dimensional mesh renderer <eos> modeling three dimensional world behind image three dimensional representation most appropriate polygon mesh promising candidate its compactness geometric properties <eos> however straightforward model polygon mesh image using neural network because conversion mesh image rendering involves discrete operation called rasterization prevents back propagation <eos> therefore work propose approximate gradient rasterization enables integration rendering into neural network <eos> using renderer perform single image three dimensional mesh reconstruction silhouette image supervision system outperforms existing voxel based approach <eos> additionally perform gradient based three dimensional mesh editing operations such three dimensional style transfer three dimensional deepdream supervision first time <eos> applications demonstrate potential integration mesh renderer into neural network effectiveness proposed renderer <eos> <eop> structured attention guided convolutional neural fields monocular depth estimation <eos> recent works shown benefit integrating conditional random fields crfs models into deep architectures improving pixel level prediction tasks <eos> following line research paper introduce novel approach monocular depth estimation <eos> similarly previous works method employs continuous crf fuse multi scale information derived different layer front end convolutional neural network cnn <eos> differently past works approach benefits structured attention model automatically regulates amount information transferred between corresponding feature different scales <eos> importantly proposed attention model seamlessly integrated into crf allowing end end training entire architecture <eos> extensive experimental evaluation demonstrates effectiveness proposed method competitive previous method kitti benchmark outperforms state art nyu depth dataset <eos> <eop> automatic three dimensional indoor scene modeling single panorama <eos> describe system automatically extracts three dimensional geometry indoor scene single panorama <eos> system recovers spatial layout finding floor walls ceiling also recovers shapes typical indoor object such furniture <eos> using sampled perspective sub views extract geometric cues lines vanishing point orientation map surface normals semantic cues saliency object detection information <eos> cues used ground plane estimation occlusion reasoning <eos> global spatial layout inferred through constraint graph line segments planar superpixels <eos> recovered layout then used guide shape estimation remaining object using their normal information <eos> experiments synthetic real datasets show approach state art both accuracy efficiency <eos> system handle cluttered scenes complex geometry challenging existing techniques <eos> <eop> extreme three dimensional face reconstruction seeing through occlusions <eos> existing single view three dimensional face reconstruction method produce beautifully detailed three dimensional result but typically only near frontal unobstructed viewpoints <eos> describe system designed provide detailed three dimensional reconstructions faces viewed under extreme conditions out plane rotations occlusions <eos> motivated concept bump mapping propose layered approach decouples estimation global shape its mid level details <eos> estimate coarse three dimensional face shape acts foundation then separately layer foundation details represented bump map <eos> show how deep convolutional encoder decoder used estimate such bump maps <eos> further show how approach naturally extends generate plausible details occluded facial region <eos> test approach its components extensively quantitatively demonstrating invariance estimated facial details <eos> further provide numerous qualitative examples showing method produces detailed three dimensional face shapes viewing conditions existing state art often break down <eos> <eop> beyond gr bner bases basis selection minimal solvers <eos> many computer vision applications require robust estimation underlying geometry terms camera motion three dimensional structure scene <eos> robust method often rely running minimal solvers ransac framework <eos> paper show how make polynomial solvers based action matrix method faster careful selection monomial bases <eos> monomial bases traditionally based gr bner basis polynomial ideal <eos> here describe how enumerate all such bases efficient way <eos> also show going beyond gr bner bases leads more efficient solvers many cases <eos> present novel basis sampling scheme evaluate number problems <eos> <eop> lions tigers bears capturing non rigid three dimensional articulated shape image <eos> animals widespread nature analysis their shape motion important many fields industries <eos> modeling three dimensional animal shape however difficult because three dimensional scanning method used capture human shape applicable wild animals natural settings <eos> consequently propose method capture detailed three dimensional shape animals image alone <eos> articulated deformable nature animals makes problem extremely challenging particularly unconstrained environments moving uncalibrated cameras <eos> make possible use strong prior model articulated animal shape fit image data <eos> then deform animal shape canonical reference pose such matches image evidence when articulated projected into multiple image <eos> method extracts significantly more three dimensional shape detail than previous method able model new species including shape extinct animal using only few video frames <eos> additionally projected three dimensional shapes accurate enough facilitate extraction realistic texture map multiple frames <eos> <eop> deep cocktail network multi source unsupervised domain adaptation category shift <eos> most existing unsupervised domain adaptation uda method based upon assumption source labeled data come identical underlying distribution <eos> whereas practical scenario labeled instances typically collected diverse sources <eos> moreover sources may completely share their categories further brings category shift challenge multi source unsupervised domain adaptation mda <eos> paper propose deep cocktail network dctn battle domain category shifts among multiple sources <eos> motivated theoretical result cite mansour domain target distribution represented weighted combination source distributions training mda via dctn then performed two alternating steps deploys multi way adversarial learning minimize discrepancy between target each multiple source domains also obtains source specific perplexity scores denote possibilities target sample belongs different source domains <eos> ii multi source category classifiers integrated perplexity scores classify target sample pseudo labeled target sample together source sample utilized update multi source category classifier representation module <eos> evaluate dctn three domain adaptation benchmarks clearly demonstrate superiority framework <eos> <eop> dota large scale dataset object detection aerial image <eos> object detection important challenging problem computer vision <eos> although past decade witnessed major advances object detection natural scenes such successes slow aerial imagery only because huge variation scale orientation shape object instances earth surface but also due scarcity well annotated datasets object aerial scenes <eos> advance object detection research earth vision also known earth observation remote sensing introduce large scale dataset object detection aerial image dota <eos> end collect aerial image different sensors platforms <eos> each image size about pixels contains object exhibiting wide variety scales orientations shapes <eos> dota image then annotated experts aerial image interpretation using common object categories <eos> fully annotated dota image contains instances each labeled arbitrary <eos> build baseline object detection earth vision evaluate state art object detection algorithms dota <eos> experiments demonstrate dota well represents real earth vision applications quite challenging <eos> <eop> finding beans burgers deep semantic visual embedding localization <eos> several works proposed learn two path neural network maps image texts respectively same shared euclidean space geometry captures useful semantic relationships <eos> such multi modal embedding trained used various tasks notably image captioning <eos> present work introduce new architecture type visual path leverages recent space aware pooling mechanisms <eos> combined textual path jointly trained scratch semantic visual embedding offers versatile model <eos> once trained under supervision captioned image yields new state art performance cross modal retrieval <eos> also allows localization new concepts embedding space into any input image delivering state art result visual grounding phrases <eos> <eop> feature super resolution make machine see more clearly <eos> identifying small size image small object notoriously challenging problem discriminative representations difficult learn limited information contained them poor quality appearance unclear object structure <eos> existing research works usually increase resolution low resolution image pixel space order provide better visual quality human viewing <eos> however improved performance such method usually limited even trivial case very small image size will show paper explicitly <eos> paper different image super resolution isr propose novel super resolution technique called feature super resolution fsr aims enhancing discriminatory power small size image order provide high recognition precision machine <eos> achieve goal propose new feature super resolution generative adversarial network fsr gan model transforms raw poor feature small size image highly discriminative ones performing super resolution feature space <eos> fsr gan consists two subnetworks feature generator network feature discriminator network <eos> training network alternative manner encourage network discover latent distribution correlations between small size large size image then use improve representations small image <eos> extensive experiment result oxford paris holidays flick datasets demonstrate proposed fsr approach effectively enhance discriminatory ability feature <eos> even when resolution query image reduced greatly <eos> original size query feature enhanced fsr approach achieves surprisingly high retrieval performance different image resolutions increases retrieval precision compared raw query feature <eos> <eop> clusternet detecting small object large scenes exploiting spatio temporal information <eos> object detection wide area motion imagery wami drawn attention computer vision research community number years <eos> wami proposes number unique challenges including extremely small object sizes both sparse densely packed object extremely large search spaces large video frames <eos> nearly all state art method wami object detection report appearance based classifiers fail challenging data instead rely almost entirely motion information form background subtraction frame differencing <eos> work experimentally verify failure appearance based classifiers wami such faster cnn heatmap based fully convolutional neural network cnn propose novel two stage spatio temporal cnn effectively efficiently combines both appearance motion information significantly surpass state art wami object detection <eos> reduce large search space first stage clusternet takes set extremely large video frames combines motion appearance information within convolutional architecture proposes region object interest roobi <eos> roobi contain one clusters several hundred object due large video frame size varying object density wami <eos> second stage foveanet then estimates centroid location all object given roobi simultaneously via heatmap estimation <eos> proposed method exceeds state art result wpafb dataset moving object nearly stopped object well being first proposed method wide area motion imagery detect completely stationary object <eos> <eop> masklab instance segmentation refining object detection semantic direction feature <eos> work tackle problem instance segmentation task simultaneously solving object detection semantic segmentation <eos> towards goal present model called masklab produces three outputs box detection semantic segmentation direction prediction <eos> building top faster rcnn object detector predicted boxes provide accurate localization object instances <eos> within each region interest masklab performs foreground background segmentation combining semantic direction prediction <eos> semantic segmentation assists model distinguishing between object different semantic classes including background while direction prediction estimating each pixel direction towards its corresponding center allows separating instances same semantic class <eos> moreover explore effect incorporating recent successful method both segmentation detection eg atrous convolution hypercolumn <eos> proposed model evaluated coco instance segmentation benchmark shows comparable performance other state art models <eos> <eop> hashing tie aware learning rank <eos> hashing learning binary embeddings data frequently used nearest neighbor retrieval <eos> paper develop learning rank formulations hashing aimed directly optimizing ranking based evaluation metrics such average precision ap normalized discounted cumulative gain ndcg <eos> first observe integer valued hamming distance often leads tied rankings propose use tie aware versions ap ndcg evaluate hashing retrieval <eos> then optimize tie aware ranking metrics derive their continuous relaxations perform gradient based optimization deep neural network <eos> result establish new state art image retrieval hamming ranking common benchmarks <eos> <eop> classification driven dynamic image enhancement <eos> convolutional neural network rely image texture structure serve discriminative feature classify image content <eos> image enhancement techniques used preprocessing steps help improve overall image quality turn improve overall effectiveness cnn <eos> existing image enhancement method however designed improve perceptual quality image human observer <eos> paper interested learning cnn emulate image enhancement restoration but overall goal improve image classification necessarily human perception <eos> end present unified cnn architecture uses range enhancement filters enhance image specific details via end end dynamic filter learning <eos> demonstrate effectiveness strategy four challenging benchmark datasets fine grained object scene texture classification cub pascal voc mit indoor dtd <eos> experiments using proposed enhancement shows promising result all datasets <eos> addition approach capable improving performance all generic cnn architectures <eos> <eop> knowledge aided consistency weakly supervised phrase grounding <eos> given natural language query phrase grounding system aims localize mentioned object image <eos> weakly supervised scenario mapping between image region <eos> proposals language available training set <eos> previous method address deficiency training grounding system via learning reconstruct language information contained input queries predicted proposals <eos> however optimization solely guided reconstruction loss language modality ignores rich visual information contained proposals useful cues external knowledge <eos> paper explore consistency contained both visual language modalities leverage complementary external knowledge facilitate weakly supervised grounding <eos> propose novel knowledge aided consistency network kac net optimized reconstructing input query proposal information <eos> leverage complementary knowledge contained visual feature introduce knowledge based pooling kbp gate focus query related proposals <eos> experiments show kac net provides significant improvement two popular datasets <eos> <eop> who let dogs out modeling dog behavior visual data <eos> introduce task directly modeling visually intelligent agent <eos> computer vision typically focuses solving various subtasks related visual intelligence <eos> depart standard approach computer vision instead directly model visually intelligent agent <eos> model takes visual information input directly predicts actions agent <eos> toward end introduce decade large scale dataset ego centric video dog perspective well her corresponding movements <eos> using data model how dog acts how dog plans her movements <eos> show under variety metrics given just visual input successfully model intelligent agent many situations <eos> moreover representation learned model encodes distinct information compared representations trained image classification learned representation generalize other domains <eos> particular show strong result task walkable surface estimation using dog modeling task representation learning <eos> <eop> pseudo mask augmented object detection <eos> work present novel effective framework facilitate object detection instance level segmentation information only supervised bounding box annotation <eos> starting joint object detection instance segmentation network propose recursively estimate pseudo ground truth object masks instance level object segmentation network training then enhance detection network top down segmentation feedbacks <eos> pseudo ground truth mask network parameters optimized alternatively mutually benefit each other <eos> obtain promising pseudo masks each iteration embed graphical inference incorporates low level image appearance consistency bounding box annotations refine segmentation masks predicted segmentation network <eos> approach progressively improves object detection performance incorporating detailed pixel wise information learned weakly supervised segmentation network <eos> extensive evaluation detection task pascal voc verifies proposed approach effective <eos> <eop> dual skipping network <eos> inspired recent neuroscience studies left right asymmetry human brain processing low high spatial frequency information paper introduces dual skipping network carries out coarse fine object categorization <eos> such network two branches simultaneously deal both coarse fine grained classification tasks <eos> specifically propose layer skipping mechanism learns gating network predict layer skip testing stage <eos> layer skipping mechanism endows network good flexibility capability practice <eos> evaluations conducted several widely used coarse fine object categorization benchmarks promising result achieved proposed network model <eos> <eop> memory matching network one shot image recognition <eos> paper introduce new ideas augmenting convolutional neural network cnn memory learning learn network parameters unlabelled image fly one shot learning <eos> specifically present memory matching network mm net novel deep architecture explores training procedure following philosophy training test conditions must match <eos> technically mm net writes feature set labelled image support set into memory reads memory when performing inference holistically leverage knowledge set <eos> meanwhile contextual learner employs memory slots sequential manner predict parameters cnn unlabelled image <eos> whole architecture trained once showing only few examples per class switching learning minibatch minibatch tailored one shot learning when presented few examples new categories test time <eos> unlike conventional one shot learning approaches mm net could output one unified model irrespective number shots categories <eos> extensive experiments conducted two public datasets <eos> omniglot emph mini imagenet superior result reported when compared state art approaches <eos> more remarkably mm net improves one shot accuracy omniglot <eos> emph mini imagenet <eos> <eop> iqa visual question answering interactive environments <eos> introduce interactive question answering iqa task answering questions require autonomous agent interact dynamic visual environment <eos> iqa presents agent scene question like there any apples fridge agent must navigate around scene acquire visual understanding scene elements interact object <eos> open refrigerators plan series actions conditioned question <eos> popular reinforcement learning approaches single controller perform poorly iqa owing large diverse state space <eos> propose hierarchical interactive memory network himn consisting factorized set controllers allowing system operate multiple levels temporal abstraction <eos> evaluate himn introduce iquad new dataset built upon ai thor simulated photo realistic environment configurable indoor scenes interactive object <eos> iquad questions each paired unique scene configuration <eos> experiments show proposed model outperforms popular single controller based method iquad <eos> sample questions result please view video youtu <eos> <eop> pose transferrable person re identification <eos> person re identification reid important task field intelligent security <eos> key challenge how capture human pose variations while existing benchmarks <eos> market dukemtmc reid cuhk etc <eos> provide sufficient pose coverage train robust reid system <eos> address issue propose pose transferrable person reid framework utilizes pose transferred sample augmentations <eos> id supervision enhance reid model training <eos> one hand novel training sample rich pose variations generated via transferring pose instances mars dataset they added into target dataset facilitate robust training <eos> other hand addition conventional discriminator gan <eos> distinguish between real fake sample propose novel guider sub network encourages generated sample <eos> novel pose towards better satisfying reid loss <eos> cross entropy reid loss triplet reid loss <eos> meantime alternative optimization procedure proposed train proposed generator guider discriminator network <eos> experimental result market dukemtmc reid cuhk show method achieves great performance improvement outperforms most state art method without elaborate designing reid model <eos> <eop> large scale fine grained categorization domain specific transfer learning <eos> transferring knowledge learned large scale datasets <eos> imagenet via fine tuning offers effective solution domain specific fine grained visual categorization fgvc tasks <eos> recognizing bird species car make model <eos> such scenarios data annotation often calls specialized domain knowledge thus difficult scale <eos> work first tackle problem large scale fgvc <eos> method won first place inaturalist large scale species classification challenge <eos> central success approach training scheme uses higher image resolution deals long tailed distribution training data <eos> next study transfer learning via fine tuning large scale datasets small scale domain specific fgvc datasets <eos> propose measure estimate domain similarity via earth mover distance demonstrate transfer learning benefits pre training source domain similar target domain measure <eos> proposed transfer learning outperforms imagenet pre training obtains state art result multiple commonly used fgvc datasets <eos> <eop> data distillation towards omni supervised learning <eos> investigate omni supervised learning special regime semi supervised learning learner exploits all available labeled data plus internet scale sources unlabeled data <eos> omni supervised learning lower bounded performance existing labeled datasets offering potential surpass state art fully supervised method <eos> exploit omni supervised setting propose data distillation method ensembles predictions multiple transformations unlabeled data using single model automatically generate new training annotations <eos> argue visual recognition models recently become accurate enough now possible apply classic ideas about self training challenging real world data <eos> experimental result show cases human keypoint detection general object detection state art models trained data distillation surpass performance using labeled data coco dataset alone <eos> <eop> object referring video language human gaze <eos> investigate problem object referring <eos> localize target object visual scene coming language description <eos> humans perceive world more continued video snippets than static image describe object only their appearance but also their spatio temporal context motion feature <eos> humans also gaze object when they issue referring expression <eos> existing works mostly focus static image only fall short providing many such cues <eos> paper addresses video language human gaze <eos> end present new video dataset object over stereo video sequences annotated their descriptions gaze <eos> further propose novel network model video integrating appearance motion gaze spatio temporal context into one network <eos> experimental result show method effectively utilizes motion cues human gaze spatio temporal context <eos> method outperforms previous method <eos> <eop> feature selective network object detection <eos> object detection usually distinct characteristics different sub region different aspect ratios <eos> however prevalent two stage object detection method region interest roi feature extracted roi pooling little emphasis translation variant feature components <eos> present feature selective network reform feature representations rois exploiting their disparities among sub region aspect ratios <eos> network produces sub region attention bank aspect ratio attention bank whole image <eos> roi based sub region attention map aspect ratio attention map selectively pooled banks then used refine original roi feature roi classification <eos> equipped light weight detection subnetwork network gets consistent boost detection performance based general convnet backbones resnet googlenet vgg <eos> without bells whistles detectors equipped resnet achieve more than map improvement compared counterparts pascal voc pascal voc ms coco datasets <eos> <eop> learning discriminative filter bank within cnn fine grained recognition <eos> compared earlier multistage frameworks using cnn feature recent end end deep approaches fine grained recognition essentially enhance mid level learning capability cnn <eos> previous approaches achieve introducing auxiliary network infuse localization information into main classification network sophisticated feature encoding method capture higher order feature statistics <eos> show mid level representation learning enhanced within cnn framework learning bank convolutional filters capture class specific discriminative patches without extra part bounding box annotations <eos> such filter bank well structured properly initialized discriminatively learned through novel asymmetric multi stream architecture convolutional filter supervision non random layer initialization <eos> experimental result show approach achieves state art three publicly available fine grained recognition datasets cub stanford cars fgvc aircraft <eos> ablation studies visualizations further provided understand approach <eos> <eop> grounding referring expressions image variational context <eos> focus grounding <eos> localizing linking referring expressions image <eos> largest elephant standing behind baby elephant <eos> general yet challenging vision language task since only require localization object but also multimodal comprehension context visual attributes <eos> largest baby relationships <eos> behind help distinguish referent other object especially same category <eos> due exponential complexity involved modeling context associated multiple image region existing work oversimplifies task pairwise region modeling multiple instance learning <eos> paper propose variational bayesian method called variational context solve problem complex context modeling referring expression grounding <eos> model exploits reciprocal relation between referent context <eos> either them influences estimation posterior distribution other thereby search space context greatly reduced <eos> also extend model unsupervised setting no annotation referent available <eos> extensive experiments various benchmarks show consistent improvement over state art method both supervised unsupervised settings <eos> code available url github <eos> com yuleiniu vc <eop> dynamic graph generation network generating relational knowledge diagrams <eos> work introduce new algorithm analyzing diagram contains visual textual information abstract integrated way <eos> whereas diagrams contain richer information compared individual image based language based data proper solutions automatically understanding them proposed due their innate characteristics multi modality arbitrariness layouts <eos> tackle problem propose unified diagram parsing network generating knowledge diagrams based object detector recurrent neural network designed graphical structure <eos> specifically propose dynamic graph generation network based dynamic memory graph theory <eos> explore dynamics information diagram activation gates gated recurrent unit gru cells <eos> publicly available diagram datasets model demonstrates state art result outperforms other baselines <eos> moreover further experiments question answering shows potentials proposed method various applications <eos> <eop> network architecture point cloud classification via automatic depth image generation <eos> propose novel neural network architecture point cloud classification <eos> key idea automatically transform three dimensional unordered input data into set useful depth image classify them exploiting well performing image classification cnn <eos> present new differentiable module designs generate depth image point cloud <eos> modules combined any network architecture processing point clouds <eos> utilize them combination state art classification network get result competitive state art point cloud classification <eos> furthermore architecture automatically produces informative image representing input point cloud could used further applications such point cloud visualization <eos> <eop> towards dense object tracking honeybee hive <eos> human crowds cells tissue detection efficient tracking multiple object dense configurations important unsolved problem <eos> past limitations image analysis restricted studies dense groups tracking one individual set marked individuals coarse grained group level dynamics all yield incomplete information <eos> here combine power convolutional neural network cnn model environment honeybee hive develop automated method recognition all individuals dense group based raw image data <eos> proposed solution create new adapted individual labeling use segmentation architecture net specific loss function predict both object location orientation <eos> additionally leverage time series image data exploit both structural temporal regularities tracked object recurrent manner <eos> allowed achieve near human level performance real world image data while dramatically reducing original network size initial parameters <eos> given novel application cnn study generate extensive problem specific image data labeled examples produced through custom interface amazon mechanical turk <eos> dataset contains over labeled bee instances moving across video frames fps sampling represents extensive resource development testing dense object recognition tracking method <eos> method correctly detect individuals location error typical body dimension orientation error degrees approximating variability labeling human raters body dimension variation position degrees orientation variation <eos> study represents important step towards efficient image based dense object tracking allowing accurate determination object location orientation across time series image data efficiently within one network architecture <eos> <eop> long term board prediction people traffic scenes under uncertainty <eos> progress towards advanced systems assisted autonomous driving leveraging recent advances recognition segmentation method <eos> yet still facing challenges bringing reliable driving inner cities composed highly dynamic scenes observed moving platform considerable speeds <eos> anticipation becomes key element order react timely prevent accidents <eos> paper argue necessary predict least second thus propose new model jointly predicts ego motion people trajectories over such large time horizons <eos> pay particular attention modeling uncertainty estimates arising non deterministic nature natural traffic scenes <eos> experimental result show indeed possible predict people trajectories desired time horizons uncertainty estimates informative prediction error <eos> also show both sequence modeling trajectories well novel method long term odometry prediction essential best performance <eos> <eop> single shot refinement neural network object detection <eos> object detection two stage approach <eos> faster cnn achieving highest accuracy whereas one stage approach <eos> ssd advantage high efficiency <eos> inherit merits both while overcoming their disadvantages paper propose novel single shot based detector called refinedet achieves better accuracy than two stage method maintains comparable efficiency one stage method <eos> refinedet consists two inter connected modules namely anchor refinement module object detection module <eos> specifically former aims filter out negative anchors reduce search space classifier coarsely adjust locations sizes anchors provide better initialization subsequent regressor <eos> latter module takes refined anchors input former further improve regression accuracy predict multi class label <eos> meanwhile design transfer connection block transfer feature anchor refinement module predict locations sizes class labels object object detection module <eos> multi task loss function enables train whole network end end way <eos> extensive experiments pascal voc pascal voc ms coco demonstrate refinedet achieves state art detection accuracy high efficiency <eos> code available github <eos> com sfzhang refinedet <eos> <eop> video captioning via hierarchical reinforcement learning <eos> video captioning task automatically generating textual description actions video <eos> although previous work <eos> sequence sequence model shown promising result abstracting coarse description short video still very challenging caption video containing multiple fine grained actions detailed description <eos> paper aims address challenge proposing novel hierarchical reinforcement learning framework video captioning high level manager module learns design sub goals low level worker module recognizes primitive actions fulfill sub goal <eos> compositional framework reinforce video captioning different levels approach significantly outperforms all baseline method newly introduced large scale dataset fine grained video captioning <eos> furthermore non ensemble model already achieved state art result widely used msr vtt dataset <eos> <eop> tips tricks visual question answering learnings challenge <eos> paper presents state art model visual question answering vqa won first place vqa challenge <eos> vqa task significant importance research artificial intelligence given its multimodal nature clear evaluation protocol potential real world applications <eos> performance deep neural network vqa very dependent choices architectures hyperparameters <eos> help further research area describe detail high performing though relatively simple model <eos> through massive exploration architectures hyperparameters representing more than gpu hours identified tips tricks lead its success namely sigmoid outputs soft training targets image feature bottom up attention gated tanh activations output embeddings initialized using glove google image large mini batches smart shuffling training data <eos> provide detailed analysis their impact performance assist others making appropriate selection <eos> <eop> learning segment every thing <eos> most method object instance segmentation require all training examples labeled segmentation masks <eos> requirement makes expensive annotate new categories restricted instance segmentation models well annotated classes <eos> goal paper propose new partially supervised training paradigm together novel weight transfer function enables training instance segmentation models large set categories all box annotations but only small fraction mask annotations <eos> contributions allow train mask cnn detect segment visual concepts using box annotations visual genome dataset mask annotations classes coco dataset <eos> evaluate approach controlled study coco dataset <eos> work first step towards instance segmentation models broad comprehension visual world <eos> <eop> self supervised adversarial hashing network cross modal retrieval <eos> thanks success deep learning cross modal retrieval made significant progress recently <eos> however there still remains crucial bottleneck how bridge modality gap further enhance retrieval accuracy <eos> paper propose self supervised adversarial hashing ssah approach lies among early attempts incorporate adversarial learning into cross modal hashing self supervised fashion <eos> primary contribution work two adversarial network leveraged maximize semantic correlation consistency representations between different modalities <eos> addition harness self supervised semantic network discover high level semantic information form multi label annotations <eos> such information guides feature learning process preserves modality relationships both common semantic space hamming space <eos> extensive experiments carried out three benchmark datasets validate proposed ssah surpasses state art method <eos> <eop> parallel attention unified framework visual object discovery through dialogs queries <eos> recognising object according pre defined fixed set class labels well studied computer vision <eos> there great many practical applications subjects may interest known beforehand so easily delineated however <eos> many cases natural language dialog natural way specify subject interest task achieving capability <eos> referring expression comprehension recently attracted attention <eos> end propose unified framework parallel attention plan network discover object image being referred variable length natural expression descriptions short phrases query long multi round dialogs <eos> plan network two attention mechanisms relate parts expressions both global visual content also directly object candidates <eos> furthermore attention mechanisms recurrent making referring process visualizable explainable <eos> attended information dual sources combined reason about referred object <eos> two attention mechanisms trained parallel find combined system outperforms state art several benchmarked datasets different length language input such refcoco refcoco guesswhat <eos> <eop> zigzag learning weakly supervised object detection <eos> paper addresses weakly supervised object detection only image level supervision training stage <eos> previous approaches train detection models entire image all once making models prone being trapped sub optimums due introduced false positive examples <eos> unlike them propose zigzag learning strategy simultaneously discover reliable object instances prevent model overfitting initial seeds <eos> towards goal first develop criterion named mean energy accumulation scores meas automatically measure rank localization difficulty image containing target object accordingly learn detector progressively feeding examples increasing difficulty <eos> way model well prepared training easy examples learning more difficult ones thus gain stronger detection ability more efficiently <eos> furthermore introduce novel masking regularization strategy over high level convolutional feature maps avoid overfitting initial sample <eos> two modules formulate zigzag learning process progressive learning endeavors discover reliable object instances masking regularization increases difficulty finding object instances properly <eos> map pascal voc surpassing state arts large margin <eos> <eop> attentive fashion grammar network fashion landmark detection clothing category classification <eos> paper proposes knowledge guided fashion network solve problem visual fashion analysis <eos> fashion landmark localization clothing category classification <eos> suggested fashion model leveraged high level human knowledge domain <eos> propose two important fashion grammars dependency grammar capturing kinematics like relation ii symmetry grammar accounting bilateral symmetry clothes <eos> introduce bidirectional convolutional recurrent neural network bcrnns efficiently approaching message passing over grammar topologies producing regularized landmark layouts <eos> enhancing clothing category classification fashion network encoded two novel attention mechanisms <eos> landmark aware attention category driven attention <eos> former enforces network focus functional parts clothes learns domain knowledge centered representations leading supervised attention mechanism <eos> latter goal driven directly enhances task related feature learned implicit top down manner <eos> experimental result large scale fashion datasets demonstrate superior performance fashion grammar network <eos> <eop> generalized zero shot learning via synthesized examples <eos> present generative framework generalized zero shot learning training test classes necessarily disjoint <eos> built upon variational autoencoder based architecture consisting probabilistic encoder probabilistic emph conditional decoder model generate novel exemplars seen unseen classes given their respective class attributes <eos> exemplars subsequently used train any off shelf classification model <eos> one key aspects encoder decoder architecture feedback driven mechanism discriminator multivariate regressor learns map generated exemplars corresponding class attribute vectors leading improved generator <eos> model ability generate leverage examples unseen classes train classification model naturally helps mitigate bias towards predicting seen classes generalized zero shot learning settings <eos> through comprehensive set experiments show model outperforms several state art method several benchmark datasets both standard well generalized zero shot learning <eos> <eop> partially shared multi task convolutional neural network local constraint face attribute learning <eos> paper study face attribute learning problem considering identity information attribute relationships simultaneously <eos> particular first introduce partially shared multi task convolutional neural network ps mcnn four task specific network tsnets one shared network snet connected partially shared ps structures learn better shared task specific representations <eos> utilize identity information further boost performance introduce local learning constraint minimizes difference between representations each sample its local geometric neighbours same identity <eos> consequently present local constraint regularized multi task network called partially shared multi task convolutional neural network local constraint ps mcnn lc ps structure local constraint integrated together help framework learn better attribute representations <eos> experimental result celeba lfwa demonstrate promise proposed method <eos> <eop> syq learning symmetric quantization efficient deep neural network <eos> inference state art deep neural network computationally expensive making them difficult deploy constrained hardware environments <eos> efficient way reduce complexity quantize weight parameters activations during training approximating their distributions limited entry codebook <eos> very low precisions such binary ternary network bit activations information loss quantization leads significant accuracy degradation due large gradient mismatches between forward backward functions <eos> paper introduce quantization method reduce loss learning symmetric codebook particular weight subgroups <eos> subgroups determined based their locality weight matrix such hardware simplicity low precision representations preserved <eos> empirically show symmetric quantization substantially improve accuracy network extremely low precision weights activations <eos> also demonstrate representation imposes minimal no hardware implications more coarse grained approaches <eos> source code available www <eos> com julianfaraone syq <eos> <eop> ds tighter lifting free convex relaxations quadratic matching problems <eos> work study convex relaxations quadratic optimisation problems over permutation matrices <eos> while existing semidefinite programming approaches achieve remarkably tight relaxations they strong disadvantage they lift original dimensional variable dimensional variable limits their practical applicability <eos> contrast here present lifting free convex relaxation provably least tight existing lifting free convex relaxations <eos> demonstrate experimentally approach superior existing convex non convex method various problems including image arrangement multi graph matching <eos> <eop> deep mutual learning <eos> model distillation effective widely used technique transfer knowledge teacher student network <eos> typical application transfer powerful large network ensemble small network order meet low memory fast execution requirements <eos> paper present deep mutual learning dml strategy <eos> different one way transfer between static pre defined teacher student model distillation dml ensemble students learn collaboratively teach each other throughout training process <eos> experiments show variety network architectures benefit mutual learning achieve compelling result both category instance recognition tasks <eos> surprisingly revealed no prior powerful teacher network necessary mutual learning collection simple student network works moreover outperforms distillation more powerful yet static teacher <eos> <eop> coupled end end transfer learning generalized fisher information <eos> transfer learning one seeks transfer related information source tasks sufficient data help learning target task only limited data <eos> paper propose novel coupled end end transfer learning cetl framework mainly consists two convolutional neural network source target connect shared decoder <eos> novel loss function coupled loss used cetl training <eos> theoretical perspective demonstrate rationale coupled loss establishing learning bound cetl <eos> moreover introduce generalized fisher information improve multi task optimization cetl <eos> practical aspect cetl provides unified highly flexible solution various learning tasks such domain adaption knowledge distillation <eos> empirical result shows superior performance cetl cross domain cross task image classification <eos> <eop> residual parameter transfer deep domain adaptation <eos> goal deep domain adaptation make possible use deep nets trained one domain there enough annotated training data another there little none <eos> most current approaches focused learning feature representations invariant changes occur when going one domain other means using same network parameters both domains <eos> while some recent algorithms explicitly model changes adapting network parameters they either severely restrict possible domain changes significantly increase number model parameters <eos> contrast introduce network architecture includes auxiliary residual network train predict parameters domain little annotated data other one <eos> architecture enables flexibly preserve similarities between domains they exist model differences when necessary <eos> demonstrate approach yields higher accuracy than state art method without undue complexity <eos> <eop> high order tensor regularization application attribute ranking <eos> when learning functions manifolds improve performance regularizing respect intrinsic manifold geometry rather than ambient space <eos> however when regularizing tensor learning calculating derivatives along intrinsic geometry possible so existing approaches limited regularizing euclidean space <eos> new method intrinsically regularizing learning tensors riemannian manifolds introduces surrogate object encapsulate geometric characteristic tensor <eos> regularizing instead allows learn non symmetric high order tensors <eos> apply approach relative attributes problem demonstrate explicitly regularizing high order relationships between pairs data point improves performance <eos> <eop> learning localize sound source visual scenes <eos> visual events usually accompanied sounds daily lives <eos> pose question machine learn correspondence between visual scene sound localize sound source only observing sound visual scene pairs like human paper propose novel unsupervised algorithm address problem localizing sound source visual scenes <eos> two stream network structure handles each modality attention mechanism developed sound source localization <eos> moreover although network formulated within unsupervised learning framework extended unified architecture simple modification supervised semi supervised learning settings well <eos> meanwhile new sound source dataset developed performance evaluation <eos> empirical evaluation shows unsupervised method eventually go through false conclusion some cases <eos> show even few supervision <eos> semi supervised setup false conclusion able corrected effectively <eos> <eop> dynamic few shot visual learning without forgetting <eos> human visual system remarkably ability able effortlessly learn novel concepts only few examples <eos> mimicking same behavior machine learning vision systems interesting very challenging research problem many practical advantages real world vision applications <eos> context goal work devise few shot visual learning system during test time will able efficiently learn novel categories only few training data while same time will forget initial categories was trained here called base categories <eos> achieve goal propose extend object recognition system attention based few shot classification weight generator redesign classifier convnet model cosine similarity function between feature representations classification weight vectors <eos> latter apart unifying recognition both novel base categories also leads feature representations generalize better unseen categories <eos> extensively evaluate approach mini imagenet manage improve prior state art few shot recognition <eos> shot shot settings respectively while same time sacrifice any accuracy base categories characteristic most prior approaches lack <eos> finally apply approach recently introduced few shot benchmark bharath girshick also achieve state art result <eos> <eop> two step quantization low bit neural network <eos> every bit matters hardware design quantized neural network <eos> however extremely low bit representation usually causes large accuracy drop <eos> thus how train extremely low bit neural network high accuracy central importance <eos> most existing network quantization approaches learn transformations low bit weights well encodings low bit activations simultaneously <eos> tight coupling makes optimization problem difficult thus prevents network learning optimal representations <eos> paper propose simple yet effective two step quantization tsq framework decomposing network quantization problem into two steps code learning transformation function learning based learned codes <eos> first step propose sparse quantization method code learning <eos> second step formulated non linear least square regression problem low bit constraints solved efficiently iterative manner <eos> extensive experiments cifar ilsvrc datasets demonstrate proposed tsq effective outperforms state art large margin <eos> especially bit activation ternary weight quantization alexnet accuracy tsq drops only about <eos> point compared full precision counterpart outperforming current state art more than point <eos> <eop> improved lossy image compression priming spatially adaptive bit rates recurrent network <eos> propose method lossy image compression based recurrent convolutional neural network outper forms bpg webp jpeg jpeg mea sured ms ssim <eos> introduce three improvements over previous research lead state art result ing single model <eos> first modify recurrent architec ture improve spatial diffusion allows network more effectively capture propagate image informa tion through network hidden state <eos> second addition lossless entropy coding use spatially adaptive bit allocation algorithm more efficiently use limited num ber bits encode visually complex image region <eos> fi nally show training pixel wise loss weighted ssim increases reconstruction quality according sev eral metrics <eos> evaluate method kodak tecnick image set compare against standard codecs well recently published method based deep neural network <eos> <eop> conditional probability models deep image compression <eos> deep neural network trained image auto encoders recently emerged promising direction advancing state art image compression <eos> key challenge learning such network twofold deal quantization control trade off between reconstruction error distortion entropy rate latent image representation <eos> paper focus latter challenge propose new technique navigate rate distortion trade off image compression auto encoder <eos> main idea directly model entropy latent representation using context model three dimensional cnn learns conditional probability model latent distribution auto encoder <eos> during training auto encoder makes use context model estimate entropy its representation context model concurrently updated learn dependencies between symbols latent representation <eos> experiments show approach when measured ms ssim yields state art image compression system based simple convolutional auto encoder <eos> <eop> deep diffeomorphic transformer network <eos> spatial transformer layer allow neural network least principle invariant large spatial transformations image data <eos> model however seen limited uptake most practical implementations support only transformations too restricted <eos> affine homographic maps destructive maps such thin plate splines <eos> investigate use exible diffeomorphic image transformations within such network demonstrate significant performance gains attained over currently used models <eos> learned transformations found both simple intuitive thereby providing insights into individual problem domains <eos> proposed framework standard convolutional neural network matches state art result face veri cation only two extra lines simple tensorflow code <eos> <eop> lov sz softmax loss tractable surrogate optimization intersection over union measure neural network <eos> jaccard index also referred intersection over union score commonly employed evaluation image segmentation result given its perceptual qualities scale invariance lends appropriate relevance small object appropriate counting false negatives comparison per pixel losses <eos> present method direct optimization mean intersection over union loss neural network context semantic image segmentation based convex lov sz extension submodular losses <eos> loss shown perform better respect jaccard index measure than traditionally used cross entropy loss <eos> show quantitative qualitative differences between optimizing jaccard index per image versus optimizing jaccard index taken over entire dataset <eos> evaluate impact method semantic segmentation pipeline show substantially improved intersection over union segmentation scores pascal voc cityscapes datasets using state art deep learning segmentation architectures <eos> <eop> generative adversarial perturbations <eos> paper propose novel generative models creating adversarial examples slightly perturbed image resembling natural image but maliciously crafted fool pre trained models <eos> present trainable deep neural network transforming image adversarial perturbations <eos> proposed models produce image agnostic image dependent perturbations targeted non targeted attacks <eos> also demonstrate similar architectures achieve impressive result fooling both classification semantic segmentation models obviating need hand crafting attack method each task <eos> using extensive experiments challenging high resolution datasets such imagenet cityscapes show perturbations achieve high fooling rates small perturbation norms <eos> moreover attacks considerably faster than current iterative method inference time <eos> <eop> learning strict identity mappings deep residual network <eos> family super deep network referred residual network resnet cite he deep achieved record beating performance various visual tasks such image recognition object detection semantic segmentation <eos> ability train very deep network naturally pushed researchers use enormous resources achieve best performance <eos> consequently many applications super deep residual network were employed just marginal improvement performance <eos> paper propose epsilon resnet allows automatically discard redundant layer produces responses smaller than threshold epsilon without any loss performance <eos> epsilon resnet architecture achieved using few additional rectified linear units original resnet <eos> method use any additional variables nor numerous trials like other hyper parameter optimization techniques <eos> layer selection achieved using single training process evaluation performed cifar cifar svhn imagenet datasets <eos> some instances achieve about reduction number parameters <eos> <eop> geometric robustness deep network analysis improvement <eos> deep convolutional neural network shown vulnerable arbitrary geometric transformations <eos> however there no systematic method measure invariance properties deep network such transformations <eos> propose manifool simple yet scalable algorithm measure invariance deep network <eos> particular algorithm measures robustness deep network geometric transformations worst case regime they problematic sensitive applications <eos> extensive experimental result show manifool used measure invariance fairly complex network high dimensional datasets values used analyzing reasons <eos> furthermore build manifool propose new adversarial training scheme show its effectiveness improving invariance properties deep neural network <eos> <eop> view extrapolation human body single image <eos> study how synthesize novel views human body single image <eos> though recent deep learning based method work well rigid object they often fail object large articulation like human bodies <eos> core step existing method fit map observable views novel views cnn however rich articulation modes human body make rather challenging cnn memorize interpolate data well <eos> address problem propose novel deep learning based pipeline explicitly estimates leverages geometry underlying human body <eos> new pipeline composition shape estimation network image generation network interface perspective transformation applied generate forward flow pixel value transportation <eos> design able factor out space data variation makes learning each step much easier <eos> empirically show performance pose varying object improved dramatically <eos> method also applied real data captured three dimensional sensors flow generated method used generating high quality result higher resolution <eos> <eop> geometry aware constrained optimization techniques deep learning <eos> paper generalize stochastic gradient descent sgd rmsprop algorithms setting riemannian optimization <eos> sgd popular method large scale optimization <eos> particular widely used train weights deep neural network <eos> however gradients computed using standard sgd large variance detrimental convergence rate algorithm <eos> other method such rmsprop adam address issue <eos> nevertheless method cannot directly applied constrained optimization problems <eos> paper extend some popular optimization algorithm riemannian constrained setting <eos> substantiate proposed extensions range relevant problems machine learning such incremental principal component analysis computating riemannian centroids spd matrices deep metric learning <eos> achieve competitive result against state art fine grained object recognition datasets <eos> <eop> pointnetvlad deep point cloud based retrieval large scale place recognition <eos> unlike its image based counterpart point cloud based retrieval place recognition remained unexplored unsolved problem <eos> largely due difficulty extracting local feature descriptors point cloud subsequently encoded into global descriptor retrieval task <eos> paper propose pointnetvlad leverage recent success deep network solve point cloud based retrieval place recognition <eos> specifically pointnetvlad combination modification existing pointnet netvlad allows end end training inference extract global descriptor given three dimensional point cloud <eos> furthermore propose lazy triplet quadruplet loss functions achieve more discriminative generalizable global descriptors tackle retrieval task <eos> create benchmark datasets point cloud based retrieval place recognition experimental result datasets show feasibility pointnetvlad <eos> <eop> efficient provable approach mixture proportion estimation using linear independence assumption <eos> paper study mixture proportion estimation mpe problem new setting given sample mixture component distributions identify proportions components mixture distribution <eos> address problem make use linear independence assumption <eos> component distributions independent each other much weaker than assumptions exploited previous mpe method <eos> based assumption propose method uniquely identifies mixture proportions whose output provably converges optimal solution computationally efficient <eos> show superiority proposed method over state art method two applications including learning label noise semi supervised learning both synthetic real world datasets <eos> <eop> voxelnet end end learning point cloud based three dimensional object detection <eos> accurate detection object three dimensional point clouds central problem many applications such autonomous navigation housekeeping robots augmented virtual reality <eos> interface highly sparse lidar point cloud region proposal network rpn most existing efforts focused hand crafted feature representations example bird eye view projection <eos> work remove need manual feature engineering three dimensional point clouds propose voxelnet generic three dimensional detection network unifies feature extraction bounding box prediction into single stage end end trainable deep network <eos> specifically voxelnet divides point cloud into equally spaced three dimensional voxels transforms group point within each voxel into unified feature representation through newly introduced voxel feature encoding vfe layer <eos> way point cloud encoded descriptive volumetric representation then connected rpn generate detections <eos> experiments kitti car detection benchmark show voxelnet outperforms state art lidar based three dimensional detection method large margin <eos> furthermore network learns effective discriminative representation object various geometries leading encouraging result three dimensional detection pedestrians cyclists based only lidar <eos> <eop> image image translation domain adaptation <eos> propose general framework unsupervised domain adaptation allows deep neural network trained source domain tested different target domain without requiring any training annotations target domain <eos> achieved adding extra network losses help regularize feature extracted backbone encoder network <eos> end propose novel use recently proposed unpaired image image translation framework constrain feature extracted encoder network <eos> specifically require feature extracted able reconstruct image both domains <eos> addition require distribution feature extracted image two domains indistinguishable <eos> many recent works seen specific cases general framework <eos> apply method domain adaptation between mnist usps svhn datasets amazon webcam dslr office datasets classification tasks also between gta cityscapes datasets segmentation task <eos> demonstrate state art performance each datasets <eos> <eop> mobilenetv inverted residuals linear bottlenecks <eos> paper describe new mobile architecture mbox mobilenetv improves state art performance mobile models multiple tasks benchmarks well across spectrum different model sizes <eos> also describe efficient ways applying mobile models object detection novel framework call mbox ssdlite <eos> additionally demonstrate how build mobile semantic segmentation models through reduced form mbox deeplabv call mobile mbox deeplabv <eos> based inverted residual structure shortcut connections between thin bottleneck layer <eos> intermediate expansion layer uses lightweight depthwise convolutions filter feature source non linearity <eos> additionally find important remove non linearities narrow layer order maintain representational power <eos> demonstrate improves performance provide intuition led design <eos> finally approach allows decoupling input output domains expressiveness transformation provides convenient framework further analysis <eos> measure performance mbox imagenet cite russakovsky ils <eos> classification coco object detection cite coco voc image segmentation cite pascal <eos> evaluate trade offs between accuracy number operations measured multiply adds madd well actual latency number parameters <eos> <eop> im struct recovering three dimensional shape structure single rgb image <eos> propose recover three dimensional shape structures single rgb image structure refers shape parts represented cuboids part relations encompassing connectivity symmetry <eos> given single image object depicted goal automatically recover cuboid structure object parts well their mutual relations <eos> develop convolutional recursive auto encoder comprised structure parsing image followed structure recovering cuboid hierarchy <eos> encoder achieved multi scale convolutional network trained task shape contour estimation thereby learning discern object structures various forms scales <eos> decoder fuses feature structure parsing network original image recursively decodes hierarchy cuboids <eos> since decoder network learned recover part relations including connectivity symmetry explicitly plausibility generality part structure recovery ensured <eos> two network jointly trained using training data contour mask cuboid structure pairs <eos> such pairs generated rendering stock three dimensional cad models coming part segmentation <eos> method achieves unprecedentedly faithful detailed recovery diverse three dimensional part structures single view image <eos> demonstrate two applications method including structure guided completion three dimensional volumes reconstructed single view image structure aware interactive editing image <eos> <eop> trust your model light field depth estimation inline occlusion handling <eos> address problem depth estimation light field image <eos> main contribution new way handle occlusions improves general accuracy quality object borders <eos> contrast all prior work work model directly incorporates both depth occlusion using local optimization scheme based patchmatch algorithm <eos> key benefit joint approach utilize all available data erroneously discard valuable information pre processing steps <eos> see benefit approach only improved object boundaries but also smooth surface reconstruction outperform even method focus good surface regularization <eos> evaluated method public light field dataset achieve state art result nine out twelve error metrics close tie remaining three <eos> <eop> baseline desensitizing translation averaging <eos> many existing translation averaging algorithms either sensitive disparate camera baselines rely extensive preprocessing improve observed epipolar geometry graph if they robust against disparate camera baselines require complicated optimization minimize highly nonlinear angular error objective <eos> paper carefully design simple yet effective bilinear objective function introducing variable perform requisite normalization <eos> objective function enjoys baseline insensitive property angular error yet amenable simple efficient optimization block coordinate descent good empirical performance <eos> rotation assisted iterative reweighted least squares scheme further put forth help deal outliers <eos> also contribute towards better understanding behavior two recent convex algorithms lud shapefit kick clarifying underlying subtle difference leads performance gap <eos> finally demonstrate algorithm achieves overall superior accuracies benchmark dataset compared state art method also several times faster <eos> <eop> mining point cloud local structures kernel correlation graph pooling <eos> unlike image semantic learning three dimensional point clouds using deep network challenging due naturally unordered data structure <eos> among existing works pointnet achieved promising result directly learning point set <eos> however take full advantage point local neighborhood contains fine grained structural information turns out helpful towards better semantic learning <eos> regard present two new operations improve pointnet more efficient exploitation local structures <eos> first one focuses local three dimensional geometric structures <eos> analogy convolution kernel image define point set kernel set learnable three dimensional point jointly respond set neighboring data point according their geometric affinities measured kernel correlation adapted similar technique point cloud registration <eos> second one exploits local high dimensional feature structures recursive feature aggregation nearest neighbor graph computed three dimensional positions <eos> experiments show network efficiently capture local information robustly achieve better performances major datasets <eos> code available www <eos> com research license kcnet <eop> large scale point cloud semantic segmentation superpoint graphs <eos> propose novel deep learning based framework tackle challenge semantic segmentation large scale point clouds millions point <eos> argue organization three dimensional point clouds efficiently captured structure called superpoint graph spg derived partition scanned scene into geometrically homogeneous elements <eos> spgs offer compact yet rich representation contextual relationships between object parts then exploited graph convolutional network <eos> framework set new state art segmenting outdoor lidar scans <eos> miou point both semantic test set well indoor scans <eos> miou point dis dataset <eos> <eop> very large scale global sfm distributed motion averaging <eos> global structure motion sfm techniques demonstrated superior efficiency accuracy than conventional incremental approach many recent studies <eos> work proposes divide conquer framework solve very large global sfm scale millions image <eos> specifically first divide all image into multiple partitions preserve strong data association well posed parallel local motion averaging <eos> then solve global motion averaging determines cameras partition boundaries similarity transformation per partition register all cameras single coordinate frame <eos> finally local global motion averaging iterated until convergence <eos> since local camera poses fixed during global motion average avoid caching whole reconstruction memory once <eos> distributed framework significantly enhances efficiency robustness large scale motion averaging <eos> <eop> scancomplete large scale scene completion semantic segmentation three dimensional scans <eos> introduce scancomplete novel data driven approach taking incomplete three dimensional scan scene input predicting complete three dimensional model along per voxel semantic labels <eos> key contribution method its ability handle large scenes varying spatial extent managing cubic growth data size scene size increases <eos> end devise fully convolutional generative three dimensional cnn model whose filter kernels invariant overall scene size <eos> model trained scene subvolumes but deployed arbitrarily large scenes test time <eos> addition propose coarse fine inference strategy order produce high resolution output while also leveraging large input context sizes <eos> extensive series experiments carefully evaluate different model design choices considering both deterministic probabilistic models completion semantic inference <eos> result show outperform other method only size environments handled processing efficiency but also regard completion quality semantic segmentation performance significant margin <eos> <eop> solving perspective point problem flying camera photo composition <eos> drone mounted flying cameras will revolutionize photo taking <eos> user instead holding camera hand manually searching viewpoint will interact directly image contents viewfinder through simple gestures flying camera will achieve desired viewpoint through autonomous flying capability drone <eos> work studies underlying viewpoint search problem composing photo two object interest common situation photo taking <eos> model perspective point problem under constrained determine six degrees freedom camera pose uniquely <eos> incorporating user composition requirements minimizing camera flying distance form constrained nonlinear optimization problem solve closed form <eos> experiments synthetic data set real flying camera system indicate promising result <eos> <eop> reflection removal large scale three dimensional point clouds <eos> large scale three dimensional point clouds ls dpcs captured terrestrial lidar scanners often exhibit reflection artifacts glasses degrade performance related computer vision techniques <eos> paper propose efficient reflection removal algorithm ls dpcs <eos> first partition unit sphere into local surface patches then classified into ordinary patches glass patches according number echo pulses emitted laser pulses <eos> then estimate glass region dominant reflection artifacts measuring reliability <eos> also detect remove virtual point using conditions reflection symmetry geometric similarity <eos> test performance proposed algorithm ls dpcs capturing real world outdoor scenes show proposed algorithm estimates valid glass region faithfully removes virtual point caused reflection artifacts successfully <eos> <eop> attentional shapecontextnet point cloud recognition <eos> tackle problem point cloud recognition <eos> unlike previous approaches point cloud either converted into volume image represented independently permutation invariant set develop new representation adopting concept shape context building block network design <eos> resulting model called shapecontextnet consists hierarchy modules relying fixed grid while still enjoying properties similar convolutional neural network being able capture propagate object part information <eos> addition find inspiration self attention based models include simple yet effective contextual modeling mechanism making contextual region selection feature aggregation feature transformation process fully automatic <eos> shapecontextnet end end model applied general point cloud classification segmentation problems <eos> observe competitive result number benchmark datasets <eos> <eop> geometry aware deep network single image novel view synthesis <eos> paper tackles problem novel view synthesis single image <eos> particular target real world scenes rich geometric structure challenging task due large appearance variations such scenes lack simple three dimensional models represent them <eos> modern learning based approaches mostly focus appearance synthesize novel views thus tend generate predictions inconsistent underlying scene structure <eos> contrast paper propose exploit three dimensional geometry scene synthesize novel view <eos> specifically approximate real world scene fixed number planes learn predict set homographies their corresponding region masks transform input image into novel view <eos> end develop new region aware geometric transform network performs multiple tasks common framework <eos> result outdoor kitti indoor scannet datasets demonstrate effectiveness network generate high quality synthetic views respect scene geometry thus outperforming state art method <eos> <eop> inversefacenet deep monocular inverse face rendering <eos> introduce inversefacenet deep convolutional inverse rendering framework faces jointly estimates facial pose shape expression reflectance illumination single input image <eos> estimating all parameters just single image advanced editing possibilities single face image such appearance editing relighting become feasible real time <eos> most previous learning based face reconstruction approaches jointly recover all dimensions severely limited terms visual quality <eos> contrast propose recover high quality facial pose shape expression reflectance illumination using deep neural network trained using large synthetically created training corpus <eos> approach builds novel loss function measures model space similarity directly parameter space significantly improves reconstruction accuracy <eos> further propose self supervised bootstrapping process network training loop iteratively updates synthetic training corpus better reflect distribution real world imagery <eos> demonstrate strategy outperforms completely synthetically trained network <eos> finally show high quality reconstructions compare approach several state art approaches <eos> <eop> sparse photometric three dimensional face reconstruction guided morphable models <eos> present novel three dimensional face reconstruction technique leverages sparse photometric stereo ps latest advances face registration modeling single image <eos> observe three dimensional morphable faces approach provides reasonable geometry proxy light position calibration <eos> specifically develop robust optimization technique calibrate per pixel lighting direction illumination very high precision without assuming uniform surface albedos <eos> next apply semantic segmentation input image geometry proxy refine hairy vs <eos> bare skin region using tailored filter <eos> experiments synthetic real data show using very small set image technique able reconstruct fine geometric details such wrinkles eyebrows whelks pores etc comparable sometimes surpassing movie quality productions <eos> <eop> texture mapping three dimensional reconstruction rgb sensor <eos> acquiring realistic texture details three dimensional models important three dimensional reconstruction <eos> however existence geometric errors caused noisy rgb sensor data always makes color image cannot accurately aligned onto reconstructed three dimensional models <eos> paper propose global local correction strategy obtain more desired texture mapping result <eos> algorithm first adaptively selects optimal image each face three dimensional model effectively remove blurring ghost artifacts produced multiple image blending <eos> then adopt non rigid global local correction step reduce seaming effect between textures <eos> effectively compensate texture geometric misalignment caused camera pose drift geometric errors <eos> evaluate proposed algorithm range complex scenes demonstrate its effective performance generating seamless high fidelity textures three dimensional models <eos> <eop> learning less more camera localization via three dimensional surface regression <eos> popular research areas like autonomous driving augmented reality renewed interest image based camera localization <eos> work address task predicting camera pose single rgb image given three dimensional environment <eos> advent neural network previous works either learned entire camera localization process multiple components camera localization pipeline <eos> key contribution demonstrate explain learning single component pipeline sufficient <eos> component fully convolutional neural network densely regressing so called scene coordinates defining correspondence between input image three dimensional scene space <eos> neural network prepended new end end trainable pipeline <eos> system efficient highly accurate robust training exhibits outstanding generalization capabilities <eos> exceeds state art consistently indoor outdoor datasets <eos> interestingly approach surpasses existing techniques even without utilizing three dimensional model scene during training since network able discover three dimensional scene geometry automatically solely single view constraints <eos> <eop> feature mapping learning fast accurate three dimensional pose inference synthetic image <eos> propose simple efficient method exploiting synthetic image when training deep network predict three dimensional pose image <eos> ability using synthetic image training deep network extremely valuable easy create virtually infinite training set made such image while capturing annotating real image very cumbersome <eos> however synthetic image resemble real image exactly using them training result suboptimal performance <eos> was recently shown exemplar based approaches possible learn mapping exemplar representations real image exemplar representations synthetic image <eos> paper show approach more general network also applied after mapping infer three dimensional pose run time given real image target object first compute feature image map them feature space synthetic image finally use resulting feature input another network predicts three dimensional pose <eos> since network trained very effectively using synthetic image performs very well practice inference faster more accurate than exemplar based approach <eos> demonstrate approach linemod dataset three dimensional object pose estimation color image nyu dataset three dimensional hand pose estimation depth maps <eos> show allows outperform state art both datasets <eos> <eop> indoor rgb compass single line plane <eos> propose novel approach estimate three degrees freedom dof drift free rotational motion rgb camera only single line plane manhattan world mw <eos> previous approaches exploit surface normal vectors vanishing point achieve accurate dof rotation estimation <eos> however they require multiple orthogonal planes many consistent lines visible throughout entire rotation estimation process otherwise approaches fail <eos> overcome limitations present new method estimates absolute camera orientation only single line single plane ransac corresponds theoretical minimal sampling dof rotation estimation <eos> once find initial rotation estimate refine camera orientation minimizing average orthogonal distance endpoints lines parallel mw axes <eos> demonstrate effectiveness proposed algorithm through extensive evaluation variety rgb datasets compare other state art method <eos> <eop> geometry aware network non rigid shape prediction single view <eos> propose method predicting three dimensional shape deformable surface single view <eos> contrast previous approaches need pre registered template surface method robust lack texture partial occlusions <eos> core approach geometry aware deep architecture tackles problem usually done analytic solutions first perform detection mesh then estimate three dimensional shape geometrically consistent image <eos> train architecture end end manner using large dataset synthetic renderings shapes under different levels deformation material properties textures lighting conditions <eos> evaluate approach test split dataset available real benchmarks consistently improving state art solutions significantly lower computational time <eos> <eop> sim real viewpoint invariant visual servoing recurrent control <eos> humans remarkably proficient controlling their limbs tools wide range viewpoints <eos> robotics ability referred visual servoing moving tool end point desired location using primarily visual feedback <eos> paper propose learning viewpoint invariant visual servoing skills robot manipulation task <eos> train deep recurrent controller automatically determine actions move end effector robotic arm desired object <eos> problem fundamentally ambiguous under severe variation viewpoint may impossible determine actions single feedforward operation <eos> instead visual servoing approach uses its memory past movements understand how actions affect robot motion current viewpoint correcting mistakes gradually moving closer target <eos> ability stark contrast previous visual servoing method assume known dynamics require calibration phase <eos> learn recurrent controller using simulated data synthetic demonstrations reinforcement learning <eos> then describe how resulting model transferred real world robot disentangling perception control only adapting visual layer <eos> adapted model servo previously unseen object novel viewpoints real world kuka iiwa robotic arm <eos> supplementary video see href www <eos> com watch olgm bnb fo www <eos> com watch olgm bnb fo <eop> docunet document image unwarping via stacked net <eos> capturing document image common way digitizing recording physical documents due ubiquitousness mobile cameras <eos> make text recognition easier often desirable digitally flatten document image when physical document sheet folded curved <eos> paper develop first learning based method achieve goal <eos> propose stacked net intermediate supervision directly predict forward mapping distorted image its rectified version <eos> because large scale real world data ground truth deformation difficult obtain create synthetic dataset approximately thousand image warping non distorted document image <eos> network trained dataset various data augmentations improve its generalization ability <eos> further create comprehensive benchmark covers various real world conditions <eos> evaluate proposed model quantitatively qualitatively proposed benchmark compare previous non learning based method <eos> <eop> analysis hand segmentation wild <eos> large number works egocentric vision concentrated action object recognition <eos> detection segmentation hands first person video however less explored <eos> many applications domain necessary accurately segment only hands camera wearer but also hands others whom he interacting <eos> here take depth look hand segmentation problem <eos> quest robust hand segmentation method evaluated performance state art semantic segmentation method off shelf fine tuned existing datasets <eos> fine tune refinenet leading semantic segmentation method hand segmentation find much better than best contenders <eos> existing hand segmentation datasets collected laboratory settings <eos> overcome limitation contribute collecting two new datasets egoyoutubehands including egocentric video containing hands wild handoverface analyze performance models presence similar appearance occlusions <eos> further explore whether conditional random fields help refine generated hand segmentations <eos> demonstrate benefit accurate hand maps train cnn hand based activity recognition achieve higher accuracy when cnn was trained using hand maps produced fine tuned refinenet <eos> finally annotate subset egohands dataset fine grained action recognition show accuracy <eos> achieved just looking single hand pose much better than chance level <eos> <eop> roadtracer automatic extraction road network aerial image <eos> mapping road network currently both expensive labor intensive <eos> high resolution aerial imagery provides promising avenue automatically infer road network <eos> prior work uses convolutional neural network cnn detect pixels belong road segmentation then uses complex post processing heuristics infer graph connectivity <eos> show segmentation method high error rates because noisy cnn outputs difficult correct <eos> propose roadtracer new method automatically construct accurate road network maps aerial image <eos> roadtracer uses iterative search process guided cnn based decision function derive road network graph directly output cnn <eos> compare approach segmentation method fifteen cities find error rate roadtracer correctly captures more junctions across cities <eos> <eop> alternating stereo vins observability analysis performance evaluation <eos> one approach improve accuracy robustness vision aided inertial navigation systems vins employ low cost inertial sensors obtain scale information stereoscopic vision <eos> processing image two cameras however computationally expensive increases latency <eos> address limitation work novel two camera alternating stereo vins presented <eos> specifically proposed system triggers left right cameras alternating fashion estimates poses corresponding left camera only introduces linear interpolation model processing alternating right camera measurements <eos> although regular stereo system alternating visual observations when employing proposed interpolation scheme still provide scale information shown analyzing observability properties vision only corresponding system <eos> finally performance gain proposed algorithm over its monocular stereo counterparts assessed using various datasets <eos> <eop> soccer your tabletop <eos> present system transforms monocular video soccer game into moving three dimensional reconstruction players field rendered interactively three dimensional viewer through augmented reality device <eos> heart paper approach estimate depth map each player using cnn trained three dimensional player data extracted soccer video games <eos> compare state art body pose depth estimation techniques show result both synthetic ground truth benchmarks real youtube soccer footage <eos> <eop> epinet fully convolutional neural network using epipolar geometry depth light field image <eos> light field cameras capture both spatial angular properties light rays space <eos> due its property one compute depth light fields uncontrolled lighting environments big advantage over active sensing devices <eos> depth computed light fields used many applications including three dimensional modelling refocusing <eos> however light field image hand held cameras very narrow baselines noise making depth estimation difficult <eos> many approaches proposed overcome limitations light field depth estimation but there clear trade off between accuracy speed method <eos> paper introduce fast accurate light field depth estimation method based fully convolutional neural network <eos> network designed considering light field geometry also overcome lack training data proposing light field specific data augmentation method <eos> achieved top rank hci light field benchmark most metrics also demonstrate effectiveness proposed method real world light field image <eos> <eop> hybrid layer decomposition model tone mapping <eos> tone mapping aims reproduce standard dynamic range image high dynamic range image visual information preserved <eos> state art tone mapping algorithms mostly decompose image into base layer detail layer process them accordingly <eos> method may problems halo artifacts over enhancement due lack proper priors imposed two layer <eos> paper propose hybrid decomposition model address problems <eos> specifically sparsity term imposed base layer model its piecewise smoothness property <eos> sparsity term imposed detail layer structural prior leads piecewise constant effect <eos> further propose multiscale tone mapping scheme based layer decomposition model <eos> experiments show tone mapping algorithm achieves visually compelling result little halo artifacts outperforming state art tone mapping algorithms both subjective objective evaluations <eos> <eop> deeply learned filter response functions hyperspectral reconstruction <eos> hyperspectral reconstruction rgb imaging recently achieved significant progress via sparse coding deep learning <eos> however largely ignored fact existing rgb cameras tuned mimic human richromatic perception thus their spectral responses necessarily optimal hyperspectral reconstruction <eos> paper rather than use rgb spectral responses simultaneously learn optimized camera spectral response functions implemented hardware mapping spectral reconstruction using end end network <eos> core idea since camera spectral filters act effect like convolution layer their response functions could optimized training standard neural network <eos> propose two types designed filters three chip setup without spatial mosaicing single chip setup bayer style filter array <eos> numerical simulations verify advantages deeply learned spectral responses compared existing rgb cameras <eos> more interestingly considering physical restrictions design process able realize deeply learned spectral response functions using modern film filter production technologies thus construct data inspired multispectral cameras snapshot hyperspectral imaging <eos> <eop> crrn multi scale guided concurrent reflection removal network <eos> removing undesired reflections image taken through glass broad application various computer vision tasks <eos> non learning based method utilize different handcrafted priors such separable sparse gradients caused different levels blurs often fail due their limited description capability properties real world reflections <eos> paper propose concurrent reflection removal network crrn tackle problem unified framework <eos> network integrates image appearance information multi scale gradient information human perception inspired loss function trained new dataset reflection image taken under diverse real world scenes <eos> extensive experiments public benchmark dataset show proposed method performs favorably against state art method <eos> <eop> single image reflection separation perceptual losses <eos> present approach separating reflection single image <eos> approach uses fully convolutional network trained end end losses exploit low level high level image information <eos> loss function includes two perceptual losses feature loss visual perception network adversarial loss encodes characteristics image transmission layer <eos> also propose novel exclusion loss enforces pixel level layer separation <eos> create dataset real world image reflection corresponding ground truth transmission layer quantitative evaluation model training <eos> validate method through comprehensive quantitative experiments show approach outperforms state art reflection removal method psnr ssim perceptual user study <eos> also extend method two other image enhancement tasks demonstrate generality approach <eos> <eop> robust method strong rolling shutter effects correction using lines automatic feature selection <eos> present robust method compensates rs distortions single image using set image curves basing knowledge they correspond three dimensional straight lines <eos> unlike existing work no priori knowledge about line directions <eos> manhattan world assumption required <eos> first formulate parametric equation projection three dimensional straight line viewed moving rolling shutter camera under uniform motion model <eos> then propose method efficiently estimates ego angular velocity separately pose parameters using least image curves <eos> moreover propose first time ransac like strategy select image curves really correspond three dimensional straight lines reject corresponding actual curves three dimensional world <eos> comparative experimental study both synthetic real data famous benchmarks shows proposed method outperforms all existing techniques state art <eos> <eop> time resolved light transport decomposition thermal photometric stereo <eos> present novel time resolved light transport decomposition method using thermal imaging <eos> because speed heat propagation much slower than speed light propagation transient transport far infrared light observed video frame rate <eos> key observation thermal image looks similar visible light image appropriately controlled environment <eos> implies conventional computer vision techniques straightforwardly applied thermal image <eos> show diffuse component thermal image separated therefore surface normals object estimated lambertian photometric stereo <eos> effectiveness method evaluated conducting real world experiments its applicability black body transparent translucent object shown <eos> <eop> efficient diverse ensemble discriminative co tracking <eos> ensemble discriminative tracking utilizes committee classifiers label data sample turn used retraining tracker localize target using collective knowledge committee <eos> committee members could vary their feature memory update schemes training data however inevitable committee members excessively agree because large overlaps their version space <eos> remove redundancy effective ensemble learning critical committee include consistent hypotheses differ one another covering version space minimum overlaps <eos> study propose online ensemble tracker directly generates diverse committee generating efficient set artificial training <eos> artificial data sampled empirical distribution sample taken both target background whereas process governed query committee shrink overlap between classifiers <eos> experimental result demonstrate proposed scheme outperforms conventional ensemble trackers public benchmarks <eos> <eop> rolling shutter radial distortion feature high frame rate multi camera tracking <eos> traditionally camera based tracking approaches treated rolling shutter radial distortion imaging artifacts overcome corrected order apply standard camera models scene reconstruction method <eos> paper introduce novel multi camera tracking approach first time jointly leverages information introduced rolling shutter radial distortion feature achieve superior performance respect high frequency camera pose estimation <eos> particular system capable attaining high tracking rates were previously unachievable <eos> approach explicitly leverages rolling shutter capture radial distortion process individual rows rather than entire image frames accurate camera motion estimation <eos> estimate per row dof pose rolling shutter camera tracking multiple point radially distorted row whose rays span curved surface three dimensional space <eos> although tracking systems rolling shutter cameras exist first leverage radial distortion measure per row pose enabling use less than half number cameras required previous state art <eos> validate system both synthetic real imagery <eos> <eop> twofold siamese network real time object tracking <eos> observing semantic feature learned image classification task appearance feature learned similarity matching task complement each other build twofold siamese network named sa siam real time object tracking <eos> sa siam composed semantic branch appearance branch <eos> each branch similarity learning siamese network <eos> important design choice sa siam separately train two branches keep heterogeneity two types feature <eos> addition propose channel attention mechanism semantic branch <eos> channel wise weights computed according channel activations around target position <eos> while inherited architecture siamfc allows tracker operate beyond real time twofold design attention mechanism significantly improve tracking performance <eos> proposed sa siam outperforms all other real time trackers large margin otb benchmarks <eos> <eop> multi cue correlation filters robust visual tracking <eos> recent years many tracking algorithms achieve impressive performance via fusing multiple types feature however most them fail fully explore context among adopted multiple feature strength them <eos> paper propose efficient multi cue analysis framework robust visual tracking <eos> combining different types feature approach constructs multiple experts through discriminative correlation filter dcf each them tracks target independently <eos> proposed robustness evaluation strategy suitable expert selected tracking each frame <eos> furthermore divergence multiple experts reveals reliability current tracking quantified update experts adaptively keep them corruption <eos> through proposed multi cue analysis tracker standard dcf deep feature achieves outstanding result several challenging benchmarks otb otb temple color vot <eos> other hand when evaluated only simple hand crafted feature method demonstrates comparable performance amongst complex non realtime trackers but exhibits much better efficiency speed fps cpu <eos> <eop> learning attentions residual attentional siamese network high performance online visual tracking <eos> offline training object tracking recently shown great potentials balancing tracking accuracy speed <eos> however still difficult adapt offline trained model target tracked online <eos> work presents residual attentional siamese network rasnet high performance object tracking <eos> rasnet model reformulates correlation filter within siamese tracking framework introduces different kinds attention mechanisms adapt model without updating model online <eos> particular exploiting offline trained general attention target adapted residual attention channel favored feature attention rasnet only mitigates over fitting problem deep network training but also enhances its discriminative capacity adaptability due separation representation learning discriminator learning <eos> proposed deep architecture trained end end takes full advantage rich spatial temporal information achieve robust visual tracking <eos> experimental result two latest benchmarks otb vot show rasnet tracker state art tracking accuracy while runs more than frames per second <eos> <eop> sint robust visual tracking via adversarial positive instance generation <eos> existing visual trackers easily disturbed occlusion blurandlargedeformation <eos> inthechallengesofocclusion motion blur large object deformation performance existing visual trackers may limited due followingissues adoptingthedensesamplingstrategyto generate positive examples will make them less diverse ii thetrainingdatawithdifferentchallengingfactorsarelimited even though through collecting large training dataset <eos> collecting even larger training dataset most intuitive paradigm but may still cover all situations positive sample still monotonous <eos> paper propose generate hard positive sample via adversarial learning visual tracking <eos> speci cally speaking assume target object all lie manifold hence introduce positive sample generation network psgn sampling massive diverse training data through traversing over constructed target object manifold <eos> generated diverse target object image enrich training dataset enhance robustness visual trackers <eos> make tracker more robust occlusion adopt hard positive transformation network hptn generate hard sample tracking algorithm recognize <eos> train network deep reinforcement learning automaticallyoccludethetargetobjectwithanegativepatch <eos> based generated hard positive sample train siamese network visual tracking experiments validate effectiveness introduced algorithm <eos> <eop> high speed tracking multi kernel correlation filters <eos> correlation filter cf based trackers currently ranked top terms their performances <eos> nevertheless only some them such kcf henriques mkcf tang feng able exploit powerful discriminability non linear kernels <eos> although mkcf achieves more powerful discriminability than kcf through introducing multi kernel learning mkl into kcf its improvement over kcf quite limited its computational burden increases significantly comparison kcf <eos> paper will introduce mkl into kcf different way than mkcf <eos> reformulate mkl version cf objective function its upper bound alleviating negative mutual interference different kernels significantly <eos> novel mkcf tracker mkcfup outperforms kcf mkcf large margins still work very high fps <eos> extensive experiments public data set show method superior state art algorithms target object small move very high speed <eos> <eop> occlusion aware unsupervised learning optical flow <eos> recently shown convolutional neural network learn optical flow estimation unsuper vised learning <eos> however performance unsuper vised method still relatively large gap compared its supervised counterpart <eos> occlusion large motion some major factors limit current unsuper vised learning optical flow method <eos> work introduce new method models occlusion explicitly new warping way facilitates learning large motion <eos> method shows promising result flying chairs mpi sintel kitti benchmark datasets <eos> espe cially kitti dataset abundant unlabeled sample exist unsupervised method outperforms its counterpart trained supervised learning <eos> <eop> revisiting video saliency large scale benchmark new model <eos> work contribute video saliency research two ways <eos> first introduce new benchmark predicting human eye movements during dynamic scene free viewing long time urged field <eos> dataset named dhf dynamic human fixation consists high quality elaborately selected video sequences spanning large range scenes motions object types background complexity <eos> existing video saliency datasets lack variety generality common dynamic scenes fall short covering challenging situations unconstrained environments <eos> contrast dhf makes significant leap terms scalability diversity difficulty expected boost video saliency modeling <eos> second propose novel video saliency model augments cnn lstm network architecture attention mechanism enable fast end end saliency learning <eos> attention mechanism explicitly encodes static saliency information thus allowing lstm focus learning more flexible temporal saliency representation across successive frames <eos> such design fully leverages existing large scale static fixation datasets avoids overfitting significantly improves training efficiency testing performance <eos> thoroughly examine performance model respect state art saliency models three large scale datasets <eos> dhf hollywood ucf sports <eos> experimental result over more than <eos> testing video containing frames demonstrate model outperforms other competitors <eos> <eop> learning spatial temporal regularized correlation filters visual tracking <eos> discriminative correlation filters dcf efficient visual tracking but suffer unwanted boundary effects <eos> spatially regularized dcf srdcf suggested resolve issue enforcing spatial penalty dcf coefficients inevitably improves tracking performance price increasing complexity <eos> tackle online updating srdcf formulates its model multiple training image further adding difficulties improving efficiency <eos> work introducing temporal regularization srdcf single sample present spatial temporal regularized correlation filters strcf <eos> strcf formulation only serve reasonable approximation srdcf multiple training sample but also provide more robust appearance model than srdcf case large appearance variations <eos> besides efficiently solved via alternating direction method multipliers admm <eos> incorporating both temporal spatial regularization strcf handle boundary effects without much loss efficiency achieve superior performance over srdcf terms accuracy speed <eos> compared srdcf strcf hand crafted feature provides speedup achieves gain <eos> auc score otb temple color respectively <eos> moreover strcf deep feature also performs favorably against state art trackers achieves auc score <eos> <eop> multimodal visual concept learning weakly supervised techniques <eos> despite availability huge amount video data accompanied descriptive texts always easy exploit information contained natural language order automatically recognize video concepts <eos> towards goal paper use textual cues means supervision introducing two weakly supervised techniques extend multiple instance learning mil framework fuzzy set multiple instance learning fsmil probabilistic labels multiple instance learning plmil <eos> former encodes spatio temporal imprecision linguistic descriptions fuzzy set while latter models different interpretations each description semantics probabilistic labels both formulated through convex optimization algorithm <eos> addition provide novel technique extract weak labels presence complex semantics consists semantic similarity computations <eos> evaluate method two distinct problems namely face action recognition challenging realistic setting movies accompanied their screenplays contained cognimuse database <eos> show both tasks method considerably outperforms state art weakly supervised approach well other baselines <eos> <eop> efficient large scale approximate nearest neighbor search opencl fpga <eos> present new method product quantization pq based approximated nearest neighbor search ann high dimensional spaces <eos> specifically first propose quantization scheme codebook coarse quantizer product quantizer rotation matrix reduce cost accessing codebooks <eos> approach also combines highly parallel selection method fused distance calculation reduce memory overhead <eos> implement proposed method intel harpv platform using opencl fpga <eos> proposed method significantly outperforms state art method cpu gpu high dimensional nearest neighbor queries billion scale datasets terms query time accuracy regardless batch size <eos> best knowledge first work demonstrate fpga performance superior cpu gpu high dimensional large scale ann datasets <eos> <eop> learning complete image indexing pipeline <eos> work scale complete image indexing system comprises two components inverted file index restrict actual search only subset should contain most items relevant query approximate distance computation mechanism rapidly scan lists <eos> while supervised deep learning recently enabled improvements latter former continues based unsupervised clustering literature <eos> work propose first system learns both components within unifying neural framework structured binary encoding <eos> <eop> transparency design closing gap between performance interpretability visual reasoning <eos> visual question answering requires high order reasoning about image fundamental capability needed machine systems follow complex directives <eos> recently modular network shown effective framework performing visual reasoning tasks <eos> while modular network were initially designed degree model transparency their performance complex visual reasoning benchmarks was lacking <eos> current state art approaches provide effective mechanism understanding reasoning process <eos> paper close performance gap between interpretable models state art visual reasoning method <eos> propose set visual reasoning primitives when composed manifest model capable performing complex reasoning tasks explicitly interpretable manner <eos> fidelity interpretability primitives outputs enable unparalleled ability diagnose strengths weaknesses resulting model <eos> critically show primitives highly performant achieving state art accuracy <eos> also show model able effectively learn generalized representations when provided small amount data containing novel object attributes <eos> using cogent generalization task show more than percentage point improvement over current state art <eos> <eop> fooling vision language models despite localization attention mechanism <eos> adversarial attacks known succeed classifiers but open question whether more complex vision systems vulnerable <eos> paper study adversarial examples vision language models incorporate natural language understanding complex structures such attention localization modular architectures <eos> particular investigate attacks dense captioning model two visual question answering vqa models <eos> evaluation shows generate adversarial examples high success rate <eos> work sheds new light understanding adversarial attacks vision systems language component shows attention bounding box localization compositional internal structures vulnerable adversarial attacks <eos> observations will inform future work towards building effective defenses <eos> <eop> categorizing concepts basic level vision language <eos> vision language tasks require unified semantic understanding visual content <eos> however information contained image video essentially ambiguous two perspectives manifested diverse understanding among different persons various understanding grains even same person <eos> inspired basic level early cognition basic concept bac category proposed work contains both consensus proper level visual content help neural network tackle above problems <eos> specifically salient concept category firstly generated intersecting labels imagenet vocabulary mscoco dataset <eos> then according observation human early cognition children make fewer mistakes basic level salient category further refined clustering concepts defined confusion degree measures difficulty convolutional neural network distinguish class pairs <eos> finally pre trained model based googlenet produced proposed bac category concept classes <eos> verify effectiveness proposed categorizing method vision language tasks two kinds experiments performed including image captioning visual question answering benchmark datasets mscoco flickr coco qa <eos> experimental result demonstrate representations derived cognition inspired bac category promote representation learning neural network vision language tasks performance improvement gained without modifying standard models <eos> <eop> don just assume look answer overcoming priors visual question answering <eos> number studies found today visual question answering vqa models heavily driven superficial correlations training data lack sufficient image grounding <eos> encourage development models geared towards latter propose new setting vqa every question type train test set different prior distributions answers <eos> specifically present new splits vqa vqa datasets call visual question answering under changing priors vqa cp vqa cp respectively <eos> first evaluate several existing vqa models under new setting show their performance degrades significantly compared original vqa setting <eos> second propose novel grounded visual question answering model gvqa contains inductive biases restrictions architecture specifically designed prevent model cheating primarily relying priors training data <eos> specifically gvqa explicitly disentangles recognition visual concepts present image identification plausible answer space given question enabling model more robustly generalize across different distributions answers <eos> gvqa built off existing vqa model stacked attention network san <eos> experiments demonstrate gvqa significantly outperforms san both vqa cp vqa cp datasets <eos> interestingly also outperforms more powerful vqa models such multimodal compact bilinear pooling mcb several cases <eos> gvqa offers strengths complementary san when trained evaluated original vqa vqa datasets <eos> finally gvqa more transparent interpretable than existing vqa models <eos> <eop> learning pixel level semantic affinity image level supervision weakly supervised semantic segmentation <eos> deficiency segmentation labels one main obstacles semantic segmentation wild <eos> alleviate issue present novel framework generates segmentation labels image given their image level class labels <eos> weakly supervised setting trained models known segment local discriminative parts rather than entire object area <eos> solution propagate such local responses nearby areas belong same semantic entity <eos> end propose deep neural network dnn called affinitynet predicts semantic affinity between pair adjacent image coordinates <eos> semantic propagation then realized random walk affinities predicted affinitynet <eos> more importantly supervision employed train affinitynet given initial discriminative part segmentation incomplete segmentation annotation but sufficient learning semantic affinities within small image areas <eos> thus entire framework relies only image level class labels require any extra data annotations <eos> pascal voc dataset dnn learned segmentation labels generated method outperforms previous models trained same level supervision even competitive relying stronger supervision <eos> <eop> lifestyle vlogs everyday interactions <eos> major stumbling block progress understanding basic human interactions such getting out bed opening refrigerator lack good training data <eos> most past efforts gathered data explicitly starting laundry list action labels then querying search engines video tagged each label <eos> work reverse search implicitly start large collection interaction rich video data then annotate analyze <eos> use internet lifestyle vlogs source surprisingly large diverse interaction data <eos> show collecting data first able achieve greater scale far greater diversity terms actions actors <eos> additionally data exposes biases built into common explicitly gathered data <eos> make sense data analyzing central component interaction hands <eos> benchmark two tasks identifying semantic object contact video level non semantic contact state frame level <eos> additionally demonstrate future prediction hands <eos> <eop> cross domain weakly supervised object detection through progressive domain adaptation <eos> detect common object variety image domains without instance level annotations paper present framework novel task cross domain weakly supervised object detection addresses question <eos> paper access image instance level annotations source domain <eos> natural image image image level annotations target domain <eos> addition classes detected target domain all subset source domain <eos> starting fully supervised object detector pre trained source domain propose two step progressive domain adaptation technique fine tuning detector two types artificially automatically generated sample <eos> test method newly collected datasets containing three image domains achieve improvement approximately percentage point terms mean average precision map compared best performing baselines <eos> <eop> rotationnet joint object categorization pose estimation using multiviews unsupervised viewpoints <eos> propose convolutional neural network cnn based model rotationnet takes multi view image object input jointly estimates its pose object category <eos> unlike previous approaches use known viewpoint labels training method treats viewpoint labels latent variables learned unsupervised manner during training using unaligned object dataset <eos> rotationnet designed use only partial set multi view image inference property makes useful practical scenarios only partial views available <eos> moreover pose alignment strategy enables one obtain view specific feature representations shared across classes important maintain high accuracy both object categorization pose estimation <eos> effectiveness rotationnet demonstrated its superior performance state art method three dimensional object classification class modelnet datasets <eos> also show rotationnet even trained without known poses achieves state art performance object pose estimation dataset <eos> <eop> end end textspotter explicit alignment attention <eos> text detection recognition natural image long considered two separate tasks processed sequentially <eos> jointly training two tasks non trivial due significant differences learning difficulties convergence rates <eos> work present conceptually simple yet efficient framework simultaneously processes two tasks united framework <eos> main contributions three fold propose novel textalignment layer allows precisely compute convolutional feature text instance arbitrary orientation key boost performance character attention mechanism introduced using character spatial information explicit supervision leading large improvements recognition two technologies together new rnn branch word recognition integrated seamlessly into single model end end trainable <eos> allows two tasks work collaboratively sharing convolutional feature critical identify challenging text instances <eos> model obtains impressive result end end recognition icdar significantly advancing most recent result improvements measure <eos> using strong weak generic lexicon respectively <eos> thanks joint training method also serve good detector achieving new state art detection performance related benchmarks <eos> code available github <eos> com tonghe textspotter <eos> <eop> wildtrack multi camera hd dataset dense unscripted pedestrian detection <eos> people detection method highly sensitive occlusions between pedestrians extremely frequent many situations cameras mounted limited height <eos> reduction camera prices allows generalization static multi camera set ups <eos> using joint visual information multiple synchronized cameras gives opportunity improve detection performance <eos> paper present new large scale high resolution dataset <eos> captured seven static cameras public open area unscripted dense groups pedestrians standing walking <eos> together camera frames provide accurate joint extrinsic intrinsic calibration well series annotated frames detection rate frames per second <eos> result over bounding boxes delimiting every person present area interest total more than individuals <eos> provide series benchmark result using baseline algorithms published over recent months multi view detection deep neural network trajectory estimation using non markovian model <eos> <eop> direct shape regression network end end face alignment <eos> face alignment extensively studied computer vision community due its fundamental role facial analysis but remains unsolved problem <eos> major challenges lie highly nonlinear relationship between face image associated facial shapes coupled underlying correlation landmarks <eos> existing method mainly rely cascaded regression suffering intrinsic shortcomings <eos> strong dependency initialization failure exploit landmark correlations <eos> paper propose direct shape regression network dsrn end end face alignment jointly handling aforementioned challenges unified framework <eos> specifically deploying doubly convolutional layer using fourier feature pooling layer proposed paper dsrn efficiently constructs strong representations disentangle highly nonlinear relationships between image shapes incorporating linear layer low rank learning dsrn effectively encodes correlations landmarks improve performance <eos> dsrn leverages strengths kernels nonlinear feature extraction neural network structured prediction provides first end end learning architecture direct face alignment <eos> its effectiveness generality validated extensive experiments five benchmark datasets including aflw celeba mafl vw <eos> all empirical result demonstrate dsrn consistently produces high performance most cases surpasses state art <eos> <eop> natural effective obfuscation head inpainting <eos> more more personal photos shared online being able obfuscate identities such photos becoming necessity privacy protection <eos> people largely resorted blacking out blurring head region but they result poor user experience while being surprisingly ineffective against state art person recognizers <eos> work propose novel head inpainting obfuscation technique <eos> generating realistic head inpainting social media photos challenging because subjects appear diverse activities head orientations <eos> thus split task into two sub tasks facial landmark generation image context <eos> body pose seamless hypothesis sensible head pose facial landmark conditioned head inpainting <eos> verify inpainting method generates realistic person image while achieving superior obfuscation performance against automatic person recognizers <eos> <eop> semantic trajectory reconstruction three dimensional pixel continuum <eos> paper presents method reconstruct dense semantic trajectory stream human interactions three dimensional synchronized multiple video <eos> interactions inherently introduce self occlusion illumination appearance shape changes resulting highly fragmented trajectory reconstruction noisy coarse semantic labels <eos> conjecture among many views there exists set views confidently recognize visual semantic label three dimensional trajectory <eos> introduce new representation called three dimensional semantic map probability distribution over semantic labels per trajectory <eos> construct three dimensional semantic map reasoning about visibility recognition confidence based view pooling <eos> finding view best represents semantics trajectory <eos> using three dimensional semantic map precisely infer all trajectory labels jointly considering affinity between long range trajectories via estimating their local rigid transformations <eos> inference quantitatively outperforms baseline approaches terms predictive validity representation robustness affinity effectiveness <eos> demonstrate algorithm robustly compute semantic labels large scale trajectory set involving real world human interactions object scenes people <eos> <eop> optimizing filter size convolutional neural network facial action unit recognition <eos> recognizing facial action units aus during spontaneous facial displays challenging problem <eos> most recently convolutional neural network cnn shown promise facial au recognition predefined fixed convolution filter sizes employed <eos> order achieve best performance optimal filter size often empirically found conducting extensive experimental validation <eos> such training process suffers expensive training cost especially network becomes deeper <eos> paper proposes novel optimized filter size cnn ofs cnn filter sizes weights all convolutional layer learned simultaneously training data along learning convolution filters <eos> specifically filter size defined continuous variable optimized minimizing training loss <eos> experimental result two au coded spontaneous databases shown proposed ofs cnn capable estimating optimal filter size varying image resolution outperforms traditional cnn best filter size obtained exhaustive search <eos> ofs cnn also beats cnn using multiple filter sizes more importantly much more efficient during testing proposed forward backward propagation algorithm <eos> <eop> posenet voxel voxel prediction network accurate three dimensional hand human pose estimation single depth map <eos> most existing deep learning based method three dimensional hand human pose estimation single depth map based common framework takes depth map directly regresses three dimensional coordinates keypoints such hand human body joints via convolutional neural network cnn <eos> first weakness approach presence perspective distortion depth map <eos> while depth map intrinsically three dimensional data many previous method treat depth maps image distort shape actual object through projection three dimensional space <eos> compels network perform perspective distortion invariant estimation <eos> second weakness conventional approach directly regressing three dimensional coordinates image highly non linear mapping causes difficulty learning procedure <eos> overcome weaknesses firstly cast three dimensional hand human pose estimation problem single depth map into voxel voxel prediction uses three dimensional voxelized grid estimates per voxel likelihood each keypoint <eos> design model three dimensional cnn provides accurate estimates while running real time <eos> system outperforms previous method almost all publicly available three dimensional hand human pose estimation datasets placed first hands frame based three dimensional hand pose estimation challenge <eos> code available github <eos> com mks posenet release <eos> <eop> ring loss convex feature normalization face recognition <eos> motivate present ring loss simple elegant feature normalization approach deep network designed augment standard loss functions such softmax <eos> argue deep feature normalization important aspect supervised classification problems require model represent each class multi class problem equally well <eos> direct approach feature normalization through hard normalization operation result non convex formulation <eos> instead ring loss applies soft normalization gradually learns constrain norm scaled unit circle while preserving convexity leading more robust feature <eos> apply ring loss large scale face recognition problems present result lfw challenging protocols ijb janus janus cs superset ijb janus celebrity frontal profile cfp megaface million distractors <eos> ring loss outperforms strong baselines matches state art performance ijb janus outperforms all other result challenging janus cs thereby achieving state art <eos> also outperform strong baselines handling extremely low resolution face matching <eos> <eop> adversarially occluded sample person re identification <eos> person re identification reid task retrieving particular persons across different cameras <eos> despite its great progress recent years still confronted challenges like pose variation occlusion similar appearance among different persons <eos> large gap between training testing performance existing models implies insufficiency generalization <eos> considering fact propose augment variation training data introducing adversarially occluded sample <eos> special sample both meaningful they resemble real scene occlusions effective they tough original model thus provide momentum jump out local optimum <eos> mine sample based trained reid model help network visualization techniques <eos> extensive experiments show proposed sample help model discover new discriminative clues body generalize much better test time <eos> strategy makes significant improvement over strong baselines three large scale reid datasets market cuhk dukemtmc reid <eos> <eop> classifier learning prior probabilities facial action unit recognition <eos> facial action units aus play important role human emotion understanding <eos> one big challenge data driven au recognition approaches lack enough au annotations since au annotation requires strong domain expertise <eos> alleviate issue propose knowledge driven method jointly learning multiple au classifiers without any au annotation leveraging prior probabilities aus including expression independent expression dependent au probabilities <eos> prior probabilities drawn facial anatomy emotion studies independent datasets <eos> incorporate prior probabilities aus constraints into objective function multiple au classifiers develop efficient learning algorithm solve formulated problem <eos> experimental result five benchmark expression databases demonstrate effectiveness proposed method especially its generalization ability power prior probabilities <eos> <eop> dfab large scale database facial expression analysis biometric applications <eos> progress currently witnessing many computer vision applications including automatic face analysis would made possible without tremendous efforts collecting annotating large scale visual databases <eos> end propose dfab new large scale database dynamic high resolution three dimensional faces over three dimensional meshes <eos> dfab contain recordings subjects captured four different sessions spanned over five year period <eos> contains video subjects displaying both spontaneous posed facial behaviours <eos> database used both face facial expression recognition well behavioural biometrics <eos> also used learn very powerful blendshapes parametrising facial behaviour <eos> paper conduct several experiments demonstrate usefulness database various applications <eos> database will made publicly available research purposes <eos> <eop> seeing small faces robust anchor perspective <eos> paper introduces novel anchor design principle support anchor based face detection superior scale invariant performance especially tiny faces <eos> achieve explicitly address problem anchor based detectors drop performance drastically faces tiny sizes <eos> less than pixels <eos> paper investigate why case <eos> discover current anchor design cannot guarantee high overlaps between tiny faces anchor boxes increases difficulty training <eos> new expected max overlapping emo score proposed theoretically explain low overlapping issue inspire several effective strategies new anchor design leading higher face overlaps including anchor stride reduction new network architectures extra shifted anchors stochastic face shifting <eos> comprehensive experiments show proposed method significantly outperforms baseline anchor based detector while consistently achieving state art result challenging face detection datasets competitive runtime speed <eos> <eop> three dimensional pose estimation action recognition using multitask deep learning <eos> action recognition human pose estimation closely related but both problems generally handled distinct tasks literature <eos> work propose multitask framework jointly three dimensional pose estimation still image human action recognition video sequences <eos> show single architecture used solve two problems efficient way still achieves state art result <eos> additionally demonstrate optimization end end leads significantly higher accuracy than separated learning <eos> proposed architecture trained data different categories simultaneously seamlessly way <eos> reported result four datasets mpii human <eos> penn action ntu demonstrate effectiveness method targeted tasks <eos> <eop> dense three dimensional regression hand pose estimation <eos> present simple effective method three dimensional hand pose estimation single depth frame <eos> opposed previous state arts based holistic three dimensional regression method works dense pixel wise estimation <eos> achieved careful design choices pose parameterization leverages both three dimensional properties depth map <eos> specifically decompose pose parameters into set per pixel estimations <eos> heat maps three dimensional heat maps unit three dimensional direction vector fields <eos> three dimensional joint heat maps three dimensional joint offsets estimated via multi task network cascades trained end end <eos> pixel wise estimations directly translated into vote casting scheme <eos> variant mean shift then used aggregate local votes explicitly handles global three dimensional estimation consensus pixel wise three dimensional estimations <eos> method efficient highly accurate <eos> msra nyu hand dataset method outperforms all previous state arts large margin <eos> icvl hand dataset method achieves similar accuracy compared state art nearly saturated outperforms other state arts <eos> code will made available <eos> <eop> camera style adaptation person re identification <eos> being cross camera retrieval task person re identification suffers image style variations caused different cameras <eos> art implicitly addresses problem learning camera invariant descriptor subspace <eos> paper explicitly consider challenge introducing camera style camstyle adaptation <eos> camstyle serve data augmentation approach smooths camera style disparities <eos> specifically cyclegan labeled training image style transferred each camera along original training sample form augmented training set <eos> method while increasing data diversity against over fitting also incurs considerable level noise <eos> effort alleviate impact noise label smooth regularization lsr adopted <eos> vanilla version method without lsr performs reasonably well few camera systems over fitting often occurs <eos> lsr demonstrate consistent improvement all systems regardless extent over fitting <eos> also report competitive accuracy compared state art <eos> <eop> posetrack benchmark human pose estimation tracking <eos> existing systems video based pose estimation tracking struggle perform well realistic video multiple people often fail output body pose trajectories consistent over time <eos> address shortcoming paper introduces posetrack new large scale benchmark video based human pose estimation articulated tracking <eos> new benchmark encompasses three tasks focusing single frame multi person pose estimation ii multi person pose estimation video iii multi person articulated tracking <eos> establish benchmark collect annotate release new dataset feature video multiple people labeled person tracks articulated pose <eos> public centralized evaluation server provided allow research community evaluate held out test set <eos> furthermore conduct extensive experimental study recent approaches articulated pose tracking provide analysis strengths weaknesses state art <eos> envision proposed benchmark will stimulate productive research both providing large representative training dataset well providing platform objectively evaluate compare proposed method <eos> benchmark freely accessible posetrack <eos> <eop> exploit unknown gradually one shot video based person re identification stepwise learning <eos> focus one shot learning video based person re identification re id <eos> unlabeled tracklets person re id tasks easily obtained pre processing such pedestrian detection tracking <eos> paper propose approach exploiting unlabeled tracklets gradually but steadily improving discriminative capability convolutional neural network cnn feature representation via stepwise learning <eos> first initialize cnn model using one labeled tracklet each identity <eos> then update cnn model following two steps iteratively <eos> sample few candidates most reliable pseudo labels unlabeled tracklets <eos> update cnn model according selected data <eos> instead static sampling strategy applied existing works propose progressive sampling method increase number selected pseudo labeled candidates step step <eos> systematically investigate way how should select pseudo labeled tracklets into training set make best use them <eos> notably rank accuracy method outperforms state art method <eos> point dukemtmc videoreid dataset <eos> <eop> pose robust face recognition via deep residual equivariant mapping <eos> face recognition achieves exceptional success thanks emergence deep learning <eos> however many contemporary face recognition models still perform relatively poor processing profile faces compared frontal faces <eos> key reason number frontal profile training faces highly imbalanced there extensively more frontal training sample compared profile ones <eos> addition intrinsically hard learn deep representation geometrically invariant large pose variations <eos> study hypothesize there inherent mapping between frontal profile faces consequently their discrepancy deep representation space bridged equivariant mapping <eos> exploit mapping formulate novel deep residual equivariant mapping dream block capable adaptively adding residuals input deep representation transform profile face representation canonical pose simplifies recognition <eos> dream block consistently enhances performance profile face recognition many strong deep network including resnet models without deliberately augmenting training data profile faces <eos> block easy use light weight implemented negligible computational overhead <eos> <eop> decidenet counting varying density crowds through attention guided detection density estimation <eos> real world crowd counting applications crowd densities vary greatly spatial temporal domains <eos> detection based counting method will estimate crowds accurately low density scenes while its reliability congested areas downgraded <eos> regression based approach other hand captures general density information crowded region <eos> without knowing location each person tends overestimate count low density areas <eos> thus exclusively using either one them sufficient handle all kinds scenes varying densities <eos> address issue novel end end crowd counting framework named decidenet detection density estimation network proposed <eos> adaptively decide appropriate counting mode different locations image based its real density conditions <eos> decidenet starts estimating crowd density generating detection regression based density maps separately <eos> capture inevitable variation densities incorporates attention module meant adaptively assess reliability two types estimations <eos> final crowd counts obtained guidance attention module adopt suitable estimations two kinds density maps <eos> experimental result show method achieves state art performance three challenging crowd counting datasets <eos> <eop> lstm pose machines <eos> observed recent state art result single image human pose estimation were achieved multi stage convolution neural network cnn <eos> notwithstanding superior performance static image application models video only computationally intensive also suffers performance degeneration flicking <eos> such suboptimal result mainly attributed inability imposing sequential geometric consistency handling severe image quality degradation <eos> motion blur occlusion well inability capturing temporal correlation among video frames <eos> paper proposed novel recurrent network tackle problems <eos> showed if were impose weight sharing scheme multi stage cnn could re written recurrent neural network rnn <eos> property decouples relationship among multiple network stages result significantly faster speed invoking network video <eos> also enables adoption long short term memory lstm units between video frames <eos> found such memory augmented rnn very effective imposing geometric consistency among frames <eos> also well handles input quality degradation video while successfully stabilizes sequential outputs <eos> experiments showed approach significantly outperformed current state art method two large scale video pose estimation benchmarks <eos> also explored memory cells inside lstm provided insights why such mechanism would benefit prediction video based pose estimations <eos> <eop> disentangling feature three dimensional face shapes joint face reconstruction recognition <eos> paper proposes encoder decoder network disentangle shape feature during three dimensional face shape reconstruction single image such tasks learning discriminative shape feature face recognition reconstructing accurate three dimensional face shapes done simultaneously <eos> unlike existing three dimensional face reconstruction method proposed method directly regresses dense three dimensional face shapes single image tackles identity residual <eos> non identity components three dimensional face shapes explicitly separately based composite three dimensional face shape model latent representations <eos> devise training process proposed network joint loss measuring both face identification error three dimensional face shape reconstruction error <eos> develop multi image three dimensional morphable model dmm fitting method multiple image subject construct training data <eos> comprehensive experiments done micc bu dfe lfw ytf databases <eos> result show method expands capacity dmm capturing discriminative shape feature facial detail thus outperforms existing method both three dimensional face reconstruction accuracy face recognition accuracy <eos> <eop> convolutional sequence sequence model human dynamics <eos> human motion modeling classic problem com puter vision graphics <eos> challenges modeling human motion include high dimensional prediction well extremely complicated dynamics <eos> present novel approach human motion modeling based convolutional neural network cnn <eos> hierarchical structure cnn makes capable capturing both spatial temporal correlations effectively <eos> proposed approach convolutional long term encoder used encode whole given motion sequence into long term hidden variable used decoder predict remainder sequence <eos> decoder itself also encoder decoder structure short term encoder encodes shorter sequence short term hidden variable spatial decoder maps long short term hidden variable motion predictions <eos> using such model able capture both invariant dynamic information human motion result more accurate predictions <eos> experiments show algorithm outperforms state art method human <eos> cmu motion capture datasets <eos> code available project website <eop> gesture recognition focus hands <eos> gestures common form human communication important human computer interfaces hci <eos> recent approaches gesture recognition use deep learning method including multi channel method <eos> show when spatial channels focused hands gesture recognition improves significantly particularly when channels fused using sparse network <eos> using technique improve performance chalearn isogd dataset previous best <eos> <eop> crowd counting via adversarial cross scale consistency pursuit <eos> crowd counting density estimation challenging task computer vision due large scale variations perspective distortions serious occlusions etc <eos> existing method generally suffers two issues model averaging effects multi scale cnn induced widely adopted regression loss inconsistent estimation across different scaled inputs <eos> explicitly address issues propose novel crowd counting density estimation framework called adversarial cross scale consistency pursuit acscp <eos> one hand net structural network designed generate density map input patch adversarial loss employed shrink solution onto realistic subspace thus attenuating blurry effects density map estimation <eos> other hand design novel scale consistency regularizer enforces sum up crowd counts local patches <eos> small scale coherent overall count their region union <eos> above losses integrated via joint training scheme so help boost density estimation performance further exploring collaboration between both objectives <eos> extensive experiments four benchmarks well demonstrated effectiveness proposed innovations well superior performance over prior art <eos> <eop> human pose estimation wild adversarial learning <eos> recently remarkable advances achieved three dimensional human pose estimation monocular image because powerful deep convolutional neural network dcnns <eos> despite their success large scale datasets collected constrained lab environment difficult obtain three dimensional pose annotations wild image <eos> therefore three dimensional human pose estimation wild still challenge <eos> paper propose adversarial learning framework distills three dimensional human pose structures learned fully annotated dataset wild image only pose annotations <eos> instead defining hard coded rules constrain pose estimation result design novel multi source discriminator distinguish predicted three dimensional poses ground truth helps enforce pose estimator generate anthropometrically valid poses even image wild <eos> also observe carefully designed information source discriminator essential boost performance <eos> thus design geometric descriptor computes pairwise relative locations distances between body joints new information source discriminator <eos> efficacy adversarial learning framework new geometric descriptor demonstrated through extensive experiments two widely used public benchmarks <eos> approach significantly improves performance compared previous state art approaches <eos> <eop> cosface large margin cosine loss deep face recognition <eos> face recognition made extraordinary progress owing advancement deep convolutional neural network cnn <eos> central task face recognition including face verification identification involves face feature discrimination <eos> however traditional softmax loss deep cnn usually lacks power discrimination <eos> address problem recently several loss functions such center loss large margin softmax loss angular softmax loss proposed <eos> all improved losses share same idea maximizing inter class variance minimizing intra class variance <eos> paper propose novel loss function namely large margin cosine loss lmcl realize idea different perspective <eos> more specifically reformulate softmax loss cosine loss normalizing both feature weight vectors remove radial variations based cosine margin term introduced further maximize decision margin angular space <eos> result minimum intra class variance maximum inter class variance achieved virtue normalization cosine decision margin maximization <eos> refer model trained lmcl cosface <eos> extensive experimental evaluations conducted most popular public domain face recognition datasets such megaface challenge youtube faces ytf labeled face wild lfw <eos> achieve state art performance benchmarks confirms effectiveness proposed approach <eos> <eop> encoding crowd interaction deep neural network pedestrian trajectory prediction <eos> pedestrian trajectory prediction challenging task because complex nature humans <eos> paper tackle problem within deep learning framework considering motion information each pedestrian its interaction crowd <eos> specifically motivated residual learning deep learning propose predict displacement between neighboring frames each pedestrian sequentially <eos> predict such displacement design crowd interaction deep neural network cidnn considers different importance different pedestrians displacement prediction target pedestrian <eos> specifically use lstm model motion information all pedestrians use multi layer perceptron map location each pedestrian high dimensional feature space inner product between feature used measurement spatial affinity between two pedestrians <eos> then weight motion feature all pedestrians based their spatial affinity target pedestrian location displacement prediction <eos> extensive experiments publicly available datasets validate effectiveness method trajectory prediction <eos> <eop> mean variance loss deep age estimation face <eos> age estimation broad application prospects many fields such video surveillance social networking human computer interaction <eos> however many published age estimation approaches simply treat age estimation exact age regression problem thus did leverage distribution robustness representing labels ambiguity such ages <eos> paper propose new loss function called mean variance loss robust age estimation via distribution learning <eos> specifically mean variance loss consists mean loss penalizes difference between mean estimated age distribution ground truth age variance loss penalizes variance estimated age distribution ensure concentrated distribution <eos> proposed mean variance loss softmax loss embedded jointly into convolutional neural network cnn age estimation network weights optimized via stochastic gradient descent sgd end end learning way <eos> experimental result number challenging face aging databases fg net morph album ii clap show proposed approach outperforms state art method large margin using single model <eos> <eop> probabilistic joint face skull modelling facial reconstruction <eos> present novel method co registration two independent statistical shape models <eos> solve problem aligning face model skull model stochastic optimization based markov chain monte carlo mcmc <eos> create probabilistic joint face skull model show how obtain distribution plausible face shapes given skull shape <eos> due environmental genetic factors there exists distribution possible face shapes arising same skull <eos> pose facial reconstruction conditional distribution plausible face shapes given skull shape <eos> because very difficult obtain distribution directly mri ct data create dataset artificial face skull pairs <eos> propose combine three data sources independent origin model joint face skull distribution face shape model skull shape model tissue depth marker information <eos> given skull compute posterior distribution faces matching tissue depth distribution metropolis hastings <eos> estimate joint face skull distribution sample posterior <eos> find faces matching unknown skull estimate probability face under joint face skull model <eos> knowledge first provide whole distribution plausible faces arising skull instead only single reconstruction <eos> show how face skull model used rank face dataset average successfully identify correct match top <eos> face ranking even works when obtaining face shapes image <eos> furthermore show how face skull model useful estimate skull position mr image <eos> <eop> learning latent super events detect multiple activities video <eos> paper introduce concept learning latent super events activity video present how benefits activity detection continuous video <eos> define super event set multiple events occurring together video particular temporal organization opposite concept sub events <eos> real world video contain multiple activities rarely segmented <eos> surveillance video learning latent super events allows model capture how events temporally related video <eos> design emph temporal structure filters enable model focus particular sub intervals video use them together soft attention mechanism learn representations latent super events <eos> super event representations combined per frame per segment cnn provide frame level annotations <eos> approach designed fully differentiable enabling end end learning latent super event representations jointly activity detector using them <eos> experiments multiple public video datasets confirm proposed concept latent super event learning significantly benefits activity detection advancing state arts <eos> <eop> temporal hallucinating action recognition few still image <eos> action recognition still image recently promoted deep learning <eos> however success deep models heavily depends huge amount training image various action categories may available practice <eos> alternatively humans classify new action categories after seeing few image since may only compare appearance similarities between image hand but also attempt recall importance motion cues relevant action video memory <eos> mimic capacity propose novel hybrid video memory hvm machine hallucinate temporal feature still image video memory order boost action recognition few still image <eos> first design temporal memory module consisting temporal hallucinating predicting <eos> temporal hallucinating generate temporal feature still image unsupervised manner <eos> hence flexibly used realistic scenarios image video categories may consistent <eos> temporal predicting effectively infer action categories query image integrating temporal feature training image video within domain adaptation manner <eos> second design spatial memory module spatial predicting <eos> spatial temporal feature complementary represent different actions apply spatial temporal prediction fusion further boost performance <eos> finally design video selection module select strongly relevant video memory <eos> case balance number image video reduce prediction bias well preserve computation efficiency <eos> show effectiveness conduct extensive experiments three challenging data set hvm outperforms number recent approaches temporal hallucinating video memory <eos> <eop> deep progressive reinforcement learning skeleton based action recognition <eos> paper propose deep progressive reinforcement learning dprl method action recognition skeleton based video aims distil most informative frames discard ambiguous frames sequences recognizing actions <eos> since choices selecting representative frames multitudinous each video model frame selection progressive process through deep reinforcement learning during progressively adjust chosen frames taking two important factors into account quality selected frames relationship between selected frames whole video <eos> moreover considering topology human body inherently lies graph based structure vertices edges represent hinged joints rigid bones respectively employ graph based convolutional neural network capture dependency between joints action recognition <eos> approach achieves very competitive performance three widely used benchmarks <eos> <eop> gaze prediction dynamic immersive video <eos> paper explores gaze prediction dynamic circ immersive video emph <eos> based history scan path vr contents predict viewer will look upcoming time <eos> tackle problem first present large scale eye tracking dynamic vr scene dataset <eos> dataset contains circ video captured dynamic scenes each video viewed least subjects <eos> analysis shows gaze prediction depends its history scan path image contents <eos> terms image contents salient object easily attract viewers attention <eos> one hand saliency related both appearance motion object <eos> considering saliency measured different scales different propose compute saliency maps different spatial scales sub image patch centered current gaze point sub image corresponding field view fov panorama image <eos> then feed both saliency maps corresponding image into convolutional neural network cnn feature extraction <eos> meanwhile also use long short term memory lstm encode history scan path <eos> then combine cnn feature lstm feature gaze displacement prediction between gaze point current time gaze point upcoming time <eos> extensive experiments validate effectiveness method gaze prediction dynamic vr scenes <eos> <eop> when will you anticipating temporal occurrences activities <eos> analyzing human actions video gained increased attention recently <eos> while most works focus classifying labeling observed video frames anticipating very recent future making long term predictions over more than just few seconds task many practical applications yet addressed <eos> paper propose two method predict considerably large amount future actions their durations <eos> both cnn rnn trained learn future video labels based previously seen content <eos> show method generate accurate predictions future even long video huge amount different actions even deal noisy erroneous input information <eos> <eop> fusing crowd density maps visual object trackers people tracking crowd scenes <eos> while people tracking greatly improved over recent years crowd scenes remain particularly challenging people tracking due heavy occlusions high crowd density significant appearance variation <eos> address challenges first design sparse kernelized correlation filter kcf suppress target response variations caused occlusions illumination changes spurious responses due similar distractor object <eos> then propose people tracking framework fuses kcf response map estimated crowd density map using convolutional neural network cnn yielding refined response map <eos> train fusion cnn propose two stage strategy gradually optimize parameters <eos> first stage train preliminary model batch mode image patches selected around targets second stage fine tune preliminary model using real frame frame tracking process <eos> density fusion framework significantly improves people tracking crowd scenes also combined other trackers improve tracking performance <eos> validate framework two crowd video datasets ucsd pets <eos> <eop> dual attention matching network context aware feature sequence based person re identification <eos> typical person re identification reid method usually describe each pedestrian single feature vector match them task specific metric space <eos> however method based single feature vector sufficient enough overcome visual ambiguity frequently occurs real scenario <eos> paper propose novel end end trainable framework called dual attention matching network duatm learn context aware feature sequences perform attentive sequence comparison simultaneously <eos> core component duatm framework dual attention mechanism both intra sequence inter sequence attention strategies used feature refinement feature pair alignment respectively <eos> thus detailed visual cues contained intermediate feature sequences automatically exploited properly compared <eos> train proposed duatm network siamese network via triplet loss assisted de correlation loss cross entropy loss <eos> conduct extensive experiments both image video based reid benchmark datasets <eos> experimental result demonstrate significant advantages approach compared state art method <eos> <eop> easy identification better constraints multi shot person re identification reference constraints <eos> multi shot person re identification msp rid utilizes multiple image same person facilitate identification <eos> considering fact motion information may discriminative nor reliable enough msp rid paper focused handling large variations visual appearances through learning discriminative visual metrics identification <eos> existing metric learning based method usually exploit pair wise triple wise similarity constraints generally demands intensive optimization metric learning leads degraded performances using sub optimal solutions <eos> addition training data significantly imbalanced learning largely dominated negative pairs thus produces unstable non discriminative result <eos> paper propose novel type similarity constraint <eos> assigns sample point set extbf reference point produce linear number extbf reference constraints <eos> several optimal transport based schemes reference constraint generation proposed studied <eos> based constraints utilizing typical regressive metric learning model closed form solution learned metric easily obtained <eos> extensive experiments comparative studies several public msp rid benchmarks validated effectiveness method its significant superiority over state art msp rid method terms both identification accuracy running speed <eos> <eop> crowd counting deep negative correlation learning <eos> deep convolutional network convnets achieved unprecedented performances many computer vision tasks <eos> however their adaptations crowd counting single image still their infancy suffer severe over fitting <eos> here propose new learning strategy produce generalizable feature way deep negative correlation learning ncl <eos> more specifically deeply learn pool decorrelated regressors sound generalization capabilities through managing their intrinsic diversities <eos> proposed method named decorrelated convnet convnet end end trainable independent backbone fully convolutional network architectures <eos> extensive experiments very deep vggnet well customized network structure indicate superiority convnet when compared several state art method <eos> implementation will released github <eos> com shizenglin deep ncl <eop> human appearance transfer <eos> propose automatic person person appearance transfer model based explicit parametric human representations learned constrained deep translation network architectures photographic image synthesis <eos> given single source image single target image each corresponding different human subjects wearing different clothing different poses goal photo realistically transfer appearance source image onto target image while preserving target shape clothing segmentation layout <eos> solution new problem formulated terms computational pipeline combines human pose body shape estimation monocular image identifying surface colors elements mesh triangles visible both image transferred directly using barycentric procedures predicting surface appearance missing first image but visible second one using deep learning based image synthesis techniques <eos> model achieves promising result supported perceptual user study participants rated around result good very good perfect well automated tests inception scores faster rcnn human detector responding very similarly real model generated image <eos> further show how proposed architecture profiled automatically generate image person dressed different clothing transferred person another image opening paths applications entertainment photo editing <eos> embodying posing friends famous actors fashion industry affordable online shopping clothing <eos> <eop> domain generalization adversarial feature learning <eos> paper tackle problem domain generalization how learn generalized feature representation unseen target domain taking advantage multiple seen source domain data <eos> present novel framework based adversarial autoencoders learn generalized latent feature representation across domains domain generalization <eos> specific extend adversarial autoencoders imposing maximum mean discrepancy mmd measure align distributions among different domains matching aligned distribution arbitrary prior distribution via adversarial feature learning <eos> way learned feature representation supposed universal seen source domains because mmd regularization expected generalize well target domain because introduction prior distribution <eos> proposed algorithm jointly train different components proposed framework <eos> extensive experiments various vision tasks demonstrate proposed framework learn better generalized feature unseen target domain compared state art domain generalization method <eos> <eop> pyramid stereo matching network <eos> recent work shown depth estimation stereo pair image formulated supervised learning task resolved convolutional neural network cnn <eos> however current architectures rely patch based siamese network lacking means exploit context information finding correspondence ill posed region <eos> tackle problem propose psmnet pyramid stereo matching network consisting two main modules spatial pyramid pooling three dimensional cnn <eos> spatial pyramid pooling module takes advantage capacity global context information aggregating context different scales locations form cost volume <eos> three dimensional cnn learns regularize cost volume using stacked multiple hourglass network conjunction intermediate supervision <eos> proposed approach was evaluated several benchmark datasets <eos> method ranked first kitti leaderboards before march <eos> codes psmnet available github <eos> com jiarenchang psmnet <eos> <eop> event based vision meets deep learning steering prediction self driving cars <eos> event cameras bio inspired vision sensors naturally capture dynamics scene filtering out redundant information <eos> paper presents deep neural network approach unlocks potential event cameras challenging motion estimation task prediction vehicle steering angle <eos> make best out sensor algorithm combination adapt state art convolutional architectures output event sensors extensively evaluate performance approach publicly available large scale event camera dataset km <eos> present qualitative quantitative explanations why event cameras allow robust steering prediction even cases traditional cameras fail <eos> challenging illumination conditions fast motion <eos> finally demonstrate advantages leveraging transfer learning traditional event based vision show approach outperforms state art algorithms based standard cameras <eop> learning answer embeddings visual question answering <eos> propose novel probabilistic model visual question answering visual qa <eos> key idea infer two set embeddings one image question jointly other answers <eos> learning objective learn best parameterization embeddings such correct answer higher likelihood among all possible answers <eos> contrast several existing approaches treating visual qa multi way classification proposed approach takes semantic relationships characterized embeddings among answers into consideration instead viewing them independent ordinal numbers <eos> thus learned embedded function used embed unseen answers training dataset <eos> properties make approach particularly appealing transfer learning open ended visual qa source dataset model learned limited overlapping target dataset space answers <eos> also developed large scale optimization techniques applying model datasets large number answers challenge properly normalize proposed probabilistic models <eos> validate approach several visual qa datasets investigate its utility transferring models across datasets <eos> empirical result shown approach performs well only domain learning but also transfer learning <eos> <eop> good view hunting learning photo composition dense view pairs <eos> finding views good photo composition challenging task machine learning method <eos> key difficulty lack well annotated large scale datasets <eos> most existing datasets only provide limited number annotations good views while ignoring comparative nature view selection <eos> work present first large scale comparative photo composition dataset contains over one million comparative view pairs annotated using cost effective crowdsourcing workflow <eos> show comparative view annotations essential training robust neural network model composition <eos> addition propose novel knowledge transfer framework train fast view proposal network runs fps achieves state art performance image cropping thumbnail generation tasks three benchmark datasets <eos> superiority method also demonstrated user study challenging experiment method significantly outperforms baseline method producing diversified well composed views <eos> <eop> cleannet transfer learning scalable image classifier training label noise <eos> paper study problem learning image classification models label noise <eos> existing approaches depending human supervision generally scalable manually identifying correct incorrect labels time consuming whereas approaches relying human supervision scalable but less effective <eos> reduce amount human supervision label noise cleaning introduce cleannet joint neural embedding network only requires fraction classes being manually verified provide knowledge label noise transferred other classes <eos> further integrate cleannet conventional convolutional neural network classifier into one framework image classification learning <eos> demonstrate effectiveness proposed algorithm both label noise detection task image classification noisy data task several large scale datasets <eos> experimental result show cleannet reduce label noise detection error rate held out classes no human supervision available <eos> compared current weakly supervised method <eos> also achieves performance gain verifying all image only <eos> image verified image classification task <eos> source code dataset will available kuanghuei <eos> <eop> independently recurrent neural network indrnn building longer deeper rnn <eos> recurrent neural network rnns widely used processing sequential data <eos> however rnns commonly difficult train due well known gradient vanishing exploding problems hard learn long term patterns <eos> long short term memory lstm gated recurrent unit gru were developed address problems but use hyperbolic tangent sigmoid action functions result gradient decay over layer <eos> consequently construction efficiently trainable deep network challenging <eos> addition all neurons rnn layer entangled together their behaviour hard interpret <eos> address problems new type rnn referred independently recurrent neural network indrnn proposed paper neurons same layer independent each other they connected across layer <eos> shown indrnn easily regulated prevent gradient exploding vanishing problems while allowing network learn long term dependencies <eos> moreover indrnn work non saturated activation functions such relu rectified linear unit still trained robustly <eos> multiple indrnns stacked construct network deeper than existing rnns <eos> experimental result shown proposed indrnn able process very long sequences over time steps used construct very deep network layer used experiment still trained robustly <eos> better performances achieved various tasks using indrnns compared traditional rnn lstm <eos> <eop> mix match network encoder decoder alignment zero pair image translation <eos> address problem image translation between domains modalities no direct paired data available <eos> zero pair translation <eos> propose mix match network based multiple encoders decoders aligned such way other encoder decoder pairs composed test time perform unseen image translation tasks between domains modalities explicit paired sample were seen during training <eos> study impact autoencoders side information losses improving alignment transferability trained pairwise translation models unseen translations <eos> show approach scalable perform colorization style transfer between unseen combinations domains <eos> evaluate system challenging cross modal setting semantic segmentation estimated depth image without explicit access any depth semantic segmentation training pairs <eos> model outperforms baselines based pix pix cyclegan models <eos> <eop> structured uncertainty prediction network <eos> paper first work propose network predict structured uncertainty distribution synthesized image <eos> previous approaches mostly limited predicting diagonal covariance matrices <eos> novel model learns predict full gaussian covariance matrix each reconstruction permits efficient sampling likelihood evaluation <eos> demonstrate model accurately reconstruct ground truth correlated residual distributions synthetic datasets generate plausible high frequency sample real face image <eos> also illustrate use predicted covariances structure preserving image denoising <eos> <eop> between class learning image classification <eos> paper propose novel learning method image classification called between class learning bc learning <eos> generate between class image mixing two image belonging different classes random ratio <eos> then input mixed image model train model output mixing ratio <eos> bc learning ability impose constraints shape feature distributions thus generalization ability improved <eos> bc learning originally method developed sounds digitally mixed <eos> mixing two image data appear make sense however argue because convolutional neural network aspect treating input data waveforms works sounds must also work image <eos> first propose simple mixing method using internal divisions surprisingly proves significantly improve performance <eos> second propose mixing method treats image waveforms leads further improvement performance <eos> top errors imagenet cifar respectively <eos> <eop> adversarial feature augmentation unsupervised domain adaptation <eos> recent works showed generative adversarial network gans successfully applied unsupervised domain adaptation given labeled source dataset unlabeled target dataset goal train powerful classifiers target sample <eos> particular was shown gan objective function used learn target feature indistinguishable source ones <eos> work extend framework forcing learned feature extractor domain invariant ii training through data augmentation feature space namely performing feature augmentation <eos> while data augmentation image space well established technique deep learning feature augmentation yet received same level attention <eos> accomplish means feature generator trained playing gan minimax game against source feature <eos> result show both enforcing domain invariance performing feature augmentation lead superior comparable performance state art result several unsupervised domain adaptation benchmarks <eos> <eop> generative image inpainting contextual attention <eos> recent deep learning based approaches shown promising result challenging task inpainting large missing region image <eos> method generate visually plausible image structures textures but often create distorted structures blurry textures inconsistent surrounding areas <eos> mainly due ineffectiveness convolutional neural network explicitly borrowing copying information distant spatial locations <eos> other hand traditional texture patch synthesis approaches particularly suitable when needs borrow textures surrounding region <eos> motivated observations propose new deep generative model based approach only synthesize novel image structures but also explicitly utilize surrounding image feature references during network training make better predictions <eos> model feed forward fully convolutional neural network process image multiple holes arbitrary locations variable sizes during test time <eos> experiments multiple datasets including faces celeba celeba hq textures dtd natural image imagenet places demonstrate proposed approach generates higher quality inpainting result than existing ones <eos> code demo models available github <eos> com jiahuiyu generative inpainting <eos> <eop> csgnet neural shape parser constructive solid geometry <eos> present neural architecture takes input three dimensional shape outputs program generates shape <eos> instructions program based constructive solid geometry principles <eos> set boolean operations shape primitives defined recursively <eos> bottom up techniques shape parsing task rely primitive detection inherently slow since search space over possible primitive combinations large <eos> contrast model uses recurrent neural network parses input shape top down manner significantly faster yields compact easy interpret sequence modeling instructions <eos> model also more effective shape detector compared existing state art detection techniques <eos> finally demonstrate network trained novel datasets without ground truth program annotations through policy gradient techniques <eos> <eop> conditional image image translation <eos> image image translation tasks widely investigated generative adversarial network gans dual learning <eos> however existing models lack ability control translated result target domain their result usually lack diversity sense fixed image usually leads almost deterministic translation result <eos> paper study new problem conditional image image translation translate image source domain target domain conditioned given image target domain <eos> requires generated image should inherit some domain specific feature conditional image target domain <eos> therefore changing conditional image target domain will lead diverse translation result fixed input image source domain therefore conditional input image helps control translation result <eos> tackle problem unpaired data based gans dual learning <eos> twist two conditional translation models one translation domain domain other one domain domain together inputs combination reconstruction while preserving domain independent feature <eos> carry out experiments men faces women faces translation edges shoes bags translations <eos> result demonstrate effectiveness proposed method <eos> <eop> continuous relaxation map inference nonconvex perspective <eos> paper study nonconvex continuous relaxation map inference discrete markov random fields mrfs <eos> show arbitrary mrfs relaxation tight discrete stationary point easily reached simple block coordinate descent algorithm <eos> addition study resolution relaxation using popular gradient method further propose more effective solution using multilinear decomposition framework based alternating direction method multipliers admm <eos> experiments many real world problems demonstrate proposed admm significantly outperforms other nonconvex relaxation based method compares favorably state art mrf optimization algorithms different settings <eos> <eop> feature generating network zero shot learning <eos> suffering extreme training data imbalance between seen unseen classes most existing state art approaches fail achieve satisfactory result challenging generalized zero shot learning task <eos> circumvent need labeled examples unseen classes propose novel generative adversarial network gan synthesizes cnn feature conditioned class level semantic information offering shortcut directly semantic descriptor class class conditional feature distribution <eos> proposed approach pairing wasserstein gan classification loss able generate sufficiently discriminative cnn feature train softmax classifiers any multimodal embedding method <eos> experimental result demonstrate significant boost accuracy over state art five challenging datasets cub flo sun awa imagenet both zero shot learning generalized zero shot learning settings <eos> <eop> joint optimization framework learning noisy labels <eos> deep neural network dnns trained large scale datasets exhibited significant performance image classification <eos> many large scale datasets collected websites however they tend contain inaccurate labels termed noisy labels <eos> training such noisy labeled datasets causes performance degradation because dnns easily overfit noisy labels <eos> overcome problem propose joint optimization framework learning dnn parameters estimating true labels <eos> framework correct labels during training alternating update network parameters labels <eos> conduct experiments noisy cifar datasets clothing dataset <eos> result indicate approach significantly outperforms other state art method <eos> <eop> convolutional image captioning <eos> image captioning important task applicable virtual assistants editing tools image indexing support disabled <eos> recent years significant progress made image captioning using recurrent neural network powered long short term memory lstm units <eos> despite mitigating vanishing gradient problem despite their compelling ability memorize dependencies lstm units complex inherently sequential across time <eos> address issue recent work shown benefits convolutional network machine translation conditional image generation <eos> inspired their success paper develop convolutional image captioning technique <eos> demonstrate its efficacy challenging mscoco dataset demonstrate performance par lstm baseline while having faster training time per number parameters <eos> also perform detailed analysis providing compelling reasons favor convolutional language generation approaches <eos> <eop> aon towards arbitrarily oriented text recognition <eos> recognizing text natural image hot research topic computer vision due its various applications <eos> despite enduring research several decades optical character recognition ocr recognizing texts natural image still challenging task <eos> because scene texts often irregular <eos> curved arbitrarily oriented seriously distorted arrangements yet well addressed literature <eos> existing method text recognition mainly work regular horizontal frontal texts cannot trivially generalized handle irregular texts <eos> paper develop arbitrary orientation network aon directly capture deep feature irregular texts combined into attention based decoder generate character sequence <eos> whole network trained end end using only image word level annotations <eos> extensive experiments various benchmarks including cute svt perspective iiit svt icdar datasets show proposed aon based method achieves state art performance irregular datasets comparable major existing method regular datasets <eos> <eop> wrapped gaussian process regression riemannian manifolds <eos> gaussian process gp regression powerful tool non parametric regression providing uncertainty estimates <eos> however limited data vector spaces <eos> fields such shape analysis diffusion tensor imaging data often lies manifold making gp regression non viable resulting predictive distribution live correct geometric space <eos> tackle problem defining wrapped gaussian processes wgps rieman nian manifolds using probabilistic setting general ize gp regression context manifold valued targets <eos> method validated empirically diffusion weighted imaging dwi data directional data sphere kendall shape space endorsing wgp regression efficient flexible tool manifold valued regression <eos> <eop> geometry guided convolutional neural network self supervised video representation learning <eos> often laborious costly manually annotate video training high quality video recognition models so there some work interest exploring alternative cheap yet often noisy indirect training signals learning video representations <eos> however signals still coarse supplying supervision whole video frame level subtle sometimes enforcing learning agent solve problems even hard humans <eos> paper instead explore geometry grand new type auxiliary supervision self supervised learning video representations <eos> particular extract pixel wise geometry information flow fields disparity maps synthetic imagery real three dimensional movies <eos> although geometry high level semantics seemingly distant topics surprisingly find convolutional neural network pre trained geometry cues effectively adapted semantic video understanding tasks <eos> addition also find progressive training strategy foster better neural network video recognition task than blindly pooling distinct sources geometry cues together <eos> extensive result video dynamic scene recognition action recognition tasks show geometry guided network significantly outperform competing method trained other types labeling free supervision signals <eos> <eop> diversenet when one right answer enough <eos> many structured prediction tasks machine vision collection acceptable answers instead one definitive ground truth answer <eos> segmentation image example subject human labeling bias <eos> similarly there multiple possible pixel values could plausibly complete occluded image region <eos> state art supervised learning method typically optimized make single test time prediction each query failing find other modes output space <eos> existing method allow sampling often sacrifice speed accuracy <eos> introduce simple method training neural network enables diverse structured predictions made each test time query <eos> single input learn predict range possible answers <eos> compare favorably method seek diversity through ensemble network <eos> such stochastic multiple choice learning faces mode collapse one more ensemble members fail receive any training signal <eos> best performing solution deployed various tasks just involves small modifications existing single mode architecture loss function training regime <eos> demonstrate method result quantitative improvements across three challenging tasks image completion three dimensional volume estimation flow prediction <eos> <eop> deep face detector adaptation without negative transfer catastrophic forgetting <eos> arguably no single face detector fits all real life scenarios <eos> often desirable some built schemes face detector automatically adapt <eos> particular user photo album target domain <eos> propose novel face detector adaptation approach works long there representative image target domain no matter they labeled more importantly without need accessing training data source domain <eos> approach explicitly accounts notorious negative transfer caveat domain adaptation thanks residual loss design <eos> moreover incur catastrophic interference knowledge learned source domain therefore adapted face detectors maintain about same performance old detectors original source domain <eos> such adaption approach face detectors analogous popular interpolation techniques language models may opens new direction progressively training face detectors domain domain <eos> report extensive experimental result verify approach two massively benchmarked face detectors <eos> <eop> analyzing filters toward efficient convnet <eos> deep convolutional neural network convnet promising approach high performance image classification <eos> behavior convnet analyzed mainly based neuron activations such visualizing them <eos> paper contrast activations focus filters main components convnets <eos> through analyzing two types filters convolution fully connected fc layer respectively various pre trained convnets present method efficiently reformulate filters contributing improving both memory size classification performance convnets <eos> they render filter bases formulated parameter free form well efficient representation fc layer <eos> experimental result image classification show method favorably applied improve various convnets including resnet trained imagenet exhibiting high transferability other datasets <eos> <eop> regularizing deep network modeling predicting label structure <eos> construct custom regularization functions use supervised training deep neural network <eos> technique applicable when ground truth labels themselves exhibit internal structure derive regularizer learning autoencoder over set annotations <eos> training thereby becomes two phase procedure <eos> first phase models labels autoencoder <eos> second phase trains actual network interest attaching auxiliary branch must predict output via hidden layer autoencoder <eos> after training discard auxiliary branch <eos> experiment context semantic segmentation demonstrating regularization strategy leads consistent accuracy boosts over baselines both when training scratch combination imagenet pretraining <eos> gains also consistent over different choices convolutional network architecture <eos> regularizer discarded after training method zero cost test time performance improvements essentially free <eos> simply able learn better network weights building abstract model label space then training network understand abstraction alongside original task <eos> <eop> place activated batchnorm memory optimized training dnns <eos> work present place activated batch normalization inplace abn novel approach drastically reduce training memory footprint modern deep neural network computationally efficient way <eos> solution substitutes conventionally used succession batchnorm activation layer single plugin layer hence avoiding invasive framework surgery while providing straightforward applicability existing deep learning frameworks <eos> obtain memory savings up dropping intermediate result recovering required information during backward pass through inversion stored forward result only minor increase <eos> also demonstrate how frequently used checkpointing approaches made computationally efficient inplace abn <eos> experiments image classification demonstrate par result imagenet state art approaches <eos> memory demanding task semantic segmentation report competitive result coco stuff set new state art result cityscapes mapillary vistas <eos> code found github <eos> com mapillary inplace abn <eos> <eop> dvqa understanding data visualizations via question answering <eos> bar charts effective way convey numeric information but today algorithms cannot parse them <eos> existing method fail when faced even minor variations appearance <eos> here present dvqa dataset tests many aspects bar chart understanding question answering framework <eos> unlike visual question answering vqa dvqa requires processing words answers unique particular bar chart <eos> state art vqa algorithms perform poorly dvqa propose two strong baselines perform considerably better <eos> work will enable algorithms automatically extract numeric semantic information vast quantities bar charts found scientific publications internet articles business reports many other areas <eos> <eop> da gan instance level image translation deep attention generative adversarial network <eos> unsupervised image translation aims translating two independent set image challenging discovering correct correspondences without paired data <eos> existing works build upon generative adversarial network gans such distribution translated image indistinguishable distribution target set <eos> however such set level constraints cannot learn instance level correspondences <eos> aligned semantic parts object transfiguration task <eos> limitation often result false positives <eos> geometric semantic artifacts further leads mode collapse problem <eos> address above issues propose novel framework instance level image translation deep attention gan da gan <eos> such design enables da gan decompose task translating sample two set into translating instances highly structured latent space <eos> specifically jointly learn deep attention encoder instance level correspondences could consequently discovered through attending learned instances <eos> therefore constraints could exploited both set level instance level <eos> comparisons against several state arts demonstrate superiority approach broad application capability <eos> pose morphing data augmentation etc <eos> pushes margin domain translation problem <eos> <eop> unsupervised learning depth ego motion monocular video using three dimensional geometric constraints <eos> present novel approach unsupervised learning depth ego motion monocular video <eos> unsupervised learning removes need separate supervisory signals depth ego motion ground truth multi view video <eos> prior work unsupervised depth learning uses pixel wise gradient based losses only consider pixels small local neighborhoods <eos> main contribution explicitly consider inferred three dimensional geometry whole scene enforce consistency estimated three dimensional point clouds ego motion across consecutive frames <eos> challenging task solved novel approximate backpropagation algorithm aligning three dimensional structures <eos> combine novel three dimensional based loss losses based photometric quality frame reconstructions using estimated depth ego motion adjacent frames <eos> also incorporate validity masks avoid penalizing areas no useful information exists <eos> test algorithm kitti dataset video dataset captured uncalibrated mobile phone camera <eos> proposed approach consistently improves depth estimates both datasets outperforms state art both depth ego motion <eos> because only require simple video learning depth ego motion large varied datasets becomes possible <eos> demonstrate training low quality uncalibrated video dataset evaluating kitti ranking among top performing prior method trained kitti itself <eos> <eop> fots fast oriented text spotting unified network <eos> incidental scene text spotting considered one most difficult valuable challenges document analysis community <eos> most existing method treat text detection recognition separate tasks <eos> work propose unified end end trainable fast oriented text spotting fots network simultaneous detection recognition sharing computation visual information among two complementary tasks <eos> specifically roirotate introduced share convolutional feature between detection recognition <eos> benefiting convolution sharing strategy fots little computation overhead compared baseline text detection network joint training method makes method perform better than two stage method <eos> experiments icdar icdar mlt icdar datasets demonstrate proposed method outperforms state art method significantly further allows develop first real time oriented text spotting system surpasses all previous state art result more than icdar text spotting task while keeping <eos> <eop> mobile video object detection temporally aware feature maps <eos> paper introduces online model object detection video real time performance mobile embedded devices <eos> approach combines fast single image object detection convolutional long short term memory lstm layer create interweaved recurrent convolutional architecture <eos> additionally propose efficient bottleneck lstm layer significantly reduces computational cost compared regular lstms <eos> network achieves temporal awareness using bottleneck lstms refine propagate feature maps across frames <eos> approach substantially faster than existing detection method video outperforming fastest single frame models model size computational cost while attaining accuracy comparable much more expensive single frame models imagenet vid dataset <eos> model reaches real time inference speed up fps mobile cpu <eos> <eop> weakly supervised phrase localization multi scale anchored transformer network <eos> paper propose novel weakly supervised model multi scale anchored transformer network matn accurately localize free form textual phrases only image level supervision <eos> proposed matn takes region proposals localization anchors learns multi scale correspondence network continuously search phrase region referring anchors <eos> way matn exploit useful cues anchors reliably reason about locations region described phrases given only image level supervision <eos> through differentiable sampling image spatial feature maps matn introduces novel training objective simultaneously minimize contrastive reconstruction loss between different phrases single image set triplet losses among multiple image similar phrases <eos> superior existing region proposal based method matn searches optimal bounding box over entire feature map instead selecting sub optimal one discrete region proposals <eos> evaluate matn flickr entities referitgame datasets <eos> experimental result show matn significantly outperforms state art method <eos> <eop> revisiting oxford paris large scale image retrieval benchmarking <eos> paper address issues image retrieval benchmarking standard popular oxford paris datasets <eos> particular annotation errors size dataset level challenge addressed new annotation both datasets created extra attention reliability ground truth <eos> three new protocols varying difficulty introduced <eos> protocols allow fair comparison between different method including using dataset pre processing stage <eos> each dataset new challenging queries introduced <eos> finally new set hard semi automatically cleaned distractors selected <eos> extensive comparison state art method performed new benchmark <eos> different types method evaluated ranging local feature based modern cnn based method <eos> best result achieved taking best two worlds <eos> most importantly image retrieval appears far being solved <eos> <eop> cross dataset adaptation visual question answering <eos> investigate problem cross dataset adaptation visual question answering visual qa <eos> goal train visual qa model source dataset but apply another target one <eos> analogous domain adaptation visual recognition setting appealing when target dataset sufficient amount labeled data learn domain model <eos> key challenge two datasets constructed differently resulting cross dataset mismatch image questions answers <eos> overcome difficulty proposing novel domain adaptation algorithm <eos> method reduces difference statistical distributions transforming feature representation data target dataset <eos> moreover maximizes likelihood answering questions target dataset correctly using visual qa model trained source dataset <eos> empirically studied effectiveness proposed approach adapting among several popular visual qa datasets <eos> show proposed method improves over baselines there no adaptation several other adaptation method <eos> both quantitatively qualitatively analyze when adaptation mostly effective <eos> <eop> globally optimal inlier set maximization atlanta frame estimation <eos> work describe man made structures via appropriate structure assumption called atlanta world contains vertical direction typically gravity direction set horizontal directions orthogonal vertical direction <eos> contrary commonly used manhattan world assumption horizontal directions atlanta world necessarily orthogonal each other <eos> while atlanta world permits encompass wider range scenes makes solution space larger problem more challenging <eos> given set inputs such lines calibrated image surface normals propose first globally optimal method inlier set maximization atlanta direction estimation <eos> define novel search space atlanta world well its parameterization solve challenging problem branch bound framework <eos> experimental result synthetic real world datasets successfully confirmed validity approach <eos> <eop> end end convolutional semantic embeddings <eos> semantic embeddings image sentences widely studied recently <eos> ability deep neural network learning rich robust visual textual representations offers opportunity develop effective semantic embedding models <eos> currently state art approaches semantic learning first employ deep neural network encode image sentences into common semantic space <eos> then learning objective ensure larger similarity between matching image sentence pairs than randomly sampled pairs <eos> usually convolutional neural network cnn recurrent neural network rnns employed learning image sentence representations respectively <eos> one hand cnn known produce robust visual feature different levels rnns known capturing dependencies sequential data <eos> therefore simple framework sufficiently effective learning visual textual semantics <eos> other hand different cnn rnns cannot produce middle level <eos> phrase level text representations <eos> result only global representations available semantic learning <eos> could potentially limit performance model due hierarchical structures image sentences <eos> work apply convolutional neural network process both image sentences <eos> consequently employ mid level representations assist global semantic learning introducing new learning objective convolutional layer <eos> experimental result show proposed textual cnn models new learning objective lead better performance than state art approaches <eos> <eop> referring image segmentation via recurrent refinement network <eos> address problem image segmentation natural language descriptions <eos> existing deep learning based method encode image representations based output last convolutional layer <eos> one general issue resulting image representation lacks multi scale semantics key components advanced segmentation systems <eos> paper utilize feature pyramids inherently existing convolutional neural network capture semantics different scales <eos> produce suitable information flow through path feature hierarchy propose recurrent refinement network rrn takes pyramidal feature input refine segmentation mask progressively <eos> experimental result four available datasets show approach outperforms multiple baselines state art <eos> <eop> two play game visual dialog discriminative question generation answering <eos> human conversation complex mechanism subtle nuances <eos> hence ambitious goal develop artificial intelligence agents participate fluently conversation <eos> while still far achieving goal recent progress visual question answering image captioning visual question generation shows dialog systems may realizable too distant future <eos> end novel dataset was introduced recently encouraging result were demonstrated particularly question answering <eos> paper demonstrate simple symmetric discriminative baseline applied both predicting answer well predicting question <eos> show method performs par state art even memory net based method <eos> addition first time visual dialog dataset assess performance system asking questions demonstrate how visual dialog generated discriminative question generation question answering <eos> <eop> generative adversarial learning towards fast weakly supervised detection <eos> weakly supervised object detection attracted extensive research efforts recent years <eos> without need annotating bounding boxes existing method usually follow two multi stage pipeline online compulsive stage extract object proposals order magnitude slower than fast fully supervised object detectors such ssd yolo <eos> paper speedup online weakly supervised object detectors orders magnitude proposing novel generative adversarial learning paradigm <eos> proposed paradigm generator one stage object detector generate bounding boxes image <eos> guide learning object level generator surrogator introduced mine high quality bounding boxes training <eos> further adapt structural similarity loss combination adversarial loss into training objective solves challenge bounding boxes produced surrogator may well capture their ground truth <eos> one stage detector outperforms all existing schemes terms detection accuracy running frames per second up faster than state art weakly supervised detectors <eos> code will available publicly soon <eos> <eop> deeper look power normalizations <eos> power normalizations pn very useful non linear operators context bag words data representations they tackle problems such feature imbalance <eos> paper reconsider operators deep learning setup introducing novel layer implements pn non linear pooling feature maps <eos> specifically using kernel formulation layer combines feature vectors their respective spatial locations feature maps produced last convolutional layer cnn <eos> linearization such kernel result positive definite matrix capturing second order statistics feature vectors pn operators applied <eos> study two types pn functions namely maxexp ii gamma addressing their role meaning context non linear pooling <eos> also provide probabilistic interpretation operators derive their surrogates well behaved gradients end end cnn learning <eos> apply theory practice implementing pn layer resnet model showcase experiments four benchmarks fine grained recognition scene recognition material classification <eos> result demonstrate state part performance across all tasks <eos> <eop> dimensionality blessing clustering image underlying distribution <eos> many high dimensional vector distances tend constant <eos> typically considered negative contrast loss phenomenon hinders clustering other machine learning techniques <eos> reinterpret contrast loss blessing <eos> re deriving contrast loss using law large numbers show result distribution instances concentrating thin hyper shell <eos> hollow center means apparently chaotically overlapping distributions actually intrinsically separable <eos> use develop distribution clustering elegant algorithm grouping data point their unknown underlying distribution <eos> distribution clustering creates notably clean clusters raw unlabeled data estimates number clusters itself inherently robust outliers form their own clusters <eos> enables trawling patterns unorganized data may key enabling machine intelligence <eos> <eop> eliminating background bias robust person re identification <eos> person re identification important topic intelligent surveillance computer vision <eos> aims accurately measure visual similarities between person image determining whether two image correspond same person <eos> state art method mainly utilize deep learning based approaches learning visual feature describing person appearances <eos> however observe existing deep learning models biased capture too much relevance between background appearances person image <eos> design series experiments newly created datasets validate influence background information <eos> solve background bias problem propose person region guided pooling deep neural network based human parsing maps learn more discriminative person part feature propose augment training data person image random background <eos> extensive experiments demonstrate robustness effectiveness proposed method <eos> <eop> learning evaluate image captioning <eos> evaluation metrics image captioning face two challenges <eos> firstly commonly used metrics such cider meteor rouge bleu often correlate well human judgments <eos> secondly each metric well known blind spots pathological caption constructions rule based metrics lack provisions repair such blind spots once identified <eos> example newly proposed spice correlates well human judgments but fails capture syntactic structure sentence <eos> address two challenges propose novel learning based discriminative evaluation metric directly trained distinguish between human machine generated captions <eos> addition further propose data augmentation scheme explicitly incorporate pathological transformations negative examples during training <eos> proposed metric evaluated three kinds robustness tests its correlation human judgments <eos> extensive experiments show proposed data augmentation scheme only makes metric more robust toward several pathological transformations but also improves its correlation human judgments <eos> metric outperforms other metrics both caption level human correlation flickr system level human correlation coco <eos> proposed approach could served learning based evaluation metric complementary existing rule based metrics <eos> <eop> single shot object detection enriched semantics <eos> propose novel single shot object detection network named detection enriched semantics des <eos> motivation enrich semantics object detection feature within typical deep detector semantic segmentation branch global activation module <eos> segmentation branch supervised weak segmentation ground truth <eos> no extra annotation required <eos> conjunction employ global activation module learns relationship between channels object classes self supervised manner <eos> comprehensive experimental result both pascal voc ms coco detection datasets demonstrate effectiveness proposed method <eos> particular vgg based des achieve map <eos> voc test map <eos> coco test dev inference speed <eos> milliseconds per image titan xp gpu <eos> lower resolution version achieve map <eos> voc inference speed <eos> milliseconds per image <eos> <eop> low shot learning imprinted weights <eos> human vision able immediately recognize novel visual categories after seeing just one few training examples <eos> describe how add similar capability convnet classifiers directly setting final layer weights novel training examples during low shot learning <eos> call process weight imprinting directly set weights new category based appropriately scaled copy embedding layer activations training example <eos> imprinting process provides valuable complement training stochastic gradient descent provides immediate good classification performance initialization any further fine tuning future <eos> show how imprinting process related proxy based embeddings <eos> however differs only single imprinted weight vector learned each novel category rather than relying nearest neighbor distance training instances typically used embedding method <eos> experiments show using averaging imprinted weights provides better generalization than using nearest neighbor instance embeddings <eos> <eop> neural motifs scene graph parsing global context <eos> investigate problem producing structured graph representations visual scenes <eos> work analyzes role motifs regularly appearing substructures scene graphs <eos> present new quantitative insights such repeated structures visual genome dataset <eos> analysis shows object labels highly predictive relation labels but vice versa <eos> also find there recurring patterns even larger subgraphs more than graphs contain motifs involving least two relations <eos> analysis motivates new baseline given object detections predict most frequent relation between object pairs given labels seen training set <eos> baseline improves previous state art average <eos> relative improvement across evaluation settings <eos> then introduce stacked motif network new architecture designed capture higher order motifs scene graphs further improves over strong baseline average <eos> code available github <eos> com rowanz neural motifs <eos> <eop> variational autoencoders deforming three dimensional mesh models <eos> three dimensional geometric contents becoming increasingly popular <eos> paper study problem analyzing deforming three dimensional meshes using deep neural network <eos> deforming three dimensional meshes exible represent three dimensional animation sequences well collections object same category allowing diverse shapes large scale non linear deformations <eos> propose novel framework call mesh variational autoencoders mesh vae explore probabilistic latent space three dimensional surfaces <eos> framework easy train requires very few training examples <eos> also propose extended model allows exibly adjusting signi cance different latent variables altering prior distribution <eos> extensive experiments demonstrate general framework able learn reasonable representation collection deformable shapes produce competitive result variety applications including shape generation shape interpolation shape space embedding shape exploration outperforming state art method <eos> <eop> fast monte carlo localization aerial vehicles using approximate continuous belief representations <eos> size weight power constrained platforms impose constraints computational resources introduce unique challenges implementing localization algorithms <eos> present framework perform fast localization such platforms enabled compressive capabilities gaussian mixture model representations point cloud data <eos> given raw structural data depth sensor pitch roll estimates board attitude reference system multi hypothesis particle filter localizes vehicle exploiting likelihood data originating mixture model <eos> demonstrate analysis likelihood vicinity ground truth pose detail its utilization particle filter based vehicle localization strategy later present result real time implementations desktop system off shelf embedded platform outperform localization result running state art algorithm same environment <eos> <eop> dels three dimensional deep localization segmentation three dimensional semantic map <eos> applications such augmented reality autonomous driving self localization camera pose estimation scene parsing crucial technologies <eos> paper propose unified framework tackle two problems simultaneously <eos> uniqueness design sensor fusion scheme integrates camera video motion sensors gps imu three dimensional semantic map order achieve robustness efficiency system <eos> specifically first initial coarse camera pose obtained consumer grade gps imu based label map rendered three dimensional semantic map <eos> then rendered label map rgb image jointly fed into pose cnn yielding corrected camera pose <eos> addition incorporate temporal information multi layer recurrent neural network rnn further deployed improve pose accuracy <eos> finally based pose rnn render new label map fed together rgb image into segment cnn produces per pixel semantic label <eos> order validate approach build dataset registered three dimensional point clouds video camera image <eos> both point clouds image semantically labeled <eos> each video frame ground truth pose highly accurate motion sensors <eos> show practically pose estimation solely relying image like posenet cite kendall iccv may fail due street view confusion important fuse multiple sensors <eos> finally various ablation studies performed demonstrate effectiveness proposed system <eos> particular show scene parsing pose estimation mutually beneficial achieve more robust accurate system <eos> <eop> lidar video driving dataset learning driving policies effectively <eos> learning autonomous driving policies one most challenging but promising tasks computer vision <eos> most researchers believe future research applications should combine cameras video recorders laser scanners obtain comprehensive semantic understanding real traffic <eos> however current approaches only learn large scale video due lack benchmarks consist precise laser scanner data <eos> paper first propose lidar video dataset provides large scale high quality point clouds scanned velodyne laser video recorded dashboard camera standard drivers behaviors <eos> extensive experiments demonstrate extra depth information help network determine driving policies indeed <eos> <eop> logo synthesis manipulation clustered generative adversarial network <eos> designing logo new brand lengthy tedious back forth process between designer client <eos> paper explore extent machine learning solve creative task designer <eos> build dataset lld logos crawled world wide web <eos> training generative adversarial network gans logo synthesis such multi modal data straightforward result mode collapse some state art method <eos> propose use synthetic labels obtained through clustering disentangle stabilize gan training validate approach cifar imagenet small demonstrate its generality <eos> able generate high diversity plausible logos demonstrate latent space exploration techniques ease logo design task interactive manner <eos> gans cope multi modal data means synthetic labels achieved through clustering result show creative potential such techniques logo synthesis manipulation <eos> dataset models publicly available data <eos> ch sagea lld <eos> <eop> egocentric basketball motion planning single first person image <eos> present model uses single first person image generate egocentric basketball motion sequence form camera configuration trajectory encodes player three dimensional location three dimensional head orientation throughout sequence <eos> first introduce future convolutional neural network cnn predicts initial sequence camera configurations aiming capture how real players move during one one basketball game <eos> also introduce goal verifier network trained verify given camera configuration consistent final goals real one one basketball players <eos> next propose inverse synthesis procedure synthesize refined sequence camera configurations sufficiently matches initial configurations predicted future cnn while maximizing output goal verifier network <eos> finally following trajectory resulting refined camera configuration sequence obtain complete motion sequence <eos> model generates realistic basketball motion sequences capture goals real players outperforming standard deep learning approaches such recurrent neural network rnns long short term memory network lstms generative adversarial network gans <eos> <eop> human centric indoor scene synthesis using stochastic grammar <eos> present human centric method sample synthesize three dimensional room layouts image thereof purpose obtaining large scale three dimensional image data perfect per pixel ground truth <eos> attributed spatial graph aog proposed represent indoor scenes <eos> aog probabilistic grammar model terminal nodes object entities including room furniture supported object <eos> human contexts contextual relations encoded markov random fields mrf terminal nodes <eos> learn distributions indoor scene dataset sample new layouts using monte carlo markov chain <eos> experiments demonstrate proposed method robustly sample large variety realistic room layouts based three criteria visual realism comparing state art room arrangement method ii accuracy affordance maps respect ground truth ii functionality naturalness synthesized rooms evaluated human subjects <eos> <eop> rotation sensitive regression oriented scene text detection <eos> text natural image arbitrary orientations requiring detection terms oriented bounding boxes <eos> normally multi oriented text detector often involves two key tasks text presence detection classification problem disregarding text orientation oriented bounding box regression concerns about text orientation <eos> previous method rely shared feature both tasks resulting degraded performance due incompatibility two tasks <eos> address issue propose perform classification regression feature different characteristics extracted two network branches different designs <eos> concretely regression branch extracts rotation sensitive feature actively rotating convolutional filters while classification branch extracts rotation invariant feature pooling rotation sensitive feature <eos> proposed method named rotation sensitive regression detector rrd achieves state art performance several oriented scene text benchmark datasets including icdar msra td rctw coco text <eos> furthermore rrd achieves significant improvement ship collection dataset demonstrating its generality oriented object detection <eos> <eop> separating self expression visual content hashtag supervision <eos> variety abundance structured nature hashtags make them interesting data source training vision models <eos> instance hashtags potential significantly reduce problem manual supervision annotation when learning vision models large number concepts <eos> however key challenge when learning hashtags they inherently subjective because they provided users form self expression <eos> consequence hashtags may synonyms different hashtags referring same visual content may polysemous same hashtag referring different visual content <eos> challenges limit effectiveness approaches simply treat hashtags image label pairs <eos> paper presents approach extends upon modeling simple image label pairs joint model image hashtags users <eos> demonstrate efficacy such approaches image tagging retrieval experiments show how joint model used perform user conditional retrieval tagging <eos> <eop> distort recover color enhancement using deep reinforcement learning <eos> learning based color enhancement approaches typically learn map input image retouched image <eos> most existing method require expensive pairs input retouched image produce result non interpretable way <eos> paper present deep reinforcement learning drl based method color enhancement explicitly model step wise nature human retouching process <eos> cast color enhancement process markov decision process actions defined global color adjustment operations <eos> then train agent learn optimal global enhancement sequence actions <eos> addition present distort recover training scheme only requires high quality reference image training instead input retouched image pairs <eos> given high quality reference image distort image color distribution form distorted reference image pairs training <eos> through extensive experiments show method produces decent enhancement result drl approach more suitable distort recover training scheme than previous supervised approaches <eos> supplementary material code available sites <eos> com view distort recover <eop> im flow motion hallucination static image action recognition <eos> existing method recognize actions static image take image their face value learning appearances object scenes body poses distinguish each action class <eos> however such models deprived rich dynamic structure motions also define human activity <eos> propose approach hallucinates unobserved future motion implied single snapshot help static image action recognition <eos> key idea learn prior over short term dynamics thousands unlabeled video infer anticipated optical flow novel static image then train discriminative models exploit both streams information <eos> main contributions twofold <eos> first devise encoder decoder convolutional neural network novel optical flow encoding translate static image into accurate flow map <eos> second show power hallucinated flow recognition successfully transferring learned motion into standard two stream network activity recognition <eos> seven datasets demonstrate power approach <eos> only achieves state art accuracy dense optical flow prediction but also consistently enhances recognition actions dynamic scenes <eos> <eop> finding weakly supervised reference aware visual grounding instructional video <eos> grounding textual phrases visual content standalone image sentence pairs challenging task <eos> when consider grounding instructional video problem becomes profoundly more complex latent temporal structure instructional video breaks independence assumptions necessitates contextual understanding resolving ambiguous visual linguistic cues <eos> furthermore dense annotations video data scale mean supervised approaches prohibitively costly <eos> work propose tackle new task weakly supervised framework reference aware visual grounding instructional video only temporal alignment between transcription video segment available supervision <eos> introduce visually grounded action graph structured representation capturing latent dependency between grounding references video <eos> optimization propose new reference aware multiple instance learning ra mil objective weak supervision grounding video <eos> evaluate approach over unconstrained video youcookii robowatch augmented new reference grounding test set annotations <eos> demonstrate jointly optimized reference aware approach simultaneously improves visual grounding reference resolution generalization unseen instructional video categories <eos> <eop> actor action video segmentation sentence <eos> paper strives pixel level segmentation actors their actions video content <eos> different existing works all learn segment fixed vocabulary actor action pairs infer segmentation natural language input sentence <eos> allows distinguish between fine grained actors same super category identify actor action instances segment pairs outside actor action vocabulary <eos> propose fully convolutional model pixel level actor action segmentation using encoder decoder architecture optimized video <eos> show potential actor action video segmentation sentence extend two popular actor action datasets more than natural language descriptions <eos> experiments demonstrate quality sentence guided segmentations generalization ability model its advantage traditional actor action segmentation compared state art <eos> <eop> egocentric activity recognition budget <eos> recent advances embedded technology enabled more pervasive machine learning <eos> one common applications field egocentric activity recognition ear users wearing device such smartphone smartglasses able receive feedback embedded device <eos> recent research activity recognition mainly focused improving accuracy using resource intensive techniques such multi stream deep network <eos> although approach provided state art result most cases neglects natural resource constraints <eos> battery wearable devices <eos> develop reinforcement learning model free method learn energy aware policies maximize use low energy cost predictors while keeping competitive accuracy levels <eos> result show policy trained egocentric dataset able use synergy between motion sensors vision effectively tradeoff energy expenditure accuracy smartglasses operating realistic real world conditions <eos> <eop> cnn mrf video object segmentation via inference cnn based higher order spatio temporal mrf <eos> paper addresses problem video object segmentation initial object mask given first frame input video <eos> propose novel spatio temporal markov random field mrf model defined over pixels handle problem <eos> unlike conventional mrf models spatial dependencies among pixels model encoded convolutional neural network cnn <eos> specifically given object probability labeling set spatially neighboring pixels predicted cnn trained specific object <eos> result higher order richer dependencies among pixels set implicitly modeled cnn <eos> temporal dependencies established optical flow resulting mrf model combines both spatial temporal cues tackling video object segmentation <eos> however performing inference mrf model very difficult due very high order dependencies <eos> end propose novel cnn embedded algorithm perform approximate inference mrf <eos> algorithm proceeds alternating between temporal fusion step feed forward cnn step <eos> when initialized appearance based one shot segmentation cnn model outperforms winning entries davis challenge without resorting model ensembling any dedicated detectors <eos> <eop> action set weakly supervised action segmentation without ordering constraints <eos> action detection temporal segmentation actions video topics increasing interest <eos> while fully supervised systems gained much attention lately full annotation each action within video costly impractical large amounts video data <eos> thus weakly supervised action detection temporal segmentation method great importance <eos> while most works area assume ordered sequence occurring actions given approach only uses set actions <eos> such action set provide much less supervision since neither action ordering nor number action occurrences known <eos> exchange they easily obtained instance meta tags while ordered sequences still require human annotation <eos> introduce system automatically learns temporally segment label actions video only supervision used action set <eos> evaluation three datasets shows method still achieves good result although amount supervision significantly smaller than other related method <eos> <eop> low latency video semantic segmentation <eos> recent years seen remarkable progress semantic segmentation <eos> yet remains challenging task apply segmentation techniques video based applications <eos> specifically high throughput video streams sheer cost running fully convolutional network together low latency requirements many real world applications <eos> autonomous driving present significant challenge design video segmentation framework <eos> tackle combined challenge develop framework video semantic segmentation incorporates two novel components feature propagation module adaptively fuses feature over time via spatially variant convolution thus reducing cost per frame computation adaptive scheduler dynamically allocate computation based accuracy prediction <eos> both components work together ensure low latency while maintaining high segmentation quality <eos> both cityscapes camvid proposed framework obtained competitive performance compared state art while substantially reducing latency ms ms <eos> <eop> fine grained video captioning sports narrative <eos> despite recent emergence video caption method how generate fine grained video descriptions <eos> long detailed commentary about individual movements multiple subjects well their frequent interactions far being solved however great applications such automatic sports narrative <eos> end work makes following contributions <eos> first facilitate novel research fine grained video caption collected novel dataset called fine grained sports narrative dataset fsn contains sports video ground truth narratives youtube <eos> second develop novel performance evaluation metric named fine grained captioning evaluation fce cope novel task <eos> considered extension widely used meteor measures only linguistic performance but also whether action details their temporal orders correctly described <eos> third propose new framework fine grained sports narrative task <eos> network feature three branches spatio temporal entity localization role discovering sub network fine grained action modeling sub network local skeleton motion description group relationship modeling sub network model interactions between players <eos> further fuse feature decode them into long narratives hierarchically recurrent structure <eos> extensive experiments fsn dataset demonstrates validity proposed framework fine grained video caption <eos> <eop> end end learning motion representation video understanding <eos> despite recent success end end learned representations hand crafted optical flow feature still widely used video analysis tasks <eos> fill gap propose tvnet novel end end trainable neural network learn optical flow like feature data <eos> tvnet subsumes specific optical flow solver tv method initialized unfolding its optimization iterations neural layer <eos> tvnet therefore used directly without any extra learning <eos> moreover naturally concatenated other task specific network formulate end end architecture thus making method more efficient than current multi stage approaches avoiding need pre compute store feature disk <eos> finally parameters tvnet further fine tuned end end training <eos> enables tvnet learn richer task specific patterns beyond exact optical flow <eos> extensive experiments two action recognition benchmarks verify effectiveness proposed approach <eos> tvnet achieves better accuracies than all compared method while being competitive fastest counterpart terms feature extraction time <eos> <eop> compressed video action recognition <eos> training robust deep video representations proven much more challenging than learning deep image representations <eos> part due enormous size raw video streams high temporal redundancy true interesting signal often drowned too much irrelevant data <eos> motivated superfluous information reduced up two orders magnitude video compression using <eos> propose train deep network directly compressed video <eos> representation higher information density found training easier <eos> addition signals compressed video provide free albeit noisy motion information <eos> propose novel techniques use them effectively <eos> times faster than res <eos> times faster than resnet <eos> task action recognition approach outperforms all other method ucf hmdb charades dataset <eos> <eop> feature multi target multi camera tracking re identification <eos> multi target multi camera tracking mtmct tracks many people through video taken several cameras <eos> person re identification re id retrieves gallery image people similar person query image <eos> learn good feature both mtmct re id convolutional neural network <eos> contributions include adaptive weighted triplet loss training new technique hard identity mining <eos> method outperforms state art both dukemtmc benchmarks tracking market dukemtmc reid benchmarks re id <eos> examine correlation between good re id good mtmct scores perform ablation studies elucidate contributions main components system <eos> <eop> ava video dataset spatio temporally localized atomic visual actions <eos> paper introduces video dataset spatio temporally localized atomic visual actions ava <eos> ava dataset densely annotates atomic visual actions minute video clips actions localized space time resulting <eos> action labels multiple labels per person occurring frequently <eos> key characteristics dataset definition atomic visual actions rather than composite actions precise spatio temporal annotations possibly multiple annotations each person exhaustive annotation atomic actions over minute video clips people temporally linked across consecutive segments using movies gather varied set action representations <eos> departs existing datasets spatio temporal action recognition typically provide sparse annotations composite actions short video clips <eos> ava its realistic scene action complexity exposes intrinsic difficulty action recognition <eos> benchmark present novel approach action localization builds upon current state art method demonstrates better performance jhmdb ucf categories <eos> while setting new state art existing datasets overall result ava low <eos> map underscoring need developing new approaches video understanding <eos> <eop> who better who best pairwise deep ranking skill determination <eos> paper presents method assessing skill video applicable variety tasks ranging surgery drawing rolling pizza dough <eos> formulate problem pairwise who better overall who best ranking video collections using supervised deep ranking <eos> propose novel loss function learns discriminative feature when pair video exhibit variance skill learns shared feature when pair video exhibit comparable skill levels <eos> result demonstrate method applicable across tasks percentage correctly ordered pairs video ranging four datasets <eos> demonstrate robustness approach via sensitivity analysis its parameters <eos> see work effort toward automated organization how video collections overall generic skill determination video <eos> <eop> mx lstm mixing tracklets vislets jointly forecast trajectories head poses <eos> recent approaches trajectory forecasting use tracklets predict future positions pedestrians exploiting long short term memory lstm architectures <eos> paper shows adding vislets short sequences head pose estimations allows increase significantly trajectory forecasting performance <eos> then propose use vislets novel framework called mx lstm capturing interplay between tracklets vislets thanks joint unconstrained optimization full covariance matrices during lstm backpropagation <eos> same time mx lstm predicts future head poses increasing standard capabilities long term trajectory forecasting approaches <eos> standard head pose estimators attentional based social pooling mixing lstm scores new trajectory forecasting state art all considered datasets zara zara ucy towncentre dramatic margin when pedestrians slow down case most forecasting approaches struggle provide accurate solution <eos> <eop> bottom up top down attention image captioning visual question answering <eos> top down visual attention mechanisms used extensively image captioning visual question answering vqa enable deeper image understanding through fine grained analysis even multiple steps reasoning <eos> work propose combined bottom up top down attention mechanism enables attention calculated level object other salient image region <eos> natural basis attention considered <eos> within approach bottom up mechanism based faster cnn proposes image region each associated feature vector while top down mechanism determines feature weightings <eos> applying approach image captioning result mscoco test server establish new state art task achieving cider spice bleu scores <eos> demonstrating broad applicability method applying same approach vqa obtain first place vqa challenge <eos> <eop> improved fusion visual language representations dense symmetric co attention visual question answering <eos> key solution visual question answering vqa exists how fuse visual language feature extracted input image question <eos> show attention mechanism enables dense bi directional interactions between two modalities contributes boost accuracy prediction answers <eos> specifically present simple architecture fully symmetric between visual language representations each question word attends image region each image region attends question words <eos> stacked form hierarchy multi step interactions between image question pair <eos> show through experiments proposed architecture achieves new state art vqa vqa <eos> despite its small size <eos> also present qualitative evaluation demonstrating how proposed attention mechanism generate reasonable attention maps image questions leads correct answer prediction <eos> <eop> flipdial generative model two way visual dialogue <eos> present flipdial generative model visual dialogue simultaneously plays role both participants visually grounded dialogue <eos> given context form image associated caption summarising contents image flipdial learns both answer questions put forward questions capable generating entire sequences dialogue question answer pairs diverse relevant image <eos> flipdial relies simple but surprisingly powerful idea uses convolutional neural network cnn encode entire dialogues directly implicitly capturing dialogue context conditional vaes learn generative model <eos> flipdial outperforms state art model sequential answering task vd visdial dataset point mean rank using generated answers <eos> first extend paradigm full two way visual dialogue vd model capable generating both questions answers sequence based visual input propose set novel evaluation measures metrics <eos> <eop> you talking me reasoned visual dialog generation through adversarial learning <eos> visual dialogue task requires agent engage conversation about image human <eos> represents extension visual question answering task agent needs answer question about image but needs so light previous dialogue taken place <eos> key challenge visual dialogue thus maintaining consistent natural dialogue while continuing answer questions correctly <eos> present novel approach combines reinforcement learning generative adversarial network gans generate more human like responses questions <eos> gan helps overcome relative paucity training data tendency typical mle based approach generate overly terse answers <eos> critically gan tightly integrated into attention mechanism generates human interpretable reasons each answer <eos> means discriminative model gan task assessing whether candidate answer generated human given provided reason <eos> significant because drives generative model produce high quality answers well supported associated reasoning <eos> method also generates state art result primary benchmark <eos> <eop> visual question generation dual task visual question answering <eos> visual question answering vqa visual question generation vqg two trending topics computer vision but they usually explored separately despite their intrinsic complementary relationship <eos> paper propose end end unified model invertible question answering network iqan introduce question generation dual task question answering improve vqa performance <eos> proposed invertible bilinear fusion module parameter sharing scheme iqan accomplish vqa its dual task vqg simultaneously <eos> jointly trained two tasks proposed dual regularizers termed dual training model better understanding interactions among image questions answers <eos> after training iqan take either question answer input output counterpart <eos> evaluated clevr vqa datasets iqan improves top accuracy prior art mutan vqa method <eos> also show proposed dual training framework consistently improve model performances many popular vqa architectures <eos> <eop> unsupervised textual grounding linking words image concepts <eos> textual grounding <eos> linking words object image challenging but important task robotics human computer interaction <eos> existing techniques benefit recent progress deep learning generally formulate task supervised learning problem selecting bounding box set possible options <eos> train deep net based approaches access large scale datasets required however constructing such dataset time consuming expensive <eos> therefore develop completely unsupervised mechanism textual grounding using hypothesis testing mechanism link words detected image concepts <eos> demonstrate approach referit game dataset flickr data outperforming baselines <eos> <eop> focal visual text attention visual question answering <eos> recent insights language vision neural network successfully applied simple single image visual question answering <eos> however tackle real life question answering problems multimedia collections such personal photos look whole collections sequences photos video <eos> when answering questions large collection natural problem identify snippets support answer <eos> paper describe novel neural network called focal visual text attention network fvta collective reasoning visual question answering both visual text sequence information such image text metadata presented <eos> fvta introduces end end approach makes use hierarchical process dynamically determine media time focus sequential data answer question <eos> fvta only answer questions well but also provides justifications system result based upon get answers <eos> fvta achieves state art performance memexqa dataset competitive result movieqa dataset <eos> <eop> segan segmenting generating invisible <eos> object often occlude each other scenes inferring their appearance beyond their visible parts plays important role scene understanding depth estimation object interaction manipulation <eos> paper study challenging problem completing appearance occluded object <eos> doing so requires knowing pixels paint segmenting invisible parts object color paint them generating invisible parts <eos> proposed novel solution segan jointly optimizes both segmentation generation invisible parts object <eos> experimental result show segan learn generate appearance occluded parts object segan outperforms state art segmentation baselines invisible parts object trained synthetic photo realistic image segan reliably segment natural image reasoning about occluder occludee relations method infer depth layering <eos> <eop> cascade cnn delving into high quality object detection <eos> object detection intersection over union iou threshold required define positives negatives <eos> object detector trained low iou threshold <eos> usually produces noisy detections <eos> however detection performance tends degrade increasing iou thresholds <eos> two main factors responsible overfitting during training due exponentially vanishing positive sample inference time mismatch between ious detector optimal input hypotheses <eos> multi stage object detection architecture cascade cnn proposed address problems <eos> consists sequence detectors trained increasing iou thresholds sequentially more selective against close false positives <eos> detectors trained stage stage leveraging observation output detector good distribution training next higher quality detector <eos> resampling progressively improved hypotheses guarantees all detectors positive set examples equivalent size reducing overfitting problem <eos> same cascade procedure applied inference enabling closer match between hypotheses detector quality each stage <eos> simple implementation cascade cnn shown surpass all single model object detectors challenging coco dataset <eos> experiments also show cascade cnn widely applicable across detector architectures achieving consistent gains independently baseline detector strength <eos> code available github <eos> com zhaoweicai cascade rcnn <eos> <eop> learning semantic concepts order image sentence matching <eos> image sentence matching made great progress recently but remains challenging due large visual semantic discrepancy <eos> mainly arises representation pixel level image usually lacks high level semantic information its matched sentence <eos> work propose semantic enhanced image sentence matching model improve image representation learning semantic concepts then organizing them correct semantic order <eos> given image first use multi regional multi label cnn predict its semantic concepts including object properties actions etc <eos> then considering different orders semantic concepts lead diverse semantic meanings use context gated sentence generation scheme semantic order learning <eos> simultaneously uses image global context containing concept relations reference groundtruth semantic order matched sentence supervision <eos> after obtaining improved image representation learn sentence representation conventional lstm then jointly perform image sentence matching sentence generation model learning <eos> extensive experiments demonstrate effectiveness learned semantic concepts order achieving state art result two public benchmark datasets <eos> <eop> functional map world <eos> present new dataset functional map world fmow aims inspire development machine learning models capable predicting functional purpose buildings land use temporal sequences satellite image rich set metadata feature <eos> metadata provided each image enables reasoning about location time sun angles physical sizes other feature when making predictions about object image <eos> dataset consists over million image over countries <eos> each image provide least one bounding box annotation containing one categories including false detection category <eos> present analysis dataset along baseline approaches reason about metadata temporal views <eos> data code pretrained models made publicly available <eos> <eop> megdet large mini batch object detector <eos> development object detection era deep learning cnn fast faster cnn recent mask cnn retinanet mainly come novel network new framework loss design <eos> how ever mini batch size key factor training deep neural network well studied object detec tion <eos> paper propose large mini batch object detector megdet enable training large mini batch size up so effectively utilize most gpus significantly shorten training time <eos> technically suggest warmup learning rate policy cross gpu batch normalization together allow successfully train large mini batch detector much less time <eos> hours hours achieve even better accuracy <eos> megdet backbone sub mission mmap <eos> coco challenge won st place detection task <eos> <eop> learning globally optimized object detector via policy gradient <eos> paper propose simple yet effective method learn globally optimized detector object detection simple modification standard cross entropy gradient inspired reinforce algorithm <eos> approach cross entropy gradient adaptively adjusted according overall mean average precision map current state each detection candidate leads more effective gradient global optimization detection result brings no computational overhead <eos> benefiting more precise gradients produced global optimization method framework significantly improves state art object detectors <eos> furthermore since method based scores bounding boxes without modification architecture object detector easily applied off shelf modern object detection frameworks <eos> <eop> photographic text image synthesis hierarchically nested adversarial network <eos> paper presents novel method deal challenging task generating photographic image conditioned semantic image descriptions <eos> method introduces accompanying hierarchical nested adversarial objectives inside network hierarchies regularize mid level representations assist generator training capture complex image statistics <eos> present extensile single stream generator architecture better adapt jointed discriminators push generated image up high resolutions <eos> adopt multi purpose adversarial loss encourage more effective image text information usage order improve semantic consistency image fidelity simultaneously <eos> furthermore introduce new visual semantic similarity measure evaluate semantic consistency generated image <eos> extensive experimental validation three public datasets method significantly improves previous state arts all datasets over different evaluation metrics <eos> <eop> illuminant spectra based source separation using flash photography <eos> real world lighting often consists multiple illuminants different spectra <eos> separating manipulating illuminants post process challenging problem requires either significant manual input calibrated scene geometry lighting <eos> work leverage flash no flash image pair analyze edit scene illuminants based their spectral differences <eos> derive novel physics based relationship between color variations observed flash no flash intensities spectra surface shading corresponding individual scene illuminants <eos> technique uses constraint automatically separate image into constituent image lit each illuminant <eos> separation used support applications like white balancing lighting editing rgb photometric stereo demonstrate result outperform state art techniques wide range image <eos> <eop> trapping light time flight <eos> propose novel imaging method near complete surround three dimensional reconstruction geometrically complex object single shot <eos> key idea augment time flight tof based three dimensional sensor multi mirror system called light trap <eos> shape trap chosen so light rays entering bounce multiple times inside trap thereby visiting every position inside trap multiple times various directions <eos> show via simulations enables light rays reach more than <eos> surface object placed inside trap even strong occlusions example lattice shaped object <eos> tof sensor provides path length each light ray along known shape trap used reconstruct complete paths all rays <eos> enables performing dense surround three dimensional reconstructions object highly complex three dimensional shapes single shot <eos> developed proof concept hardware prototype consisting pulsed tof sensor light trap built planar mirrors <eos> demonstrate effectiveness light trap based three dimensional reconstruction method variety object broad range geometry reflectance properties <eos> <eop> perception distortion tradeoff <eos> image restoration algorithms typically evaluated some distortion measure <eos> psnr ssim ifc vif human opinion scores quantify perceived perceptual quality <eos> paper prove mathematically distortion perceptual quality odds each other <eos> specifically study optimal probability correctly discriminating outputs image restoration algorithm real image <eos> show mean distortion decreases probability must increase indicating worse perceptual quality <eos> opposed common belief result holds true any distortion measure only problem psnr ssim criteria <eos> however show experimentally some measures less severe <eos> distance between vgg feature <eos> also show generative adversarial nets gans provide principled way approach perception distortion bound <eos> constitutes theoretical support their observed success low level vision tasks <eos> based analysis propose new methodology evaluating image restoration method use perform extensive comparison between recent super resolution algorithms <eos> <eop> label denoising adversarial network ldan inverse lighting faces <eos> lighting estimation faces important task applications many areas such image editing intrinsic image decomposition image forgery detection <eos> propose train deep convolutional neural network cnn regress lighting parameters single face image <eos> lacking massive ground truth lighting labels face image wild use existing method estimate lighting parameters treated ground truth noise <eos> alleviate effect such noise utilize idea generative adversarial network gan propose label denoising adversarial network ldan <eos> ldan makes use synthetic data accurate ground truth help train deep cnn lighting regression real face image <eos> experiments show network outperforms existing method producing consistent lighting parameters different faces under similar lighting conditions <eos> further evaluate proposed method also apply regress object key point ground truth labels available <eos> experiments demonstrate its effectiveness application <eos> <eop> optimal structured light la carte <eos> consider problem automatically generating sequences structured light patterns active stereo triangulation static scene <eos> unlike existing approaches use predetermined patterns reconstruction algorithms tied them generate patterns fly response generic specifications number patterns projector camera arrangement workspace constraints spatial frequency content etc <eos> pattern sequences specifically optimized minimize expected rate correspondence errors under specifications unknown scene coupled sequence independent algorithm per pixel disparity estimation <eos> achieve derive objective function easy optimize follows first principles within maximum likelihood framework <eos> minimizing demonstrate automatic discovery pattern sequences under three minutes laptop outperform state art triangulation techniques <eos> <eop> tracking multiple object outside line sight using speckle imaging <eos> paper presents techniques tracking non line sight nlos object using speckle imaging <eos> develop novel speckle formation motion model both sensor source view object only indirectly via diffuse wall <eos> show nlos imaging scenario analogous direct los imaging wall acting virtual bare lens less sensor <eos> enables tracking single rigidly moving nlos object using existing speckle based motion estimation techniques <eos> however when imaging multiple nlos object speckle components due different object superimposed virtual bare sensor image cannot analyzed separately recovering motion individual object <eos> develop novel clustering algorithm based statistical geometrical properties speckle image enables identifying motion trajectories multiple independently moving nlos object <eos> demonstrate first time tracking individual trajectories multiple object around corner extreme precision microns using only off shelf imaging components <eos> <eop> inferring light fields shadows <eos> present method inferring light field hidden scene shadows cast known occluder diffuse wall <eos> determining how light naturally reflected off surfaces hidden scene interacts occluder <eos> modeling light transport linear system incorporating prior knowledge about light field structures invert system recover hidden scene <eos> demonstrate result inference method across simulations experiments different types occluders <eos> instance using shadow cast real house plant able recover low resolution light fields different levels texture parallax complexity <eos> provide two experimental result human subject two planar elements different depths <eos> <eop> modifying non local variations across multiple views <eos> present algorithm modifying small non local variations between repeating structures patterns multiple image same scene <eos> modification consistent across views even though image could photographed different view point under different lighting conditions <eos> show when modifying each image independently correspondence between them breaks geometric structure scene gets distorted <eos> approach modifies views while maintaining correspondence hence succeed modifying appearance structure variations consistently <eos> demonstrate method number challenging examples photographed different lighting scales view point <eos> <eop> sfsnet learning shape reflectance illuminance faces wild <eos> present sfsnet end end learning framework producing accurate decomposition unconstrained human face image into shape reflectance illuminance <eos> sfsnet designed reflect physical lambertian rendering model <eos> sfsnet learns mixture labeled synthetic unlabeled real world image <eos> allows network capture low frequency variations synthetic high frequency details real image through photometric reconstruction loss <eos> sfsnet consists new decomposition architecture residual blocks learns complete separation albedo normal <eos> used along original image predict lighting <eos> sfsnet produces significantly better quantitative qualitative result than state art method inverse rendering independent normal illumination estimation <eos> <eop> deep photo enhancer unpaired learning image enhancement photographs gans <eos> paper proposes unpaired learning method image enhancement <eos> given set photographs desired characteristics proposed method learns photo enhancer transforms input image into enhanced image characteristics <eos> method based framework two way generative adversarial network gans several improvements <eos> first augment net global feature show more effective <eos> global net acts generator gan model <eos> second improve wasserstein gan wgan adaptive weighting scheme <eos> scheme training converges faster better less sensitive parameters than wgan gp <eos> finally propose use individual batch normalization layer generators two way gans <eos> helps generators better adapt their own input distributions <eos> all together they significantly improve stability gan training application <eos> both quantitative visual result show proposed method effective enhancing image <eos> <eop> lime live intrinsic material estimation <eos> present first end end approach real time material estimation general object shapes uniform material only requires single color image input <eos> addition lambertian surface properties approach fully automatically computes specular albedo material shininess foreground segmentation <eos> tackle challenging ill posed inverse rendering problem using recent advances image image translation techniques based deep convolutional encoder decoder architectures <eos> underlying core representations approach specular shading diffuse shading mirror image allow learn effective accurate separation diffuse specular albedo <eos> addition propose novel highly efficient perceptual rendering loss mimics real world image formation obtains intermediate result even during run time <eos> estimation material parameters real time frame rates enables exciting mixed reality applications such seamless illumination consistent integration virtual object into realworld scenes virtual material cloning <eos> demonstrate approach live setup compare state art demonstrate its effectiveness through quantitative qualitative evaluation <eos> <eop> learning detect feature texture image <eos> local feature detection fundamental task computer vision hand crafted feature detectors such sift shown success applications including image based localization registration <eos> recent work used feature detected texture image precise global localization but limited performance existing feature detectors textures opposed natural image <eos> propose effective scalable method learning feature detectors textures combines existing ranking loss efficient fully convolutional architecture well new training loss term maximizes peakedness response map <eos> demonstrate detector more repeatable than existing method leading improvements real world texture based localization application <eos> <eop> learning extract video sequence single motion blurred image <eos> present method extract video sequence single motion blurred image <eos> motion blurred image result averaging process instant frames accumulated over time during exposure sensor <eos> unfortunately reversing process nontrivial <eos> firstly averaging destroys temporal ordering frames <eos> secondly recovery single frame blind deconvolution task highly ill posed <eos> present deep learning scheme gradually reconstructs temporal ordering sequentially extracting pairs frames <eos> main contribution introduce loss functions invariant temporal order <eos> lets neural network choose during training frame output among possible combinations <eos> also address ill posedness deblurring designing network large receptive field implemented via resampling achieve higher computational efficiency <eos> proposed method successfully retrieve sharp image sequences single motion blurred image generalize well synthetic real datasets captured different cameras <eos> <eop> lose views limited angle ct reconstruction via implicit sinogram completion <eos> computed tomography ct reconstruction fundamental component wide variety applications ranging security healthcare <eos> classical techniques require measuring projections called sinograms full degree view object <eos> however obtaining full view always feasible such when scanning irregular object limit flexibility scanner rotation <eos> resulting limited angle sinograms known produce highly artifact laden reconstructions existing techniques <eos> paper propose address problem using ctnet system convolutional neural network operates directly limited angle sinogram predict reconstruction <eos> use ray transform prediction obtain completed sinogram if came full degree view <eos> feed standard analytical iterative reconstruction techniques obtain final reconstruction <eos> show extensive experimentation challenging real world dataset combined strategy outperforms many competitive baselines <eos> also propose measure confidence reconstruction enables practitioner gauge reliability prediction made ctnet <eos> show measure strong indicator quality measured psnr while requiring ground truth test time <eos> finally using segmentation experiment show reconstruction also preserves three dimensional structure object better than existing solutions <eos> <eop> common framework interactive texture transfer <eos> paper present general purpose solution interactive texture transfer problems better preserves both local structure visual richness <eos> challenging due diversity tasks simplicity required user guidance <eos> core idea common framework use multiple custom channels dynamically guide synthesis process <eos> interactivity users control spatial distribution stylized textures via semantic channels <eos> structure guidance acquired two stages automatic extraction propagation structure information provides prior initialization preserves salient structure searching nearest neighbor fields nnf structure coherence <eos> meanwhile texture coherence also exploited maintain similar style source image <eos> addition leverage improved patchmatch extended nnf matrix operations obtain transformable source patches richer geometric information high speed <eos> demonstrate effectiveness superiority method variety scenes through extensive comparisons state art algorithms <eos> <eop> amnet memorability estimation attention <eos> paper present design evaluation end end trainable deep neural network visual attention mechanism memorability estimation still image <eos> analyze suitability transfer learning deep models image classification memorability task <eos> further study impact attention mechanism memorability estimation evaluate network sun memorability lamem dataset only large dataset memorability labels date <eos> network outperforms existing state art models both lamem sun datasets term spearman rank correlation well mean squared error approaching human consistency <eos> <eop> blind predicting similar quality map image quality assessment <eos> key problem blind image quality assessment biqa how effectively model properties human visual system data driven manner <eos> paper propose simple efficient biqa model based novel framework consists fully convolutional neural network fcnn pooling network solve problem <eos> principle fcnn capable predicting pixel pixel similar quality map only distorted image using intermediate similarity maps derived conventional full reference image quality assessment method <eos> predicted pixel pixel quality maps good consistency distortion correlations between reference distorted image <eos> finally deep pooling network regresses quality map into score <eos> experiments demonstrated predictions outperform many state art biqa method <eos> <eop> deep end end time flight imaging <eos> present end end image processing framework time flight tof cameras <eos> existing tof image processing pipelines consist sequence operations including modulated exposures denoising phase unwrapping multipath interference correction <eos> while cascaded modular design offers several benefits such closed form solutions power efficient processing also suffers error accumulation information loss each module only observe output its direct predecessor resulting erroneous depth estimates <eos> depart conventional pipeline model propose deep convolutional neural network architecture recovers scene depth directly dual frequency raw tof correlation measurements <eos> train network simulate tof image variety scenes using time resolved renderer devise depth specific losses apply normalization augmentation strategies generalize model real captures <eos> demonstrate proposed network efficiently exploit spatio temporal structures tof frequency measurements validate performance joint multipath removal denoising phase unwrapping method wide range challenging scenes <eos> <eop> aperture supervision monocular depth estimation <eos> present novel method train machine learning algorithms estimate scene depths single image using information provided camera aperture supervision <eos> prior works use depth sensor outputs image same scene alternate viewpoints supervision while method instead uses image same viewpoint taken varying camera aperture <eos> enable learning algorithms use aperture effects supervision introduce two differentiable aperture rendering functions use input image predicted depths simulate depth field effects caused real camera apertures <eos> train monocular depth estimation network end end predict scene depths best explain finite aperture image defocus blurred renderings input all focus image <eos> <eop> seeing temporal modulation lights standard cameras <eos> paper propose novel method measuring temporal modulation lights using off shelf cameras <eos> particular show invisible flicker patterns various lights such fluorescent lights measured simple combination off shelf camera any moving object specular reflection <eos> unlike existing method need high speed cameras nor specially designed coded exposure cameras <eos> based extracted flicker patterns environment lights also propose efficient method deblurring motion blurs image <eos> proposed method enables deblur image better frequency characteristics induced flicker patterns environment lights <eos> real image experiments show efficiency proposed method <eos> <eop> statistical tomography microscopic life <eos> achieve tomography three dimensional volumetric natural object each projected image corresponds different specimen <eos> each specimen unknown random three dimensional orientation location scale <eos> imaging scenario relevant microscopic mesoscopic organisms aerosols hydrosols viewed naturally microscope <eos> class scale variation inhibits prior single particle reconstruction method <eos> thus generalize tomographic recovery account all degrees freedom similarity transformation <eos> enables geometric self calibration imaging transparent object <eos> make computational load manageable reach good quality reconstruction short time <eos> enables extraction statistics important scientific study specimen populations specifically size distribution parameters <eos> apply method study plankton <eos> <eop> divide conquer full resolution light field deblurring <eos> increasing popularity computational light field lf cameras necessitated need tackling motion blur ubiquitous phenomenon hand held photography <eos> state art method blind deblurring lfs general three dimensional scenes limited handling only downsampled lf both spatial angular resolution <eos> due computational overhead involved processing data hungry full resolution lf altogether <eos> moreover method warrants high end gpus optimization ineffective wide angle settings irregular camera motion <eos> paper introduce new blind motion deblurring strategy lfs alleviates limitations significantly <eos> model achieves isolating lf motion blur across subaperture image thus paving way independent deblurring subaperture image <eos> furthermore model accommodates common camera motion parameterization across subaperture image <eos> consequently blind deblurring any single subaperture image elegantly paves way cost effective non blind deblurring other subaperture image <eos> approach cpu efficient computationally effectively deblur full resolution lfs <eos> <eop> multispectral image intrinsic decomposition via subspace constraint <eos> multispectral image contain many clues surface characteristics object thus used many computer vision tasks <eos> however due complex geometry structure natural scenes spectra curves same surface look very different under different illuminations different angles <eos> paper new multispectral image intrinsic decomposition model miid presented decompose shading reflectance single multispectral image <eos> extend retinex model proposed rgb image intrinsic decomposition multispectral domain <eos> based subspace constraint introduced both shading reflectance spectral space reduce ill posedness problem make problem solvable <eos> dataset scenes given ground truth shadings reflectance facilitate objective evaluations <eos> experiments demonstrate effectiveness proposed method <eos> <eop> improving color reproduction accuracy cameras <eos> one key operations performed digital camera map sensor specific color space standard perceptual color space <eos> procedure involves application white balance correction followed color space transform <eos> current approach colorimetric mapping based interpolation pre calibrated color space transforms computed two fixed illuminations <eos> two white balance settings <eos> image captured under different illuminations subject less color accuracy due use interpolation process <eos> paper discuss limitations current colorimetric mapping approach propose two method able improve color accuracy <eos> evaluate approach seven different cameras show improvements up dslr cameras mobile phone cameras terms color reproduction error <eos> <eop> closer look spatiotemporal convolutions action recognition <eos> paper discuss several forms spatiotemporal convolutions video analysis study their effects action recognition <eos> motivation stems observation cnn applied individual frames video remained solid performers action recognition <eos> work empirically demonstrate accuracy advantages three dimensional cnn over cnn within framework residual learning <eos> furthermore show factorizing three dimensional convolutional filters into separate spatial temporal components yields significantly gains accuracy <eos> empirical study leads design new spatiotemporal convolutional block produces cnn achieve result comparable superior state art sports kinetics ucf hmdb <eos> <eop> inferring shared attention social scene video <eos> paper addresses new problem inferring shared attention third person social scene video <eos> shared attention phenomenon two more individuals simultaneously look common target social scenes <eos> perceiving identifying shared attention video plays crucial roles social activities social scene understanding <eos> propose spatial temporal neural network detect shared attention intervals video predict shared attention locations frames <eos> each video frame human gaze directions potential target boxes two key feature spatially detecting shared attention social scene <eos> temporal domain convolutional long short term memory network utilizes temporal continuity transition constraints optimize predicted shared attention heatmap <eos> collect new dataset videocoatt public tv show video containing complex video sequences more than frames include diverse social scenes shared attention study <eos> experiments dataset show model effectively infer shared attention video <eos> also empirically verify effectiveness different components model <eos> <eop> making convolutional network recurrent visual sequence learning <eos> recurrent neural network rnns emerged powerful model broad range machine learning problems involve sequential data <eos> while abundance work exists understand improve rnns context language audio signals such language modeling speech recognition relatively little attention paid analyze modify rnns visual sequences nature distinct properties <eos> paper aim bridge gap present first large scale exploration rnns visual sequence learning <eos> particular intention leveraging strong generalization capacity pre trained convolutional neural network cnn propose novel effective approach prernn make pre trained cnn recurrent transforming convolutional layer fully connected layer into recurrent layer <eos> conduct extensive evaluations three representative visual sequence learning tasks sequential face alignment dynamic hand gesture recognition action recognition <eos> experiments reveal prernn consistently outperforms traditional rnns achieves state art result three applications suggesting prernn more suitable visual sequence learning <eos> <eop> real world anomaly detection surveillance video <eos> surveillance video able capture variety realistic anomalies <eos> paper propose learn anomalies exploiting both normal anomalous video <eos> avoid annotating anomalous segments clips training video very time consuming propose learn anomaly through deep multiple instance ranking framework leveraging weakly labeled training video ie training labels anomalous normal video level instead clip level <eos> approach consider normal anomalous video bags video segments instances multiple instance learning mil automatically learn deep anomaly ranking model predicts high anomaly scores anomalous video segments <eos> furthermore introduce sparsity temporal smoothness constraints ranking loss function better localize anomaly during training <eos> also introduce new large scale first its kind dataset hours video <eos> consists long untrimmed real world surveillance video realistic anomalies such fighting road accident burglary robbery etc <eos> well normal activities <eos> dataset used two tasks <eos> first general anomaly detection considering all anomalies one group all normal activities another group <eos> second recognizing each anomalous activities <eos> experimental result show mil method anomaly detection achieves significant improvement anomaly detection performance compared state art approaches <eos> provide result several recent deep learning baselines anomalous activity recognition <eos> low recognition performance baselines reveals dataset very challenging opens more opportunities future work <eos> <eop> viewpoint aware attentive multi view inference vehicle re identification <eos> vehicle re identification re id huge potential contribute intelligent video surveillance <eos> however suffers challenges different vehicle identities similar appearance little inter instance discrepancy while one vehicle usually large intra instance differences under viewpoint illumination variations <eos> previous method address vehicle re id simply using visual feature originally captured views usually exploit spatial temporal information vehicles refine result <eos> paper propose viewpoint aware attentive multi view inference vami model only requires visual information solve multi view vehicle re id problem <eos> given vehicle image arbitrary viewpoints vami extracts single view feature each input image aims transform feature into global multi view feature representation so pairwise distance metric learning better optimized such viewpoint invariant feature space <eos> vami adopts viewpoint aware attention model select core region different viewpoints implement effective multi view feature inference adversarial training architecture <eos> extensive experiments validate effectiveness each proposed component illustrate approach achieves consistent improvements over state art vehicle re id method two public datasets veri vehicleid <eos> <eop> efficient video object segmentation via network modulation <eos> video object segmentation targets segmenting specific object throughout video sequence when given only annotated first frame <eos> recent deep learning based approaches find effective fine tune general purpose segmentation model annotated frame using hundreds iterations gradient descent <eos> despite high accuracy method achieve fine tuning process inefficient fails meet requirements real world applications <eos> propose novel approach uses single forward pass adapt segmentation model appearance specific object <eos> specifically second meta neural network named modulator trained manipulate intermediate layer segmentation network given limited visual spatial information target object <eos> experiments show approach times faster than fine tuning approaches achieves similar accuracy <eos> <eop> weakly supervised action segmentation iterative soft boundary assignment <eos> work address task weakly supervised human action segmentation long untrimmed video <eos> recent method relied expensive learning models such recurrent neural network rnn hidden markov models hmm <eos> however method suffer expensive computational cost thus unable deployed large scale <eos> overcome limitations keys design efficiency scalability <eos> propose novel action modeling framework consists new temporal convolutional network named temporal convolutional feature pyramid network tcfpn predicting frame wise action labels novel training strategy weakly supervised sequence modeling named iterative soft boundary assignment isba align action sequences update network iterative fashion <eos> proposed framework evaluated two benchmark datasets breakfast hollywood extended four different evaluation metrics <eos> extensive experimental result show method achieve competitive superior performance state art method <eos> <eop> depth aware stereo video retargeting <eos> compared traditional video retargeting stereo video retargeting poses new challenges because stereo video contains depth information salient object its time dynamics <eos> work propose depth aware stereo video retargeting method imposing depth fidelity constraint <eos> proposed depth aware retargeting method reconstructs three dimensional scene obtain depth information salient object <eos> cast constrained optimization problem total cost function includes shape temporal depth distortions salient object <eos> result solution preserve shape temporal depth fidelity salient object simultaneously <eos> demonstrated experimental result depth aware retargeting method achieves higher retargeting quality provides better user experience <eos> <eop> instance embedding transfer unsupervised video object segmentation <eos> propose method unsupervised video object segmentation transferring knowledge encapsulated image based instance embedding network <eos> instance embedding network produces embedding vector each pixel enables identifying all pixels belonging same object <eos> though trained static image instance embeddings stable over consecutive video frames allows link object together over time <eos> thus adapt instance network trained static image video object segmentation incorporate embeddings objectness optical flow feature without model retraining online fine tuning <eos> proposed method outperforms state art unsupervised segmentation method davis dataset fbms dataset <eos> <eop> future frame prediction anomaly detection new baseline <eos> anomaly detection video refers identification events conform expected behavior <eos> however almost all existing method tackle problem minimizing reconstruction errors training data cannot guarantee larger reconstruction error abnormal event <eos> paper propose tackle anomaly detection problem within video prediction framework <eos> best knowledge first work leverages difference between predicted future frame its ground truth detect abnormal event <eos> predict future frame higher quality normal events other than commonly used appearance spatial constraints intensity gradient also introduce motion temporal constraint video prediction enforcing optical flow between predicted frames ground truth frames consistent first work introduces temporal constraint into video prediction task <eos> such spatial motion constraints facilitate future frame prediction normal events consequently facilitate identify abnormal events conform expectation <eos> extensive experiments both toy dataset some publicly available datasets validate effectiveness method terms robustness uncertainty normal events sensitivity abnormal events <eos> <eop> spatiotemporal three dimensional cnn retrace history cnn imagenet <eos> purpose study determine whether current video datasets sufficient data training very deep convolutional neural network cnn spatio temporal three dimensional three dimensional kernels <eos> recently performance levels three dimensional cnn field action recognition improved significantly <eos> however date conventional research only explored relatively shallow three dimensional architectures <eos> examine architectures various three dimensional cnn relatively shallow very deep ones current video datasets <eos> based result experiments following conclusions could obtained resnet training resulted significant overfitting ucf hmdb activitynet but kinetics <eos> ii kinetics dataset sufficient data training deep three dimensional cnn enables training up resnets layer interestingly similar resnets imagenet <eos> average accuracy kinetics test set <eos> iii kinetics pretrained simple three dimensional architectures outperforms complex architectures pretrained resnext achieved <eos> ucf hmdb respectively <eos> use cnn trained imagenet produced significant progress various tasks image <eos> believe using deep three dimensional cnn together kinetics will retrace successful history cnn imagenet stimulate advances computer vision video <eos> codes pretrained models used study publicly available <eos> com kenshohara three dimensional resnets pytorch <eop> dynamic video segmentation network <eos> paper present detailed design dynamic video segmentation network dvsnet fast efficient semantic video segmentation <eos> dvsnet consists two convolutional neural network segmentation network flow network <eos> former generates highly accurate semantic segmentations but deeper slower <eos> latter much faster than former but its output requires further processing generate less accurate semantic segmentations <eos> explore use decision network adaptively assign different frame region different network based metric called expected confidence score <eos> frame region higher expected confidence score traverse flow network <eos> frame region lower expected confidence score pass through segmentation network <eos> extensively performed experiments various configurations dvsnet investigated number variants proposed decision network <eos> experimental result show dvsnet able achieve up <eos> fps cityscape dataset <eos> high speed version dvsnet able deliver fps <eos> miou same dataset <eos> dvsnet also able reduce up computational workloads <eos> <eop> recognize actions disentangling components dynamics <eos> despite remarkable progress action recognition over past several years existing method remain limited efficiency effectiveness <eos> method treating appearance motion separate streams usually subject cost optical flow computation while relying three dimensional convolution original video frames often yield inferior performance practice <eos> paper propose new convnet architecture video representation learning derive disentangled components dynamics purely raw video frames without need optical flow estimation <eos> particularly learned representation comprises three components representing static appearance apparent motion appearance changes <eos> introduce three dimensional pooling cost volume processing warped feature differences respectively extracting three components above <eos> modules incorporated three branches unified network share underlying feature learned jointly end end manner <eos> two large datasets ucf kinetics method obtained competitive performances high efficiency using only rgb frame sequence input <eos> <eop> motion appearance co memory network video question answering <eos> video question answering qa important task understanding video temporal structure <eos> observe there three unique attributes video qa compared image qa deals long sequences image containing richer information only quantity but also variety motion appearance information usually correlated each other able provide useful attention cues other different questions require different number frames infer answer <eos> based observations propose motion appearance co memory network video qa <eos> network built concepts dynamic memory network dmn introduces new mechanisms video qa <eos> specifically there three salient aspects co memory attention mechanism utilizes cues both motion appearance generate attention temporal conv deconv network generate multi level contextual facts dynamic fact ensemble method construct temporal representation dynamically different questions <eos> evaluate method tgif qa dataset result outperform state art significantly all four tasks tgif qa <eos> <eop> learning understand image blur <eos> while many approaches proposed estimate remove blur photo few efforts were made algorithm automatically understand blur desirability whether blur desired how affects quality photo <eos> such task only relies low level visual feature identify blurry region but also requires high level understanding image content well user intent during photo capture <eos> paper propose unified framework estimate spatially varying blur map understand its desirability terms image quality same time <eos> particular use dilated fully convolutional neural network pyramid pooling boundary refinement layer generate high quality blur response maps <eos> if blur exists classify its desirability three levels ranging good bad distilling high level semantics learning attention map adaptively localize important content image <eos> whole framework end end jointly trained both supervisions pixel wise blur responses image wise blur desirability levels <eos> considering limitations existing image blur datasets collected new large scale dataset both annotations facilitate training <eos> proposed method extensively evaluated two datasets demonstrate state art performance both tasks <eos> <eop> dense decoder shortcut connections single pass semantic segmentation <eos> propose novel end end trainable deep encoder decoder architecture single pass semantic segmentation <eos> approach based cascaded architecture feature level long range skip connections <eos> encoder incorporates structure resnext residual building blocks adopts strategy repeating building block aggregates set transformations same topology <eos> decoder feature novel architecture consisting blocks capture context information ii generate semantic feature iii enable fusion between different output resolutions <eos> crucially introduce dense decoder shortcut connections allow decoder blocks use semantic feature maps all previous decoder levels <eos> all higher level feature maps <eos> dense decoder connections allow effective information propagation one decoder block another well multi level feature fusion significantly improves accuracy <eos> importantly connections allow method obtain state art performance several challenging datasets without need time consuming multi scale averaging previous works <eos> <eop> generative adversarial image synthesis decision tree latent controller <eos> paper proposes decision tree latent controller generative adversarial network dtlc gan extension gan learn hierarchically interpretable representations without relying detailed supervision <eos> impose hierarchical inclusion structure latent variables incorporate new architecture called dtlc into generator input <eos> dtlc multiple layer tree structure off child node codes controlled parent node codes <eos> using architecture hierarchically obtain latent space lower layer codes selectively used depending higher layer ones <eos> make latent codes capture salient semantic feature image hierarchically disentangled manner dtlc also propose hierarchical conditional mutual information regularization optimize newly defined curriculum learning method propose well <eos> makes possible discover hierarchically interpretable representations layer layer manner basis information gain only using single dtlc gan model <eos> evaluated dtlc gan various datasets <eos> mnist cifar tiny imagenet three dimensional faces celeba confirmed dtlc gan learn hierarchically interpretable representations either unsupervised weakly supervised settings <eos> furthermore applied dtlc gan image retrieval tasks showed its effectiveness representation learning <eos> <eop> learning discriminative prior blind image deblurring <eos> present effective blind image deblurring method based data driven discriminative prior <eos> work motivated fact good image prior should favor clear image over blurred image <eos> obtain such image prior deblurring formulate image prior binary classifier achieved deep convolutional neural network cnn <eos> learned image prior significant discriminative property able distinguish whether image clear <eos> embedded into maximum posterior map framework helps blind deblurring various scenarios including natural face text low illumination image <eos> however difficult optimize deblurring method learned image prior involves non linear cnn <eos> therefore develop efficient numerical approach based half quadratic splitting method gradient decent algorithm solve proposed model <eos> furthermore proposed model easily extended non uniform deblurring <eos> both qualitative quantitative experimental result show method performs favorably against state art algorithms well domain specific image deblurring approaches <eos> <eop> frame recurrent video super resolution <eos> recent advances video super resolution shown convolutional neural network combined motion compensation able merge information multiple low resolution lr frames generate high quality image <eos> current state art method process batch lr frames generate single high resolution hr frame run scheme sliding window fashion over entire video effectively treating problem large number separate multi frame super resolution tasks <eos> approach two main weaknesses each input frame processed warped multiple times increasing computational cost each output frame estimated independently conditioned input frames limiting system ability produce temporally consistent result <eos> work propose end end trainable frame recurrent video super resolution framework uses previously inferred hr estimate super resolve subsequent frame <eos> naturally encourages temporally consistent result reduces computational cost warping only one image each step <eos> furthermore due its recurrent nature proposed method ability assimilate large number previous frames without increased computational demands <eos> extensive evaluations comparisons previous method validate strengths approach demonstrate proposed framework able significantly outperform current state art <eos> <eop> discovering point lights intensity distance fields <eos> introduce light localization problem <eos> scene illuminated set unobserved isotropic point lights <eos> given geometry materials illuminated appearance scene light localization problem completely recover number positions intensities lights <eos> first present scene transform identifies likely light positions <eos> based transform develop iterative algorithm locate remaining lights determine all light intensities <eos> demonstrate success method large set synthetic scenes show extends three dimensional both synthetic scenes real world scenes <eos> <eop> video rain streak removal multiscale convolutional sparse coding <eos> video captured outdoor surveillance equipments sometimes contain unexpected rain streaks brings difficulty subsequent video processing tasks <eos> rain streak removal video thus important topic recent computer vision research <eos> paper raise two intrinsic characteristics specifically possessed rain streaks <eos> firstly rain streaks video contain repetitive local patterns sparsely scattered over different positions video <eos> secondly rain streaks multiscale configurations due their occurrence positions different distances cameras <eos> based such understanding specifically formulate both characteristics into multiscale convolutional sparse coding ms csc model video rain streak removal task <eos> specifically use multiple convolutional filters convolved sparse feature maps deliver former characteristic further use multiscale filters represent different scales rain streaks <eos> such new encoding manner makes proposed method capable properly extracting rain streaks video thus getting fine video deraining effects <eos> experiments implemented synthetic real video verify superiority proposed method compared state art ones along research line both visually quantitatively <eos> <eop> stereoscopic neural style transfer <eos> paper presents first attempt stereoscopic neural style transfer responds emerging demand three dimensional movies ar vr <eos> start careful examination applying existing monocular style transfer method left right views stereoscopic image separately <eos> reveals original disparity consistency cannot well preserved final stylization result causes three dimensional fatigue viewers <eos> address issue incorporate new disparity loss into widely adopted style loss function enforcing bidirectional disparity constraint non occluded region <eos> practical real time solution propose first feed forward network jointly training stylization sub network disparity sub network integrate them feature level middle domain <eos> disparity sub network also first end end network simultaneous bidirectional disparity occlusion mask estimation <eos> finally network effectively extended stereoscopic video considering both temporal coherence disparity consistency <eos> will show proposed method clearly outperforms baseline algorithms both quantitatively qualitatively <eos> <eop> multi frame quality enhancement compressed video <eos> past few years witnessed great success applying deep learning enhance quality compressed image video <eos> existing approaches mainly focus enhancing quality single frame ignoring similarity between consecutive frames <eos> paper investigate heavy quality fluctuation exists across compressed video frames thus low quality frames enhanced using neighboring high quality frames seen multi frame quality enhancement mfqe <eos> accordingly paper proposes mfqe approach compressed video first attempt direction <eos> approach firstly develop support vector machine svm based detector locate peak quality frames pqfs compressed video <eos> then novel multi frame convolutional neural network mf cnn designed enhance quality compressed video non pqf its nearest two pqfs input <eos> mf cnn compensates motion between non pqf pqfs through motion compensation subnet mc subnet <eos> subsequently quality enhancement subnet qe subnet reduces compression artifacts non pqf help its nearest pqfs <eos> finally experiments validate effectiveness generality mfqe approach advancing state art quality enhancement compressed video <eos> code mfqe approach available github <eos> com ryangbuaa mfqe <eos> <eop> cnn based learning using reflection retinex models intrinsic image decomposition <eos> most traditional work intrinsic image decomposition rely deriving priors about scene characteristics <eos> other hand recent research use deep learning models out black box consider well established traditional image formation process basis their intrinsic learning process <eos> consequence although current deep learning approaches show superior performance when considering quantitative benchmark result traditional approaches still dominant achieving high qualitative result <eos> paper aim exploit best two worlds <eos> method proposed empowered deep learning capabilities considers physics based reflection model steer learning process exploits traditional approach obtain intrinsic image exploiting reflectance shading gradient information <eos> proposed model fast compute allows integration all intrinsic components <eos> train new model object centered large scale datasets intrinsic ground truth image created <eos> evaluation result demonstrate new model outperforms existing method <eos> visual inspection shows image formation loss function augments color reproduction use gradient information produces sharper edges <eos> datasets models higher resolution image available ivi <eos> nl cv retinet <eos> <eop> image restoration estimating frequency distribution local patches <eos> paper propose method solve image restoration problem tries restore details corrupted image especially due loss caused jpeg compression <eos> treated image frequency domain explicitly restore frequency components lost during image compression <eos> doing so distribution frequency domain learned using cross entropy loss <eos> unlike recent approaches reconstructed details image without using scheme adversarial training <eos> rather image restoration problem treated classification problem determine frequency coefficient each frequency band image patch <eos> paper show proposed method effectively restores jpeg compressed image more detailed high frequency components making restored image more vivid <eos> <eop> latent ransac <eos> present method evaluate ransac hypothesis constant time <eos> independent size data <eos> key observation here correct hypotheses tightly clustered together latent parameter domain <eos> manner similar generalized hough transform seek find cluster only need few two votes successful detection <eos> rapidly locating such pairs similar hypotheses made possible adapting recent random grids range search technique <eos> only perform usual costly hypothesis verification stage upon discovery close pair hypotheses <eos> show event rarely happens incorrect hypotheses enabling significant speedup ransac pipeline <eos> suggested approach applied tested three robust estimation problems camera localization three dimensional rigid alignment homography estimation <eos> perform rigorous testing both synthetic real datasets demonstrating improvement efficiency without compromise accuracy <eos> furthermore achieve state art three dimensional alignment result challenging redwood loop closure challenge <eos> <eop> two stream convolutional network dynamic texture synthesis <eos> introduce two stream model dynamic texture synthesis <eos> model based pre trained convolutional network convnets target two independent tasks object recognition ii optical flow prediction <eos> given input dynamic texture statistics filter responses object recognition convnet encapsulate per frame appearance input texture while statistics filter responses optical flow convnet model its dynamics <eos> generate novel texture randomly initialized input sequence optimized match feature statistics each stream example texture <eos> inspired recent work image style transfer enabled two stream model also apply synthesis approach combine texture appearance one texture dynamics another generate entirely novel dynamic textures <eos> show approach generates novel high quality sample match both framewise appearance temporal evolution input texture <eos> finally quantitatively evaluate texture synthesis approach thorough user study <eos> <eop> towards open set identity preserving face synthesis <eos> propose framework based generative adversarial network disentangle identity attributes faces such conveniently recombine different identities attributes identity preserving face synthesis open domains <eos> previous identity preserving face synthesis processes largely confined synthesizing faces known identities already training dataset <eos> synthesize face identity outside training dataset framework requires one input image subject produce identity vector any other input face image extract attribute vector capturing <eos> pose emotion illumination even background <eos> then recombine identity vector attribute vector synthesize new face subject extracted attribute <eos> proposed framework need annotate attributes faces any way <eos> trained asymmetric loss function better preserve identity stabilize training process <eos> also effectively leverage large amounts unlabeled training face image further improve fidelity synthesized faces subjects presented labeled training face dataset <eos> experiments demonstrate efficacy proposed framework <eos> also present its usage much broader set applications including face frontalization face attribute morphing face adversarial example detection <eos> <eop> revised underwater image formation model <eos> current underwater image formation model descends atmospheric dehazing equations attenuation weak function wavelength <eos> recently showed model introduces significant errors dependencies estimation direct transmission signal because underwater light attenuates wavelength dependent manner <eos> here show backscattered signal derived current model also suffers dependencies were previously unaccounted <eos> doing so use oceanographic measurements derive physically valid space backscatter further show wideband coefficients govern backscatter different than govern direct transmission even though current model treats them same <eos> propose revised equation underwater image formation takes differences into account validate through situ experiments underwater <eos> revised model might explain frequent instabilities current underwater color reconstruction models calls development new method <eos> <eop> graph cut ransac <eos> novel method robust estimation called graph cut ransac gc ransac short introduced <eos> separate inliers outliers runs graph cut algorithm local optimization lo step applied when so far best model found <eos> proposed lo step conceptually simple easy implement globally optimal efficient <eos> gc ransac shown experimentally both synthesized tests real image pairs more geometrically accurate than state art method range problems <eos> line fitting homography affine transformation fundamental essential matrix estimation <eos> runs real time many problems speed approximately equal less accurate alternatives milliseconds standard cpu <eos> <eop> temporal deformable residual network action segmentation video <eos> paper about temporal segmentation human actions video <eos> introduce new model temporal deformable residual network tdrn aimed analyzing video intervals multiple temporal scales labeling video frames <eos> tdrn computes two parallel temporal streams residual stream analyzes video information its full temporal resolution ii pooling unpooling stream captures long range video information different scales <eos> former facilitates local fine scale action segmentation latter uses multiscale context improving accuracy frame classification <eos> two streams computed set temporal residual modules deformable convolutions fused temporal residuals full video resolution <eos> evaluation university dundee salads georgia tech egocentric activities jhu isi gesture skill assessment working set demonstrates tdrn outperforms state art frame wise segmentation accuracy segmental edit score segmental overlap score <eos> <eop> weakly supervised action localization sparse temporal pooling network <eos> propose weakly supervised temporal action localization algorithm untrimmed video using convolutional neural network <eos> algorithm learns video level class labels predicts temporal intervals human actions no requirement temporal localization annotations <eos> design network identify sparse subset key segments associated target actions video using attention module fuse key segments through adaptive temporal pooling <eos> loss function comprised two terms minimize video level action classification error enforce sparsity segment selection <eos> inference time extract score temporal proposals using temporal class activations class agnostic attentions estimate time intervals correspond target actions <eos> proposed algorithm attains state art result thumos dataset outstanding performance activitynet <eos> even its weak supervision <eos> <eop> poseflow deep motion representation understanding human behaviors video <eos> motion human body critical cue understanding characterizing human behavior video <eos> most existing approaches explore motion cue using optical flows <eos> however optical flow usually contains motion both interested human bodies undesired background <eos> noisy motion representation makes very challenging pose estimation action recognition real scenarios <eos> address issue paper presents novel deep motion representation called poseflow reveals human motion video while suppressing background motion blur being robust occlusion <eos> learning poseflow mild computational cost propose functionally structured spatial temporal deep network poseflow net pfn jointly solve skeleton localization matching problems poseflow <eos> comprehensive experiments show pfn outperforms state art deep flow estimation models generating poseflow <eos> moreover poseflow demonstrates its potential improving two challenging tasks human video analysis pose estimation action recognition <eos> <eop> ffnet video fast forwarding via reinforcement learning <eos> many intelligent applications limited computation communication storage energy resources there imperative need vision method could select informative subset input video efficient processing near real time <eos> literature there two relevant groups approaches generating trailer video fast forwarding while watching processing video <eos> first group supported video summarization techniques require processing entire video select important subset showing users <eos> second group current fast forwarding method depend either manual control automatic adaptation playback speed often present accurate representation may still require processing every frame <eos> paper introduce fastforwardnet ffnet reinforcement learning agent gets inspiration video summarization fast forwarding differently <eos> online framework automatically fast forwards video presents representative subset frames users fly <eos> require processing entire video but just portion selected fast forward agent makes process very computationally efficient <eos> online nature proposed method also enables users begin fast forwarding any point video <eos> experiments two real world datasets demonstrate method provide better representation input video about improvement coverage important frames much less processing requirement more than reduction number frames processed <eos> <eop> multi shot pedestrian re identification via sequential decision making <eos> multi shot pedestrian re identification problem core surveillance video analysis <eos> matches two tracks pedestrians different cameras <eos> contrary existing works aggregate single frames feature time series model such recurrent neural network paper propose interpretable reinforcement learning based approach problem <eos> particularly train agent verify pair image each time <eos> agent could choose output result same different request another pair image verify unsure <eos> way model implicitly learns difficulty image pairs postpone decision when model accumulate enough evidence <eos> moreover adjusting reward unsure action easily trade off between speed accuracy <eos> three open benchmarks method competitive state art method while only using image <eos> promising result demonstrate method favorable both efficiency performance <eos> <eop> attend interact higher order object interactions video understanding <eos> human actions often involve complex interactions across several inter related object scene <eos> however existing approaches fine grained video understanding visual relationship detection often rely single object representation pairwise object relationships <eos> furthermore learning interactions across multiple object hundreds frames video computationally infeasible performance may suffer since large combinatorial space modeled <eos> paper propose efficiently learn higher order interactions between arbitrary subgroups object fine grained video understanding <eos> demonstrate modeling object interactions significantly improves accuracy both action recognition video captioning while saving more than times computation over traditional pairwise relationships <eos> proposed method validated two large scale datasets kinetics activitynet captions <eos> sinet sinet caption achieve state art performances both datasets even though video sampled maximum fps <eos> best knowledge first work modeling object interactions open domain large scale video datasets additionally model higher order object interactions improves performance low computational costs <eos> <eop> why they looking jointly inferring human attention intentions complex tasks <eos> paper addresses new problem jointly inferring human attention intentions tasks video <eos> given rgb video human performs task answer three questions simultaneously human looking attention prediction why human looking there intention prediction task human performing task recognition <eos> propose hierarchical model human attention object hao represents tasks intentions attention under unified framework <eos> task represented sequential intentions transition each other <eos> intention composed human pose attention object <eos> beam search algorithm adopted inference hao graph output attention intention task result <eos> built new video dataset tasks intentions attention <eos> contains task classes intention categories object classes video approximately frames <eos> experiments show approach outperforms existing approaches <eos> <eop> fully convolutional adaptation network semantic segmentation <eos> recent advances deep neural network convincingly demonstrated high capability learning vision models large datasets <eos> nevertheless collecting expert labeled datasets especially pixel level annotations extremely expensive process <eos> appealing alternative render synthetic data <eos> computer games generate ground truth automatically <eos> however simply applying models learnt synthetic image may lead high generalization error real image due domain shift <eos> paper facilitate issue perspectives both visual appearance level representation level domain adaptation <eos> former adapts source domain image appear if drawn style target domain latter attempts learn domain invariant representations <eos> specifically present fully convolutional adaptation network fcan novel deep architecture semantic segmentation combines appearance adaptation network aan representation adaptation network ran <eos> aan learns transformation one domain other pixel space ran optimized adversarial learning manner maximally fool domain discriminator learnt source target representations <eos> extensive experiments conducted transfer gta game video cityscapes urban street scenes semantic segmentation proposal achieves superior result when comparing state art unsupervised adaptation techniques <eos> more remarkably obtain new record miou <eos> bdds drive cam video unsupervised setting <eos> <eop> semantic video segmentation gated recurrent flow propagation <eos> semantic video segmentation challenging due sheer amount data needs processed labeled order construct accurate models <eos> paper present deep end end trainable methodology video segmentation capable leveraging information present unlabeled data besides sparsely labeled frames order improve semantic estimates <eos> model combines convolutional architecture spatio temporal transformer recurrent layer able temporally propagate labeling information means optical flow adaptively gated based its locally estimated uncertainty <eos> flow recognition gated temporal propagation modules trained jointly end end <eos> temporal gated recurrent flow propagation component model plugged into any static semantic segmentation architecture turn into weakly supervised video processing one <eos> experiments challenging cityscapes camvid datasets multiple deep architectures indicate resulting model leverage unlabeled temporal frames next labeled one order improve both video segmentation accuracy consistency its temporal labeling no additional annotation cost little extra computation <eos> <eop> interpretable video captioning via trajectory structured localization <eos> automatically describing open domain video natural language attracting increasing interest field artificial intelligence <eos> most existing method simply borrow ideas image captioning obtain compact video representation ensemble global image feature before feeding rnn decoder outputs sentence variable length <eos> however only arduous generator focus specific salient object different time given global video representation more formidable capture fine grained motion information relation between moving instances more subtle linguistic descriptions <eos> paper propose trajectory structured attentional encoder decoder tsa ed neural network framework more elaborate video captioning works integrating local spatial temporal representation trajectory level through structured attention mechanism <eos> proposed method based lstm based encoder decoder framework incorporates attention modeling scheme adaptively learn correlation between sentence structure moving object video consequently generates more accurate meticulous statement description decoding stage <eos> experimental result demonstrate feature representation structured attention mechanism based trajectory cluster efficiently obtain local motion information video help generate more fine grained video description achieve state art performance well known charades msvd datasets <eos> <eop> deep hashing via discrepancy minimization <eos> paper presents discrepancy minimizing model address discrete optimization problem hashing learning <eos> discrete optimization introduced binary constraint np hard mixed integer programming problem <eos> usually addressed relaxing binary variables into continuous variables adapt gradient based learning hashing functions especially training deep neural network <eos> deal objective discrepancy caused relaxation transform original binary optimization into differentiable optimization problem over hash functions through series expansion <eos> transformation decouples binary constraint similarity preserving hashing function optimization <eos> transformed objective optimized tractable alternating optimization framework gradual discrepancy minimization <eos> extensive experimental result three benchmark datasets validate efficacy proposed discrepancy minimizing hashing <eos> <eop> shufflenet extremely efficient convolutional neural network mobile devices <eos> introduce extremely computation efficient cnn architecture named shufflenet designed specially mobile devices very limited computing power <eos> new architecture utilizes two new operations pointwise group convolution channel shuffle greatly reduce computation cost while maintaining accuracy <eos> experiments imagenet classification ms coco object detection demonstrate superior performance shufflenet over other structures <eos> lower top error absolute <eos> than recent mobilenet cite howard mobilenets imagenet classification task under computation budget mflops <eos> arm based mobile device shufflenet achieves sim imes actual speedup over alexnet while maintaining comparable accuracy <eos> <eop> zero shot recognition via semantic embeddings knowledge graphs <eos> consider problem zero shot recognition learning visual classifier category zero training examples just using word embedding category its relationship other categories visual data provided <eos> key dealing unfamiliar novel category transfer knowledge obtained familiar classes describe unfamiliar class <eos> paper build upon recently introduced graph convolutional network gcn propose approach uses both semantic embeddings categorical relationships predict classifiers <eos> given learned knowledge graph kg approach takes input semantic embeddings each node representing visual category <eos> after series graph convolutions predict visual classifier each category <eos> during training visual classifiers few categories given learn gcn parameters <eos> test time filters used predict visual classifiers unseen categories <eos> show approach robust noise kg <eos> more importantly approach provides significant improvement performance compared current state art result some metrics whopping few <eos> <eop> referring relationships <eos> image simply set object each image represents web interconnected relationships <eos> relationships between entities carry semantic meaning help viewer differentiate between instances entity <eos> example image soccer match there may multiple persons present but each participates different relationships one kicking ball other guarding goal <eos> paper formulate task utilizing referring relationships disambiguate between entities same category <eos> introduce iterative model localizes two entities referring relationship conditioned one another <eos> formulate cyclic condition between entities relationship modelling predicates connect entities shifts attention one entity another <eos> demonstrate model only outperform existing approaches three datasets clevr vrd visual genome but also produces visually meaningful predicate shifts instance interpretable neural network <eos> finally show modelling predicates attention shifts even localize entities absence their category allowing model find completely unseen categories <eos> <eop> improving object localization fitness nms bounded iou loss <eos> demonstrate many detection method designed identify only sufficently accurate bounding box rather than best available one <eos> address issue propose simple fast modification existing method called fitness nms <eos> method tested denet model obtains significantly improved map greater localization accuracies without loss evaluation rate used conjunction soft nms additional improvements <eos> next derive novel bounding box regression loss based set iou upper bounds better matches goal iou maximization while still providing good convergence properties <eos> following novelties investigate roi clustering schemes improving evaluation rates denet wide model variants provide analysis localization performance various input image dimensions <eos> hz mscoco titan maxwell <eos> <eop> end end deep kronecker product matching person re identification <eos> person re identification aims robustly measure similarities between person image <eos> significant variation person poses viewing angles challenges accurate person re identification <eos> spatial layout correspondences between query person image vital information tackling problem but ignored most state art method <eos> paper propose novel kronecker product matching module match feature maps different persons end end trainable deep neural network <eos> novel feature soft warping scheme designed aligning feature maps based matching result shown crucial achieving superior accuracy <eos> multi scale feature based hourglass like network self residual attention also exploited further boost re identification performance <eos> proposed approach outperforms state art method market cuhk dukemtmc datasets demonstrates effectiveness generalization ability proposed approach <eos> <eop> semantic visual localization <eos> robust visual localization under wide range viewing conditions fundamental problem computer vision <eos> handling difficult cases problem only very challenging but also high practical relevance <eos> context life long localization augmented reality autonomous robots <eos> paper propose novel approach based joint three dimensional geometric semantic understanding world enabling succeed under conditions previous approaches failed <eos> method leverages novel generative model descriptor learning trained semantic scene completion auxiliary task <eos> resulting three dimensional descriptors robust missing observations encoding high level three dimensional geometric semantic information <eos> experiments several challenging large scale localization datasets demonstrate reliable localization under extreme viewpoint illumination geometry changes <eos> <eop> object context detecting their semantic parts <eos> present semantic part detection approach effectively leverages object information <eos> use object appearance its class indicators parts expect <eos> also model expected relative location parts inside object based their appearance <eos> achieve new network module called offsetnet efficiently predicts variable number part locations within given object <eos> model incorporates all cues detect parts context their object <eos> leads considerably higher performance challenging task part detection compared using part appearance alone map pascal part dataset <eos> also compare other part detection method both pascal part cub datasets <eos> <eop> end end weakly supervised semantic alignment <eos> tackle task semantic alignment goal compute dense semantic correspondence aligning two image depicting object same category <eos> challenging task due large intra class variation changes viewpoint background clutter <eos> present following three principal contributions <eos> first develop convolutional neural network architecture semantic alignment trainable end end manner weak image level supervision form matching image pairs <eos> outcome parameters learnt rich appearance variation present different but semantically related image without need tedious manual annotation correspondences training time <eos> second main component architecture differentiable soft inlier scoring module inspired ransac inlier scoring procedure computes quality alignment based only geometrically consistent correspondences thereby reducing effect background clutter <eos> third demonstrate proposed approach achieves state art performance multiple standard benchmarks semantic alignment <eos> <eop> dynamic zoom network fast object detection large image <eos> introduce generic framework reduces computational cost object detection while retaining accuracy scenarios object varied sizes appear high resolution image <eos> detection progresses coarse fine manner first down sampled version image then sequence higher resolution region identified likely improve detection accuracy <eos> built upon reinforcement learning approach consists model net uses coarse detection result predict potential accuracy gain analyzing region higher resolution another model net sequentially selects region zoom <eos> experiments caltech pedestrians dataset show approach reduces number processed pixels over without drop detection accuracy <eos> merits approach become more significant high resolution test set collected yfcc dataset approach maintains high detection performance while reducing number processed pixels about detection time over <eos> <eop> learning markov clustering network scene text detection <eos> novel framework named markov clustering network mcn proposed fast robust scene text detection <eos> mcn predicts instance level bounding boxes firstly converting image into stochastic flow graph sfg then performing markov clustering graph <eos> method detect text object arbitrary size orientation without prior knowledge object size <eos> stochastic flow graph encode object local correlation semantic information <eos> object modeled strongly connected nodes allows flexible bottom up detection scale varying rotated object <eos> mcn generates bounding boxes without using non maximum suppression fully parallelized gpus <eos> evaluation public benchmarks shows method outperforms existing method large margin detecting multioriented text object <eos> mcn achieves new state art performance challenging msra td dataset precision <eos> also mcn achieves realtime inference frame rate fps <eos> imes speedup when compared fastest scene text detection algorithm <eos> <eop> deep reinforcement learning region proposal network object detection <eos> propose drl rpn deep reinforcement learning based visual recognition model consisting sequential region proposal network rpn object detector <eos> contrast typical rpns candidate object region rois selected greedily via class agnostic nms drl rpn optimizes objective closer final detection task <eos> achieved replacing greedy roi selection process sequential attention mechanism trained via deep reinforcement learning rl <eos> model capable accumulating class specific evidence over time potentially affecting subsequent proposals classification scores show such context integration significantly boosts detection accuracy <eos> moreover drl rpn automatically decides when stop search process benefit being able jointly learn parameters policy detector both represented deep network <eos> model further learn search over wide range exploration accuracy trade offs making possible specify adapt exploration extent test time <eos> resulting search trajectories image category dependent yet rely only single policy over all object categories <eos> result ms coco pascal voc challenges show approach outperforms established typical state art object detection pipelines <eos> <eop> beyond holistic object recognition enriching image understanding part states <eos> important high level vision tasks require rich semantic descriptions object part level <eos> based upon previous work part localization paper address problem inferring rich semantics imparted object part still image <eos> specifically propose tokenize semantic space discrete set part states <eos> modeling part state spatially localized therefore formulate part state inference problem pixel wise annotation problem <eos> iterative part state inference neural network efficient time accurate performance specifically designed task <eos> extensive experiments demonstrate proposed method effectively predict semantic states parts simultaneously improve part segmentation thus benefiting number visual understanding applications <eos> other contribution paper part state dataset contains rich part level semantic annotations <eos> <eop> discriminability objective training descriptive captions <eos> one property remains lacking image captions generated contemporary method discriminability being able tell two image apart given caption one them <eos> propose way improve aspect caption generation <eos> incorporating into captioning training objective loss component directly related ability machine disambiguate image caption matches obtain systems produce much more discriminative caption according human evaluation <eos> remarkably approach leads improvement other aspects generated captions reflected battery standard scores such bleu spice etc <eos> approach modular applied variety model loss combinations commonly proposed image captioning <eos> <eop> visual question answering memory augmented network <eos> paper exploit memory augmented neural network predict accurate answers visual questions even when answers rarely occur training set <eos> memory network incorporates both internal external memory blocks selectively pays attention each training exemplar <eos> show memory augmented neural network able maintain relatively long term memory scarce training exemplars important visual question answering due heavy tailed distribution answers general vqa setting <eos> experimental result two large scale benchmark datasets show favorable performance proposed algorithm comparison state art <eos> <eop> structure inference net object detection using scene level context instance level relationships <eos> context important accurate visual recognition <eos> work propose object detection algorithm only considers object visual appearance but also makes use two kinds context including scene contextual information object relationships within single image <eos> therefore object detection regarded both cognition problem reasoning problem when leveraging structured information <eos> specifically paper formulates object detection problem graph structure inference given image object treated nodes graph relationships between object modeled edges such graph <eos> end present so called structure inference network sin detector incorporates into typical detection framework <eos> faster cnn graphical model aims infer object state <eos> comprehensive experiments pascal voc ms coco datasets indicate scene context object relationships truly improve performance object detection more desirable reasonable outputs <eos> <eop> occluded pedestrian detection through guided attention cnn <eos> pedestrian detection progressed significantly last years <eos> however occluded people notoriously hard detect their appearance varies substantially depending wide range partial occlusions <eos> paper aim propose simple compact method based fasterrcnn architecture occluded pedestrian detection <eos> start interpreting cnn channel feature pedestrian detector find different channels activate responses different body parts respectively <eos> findings strongly motivate employ attention mechanism across channels represent various occlusion patterns one single model each occlusion pattern formulated some specific combination body parts <eos> therefore attention network self external guidances proposed add baseline fasterrcnn detector <eos> when evaluating heavy occlusion subset achieve significant improvement pp baseline fasterrcnn detector citypersons caltech outperform state art method pp <eos> <eop> reward learning narrated demonstrations <eos> humans effortlessly program one another communicating goals desires natural language <eos> contrast humans program robotic behaviours indicating desired object locations poses achieved providing rgb image goal configurations supplying demonstration imitated <eos> none method generalize across environment variations they convey goal awkward technical terms <eos> work proposes joint learning natural language grounding instructable behavioural policies reinforced perceptual detectors natural language expressions grounded sensory inputs robotic agent <eos> supervision narrated visual demonstrations nvd visual demonstrations paired verbal narration opposed being silent <eos> introduce dataset nvd teachers perform activities while describing them detail <eos> map teachers descriptions perceptual reward detectors use them train corresponding behavioural policies simulation <eos> empirically show instructable agents learn visual reward detectors using small number examples exploiting hard negative mined configurations demonstration dynamics ii develop pick place policies using learned visual reward detectors iii benefit object factorized state representations mimic syntactic structure natural language goal expressions iv execute behaviours involve novel object novel locations test time instructed natural language <eos> <eop> weakly supervised semantic segmentation network deep seeded region growing <eos> paper studies problem learning image semantic segmentation network only using image level labels supervision important since significantly reduce human annotation efforts <eos> recent state art method problem first infer sparse discriminative region each object class using deep classification network then train semantic segmentation network using discriminative region supervision <eos> inspired traditional image segmentation method seeded region growing propose train semantic segmentation network starting discriminative region progressively increase pixel level supervision using seeded region growing <eos> seeded region growing module integrated deep segmentation network benefit deep feature <eos> different conventional deep network fixed static labels proposed weakly supervised network generates new labels using contextual information within image <eos> proposed method significantly outperforms weakly supervised semantic segmentation method using static labels obtains state art performance <eos> miou score pascal voc test set <eos> miou score coco dataset <eos> <eop> potion pose motion representation action recognition <eos> most state art method action recognition rely two stream architecture processes appearance motion independently <eos> paper claim considering them jointly offers rich information action recognition <eos> introduce novel representation gracefully encodes movement some semantic keypoints <eos> use human joints keypoints term pose motion representation potion <eos> specifically first run state art human pose estimator extract heatmaps human joints each frame <eos> obtain potion representation temporally aggregating probability maps <eos> achieved colorizing each them depending relative time frames video clip summing them <eos> fixed size representation entire video clip suitable classify actions using shallow convolutional neural network <eos> experimental evaluation shows potion outperforms other state art pose representations <eos> furthermore complementary standard appearance motion streams <eos> when combining potion recent two stream approach obtain state art performance jhmdb hmdb ucf datasets <eos> <eop> bilateral ordinal relevance multi instance regression facial action unit intensity estimation <eos> automatic intensity estimation facial action units aus challenging two aspects <eos> first capturing subtle changes facial appearance quiet difficult <eos> second annotation au intensity scarce expensive <eos> intensity annotation requires strong domain knowledge thus only experts qualified <eos> majority method directly apply supervised learning techniques au intensity estimation while few method exploit unlabeled sample improve performance <eos> paper propose novel weakly supervised regression model bilateral ordinal relevance multi instance regression bormir learns frame level intensity estimator weakly labeled sequences <eos> new perspective introduce relevance model sequential data consider two bag labels each bag <eos> au intensity estimation formulated joint regressor relevance learning problem <eos> temporal dynamics both relevance au intensity leveraged build connections among labeled unlabeled image frames provide weak supervision <eos> also develop efficient algorithm optimization based alternating minimization framework <eos> evaluations three expression databases demonstrate effectiveness proposed model <eos> <eop> pulling actions out context explicit separation effective combination <eos> ability recognize human actions video many potential applications <eos> human action recognition however tremendously challenging computers due complexity video data subtlety human actions <eos> most current recognition systems flounder inability separate human actions co occurring factors usually dominate subtle human actions <eos> paper propose novel approach training human action recognizer one explicitly factorize human actions co occurring factors deliberately build model human actions separate model all correlated contextual elements effectively combine models human action recognition <eos> approach exploits benefits conjugate sample human actions video clips contextually similar human action sample but contain action <eos> experiments actionthread pascal voc ucf hollywood datasets demonstrate ability separate action context proposed approach <eos> <eop> dynamic feature learning partial face recognition <eos> partial face recognition pfr unconstrained environment very important task especially video surveillance mobile devices etc <eos> however few studies tackled how recognize arbitrary patch face image <eos> study combines fully convolutional network fcn sparse representation classification src propose novel partial face recognition approach called dynamic feature matching dfm address partial face image regardless sizes <eos> based dfm propose sliding loss optimize fcn reducing intra variation between face patch face image subject further improves performance dfm <eos> proposed dfm evaluated several partial face databases including lfw ytf casia nir distance databases <eos> experimental result demonstrate effectiveness advantages dfm comparison state art pfr method <eos> <eop> exploiting transitivity learning person re identification models budget <eos> minimization labeling effort person re identification camera network important problem most existing popular method supervised they require large amount manual annotations acquiring tedious job <eos> work focus labeling effort minimization problem approach subset selection task objective select optimal subset image pairs labeling without compromising performance <eos> towards goal proposed scheme first represents any camera network number cameras edge weighted complete partite graph each vertex denotes person similarity scores between persons used edge weights <eos> then second stage algorithm selects optimal subset pairs solving triangle free subgraph maximization problem partite graph <eos> sub graph weight maximization problem np hard least means large datasets optimization problem becomes intractable <eos> order make framework scalable propose two polynomial time approximately optimal algorithms <eos> first algorithm approximation algorithm runs linear time number edges <eos> second algorithm greedy algorithm sub quadratic number edges time complexity <eos> experiments three state art datasets depict proposed approach requires average only manually labeled pairs order achieve performance when all pairs manually annotated <eos> <eop> deep spatial feature reconstruction partial person re identification alignment free approach <eos> partial person re identification re id challenging problem only partial observation person image available matching <eos> however few studies offered solution how identify arbitrary patch person image <eos> paper propose fast accurate matching method address problem <eos> proposed method leverages fully convolutional network fcn generate correspondingly size spatial feature maps such pixel level feature consistent <eos> match pair person image different sizes novel method called deep spatial feature reconstruction dsr further developed avoid explicit alignment <eos> specifically exploit reconstructing error dictionary learning calculate similarity between different spatial feature maps <eos> way expect proposed fcn decrease similarity coupled image different persons vice versa <eos> experimental result two partial person datasets demonstrate efficiency effectiveness proposed method comparison several state art partial person re id approaches <eos> <eop> every smile unique landmark guided diverse smile generation <eos> each smile unique one person surely smiles different ways <eos> closing opening eyes mouth <eos> given one input image neutral face generate multiple smile video distinctive characteristics tackle one many video generation problem propose novel deep learning architecture named conditional multi mode network cmm net <eos> better encode dynamics facial expressions cmm net explicitly exploits facial landmarks generating smile sequences <eos> specifically variational auto encoder used learn facial landmark embedding <eos> single embedding then exploited conditional recurrent network generates landmark embedding sequence conditioned specific expression <eos> next generated landmark embeddings fed into multi mode recurrent landmark generator producing set landmark sequences still associated given smile class but clearly distinct each other <eos> finally landmark sequences translated into face video <eos> experimental result demonstrate effectiveness cmm net generating realistic video multiple smile expressions <eos> <eop> uv gan adversarial facial uv map completion pose invariant face recognition <eos> recently proposed robust three dimensional face alignment method establish either dense sparse correspondence between three dimensional face model facial image <eos> use method presents new challenges well opportunities facial texture analysis <eos> particular sampling image using fitted model facial uv created <eos> unfortunately due self occlusion such uv map always incomplete <eos> paper propose framework training deep convolutional neural network dcnn complete facial uv map extracted wild image <eos> end first gather complete uv maps fitting three dimensional morphable model dmm various multiview image video datasets well leveraging new three dimensional dataset over identities <eos> second devise meticulously designed architecture combines local global adversarial dcnns learn identity preserving facial uv completion model <eos> demonstrate attaching completed uv fitted mesh generating instances arbitrary poses increase pose variations training deep face recognition verification models minimise pose discrepancy during testing lead better performance <eos> experiments both controlled wild uv datasets prove effectiveness adversarial uv completion model <eos> achieve state art verification accuracy <eos> under cfp frontal profile protocol only combining pose augmentation during training pose discrepancy reduction during testing <eos> will release first wild uv dataset refer wilduv comprises complete facial uv maps identities research purposes <eos> <eop> cascaded pyramid network multi person pose estimation <eos> topic multi person pose estimation beenlargely improved recently especially developmentof convolutional neural network <eos> however there still exista lot challenging cases such occluded keypoints visible keypoints complex background cannot bewell addressed <eos> paper present novel networkstructure called cascaded pyramid network cpn whichtargets relieve problem hard keypoints <eos> more specifically algorithm includes two stages glob alnet refinenet <eos> globalnet feature pyramid net work successfully localize simple key point like eyes hands but may fail precisely rec ognize occluded invisible keypoints <eos> refinenettries explicitly handling hard keypoints integrat ing all levels feature representations global net together online hard keypoint mining loss <eos> ingeneral address multi person pose estimation prob lem top down pipeline adopted first generate setof human bounding boxes based detector followed byour cpn keypoint localization each human boundingbox <eos> based proposed algorithm achieve state art result coco keypoint benchmark averageprecision <eos> coco test dev dataset <eos> onthe coco test challenge dataset relativeimprovement compared <eos> coco key point challenge <eos> code detection result personused will publicly available further research <eos> <eop> face face neural conversation model <eos> neural network recently become good engaging dialog <eos> however current approaches based solely verbal text lacking richness real face face conversation <eos> propose neural conversation model aims read generate facial gestures alongside text <eos> allows model adapt its response based mood conversation <eos> particular introduce rnn encoder decoder exploits movement facial muscles well verbal conversation <eos> decoder consists two layer lower layer aims generating verbal response coarse facial expressions while second layer fills subtle gestures making generated output more smooth natural <eos> train neural network having watch movies <eos> showcase joint face text model generating more natural conversations through automatic metrics human study <eos> demonstrate example application face face chatting avatar <eos> <eop> end end recovery human shape pose <eos> describe human mesh recovery hmr end end framework reconstructing full three dimensional mesh human body single rgb image <eos> contrast most current method compute three dimensional joint locations produce richer more useful mesh representation parameterized shape three dimensional joint angles <eos> main objective minimize reprojection loss keypoints allows model trained using wild image only ground truth annotations <eos> however reprojection loss alone highly underconstrained <eos> work address problem introducing adversary trained tell whether human body shape pose real using large database three dimensional human meshes <eos> show hmr trained without using any paired three dimensional supervision <eos> rely intermediate keypoint detections infer three dimensional pose shape parameters directly image pixels <eos> model runs real time given bounding box containing person <eos> demonstrate approach various image wild out perform previous optimization based method output three dimensional meshes show competitive result tasks such three dimensional joint location estimation part segmentation <eos> <eop> squeeze excitation network <eos> convolutional neural network built upon convolution operation extracts informative feature fusing spatial channel wise information together within local receptive fields <eos> order boost representational power network several recent approaches shown benefit enhancing spatial encoding <eos> work focus channel relationship propose novel architectural unit term squeeze excitation se block adaptively recalibrates channel wise feature responses explicitly modelling interdependencies between channels <eos> demonstrate stacking blocks together construct senet architectures generalise extremely well across challenging datasets <eos> crucially find se blocks produce significant performance improvements existing state art deep architectures minimal additional computational cost <eos> senets formed foundation ilsvrc classification submission won first place significantly reduced top error <eos> achieving relative improvement over winning entry <eos> code models available github <eos> com hujie frank senet <eos> <eop> revisiting salient object detection simultaneous detection ranking subitizing multiple salient object <eos> salient object detection problem considered detail many solutions proposed <eos> paper argue work date addressed problem relatively ill posed <eos> specifically there universal agreement about constitutes salient object when multiple observers queried <eos> implies some object more likely judged salient than others implies relative rank exists salient object <eos> solution presented paper solves more general problem considers relative rank propose data metrics suitable measuring success relative object saliency landscape <eos> novel deep learning solution proposed based hierarchical representation relative saliency stage wise refinement <eos> also show problem salient object subitizing addressed same network approach exceeds performance any prior work across all metrics considered both traditional newly proposed <eos> <eop> context encoding semantic segmentation <eos> recent work made significant progress improving spatial resolution pixelwise labeling fully convolutional network fcn framework employing dilated atrous convolution utilizing multi scale feature refining boundaries <eos> paper explore impact global contextual information semantic segmentation introducing context encoding module captures semantic context scenes selectively highlights class dependent featuremaps <eos> proposed context encoding module significantly improves semantic segmentation result only marginal extra computation cost over fcn <eos> approach achieved new state art result <eos> miou pascal context <eos> miou pascal voc <eos> single model achieves final score <eos> ade test set surpass winning entry coco place challenge <eos> addition also explore how context encoding module improve feature representation relatively shallow network image classification cifar dataset <eos> layer network achieved error rate <eos> comparable state art approaches over times more layer <eos> source code complete system publicly available <eos> <eop> creating capsule wardrobes fashion image <eos> propose automatically create emph capsule wardrobes <eos> given inventory candidate garments accessories algorithm must assemble minimal set items provides maximal mix match outfits <eos> pose task subset selection problem <eos> permit efficient subset selection over space all outfit combinations develop submodular objective functions capturing key ingredients visual compatibility versatility user specific preference <eos> since adding garments capsule only expands its possible outfits devise iterative approach allow near optimal submodular function maximization <eos> finally present unsupervised approach learn visual compatibility wild full body outfit photos compatibility metric translates well cleaner catalog photos improves over existing method <eos> result thousands pieces popular fashion websites show automatic capsule creation potential mimic skilled fashionistas assembling flexible wardrobes while being significantly more scalable <eos> <eop> webly supervised learning meets zero shot learning hybrid approach fine grained classification <eos> fine grained image classification targets distinguishing subtle distinctions among various subordinate categories remains very difficult task due high annotation cost enormous fine grained categories <eos> cope scarcity well labeled training image existing works mainly follow two research directions utilize freely available web image without human annotation only annotate some fine grained categories transfer knowledge other fine grained categories falls into scope zero shot learning zsl <eos> however above two directions their own drawbacks <eos> first direction labels web image very noisy data distribution between web image test image considerably different <eos> second direction performance gap between zsl traditional supervised learning still very large <eos> drawbacks above two directions motivate design new framework jointly leverage both web data auxiliary labeled categories predict test categories associated any well labeled training image <eos> comprehensive experiments three benchmark datasets demonstrate effectiveness proposed framework <eos> <eop> look imagine match improving textual visual cross modal retrieval generative models <eos> textual visual cross modal retrieval hot research topic both computer vision natural language processing communities <eos> learning appropriate representations multi modal data crucial cross modal retrieval performance <eos> unlike existing image text retrieval approaches embed image text pairs single feature vectors common representational space propose incorporate generative processes into cross modal feature embedding through able learn only global abstract feature but also local grounded feature <eos> extensive experiments show framework well match image sentences complex content achieve state art cross modal retrieval result mscoco dataset <eos> <eop> bidirectional attentive fusion context gating dense video captioning <eos> dense video captioning newly emerging task aims both localizing describing all events video <eos> identify tackle two challenges task namely how utilize both past future contexts accurate event proposal predictions how construct informative input decoder generating natural event descriptions <eos> first previous works predominantly generate temporal event proposals forward direction neglects future video contexts <eos> propose bidirectional proposal method effectively exploits both past future contexts make proposal predictions <eos> second different events ending nearly same time indistinguishable previous works resulting same captions <eos> solve problem representing each event attentive fusion hidden states proposal module video contents <eos> further propose novel context gating mechanism balance contributions current event its surrounding contexts dynamically <eos> empirically show attentively fused event representation superior proposal hidden states video contents alone <eos> coupling proposal captioning modules into one unified framework model outperforms state arts activitynet captions dataset relative gain over meteor score increases <eos> <eop> inloc indoor visual localization dense matching view synthesis <eos> seek predict degree freedom dof pose query photograph respect large indoor three dimensional map <eos> contributions work three fold <eos> first develop new large scale visual localization method targeted indoor environments <eos> method proceeds along three steps efficient retrieval candidate poses ensures scalability large scale environments ii pose estimation using dense matching rather than local feature deal textureless indoor scenes iii pose verification virtual view synthesis cope significant changes viewpoint scene layout occluders <eos> second collect new dataset reference dof poses large scale indoor localization <eos> query photographs captured mobile phones different time than reference three dimensional map thus presenting realistic indoor localization scenario <eos> third demonstrate method significantly outperforms current state art indoor localization approaches new challenging data <eos> <eop> towards high performance video object detection <eos> there significant progresses image object detection recently <eos> nevertheless video object detection received little attention although more challenging more important practical scenarios <eos> built upon recent works work proposes unified viewpoint based principle multi frame end end learning feature cross frame motion <eos> approach extends prior works three new techniques steadily pushes forward performance envelope speed accuracy tradeoff towards high performance video object detection <eos> <eop> neural baby talk <eos> introduce novel framework image captioning produce natural language explicitly grounded entities object detectors find image <eos> approach reconciles classical slot filling approaches generally better grounded image modern neural captioning approaches generally more natural sounding accurate <eos> approach first generates sentence template slot locations explicitly tied specific image region <eos> slots then filled visual concepts identified region object detectors <eos> entire architecture sentence template generation slot filling object detectors end end differentiable <eos> verify effectiveness proposed model different image captioning tasks <eos> standard image captioning novel object captioning model reaches state art both coco flickr datasets <eos> also demonstrate model unique advantages when train test distributions scene compositions hence language priors associated captions different <eos> code made available github <eos> com jiasenlu neuralbabytalk <eop> few shot image recognition predicting parameters activations <eos> paper interested few shot learning problem <eos> particular focus challenging scenario number categories large number examples per novel category very limited <eos> motivated close relationship between parameters activations neural network associated same category propose novel method adapt pre trained neural network novel categories directly predicting parameters activations <eos> zero training required adaptation novel categories fast inference realized single forward pass <eos> evaluate method doing few shot image recognition imagenet dataset achieves state art classification accuracy novel categories significant margin while keeping comparable performance large scale categories <eos> also test method miniimagenet dataset strongly outperforms previous state art method <eos> <eop> iterative visual reasoning beyond convolutions <eos> present novel framework iterative visual reasoning <eos> framework goes beyond current recognition systems lack capability reason beyond stack convolutions <eos> framework consists two core modules local module uses spatial memory store previous beliefs parallel global graph reasoning module <eos> graph three components knowledge graph represent classes nodes build edges encode different types semantic relationships between them region graph current image region image nodes spatial relationships between region edges assignment graph assigns region class nodes <eos> both local module global module roll out iteratively cross feed predictions each other refine estimates <eos> final predictions made combining best both modules attention mechanism <eos> show strong performance over plain convnets eg achieving <eos> absolute improvement ade measured per class average precision <eos> analysis also shows framework resilient missing region reasoning <eos> <eop> visual question reasoning general dependency tree <eos> collaborative reasoning understanding each image question pair very critical but under explored interpretable visual question answering vqa system <eos> although very recent works also tried explicit compositional processes assemble multiple sub tasks embedded questions their models heavily rely annotations hand crafted rules obtain valid reasoning layout leading either heavy labor poor performance composition reasoning <eos> paper enable global context reasoning better aligning image language domains diverse unrestricted cases propose novel reasoning network called adversarial composition modular network acmn <eos> network comprises two collaborative modules adversarial attention module exploit local visual evidence each word parsed question ii residual composition module compose previously mined evidence <eos> given dependency parse tree each question adversarial attention module progressively discovers salient region one word densely combining region child word nodes adversarial manner <eos> then residual composition module merges hidden representations arbitrary number children through sum pooling residual connection <eos> acmn thus capable building interpretable vqa system gradually dives image cues following question driven reasoning route makes global reasoning incorporating learned knowledge all attention modules principled manner <eos> experiments relational datasets demonstrate superiority acmn visualization result show explainable capability reasoning system <eos> <eop> cvm net cross view matching network image based ground aerial geo localization <eos> problem localization geo referenced aerial satellite map given query ground view image remains challenging due drastic change viewpoint causes traditional image descriptors based matching fail <eos> leverage recent success deep learning propose cvm net cross view image based ground aerial geo localization task <eos> specifically network based siamese architecture metric learning matching task <eos> first use fully convolutional layer extract local image feature then encoded into global image descriptors using powerful netvlad <eos> part training procedure also introduce simple yet effective weighted soft margin ranking loss function only speeds up training convergence but also improves final matching accuracy <eos> experimental result show proposed network significantly outperforms state art approaches two existing benchmarking datasets <eos> <eop> revisiting dilated convolution simple approach weakly semi supervised semantic segmentation <eos> despite remarkable progress weakly supervised segmentation method still inferior their fully supervised counterparts <eos> obverse performance gap mainly comes inability producing dense integral pixel level object localization training image only image level labels <eos> work revisit dilated convolution proposed shed light how enables classification network generate dense object localization <eos> substantially enlarging receptive fields convolutional kernels different dilation rates classification network localize object region even when they so discriminative classification finally produce reliable object region benefiting both weakly semi supervised semantic segmentation <eos> despite apparent simplicity dilated convolution able obtain superior performance semantic segmentation tasks <eos> mean intersection over union miou pascal voc test set weakly only image level labels available semi segmentation masks available settings new state arts <eos> <eop> low shot learning imaginary data <eos> humans quickly learn new visual concepts perhaps because they easily visualize imagine novel object look like different views <eos> incorporating ability hallucinate novel instances new concepts might help machine vision systems perform better low shot learning <eos> learning concepts few examples <eos> present novel approach low shot learning uses idea <eos> approach builds recent progress meta learning learning learn combining meta learner hallucinator produces additional training examples optimizing both models jointly <eos> hallucinator incorporated into variety meta learners provides significant gains up point boost classification accuracy when only single training example available yielding state art performance challenging imagenet low shot classification benchmark <eos> <eop> doublefusion real time capture human performances inner body shapes single depth sensor <eos> propose doublefusion new real time system combines volumetric dynamic reconstruction data driven template fitting simultaneously reconstruct detailed geometry non rigid motion inner human body shape single depth camera <eos> one key contributions method double layer representation consisting complete parametric body shape inside gradually fused outer surface layer <eos> pre defined node graph body surface parameterizes non rigid deformations near body free form dynamically changing graph parameterizes outer surface layer far body allowing more general reconstruction <eos> further propose joint motion tracking method based double layer representation enable robust fast motion tracking performance <eos> moreover inner body shape optimized online forced fit inside outer surface layer <eos> overall method enables increasingly denoised detailed complete surface reconstructions fast motion tracking performance plausible inner body shape reconstruction real time <eos> particular experiments show improved fast motion tracking loop closure performance more challenging scenarios <eos> <eop> densepose dense human pose estimation wild <eos> work establish dense correspondences between rgb image surface based representation human body task refer dense human pose estimation <eos> gather dense correspondences persons appearing coco dataset introducing efficient annotation pipeline <eos> then use dataset train cnn based systems deliver dense correspondence wild namely presence background occlusions scale variations <eos> improve training set effectiveness training inpainting network fill missing ground truth values report improvements respect best result would achievable past <eos> experiment fully convolutional network region based models observe superiority latter <eos> further improve accuracy through cascading obtaining system delivers highly accurate result multiple frames per second single gpu <eos> supplementary materials data code video provided project page densepose <eos> <eop> ordinal depth supervision three dimensional human pose estimation <eos> ability train end end systems three dimensional human pose estimation single image currently constrained limited availability three dimensional annotations natural image <eos> most datasets captured using motion capture mocap systems studio setting difficult reach variability human pose datasets like mpii lsp <eos> alleviate need accurate three dimensional ground truth propose use weaker supervision signal provided ordinal depths human joints <eos> information acquired human annotators wide range image poses <eos> showcase effectiveness flexibility training convolutional network convnets ordinal relations different settings always achieving competitive performance convnets trained accurate three dimensional joint coordinates <eos> additionally demonstrate potential approach augment popular lsp mpii datasets ordinal depth annotations <eos> extension allows present quantitative qualitative evaluation non studio conditions <eos> simultaneously ordinal annotations easily incorporated training procedure typical convnets three dimensional human pose <eos> through inclusion achieve new state art performance relevant benchmarks validate effectiveness ordinal depth supervision three dimensional human pose <eos> <eop> consensus maximization semantic region correspondences <eos> propose novel method geometric registration semantically labeled region <eos> approximate semantic region ellipsoids leverage their convexity formulate correspondence search effectively constrained optimization problem maximizes number matched region solve globally optimal branch bound fashion <eos> end derive suitable linear matrix inequality constraints describe ellipsoid ellipsoid assignment conditions <eos> approach robust large percentages outliers thus applicable difficult correspondence search problems <eos> multiple experiments demonstrate flexibility robustness approach number challenging vision problems <eos> <eop> robust hough transform based three dimensional reconstruction circular light fields <eos> light field imaging based image taken regular grid <eos> thus high quality three dimensional reconstructions obtainable analyzing orientations epipolar plane image epis <eos> unfortunately such data only allows evaluate one side object <eos> moreover constant intensity along each orientation mandatory most approaches <eos> paper presents novel method allows reconstruct depth information data acquired circular camera motion termed circular light fields <eos> approach possible determine full degree view target object <eos> additionally circular light fields allow retrieving depth datasets acquired telecentric lenses possible linear light fields <eos> proposed method finds trajectories three dimensional point epis means modified hough transform <eos> purpose binary epi edge image used only allow obtain reliable depth information but also overcome limitation constant intensity along trajectories <eos> experimental result synthetic real datasets demonstrate quality proposed algorithm <eos> <eop> alive caricature <eos> caricature art form expresses subjects abstract simple exaggerated views <eos> while many caricatures image paper presents algorithm creating expressive three dimensional caricatures caricature image minimum user interaction <eos> key idea approach introduce intrinsic deformation representation capability extrapolation enabling create deformation space standard face datasets maintains face constraints meanwhile sufficiently large producing exaggerated face models <eos> built upon proposed deformation representation optimization model formulated find three dimensional caricature captures style caricature image automatically <eos> experiments show approach better capability expressing caricatures than fitting approaches directly using classical parametric face models such dmm facewarehouse <eos> moreover approach based standard face datasets avoids constructing complicated three dimensional caricature training set provides great flexibility real applications <eos> <eop> nonlinear three dimensional face morphable model <eos> classic statistical model three dimensional facial shape texture three dimensional morphable model dmm widely used facial analysis <eos> model fitting image synthesis <eos> conventional dmm learned set well controlled face image associated three dimensional face scans represented two set pca basis functions <eos> due type amount training data well linear bases representation power dmm limited <eos> address problems paper proposes innovative framework learn nonlinear dmm model large set unconstrained face image without collecting three dimensional face scans <eos> specifically given face image input network encoder estimates projection shape texture parameters <eos> two decoders serve nonlinear dmm map shape texture parameters three dimensional shape texture respectively <eos> projection parameter three dimensional shape texture novel analytically differentiable rendering layer designed reconstruct original input face <eos> entire network end end trainable only weak supervision <eos> demonstrate superior representation power nonlinear dmm over its linear counterpart its contribution face alignment three dimensional reconstruction <eos> <eop> through wall human pose estimation using radio signals <eos> paper demonstrates accurate human pose estimation through walls occlusions <eos> leverage fact wireless signals wifi frequencies traverse walls reflect off human body <eos> introduce deep neural network approach parses such radio signals estimate poses <eos> since humans cannot annotate radio signals use state art vision model provide cross modal supervision <eos> specifically during training system uses synchronized wireless visual inputs extracts pose information visual stream uses guide training process <eos> once trained network uses only wireless signal pose estimation <eos> show when tested visible scenes radio based system almost accurate vision based system used train <eos> yet unlike vision based pose estimation radio based system estimate poses through walls despite never trained such scenarios <eos> demo video available website rfpose <eos> <eop> makes video video analyzing temporal information video understanding models datasets <eos> ability capture temporal information critical development video understanding models <eos> while there numerous attempts modeling motion video explicit analysis effect temporal information video understanding still missing <eos> work aim bridge gap ask following question how important motion video recognizing action end propose two novel frameworks class agnostic temporal generator ii motion invariant frame selector reduce remove motion ablation analysis without introducing other artifacts <eos> isolates analysis motion other aspects video <eos> proposed frameworks provide much tighter estimate effect motion ucf kinetics compared baselines analysis <eos> analysis provides critical insights about existing models like how could made achieve comparable result sparser set frames <eos> <eop> fast video object segmentation reference guided mask propagation <eos> present efficient method semi supervised video object segmentation <eos> method achieves accuracy competitive state art method while running fraction time compared others <eos> end propose deep siamese encoder decoder network designed take advantage mask propagation object detection while avoiding weaknesses both approaches <eos> network learned through two stage training process exploits both synthetic real data works robustly without any online learning post processing <eos> validate method four benchmark set cover single multiple object segmentation <eos> all benchmark set method shows comparable accuracy while having order magnitude faster runtime <eos> also provide extensive ablation add studies analyze evaluate framework <eos> <eop> neuralnetwork viterbi framework weakly supervised video learning <eos> video learning important task computer vision experienced increasing interest over recent years <eos> since even small amount video easily comprises several million frames method rely frame level annotation special importance <eos> work propose novel learning algorithm viterbi based loss allows online incremental learning weakly annotated video data <eos> moreover show explicit context length modeling leads huge improvements video segmentation labeling tasks include models into framework <eos> several action segmentation benchmarks obtain improvement up compared current state art method <eos> <eop> actor observer joint modeling first third person video <eos> several theories cognitive neuroscience suggest when people interact world simulate interactions they so first person egocentric perspective seamlessly transfer knowledge between third person observer first person actor <eos> despite learning such models human action recognition achievable due lack data <eos> paper takes step direction introduction charades ego large scale dataset paired first person third person video involving people paired video <eos> enables learning link between two actor observer perspectives <eos> thereby address one biggest bottlenecks facing egocentric vision research providing link first person abundant third person data web <eos> use data learn joint representation first third person video only weak supervision show its effectiveness transferring knowledge third person first person domain <eos> <eop> hsa rnn hierarchical structure adaptive rnn video summarization <eos> although video summarization achieved great success recent years few approaches realized influence video structure summarization result <eos> know video data follow hierarchical structure <eos> video composed shots shot composed several frames <eos> generally shots provide activity level information people understand video content <eos> while few existing summarization approaches pay attention shot segmentation procedure <eos> they generate shots some trivial strategies such fixed length segmentation may destroy underlying hierarchical structure video data further reduce quality generated summaries <eos> address problem propose structure adaptive video summarization approach integrates shot segmentation video summarization into hierarchical structure adaptive rnn denoted hsa rnn <eos> evaluate proposed approach four popular datasets <eos> summe tvsum cosum vtw <eos> experimental result demonstrated effectiveness hsa rnn video summarization task <eos> <eop> fast accurate online video object segmentation via tracking parts <eos> online video object segmentation challenging task entails process image sequence timely accurately <eos> segment target object through video numerous cnn based method developed heavily finetuning object mask first frame time consuming online applications <eos> paper propose fast accurate video object segmentation algorithm immediately start segmentation process once receiving image <eos> first utilize part based tracking method deal challenging factors such large deformation occlusion cluttered background <eos> based tracked bounding boxes parts construct region interest segmentation network generate part masks <eos> finally similarity based scoring function adopted refine object parts comparing them visual information first frame <eos> method performs favorably against state art algorithms accuracy davis benchmark dataset while achieving much faster runtime performance <eos> <eop> now you shake me towards automatic cinema <eos> interested enabling automatic cinema parsing physical special effects untrimmed movies <eos> include effects such physical interactions water splashing light shaking grounded either character scene camera <eos> collect new dataset referred movie dataset annotates over effects movies <eos> propose conditional random field model atop neural network brings together visual audio information well semantics form person tracks <eos> model further exploits correlations effects between different characters clip well across movie threads <eos> propose effect detection classification two tasks present result along ablation studies dataset paving way towards cinema everyone homes <eos> <eop> viewpoint aware video summarization <eos> paper introduces novel variant video summarization namely building summary depends particular aspect video viewer focuses <eos> infer desired viewpoint may assume several other video available especially groups video <eos> folders person phone laptop <eos> semantic similarity between video group vs <eos> dissimilarity between groups used produce viewpoint specific summaries <eos> considering similarity well avoiding redundancy output summary should diverse representative video same group discriminative against video different groups <eos> satisfy requirements simultaneously proposed novel video summarization method multiple groups video <eos> inspired fisher discriminant criteria selects summary optimizing combination three terms inner summary inner group between group variances defined feature representation summary simply represent <eos> moreover developed novel dataset investigate how well generated summary reflects underlying viewpoint <eos> quantitative qualitative experiments conducted dataset demonstrate effectiveness proposed method <eos> <eop> photometric stereo participating media considering shape dependent forward scatter <eos> image captured participating media such murky water fog smoke degraded scattered light <eos> thus use traditional three dimensional three dimensional reconstruction techniques such environments difficult <eos> paper propose photometric stereo method participating media <eos> proposed method differs previous studies respect modeling shape dependent forward scatter <eos> proposed model forward scatter described analytical form using lookup tables represented spatially variant kernels <eos> also propose approximation large scale dense matrix sparse matrix enables removal forward scatter <eos> experiments real synthesized data demonstrate proposed method improves three dimensional reconstruction participating media <eos> <eop> direction aware spatial context feature shadow detection <eos> shadow detection fundamental challenging task since requires understanding global image semantics there various backgrounds around shadows <eos> paper presents novel network shadow detection analyzing image context direction aware manner <eos> achieve first formulate direction aware attention mechanism spatial recurrent neural network rnn introducing attention weights when aggregating spatial context feature rnn <eos> learning weights through training recover direction aware spatial context dsc detecting shadows <eos> design developed into dsc module embedded cnn learn dsc feature different levels <eos> moreover weighted cross entropy loss designed make training more effective <eos> employ two common shadow detection benchmark datasets perform various experiments evaluate network <eos> experimental result show network outperforms state art method achieves accuracy reduction balance error rate <eos> <eop> discriminative learning latent feature zero shot recognition <eos> zero shot learning zsl aims recognize unseen image categories learning embedding space between image semantic representations <eos> years among existing works center task learn proper mapping matrices aligning visual semantic space whilst importance learn discriminative representations zsl ignored <eos> work retrospect existing method demonstrate necessity learn discriminative representations both visual semantic instances zsl <eos> propose end end network capable automatically discovering discriminative region zoom network learning discriminative semantic representations augmented space introduced both user defined latent attributes <eos> proposed method tested extensively two challenging zsl datasets experiment result show proposed method significantly outperforms state art method <eos> <eop> learning adapt structured output space semantic segmentation <eos> convolutional neural network based approaches semantic segmentation rely supervision pixel level ground truth but may generalize well unseen image domains <eos> labeling process tedious labor intensive developing algorithms adapt source ground truth labels target domain great interest <eos> paper propose adversarial learning method domain adaptation context semantic segmentation <eos> considering semantic segmentations structured outputs contain spatial similarities between source target domains adopt adversarial learning output space <eos> further enhance adapted model construct multi level adversarial network effectively perform output space domain adaptation different feature levels <eos> further improve method utilize multi level output adaptation based feature maps different levels <eos> extensive experiments ablation study conducted under various domain adaptation settings including synthetic real cross city scenarios <eos> show proposed method performs favorably against state art method terms accuracy visual quality <eos> <eop> multi task learning using uncertainty weigh losses scene geometry semantics <eos> numerous deep learning applications benefit multi task learning multiple regression classification objectives <eos> paper make observation performance such systems strongly dependent relative weighting between each task loss <eos> tuning weights hand difficult expensive process making multi task learning prohibitive practice <eos> propose principled approach multi task deep learning weighs multiple loss functions considering homoscedastic uncertainty each task <eos> allows simultaneously learn various quantities different units scales both classification regression settings <eos> demonstrate model learning per pixel depth regression semantic instance segmentation monocular input image <eos> perhaps surprisingly show model learn multi task weightings outperform separate models trained individually each task <eos> <eop> jointly localizing describing events dense video captioning <eos> automatically describing video natural language regarded fundamental challenge computer vision <eos> problem nevertheless trivial especially when video contains multiple events worthy mention often happens real video <eos> valid question how temporally localize then describe events known dense video captioning <eos> paper present novel framework dense video captioning unifies localization temporal event proposals sentence generation each proposal jointly training them end end manner <eos> combine two worlds integrate new design namely descriptiveness regression into single shot detection structure infer descriptive complexity each detected proposal via sentence generation <eos> turn adjusts temporal locations each event proposal <eos> model differs existing dense video captioning method since propose joint global optimization detection captioning framework uniquely capitalizes attribute augmented video captioning architecture <eos> extensive experiments conducted activitynet captions dataset framework shows clear improvements when compared state art techniques <eos> more remarkably obtain new record meteor <eos> activitynet captions official test set <eos> <eop> going image video saliency augmenting image salience dynamic attentional push <eos> present novel method incorporate recent advent static saliency models predict saliency video <eos> model augments static saliency models attentional push effect photographer scene actors shared attention setting <eos> demonstrate only imperative use static attentional push cues noticeable performance improvement achievable learning time varying nature attentional push <eos> propose multi stream convolutional long short term memory network convlstm structure augments state art static saliency models dynamic attentional push <eos> network contains four pathways saliency pathway three attentional push pathways <eos> multi pathway structure followed augmenting convnet learns combine complementary time varying outputs convlstms minimizing relative entropy between augmented saliency viewers fixation patterns video <eos> evaluate model comparing performance several augmented static saliency models state art spatiotemporal saliency three largest dynamic eye tracking datasets hollywood ucf sport diem <eos> experimental result illustrates solid performance gain achievable using proposed methodology <eos> <eop> multimodal memory modelling video captioning <eos> video captioning automatically translates video clips into natural language sentences very important task computer vision <eos> virtue recent deep learning technologies video captioning made great progress <eos> however learning effective mapping visual sequence space language space still challenging problem due long term multimodal dependency modelling semantic misalignment <eos> inspired facts memory modelling poses potential advantages long term sequential problems working memory key factor visual attention propose multimodal memory model describe video builds visual textual shared memory model long term visual textual dependency further guide visual attention described visual targets solve visual textual alignments <eos> specifically similar proposed attaches external memory store retrieve both visual textual contents interacting video sentence multiple read write operations <eos> evaluate proposed model perform experiments two public datasets msvd msr vtt <eos> experimental result demonstrate method outperforms most state art method terms bleu meteor <eos> <eop> emotional attention study image sentiment visual attention <eos> image sentiment influences visual perception <eos> emotion eliciting stimuli such happy faces poisonous snakes generally prioritized human attention <eos> however little research evaluated interrelationships image sentiment visual saliency <eos> paper present first study focus relation between emotional properties image visual attention <eos> first create emotional attention dataset emod <eos> diverse set emotion eliciting image each image eye tracking data collected subjects intensive image context labels including object contour object sentiment object semantic category high level perceptual attributes such image aesthetics elicited emotions <eos> perform extensive analyses emod identify how image sentiment relates human attention <eos> discover emotion prioritization effect image emotion eliciting content attracts human attention strongly but such advantage diminishes dramatically after initial fixation <eos> aiming model human emotion prioritization computationally design deep neural network saliency prediction includes novel subnetwork learns spatial semantic context image scene <eos> proposed network outperforms state art three benchmark datasets effectively capturing relative importance human attention within image <eos> code models dataset available online nus sesame <eos> <eop> low power high throughput fully event based stereo system <eos> introduce stereo correspondence system implemented fully event based digital hardware using fully graph based non von neumann computation model no frames arrays any other such data structures used <eos> first time end end stereo pipeline image acquisition rectification multi scale spatio temporal stereo correspondence winner take all disparity regularization implemented fully event based hardware <eos> using cluster truenorth neurosynaptic processors demonstrate their ability process bilateral event based inputs streamed live dynamic vision sensors dvs up disparity maps per second producing high fidelity disparities turn used reconstruct low power depth events produced rapidly changing scenes <eos> experiments real world sequences demonstrate ability system take full advantage asynchronous sparse nature dvs sensors low power depth reconstruction environments conventional frame based cameras connected synchronous processors would inefficient rapidly moving object <eos> system evaluation event based sequences demonstrates improvement terms power per pixel per disparity map compared closest state art maximum latencies up ms spike injection disparity map ejection <eos> <eop> viton image based virtual try network <eos> present image based viirtual try network viton without using three dimensional information any form seamlessly transfers desired clothing item onto corresponding region person using coarse fine strategy <eos> conditioned upon new clothing agnostic yet descriptive person representation framework first generates coarse synthesized image target clothing item overlaid same person same pose <eos> further enhance initial blurry clothing area refinement network <eos> network trained learn how much detail utilize target clothing item apply person order synthesize photo realistic image target item deforms naturally clear visual patterns <eos> experiments newly collected zalando dataset demonstrate its promise image based virtual try task over state art generative models <eos> <eop> multi oriented scene text detection via corner localization region segmentation <eos> previous deep learning based state art scene text detection method roughly classified into two categories <eos> first category treats scene text type general object follows general object detection paradigm localize scene text regressing text box locations but troubled arbitrary orientation large aspect ratios scene text <eos> second one segments text region directly but mostly needs complex post processing <eos> paper present method combines ideas two types method while avoiding their shortcomings <eos> propose detect scene text localizing corner point text bounding boxes segmenting text region relative positions <eos> inference stage candidate boxes generated sampling grouping corner point further scored segmentation maps suppressed nms <eos> compared previous method method handle long oriented text naturally doesn need complex post processing <eos> experiments icdar icdar msra td mlt coco text demonstrate proposed algorithm achieves better comparable result both accuracy efficiency <eos> based vgg achieves measure icdar msra td <eos> <eop> multi content gan few shot font style transfer <eos> work focus challenge taking partial observations highly stylized text generalizing observations generate unobserved glyphs ornamented typeface <eos> generate set multi content image following consistent style very few examples propose end end stacked conditional gan model considering content along channels style along network layer <eos> proposed network transfers style given glyphs contents unseen ones capturing highly stylized fonts found real world such movie posters infographics <eos> seek transfer both typographic stylization ex <eos> serifs ears well textual stylization ex <eos> color gradients effects <eos> base experiments collected data set including fonts different styles demonstrate effective generalization very small number observed glyphs <eos> <eop> audio body dynamics <eos> present method gets input audio violin piano playing outputs video skeleton predictions further used animate avatar <eos> key idea create animation avatar moves their hands similarly how pianist violinist would just audio <eos> notably clear if body movement predicted music all aim work explore possibility <eos> paper present first result shows natural body dynamics predicted <eos> built lstm network trained violin piano recital video uploaded internet <eos> predicted point applied onto rigged avatar create animation <eos> <eop> weakly supervised coupled network visual sentiment analysis <eos> automatic assessment sentiment visual content gained considerable attention increasing tendency expressing opinions line <eos> paper solve problem visual sentiment analysis using high level abstraction recognition process <eos> existing method based convolutional neural network learn sentiment representations holistic image appearance <eos> however different image region different influence intended expression <eos> paper presents weakly supervised coupled convolutional network two branches leverage localized information <eos> first branch detects sentiment specific soft map training fully convolutional network cross spatial pooling strategy only requires image level labels thereby significantly reducing annotation burden <eos> second branch utilizes both holistic localized information coupling sentiment map deep feature robust classification <eos> integrate sentiment detection classification branches into unified deep framework optimize network end end manner <eos> extensive experiments six benchmark datasets demonstrate proposed method performs favorably against state ofthe art method visual sentiment analysis <eos> <eop> future person localization first person video <eos> present new task predicts future locations people observed first person video <eos> consider first person video stream continuously recorded wearable camera <eos> given short clip person extracted complete stream aim predict person location future frames <eos> facilitate future person localization ability make following three key observations first person video typically involve significant ego motion greatly affects location target person future frames scales target person act salient cue estimate perspective effect first person video first person video often capture people up close making easier leverage target poses <eos> they look predicting their future locations <eos> incorporate three observations into prediction framework multi stream convolution deconvolution architecture <eos> experimental result reveal method effective new dataset well public social interaction dataset <eos> <eop> preserving semantic relations zero shot learning <eos> zero shot learning gained popularity due its potential scale recognition models without requiring additional training data <eos> usually achieved associating categories their semantic information like attributes <eos> however believe potential offered paradigm yet fully exploited <eos> work propose utilize structure space spanned attributes using set relations <eos> devise objective functions preserve relations embedding space thereby inducing semanticity embedding space <eos> through extensive experimental evaluation five benchmark datasets demonstrate inducing semanticity embedding space beneficial zero shot learning <eos> proposed approach outperforms state art standard zero shot setting well more realistic generalized zero shot setting <eos> also demonstrate how proposed approach useful making approximate semantic inferences about image belonging category attribute information available <eos> <eop> show me story towards coherent neural story illustration <eos> propose end end network visual illustration sequence sentences forming story <eos> core model ability model inter related nature sentences within story well ability learn coherence support reference resolution <eos> framework takes form encoder decoder architecture sentences encoded using hierarchical two level sentence story gru combined encoding coherence sequentially decoded using predicted feature representation into consistent illustrative image sequence <eos> optimize all parameters network end end fashion respect order embedding loss encoding entailment between image sentences <eos> experiments vist storytelling dataset cite vist highlight importance algorithmic choices efficacy overall model <eos> <eop> reconstruction network video captioning <eos> paper problem describing visual contents video sequence natural language addressed <eos> unlike previous video captioning work mainly exploiting cues video contents make language description propose reconstruction network recnet novel encoder decoder reconstructor architecture leverages both forward video sentence backward sentence video flows video captioning <eos> specifically encoder decoder makes use forward flow produce sentence description based encoded video semantic feature <eos> two types reconstructors customized employ backward flow reproduce video feature based hidden state sequence generated decoder <eos> generation loss yielded encoder decoder reconstruction loss introduced reconstructor jointly drawn into training proposed recnet end end fashion <eos> experimental result benchmark datasets demonstrate proposed reconstructor could boost encoder decoder models leads significant gains video caption accuracy <eos> <eop> fast spectral ranking similarity search <eos> despite success deep learning representing image particular object retrieval recent studies show learned representations still lie manifolds high dimensional space <eos> makes euclidean nearest neighbor search biased task <eos> exploring manifolds online remains expensive even if nearest neighbor graph computed offline <eos> work introduces explicit embedding reducing manifold search euclidean search followed dot product similarity search <eos> equivalent linear graph filtering sparse signal frequency domain <eos> speed up online search compute approximate fourier basis graph offline <eos> improve state art particular object retrieval datasets including challenging instre dataset containing small object <eos> scale image offline cost only few hours while query time comparable standard similarity search <eos> <eop> mining manifolds metric learning without labels <eos> work present novel unsupervised framework hard training example mining <eos> only input method collection image relevant target application meaningful initial representation provided <eos> pre trained cnn <eos> positive examples distant point single manifold while negative examples nearby point different manifolds <eos> both types examples revealed disagreements between euclidean manifold similarities <eos> discovered examples used training any discriminative loss <eos> method applied unsupervised fine tuning pre trained network fine grained classification particular object retrieval <eos> models par outperforming prior models fully partially supervised <eos> <eop> pixor real time three dimensional object detection point clouds <eos> address problem real time three dimensional object detection point clouds context autonomous driving <eos> speed critical detection necessary component safety <eos> existing approaches however expensive computation due high dimensionality point clouds <eos> utilize three dimensional data more efficiently representing scene bird eye view bev propose pixor proposal free single stage detector outputs oriented three dimensional object estimates decoded pixel wise neural network predictions <eos> input representation network architecture model optimization specially designed balance high accuracy real time efficiency <eos> validate pixor two datasets kitti bev object detection benchmark large scale three dimensional vehicle detection benchmark <eos> both datasets show proposed detector surpasses other state art method notably terms average precision ap while still runs fps <eos> <eop> leveraging unlabeled data crowd counting learning rank <eos> propose novel crowd counting approach leverages abundantly available unlabeled crowd imagery learning rank framework <eos> induce ranking cropped image use observation any sub image crowded scene image guaranteed contain same number fewer persons than super image <eos> allows address problem limited size existing datasets crowd counting <eos> collect two crowd scene datasets google using keyword searches query example image retrieval respectively <eos> demonstrate how efficiently learn unlabeled datasets incorporating learning rank multi task network simultaneously ranks image estimates crowd density maps <eos> experiments two most challenging crowd counting datasets show approach obtains state art result <eos> <eop> zero shot kernel learning <eos> paper address open problem zero shot learning <eos> its principle based learning mapping associates feature vectors extracted <eos> image attribute vectors describe object scenes interest <eos> turns allows classifying unseen object classes scenes matching feature vectors via mapping newly defined attribute vector describing new class <eos> due importance such learning task there exist many method learn semantic probabilistic linear piece wise linear mappings <eos> contrast apply well established kernel method learn non linear mapping between feature attribute spaces <eos> propose easy learning objective orthogonality constraints inspired linear discriminant analysis kernel target alignment kernel polarization method <eos> evaluate performance algorithm polynomial well shift invariant gaussian cauchy kernels <eos> despite simplicity approach obtain state art result several zero shot learning datasets benchmarks including very recent awa dataset <eos> <eop> differential attention visual question answering <eos> paper aim answer questions based image when provided dataset question answer pairs number image during training <eos> number method focused solving problem using image based attention <eos> done focusing specific part image while answering question <eos> humans also so when solving problem <eos> however region previous systems focus correlated region humans focus <eos> accuracy limited due drawback <eos> paper propose solve problem using exemplar based method <eos> obtain one more supporting opposing exemplars obtain differential attention region <eos> differential attention closer human attention than other image based attention method <eos> also helps obtaining improved accuracy when answering questions <eos> method evaluated challenging benchmark datasets <eos> perform better than other image based attention method competitive other state art method focus both image questions <eos> <eop> learning noisy web data category level supervision <eos> learning web data increasingly popular due abundant free web resources <eos> however performance gap between webly supervised learning traditional supervised learning still very large due label noise web data <eos> fill gap most existing method propose purify augment web data using instance level supervision generally requires heavy annotation <eos> instead propose address label noise using more accessible category level supervision <eos> particular build deep probabilistic framework upon variational autoencoder vae classification network vae jointly leverage category level hybrid information <eos> extensive experiments three benchmark datasets demonstrate effectiveness proposed method <eos> <eop> toward driving scene understanding dataset learning driver behavior causal reasoning <eos> driving scene understanding key ingredient intelligent transportation systems <eos> achieve systems operate complex physical social environment they need understand learn how humans drive interact traffic scenes <eos> present honda research institute driving dataset hdd challenging dataset enable research learning driver behavior real life environments <eos> dataset includes hours real human driving san francisco bay area collected using instrumented vehicle equipped different sensors <eos> provide detailed analysis hdd comparison other driving datasets <eos> novel annotation methodology introduced enable research driver behavior understanding untrimmed data sequences <eos> first step baseline algorithms driver behavior detection trained tested demonstrate feasibility proposed task <eos> <eop> learning attribute representations localization flexible fashion search <eos> paper investigate ways conducting detailed fashion search using query image attributes <eos> credible fashion search platform should able find image share same attributes query image allow users manipulate certain attributes <eos> replace collar attribute round neck handle region specific attribute manipulations <eos> replacing color attribute sleeve region without changing color attribute other region <eos> key challenge addressed fashion products multiple attributes important each attributes representative feature <eos> address challenges propose fashionsearchnet uses weakly supervised localization method extract region attributes <eos> doing so unrelated feature ignored thus improving similarity learning <eos> also fashionsearchnet incorporates new procedure enables region awareness able handle region specific requests <eos> fashionsearchnet outperforms most recent fashion search techniques shown able carry out different search scenarios using dynamic queries <eos> <eop> bidirectional retrieval made simple <eos> paper provides very simple yet effective character level architecture learning bidirectional retrieval models <eos> aligning multimodal content particularly challenging considering difficulty finding semantic correspondence between image descriptions <eos> introduce efficient character level inception module designed learn textual semantic embeddings convolving raw characters distinct granularity levels <eos> approach capable explicitly encoding hierarchical information distinct base level representations <eos> characters words sentences into shared multimodal space maps semantic correspondence between image descriptions via contrastive pairwise loss function minimizes order violations <eos> models generated approach far more robust input noise than state art strategies based word embeddings <eos> despite being conceptually much simpler requiring fewer parameters models outperform state art approaches <eos> task description retrieval <eos> absolute values task image retrieval popular ms coco retrieval dataset <eos> finally show models present solid performance text classification well specially multilingual noisy domains <eos> <eop> learning multi instance enriched image representations via non greedy ratio maximization norm distances <eos> multi instance learning mil demonstrated its usefulness many real world image applications recent years <eos> however two critical challenges prevent one effectively using mil practice <eos> first existing mil method routinely model predictive targets using instances input image but rarely utilize input image whole <eos> result useful information conveyed holistic representation input image could potentially lost <eos> second varied numbers instances input image data set make infeasible use traditional learning models only deal single vector inputs <eos> tackle two challenges paper propose novel image representation learning method integrate local patches instances input image bag its holistic representation into one single vector representation <eos> new method first learns projection preserve both global local consistencies instances input image <eos> then projects holistic representation same image into learned subspace information enrichment <eos> taking into account content characterization variations natural scenes photos develop objective maximizes ratio summations number norm distances difficult solve general <eos> solve objective derive new efficient non greedy iterative algorithm rigorously prove its convergence <eos> promising result extensive experiments demonstrated improved performances new method validate its effectiveness <eos> <eop> learning visual knowledge memory network visual question answering <eos> visual question answering vqa requires joint comprehension image natural language questions many questions directly clearly answered visual content but require reasoning structured human knowledge confirmation visual content <eos> paper proposes visual knowledge memory network vkmn address issue seamlessly incorporates structured human knowledge deep visual feature into memory network end end learning framework <eos> comparing existing method leveraging external knowledge supporting vqa paper stresses more two missing mechanisms <eos> first mechanism integrating visual contents knowledge facts <eos> vkmn handles issue embedding knowledge triples subject relation target deep visual feature jointly into visual knowledge feature <eos> second mechanism handling multiple knowledge facts expanding question answer pairs <eos> vkmn stores joint embedding using key value pair structure memory network so easy handle multiple facts <eos> experiments show proposed method achieves promising result both vqa <eos> benchmarks while outperforms state art method knowledge reasoning related questions <eos> <eop> visual grounding via accumulated attention <eos> visual grounding vg aims locate most relevant object region image based natural language query <eos> query phrase sentence even multi round dialogue <eos> there three main challenges vg main focus query how understand image how locate object <eos> most existing method combine all information curtly may suffer problem information redundancy <eos> ambiguous query complicated image large number object <eos> paper formulate challenges three attention problems propose accumulated attention att mechanism reason among them jointly <eos> att mechanism circularly accumulate attention useful information image query object while noises ignored gradually <eos> evaluate performance att four popular datasets namely refercoco refercoco refercocog guesswhat experimental result show superiority proposed method term accuracy <eos> <eop> beyond trade off accelerate fcn based face detector higher accuracy <eos> fully convolutional neural network fcn dominating game face detection task few years its congenital capability sliding window searching shared kernels boiled down all redundant calculation most recent state art method such faster rcnn ssd yolo fpn use fcn their backbone <eos> so here comes one question find universal strategy further accelerate fcn higher accuracy so could accelerate all recent fcn based method analyze decompose face searching space into two orthogonal directions scale spatial <eos> only few coordinates space expanded two base vectors indicate foreground <eos> so if fcn could ignore most other point searching space false alarm should significantly boiled down <eos> based philosophy novel method named scale estimation spatial attention proposal ap proposed pay attention some specific scales image pyramid valid locations each scales layer <eos> furthermore adopt masked convolution operation based attention result accelerate fcn calculation <eos> experiments show fcn based method rpn accelerated about help ap masked fcn same time also achieve state art fddb afw malf face detection benchmarks well <eos> <eop> packnet adding multiple tasks single network iterative pruning <eos> paper presents method adding multiple tasks single deep neural network while avoiding catastrophic forgetting <eos> inspired network pruning techniques exploit redundancies large deep network free up parameters then employed learn new tasks <eos> performing iterative pruning network re training able sequentially pack multiple tasks into single network while ensuring minimal drop performance minimal storage overhead <eos> unlike prior work uses proxy losses maintain accuracy older tasks always optimize task hand <eos> perform extensive experiments variety network architectures large scale datasets observe much better robustness against catastrophic forgetting than prior work <eos> particular able add three fine grained classification tasks single imagenet trained vgg network achieve accuracies close separately trained network each task <eos> <eop> repulsion loss detecting pedestrians crowd <eos> detecting individual pedestrians crowd remains challenging problem since pedestrians often gather together occlude each other real world scenarios <eos> paper first explore how state art pedestrian detector harmed crowd occlusion via experimentation providing insights into crowd occlusion problem <eos> then propose novel bounding box regression loss specifically designed crowd scenes termed repulsion loss <eos> loss driven two motivations attraction target repulsion other surrounding object <eos> repulsion term prevents proposal shifting surrounding object thus leading more crowd robust localization <eos> detector trained repulsion loss outperforms state art method significant improvement occlusion cases <eos> <eop> neural sign language translation <eos> sign language recognition slr active research field last two decades <eos> however most research date considered slr naive gesture recognition problem <eos> slr seeks recognize sequence continuous signs but neglects underlying rich grammatical linguistic structures sign language differ spoken language <eos> contrast introduce sign language translation slt problem <eos> here objective generate spoken language translations sign language video taking into account different word orders grammar <eos> formalize slt framework neural machine translation nmt both end end pretrained settings using expert knowledge <eos> allows jointly learn spatial representations underlying language model mapping between sign spoken language <eos> evaluate performance neural slt collected first publicly available continuous slt dataset rwth phoenix weather <eos> provides spoken language translations gloss level annotations german sign language video weather broadcasts <eos> dataset contains over <eos> frames signs sign vocabulary words german vocabulary <eos> report quantitative qualitative result various slt setups underpin future research newly established field <eos> upper bound translation performance calculated <eos> bleu while end end frame level gloss level tokenization network were able achieve <eos> <eop> non local neural network <eos> both convolutional recurrent operations building blocks process one local neighborhood time <eos> paper present non local operations generic family building blocks capturing long range dependencies <eos> inspired classical non local means method computer vision non local operation computes response position weighted sum feature all positions <eos> building block plugged into many computer vision architectures <eos> task video classification even without any bells whistles non local models compete outperform current competition winners both kinetics charades datasets <eos> static image recognition non local models improve object detection segmentation pose estimation coco suite tasks <eos> code will made available <eos> <eop> lamv learning align match video kernelized temporal layer <eos> paper considers learnable approach comparing aligning video <eos> architecture builds upon revisits temporal match kernels within neural network propose new temporal layer finds temporal alignments maximizing scores between two sequences vectors according time sensitive similarity metric parametrized fourier domain <eos> learn layer temporal proposal strategy minimize triplet loss takes into account both localization accuracy recognition rate <eos> evaluate approach video alignment copy detection event retrieval <eos> approach outperforms state art temporal video alignment video copy detection datasets comparable setups <eos> also attains best reported result particular event search while precisely aligning video <eos> <eop> optimizing video object detection via scale time lattice <eos> high performance object detection relies expensive convolutional network compute feature often leading significant challenges applications <eos> re quire detecting object video streams real time <eos> key problem trade accuracy efficiency effective way <eos> reducing computing cost while maintaining competitive performance <eos> seek good balance previous efforts usually focus optimizing model architectures <eos> paper explores alternative approach reallocate computation over scale time space <eos> basic idea perform expensive detection sparsely propagate result across both scales time substantially cheaper network exploiting strong correlations among them <eos> specifically present unified framework integrates detection temporal propagation across scale refinement scale time lattice <eos> framework one explore various strategies balance performance cost <eos> taking advantage flexibility further develop adaptive scheme detector invoked demand thus obtain improved tradeoff <eos> imagenet vid dataset proposed method achieve competitive map <eos> fps performance speed tradeoff <eos> <eop> learning compressible video isomers <eos> standard video encoders developed conventional narrow field view video widely applied video well reasonable result <eos> however while approach commits arbitrarily projection spherical frames observe some orientations video once projected more compressible than others <eos> introduce approach predict sphere rotation will yield maximal compression rate <eos> given video clips their original encoding convolutional neural network learns association between clip visual content its compressibility different rotations cubemap projection <eos> given novel video learning based approach efficiently infers most compressible direction one shot without repeated rendering compression source video <eos> validate idea thousands video clips multiple popular video codecs <eos> result show untapped dimension compression substantial potential good rotations typically more compressible than bad ones learning approach predict them reliably time <eos> <eop> attention clusters purely attention based local feature integration video classification <eos> recently substantial research effort focused how apply cnn rnns better capture temporal patterns video so improve accuracy video classification <eos> paper however show temporal information especially longer term patterns may necessary achieve competitive result common trimmed video classification datasets <eos> investigate potential purely attention based local feature integration <eos> accounting characteristics such feature video classification propose local feature integration framework based attention clusters introduce shifting operation capture more diverse signals <eos> carefully analyze compare effect different attention mechanisms cluster sizes use shifting operation also investigate combination attention clusters multimodal integration <eos> demonstrate effectiveness framework three real world video classification datasets <eos> model achieves competitive result across all <eos> particular large scale kinetics dataset framework obtains excellent single model accuracy <eos> terms top accuracy validation set <eos> <eop> learned deep representations action recognition <eos> success deep models led their deployment all areas computer vision increasingly important understand how representations work they capturing <eos> paper shed light deep spatiotemporal representations visualizing two stream models learned order recognize actions video <eos> show local detectors appearance motion object arise form distributed representations recognizing human actions <eos> key observations include following <eos> first cross stream fusion enables learning true spatiotemporal feature rather than simply separate appearance motion feature <eos> second network learn local representations highly class specific but also generic representations serve range classes <eos> third throughout hierarchy network feature become more abstract show increasing invariance aspects data unimportant desired distinctions <eos> motion patterns across various speeds <eos> fourth visualizations used only shed light learned representations but also reveal idiosyncracies training data explain failure cases system <eos> <eop> controllable video generation sparse trajectories <eos> video generation manipulation important yet challenging task computer vision <eos> existing method usually lack ways explicitly control synthesized motion <eos> work present conditional video generation model allows detailed control over motion generated video <eos> given first frame sparse motion trajectories specified users model synthesize video corresponding appearance motion <eos> propose combine advantage copying pixels given frame hallucinating lightness difference scratch help generate sharp video while keeping model robust occlusion lightness change <eos> also propose training paradigm calculate trajectories video clips eliminated need annotated training data <eos> experiments several standard benchmarks demonstrate approach generate realistic video comparable state art video generation video prediction method while motion generated video correspond well user input <eos> <eop> representing learning high dimensional data optimal transport map probabilistic viewpoint <eos> paper propose generative model space diffeomorphic deformation maps <eos> more precisely utilize kantarovich wasserstein metric accompanying geometry represent image deformation templates <eos> moreover incorporate probabilistic viewpoint assuming each image locally generated reference image <eos> capture local structure modelling tangent planes reference image <eos> assume each image generated one finite number tangent planes <eos> unobserved discrete random variable indexes tangent plane image belongs <eos> once basis vectors each tangent plane learned via probabilistic pca sample local coordinate inverted back image space exactly <eos> experiments using different datasets show generative tangent plane model optimal transport ot manifold learned small numbers image used create infinitely many unseen image <eos> addition bayesian classification accompanied probabilist modeling tangent planes shows improved accuracy over done image space <eos> combining result experiments supports claim certain datasets better represented kantarovich wasserstein metric <eos> envision proposed method could practical solution learning representing data generated templates situatons only limited numbers data point available <eos> <eop> clip deep network compression learning parallel pruning quantization <eos> deep neural network enable state art accuracy visual recognition tasks such image classification object detection <eos> however modern deep network contain millions learned weights more efficient utilization computation resources would assist variety deployment scenarios embedded platforms resource constraints computing clusters running ensembles network <eos> paper combine network pruning weight quantization single learning framework performs pruning quantization jointly parallel fine tuning <eos> allows take advantage complementary nature pruning quantization recover premature pruning errors possible current two stage approaches <eos> proposed clip method compression learning parallel pruning quantization compresses alexnet fold googlenet fold resnet fold while preserving uncompressed network accuracies imagenet <eos> <eop> inference higher order mrf map problems small large cliques <eos> higher order mrf map formulation popular technique solving many problems computer vision <eos> inference general mrf map problem np hard but performed polynomial time special case when potential functions submodular <eos> two popular combinatorial approaches solving such formulations flow based polyhedral approaches <eos> flow based approaches work well small cliques mode handle problems millions variables <eos> polyhedral approaches handle large cliques but small numbers <eos> show paper variables seemingly disparate techniques mapped each other <eos> allows combine two styles joint framework exploiting strength both them <eos> using proposed joint framework able perform tractable inference mrf map problems millions variables mix small large cliques formulation solved either two styles individually <eos> show applicability hybrid framework object segmentation problem example situation quality result significantly better than systems based only use small large cliques <eos> <eop> road reality oriented adaptation semantic segmentation urban scenes <eos> exploiting synthetic data learn deep models attracted increasing attention recent years <eos> however intrinsic domain difference between synthetic real image usually causes significant performance drop when applying learned model real world scenarios <eos> mainly due two reasons model overfits synthetic image making convolutional filters incompetent extract informative representation real image there distribution difference between synthetic real data also known domain adaptation problem <eos> end propose new reality oriented adaptation approach urban scene semantic segmentation learning synthetic data <eos> first propose target guided distillation approach learn real image style achieved training segmentation model imitate pretrained real style model using real image <eos> second further take advantage intrinsic spatial structure presented urban scene image propose spatial aware adaptation scheme effectively align distribution two domains <eos> two modules readily integrated existing state art semantic segmentation network improve their generalizability when adapting synthetic real urban scenes <eos> evaluate proposed method cityscapes dataset adapting gtav synthia datasets result demonstrate effectiveness method <eos> <eop> eye painting exemplar generative adversarial network <eos> paper introduces novel approach painting identity object remove change preserved accounted inference time exemplar gans exgans <eos> exgans type conditional gan utilize exemplar information produce high quality personalized painting result <eos> propose using exemplar information form reference image region paint perceptual code describing object <eos> unlike previous conditional gan formulations extra information inserted multiple point within adversarial network thus increasing its descriptive power <eos> show exgans produce photo realistic personalized painting result both perceptually semantically plausible applying them task closed open eye painting natural pictures <eos> new benchmark dataset also introduced task eye painting future comparisons <eos> <eop> clcnet improving efficiency convolutional neural network using channel local convolutions <eos> depthwise convolution grouped convolution successfully applied improve efficiency convolutional neural network cnn <eos> suggest models considered special cases generalized convolution operation named channel local convolution clc output channel computed using subset input channels <eos> definition entails computation dependency relations between input output channels represented channel dependency graph cdg <eos> modifying cdg grouped convolution new clc kernel named interlaced grouped convolution igc created <eos> stacking igc gc kernels result convolution block named clc block approximating regular convolution <eos> resorting cdg analysis tool derive rule setting meta parameters igc gc framework minimizing computational cost <eos> new cnn model named clcnet then constructed using clc blocks shows significantly higher computational efficiency fewer parameters compared state art network when being tested using imagenet dataset <eos> <eop> towards effective low bitwidth convolutional neural network <eos> paper tackles problem training deep convolutional neural network both low precision weights low bitwidth activations <eos> optimizing low precision network very challenging since training process easily get trapped poor local minima result substantial accuracy loss <eos> mitigate problem propose three simple yet effective approaches improve network training <eos> first propose use two stage optimization strategy progressively find good local minima <eos> specifically propose first optimize net quantized weights then quantized activations <eos> contrast traditional method optimize them simultaneously <eos> second following similar spirit first method propose another progressive optimization approach progressively decreases bit width high precision low precision during course training <eos> third adopt novel learning scheme jointly train full precision model alongside low precision one <eos> doing so full precision model provides hints guide low precision model training <eos> extensive experiments various datasets ie cifar imagenet show effectiveness proposed method <eos> highlight using method train bit precision network leads no performance decrease comparison its full precision counterpart standard network architectures ie alexnet resnet <eos> <eop> stochastic downsampling cost adjustable inference improved regularization convolutional network <eos> desirable train convolutional network cnn run more efficiently during inference <eos> many cases however computational budget system inference cannot known beforehand during training inference budget dependent changing real time resource availability <eos> thus inadequate train just inference efficient cnn whose inference costs adjustable cannot adapt varied inference budgets <eos> propose novel approach cost adjustable inference cnn stochastic downsampling point sdpoint <eos> during training sdpoint applies feature map downsampling random point layer hierarchy random downsampling ratio <eos> different stochastic downsampling configurations known sdpoint instances same model computational costs different each other while being trained minimize same prediction loss <eos> sharing network parameters across different instances provides significant regularization boost <eos> during inference one may handpick sdpoint instance best fits inference budget <eos> effectiveness sdpoint both cost adjustable inference approach regularizer validated through extensive experiments image classification <eos> <eop> face aging identity preserved conditional generative adversarial network <eos> face aging great importance cross age recognition entertainment related applications <eos> however lack labeled faces same person across long age range makes challenging <eos> because different aging speed different persons face aging approach aims synthesizing face whose target age lies some given age group instead synthesizing face certain age <eos> grouping faces target age together objective face aging equivalent transferring aging patterns faces within target age group face whose aged face synthesized <eos> meanwhile synthesized face should same identity input face <eos> thus propose identity preserved conditional generative adversarial network ipcgans framework conditional generative adversarial network module functions generating face looks realistic target age identity preserved module preserves identity information age classifier forces generated face target age <eos> both qualitative quantitative experiments show method generate more realistic faces terms image quality person identity age consistency human observations <eos> <eop> unsupervised cross dataset person re identification transfer learning spatial temporal patterns <eos> most proposed person re identification algorithms conduct supervised training testing single labeled datasets small size so directly deploying trained models large scale real world camera network may lead poor performance due underfitting <eos> challenging incrementally optimize models using abundant unlabeled data collected target domain <eos> address challenge propose unsupervised incremental learning algorithm tfusion aided transfer learning pedestrians spatio temporal patterns target domain <eos> specifically algorithm firstly transfers visual classifier trained small labeled source dataset unlabeled target dataset so learn pedestrians spatial temporal patterns <eos> secondly bayesian fusion model proposed combine learned spatio temporal patterns visual feature achieve significantly improved classifier <eos> finally propose learning rank based mutual promotion procedure incrementally optimize classifiers based unlabeled data target domain <eos> comprehensive experiments based multiple real surveillance datasets conducted result show algorithm gains significant improvement compared state art cross dataset unsupervised person re identification algorithms <eos> <eop> feature quantization defending against distortion image <eos> work address problem improving robustness convolutional neural network cnn image distortion <eos> argue higher moment statistics feature distributions shifted due image distortion shift leads performance decrease cannot reduced ordinary normalization method observed experimental analyses <eos> order mitigate effect propose approach base feature quantization <eos> specific propose employ three different types additional non linearity cnn floor function scalable resolution ii power function learnable exponents iii power function data dependent exponents <eos> experiments observe cnn employ proposed method obtain better performance both generalization performance robustness various distortion types large scale benchmark datasets <eos> instance resnet model equipped proposed method hpow obtains <eos> better accuracy ilsvrc classification tasks using image distorted motion blur salt pepper mixed distortions <eos> <eop> tagging like humans diverse distinct image annotation <eos> work propose new automatic image annotation model dubbed diverse distinct image annotation ia <eos> generative model ia inspired ensemble human annotations create semantically relevant yet distinct diverse tags <eos> ia generate relevant distinct tag subset tags relevant image contents semantically distinct each other using sequential sampling determinantal point process dpp model <eos> multiple such tag subsets cover diverse semantic aspects diverse semantic levels image contents generated randomly perturbing dpp sampling process <eos> leverage generative adversarial network gan model train ia <eos> perform extensive experiments including quantitative qualitative comparisons well human subject studies two benchmark datasets demonstrate proposed model produce more diverse distinct tags than state arts <eos> <eop> re weighted adversarial adaptation network unsupervised domain adaptation <eos> unsupervised domain adaptation uda aims transfer domain knowledge existing well defined tasks new ones labels unavailable <eos> real world applications domain task discrepancies usually uncontrollable significantly motivated match feature distributions even if domain discrepancies disparate <eos> additionally no label available target domain how successfully adapt classifier source target domain still remains open question <eos> paper propose re weighted adversarial adaptation network raan reduce feature distribution divergence adapt classifier when domain discrepancies disparate <eos> specifically alleviate need common supports matching feature distribution choose minimize optimal transport ot based earth mover em distance reformulate minimax objective function <eos> utilizing raan trained end end adversarial manner <eos> further adapt classifier propose match label distribution embed into adversarial training <eos> finally after extensive evaluation method using uda datasets varying difficulty raan achieved state art result outperformed other method large margin when domain shifts disparate <eos> <eop> inferring semantic layout hierarchical text image synthesis <eos> propose novel hierarchical approach text image synthesis inferring semantic layout <eos> instead learning direct mapping text image algorithm decomposes generation process into multiple steps first constructs semantic layout text layout generator converts layout image image generator <eos> proposed layout generator progressively constructs semantic layout coarse fine manner generating object bounding boxes refining each box estimating object shapes inside box <eos> image generator synthesizes image conditioned inferred semantic layout provides useful semantic structure image matching text description <eos> model only generates semantically more meaningful image but also allows automatic annotation generated image user controlled generation process modifying generated scene layout <eos> demonstrate capability proposed model challenging ms coco dataset show model substantially improve image quality interpretability output semantic alignment input text over existing approaches <eos> <eop> regularizing rnns caption generation reconstructing past present <eos> recently caption generation encoder decoder framework extensively studied applied different domains such image captioning code captioning so <eos> paper propose novel architecture namely auto reconstructor network arnet coupling conventional encoder decoder framework works end end fashion generate captions <eos> arnet aims reconstructing previous hidden state present one besides behaving input dependent transition operator <eos> therefore arnet encourages current hidden state embed more information previous one help regularize transition dynamics recurrent neural network rnns <eos> extensive experimental result show proposed arnet boosts performance over existing encoder decoder models both image captioning source code captioning tasks <eos> additionally arnet remarkably reduces discrepancy between training inference processes caption generation <eos> furthermore performance permuted sequential mnist demonstrates arnet effectively regularize rnn especially modeling long term dependencies <eos> code available github <eos> com chenxinpeng arnet <eos> <eop> unsupervised domain adaptation similarity learning <eos> objective unsupervised domain adaptation leverage feature labeled source domain learn classifier unlabeled target domain similar but different data distribution <eos> most deep learning approaches consist two steps learn feature preserve low risk labeled sample source domain ii make feature both domains indistinguishable possible so classifier trained source also applied target domain <eos> general classifiers step consist fully connected layer applied directly indistinguishable feature learned ii <eos> paper propose different way classification using similarity learning <eos> proposed method learns pairwise similarity function classification performed computing distances between prototype representations each category <eos> domain invariant feature categorical prototype representations learned jointly end end fashion <eos> inference time image target domain compared prototypes label associated one best matches image outputed <eos> approach simple scalable effective <eos> show model achieves state art performance different large scale unsupervised domain adaptation scenarios <eos> <eop> learning deep sketch abstraction <eos> human free hand sketches studied various contexts including sketch recognition synthesis fine grained sketch based image retrieval fg sbir <eos> fundamental challenge sketch analysis deal drastically different human drawing styles particularly terms abstraction level <eos> work propose first stroke level sketch abstraction model based insight sketch abstraction process trading off between recognizability sketch number strokes used draw <eos> concretely train model abstract sketch generation through reinforcement learning stroke removal policy learns predict strokes safely removed without affecting recognizability <eos> show abstraction model used various sketch analysis tasks including modeling stroke saliency understanding decision sketch recognition models synthesizing sketches variable abstraction given category reference object instance photo training fg sbir model photos only bypassing expensive photo sketch pair collection step <eos> <eop> matching adversarial network <eos> generative adversarial nets gans conditonal gans cgans show using trained network loss function discriminator enables synthesize highly structured outputs <eos> however applying discriminator network universal loss function common supervised tasks <eos> semantic segmentation line detection depth estimation considerably less successful <eos> argue main difficulty applying cgans supervised tasks generator training consists optimizing loss function depend directly ground truth labels <eos> overcome propose replace discriminator matching network taking into account both ground truth outputs well generated examples <eos> consequence generator loss function also depends targets training examples thus facilitating learning <eos> demonstrate three computer vision tasks approach significantly outperform cgans achieving comparable superior result task specific solutions result stable training <eos> importantly general approach require use task specific loss functions <eos> <eop> sos rsc sum squares polynomial approach robustifying subspace clustering algorithms <eos> paper addresses problem subspace clustering presence outliers <eos> typically scenario handled through regularized optimization whose computational complexity scales polynomially size data <eos> further regularization terms need manually tuned achieve optimal performance <eos> circumvent difficulties paper propose outlier removal algorithm based evaluating suitable sum ofsquares polynomial computed directly data <eos> algorithm only requires performing two singular value decompositions fixed size provides certificates probability misclassifying outliers inliers <eos> <eop> resource aware person re identification across multiple resolutions <eos> all people equally easy identify color statistics might enough some cases while others might require careful reasoning about high low level details <eos> however prevailing person re identification re id method use one size fits all high level embeddings deep convolutional network all cases <eos> might limit their accuracy difficult examples makes them needlessly expensive easy ones <eos> remedy present new person re id model combines effective embeddings built multiple convolutional network layer trained deep supervision <eos> traditional re id benchmarks method improves substantially over previous state art result all five datasets evaluate <eos> then propose two new formulations person re id problem under resource constraints show how model used effectively trade off accuracy computation presence resource constraints <eos> <eop> learning using arrow time <eos> seek understand arrow time video makes video look like they playing forwards backwards visualize cues arrow time supervisory signal useful activity analysis end build three large scale video datasets apply learning based approach tasks <eos> learn arrow time efficiently reliably design convnet suitable extended temporal footprints class activation visualization study effect artificial cues such cinematographic conventions learning <eos> trained model achieves state art performance large scale real world video datasets <eos> through cluster analysis localization important region prediction examine learned visual cues consistent among many sample show when they occur <eos> lastly use trained convnet two applications self supervision action recognition video forensics determining whether hollywood film clips deliberately reversed time often used special effects <eos> <eop> neural style transfer via meta network <eos> paper propose noval method generate specified network parameters through one feed forward propagation meta network neural style transfer <eos> recent works style transfer typically need train image transformation network every new style style encoded network parameters enormous iterations stochastic gradient descent lacks generalization ability new style inference stage <eos> tackle issues build meta network takes style image generates corresponding image transformation network directly <eos> compared optimization based method every style meta network handle arbitrary new style within milliseconds one modern gpu card <eos> fast image transformation network generated meta network only kb capable real time running mobile device <eos> also investigate manifold style transfer network operating hidden feature meta network <eos> experiments well validated effectiveness method <eos> code trained models will released <eos> <eop> people penguins petri dishes adapting object counting models new visual domains object types without forgetting <eos> paper propose technique adapt convolutional neural network cnn based object counter additional visual domains object types while still preserving original counting function <eos> domain specific normalisation scaling operators trained allow model adjust statistical distributions various visual domains <eos> developed adaptation technique used produce singular patch based counting regressor capable counting various object types including people vehicles cell nuclei wildlife <eos> part study challenging new cell counting dataset context tissue culture patient diagnosis constructed <eos> new collection referred dublin cell counting dcc dataset first its kind made available wider computer vision community <eos> state art object counting performance achieved both shanghaitech parts penguins datasets while competitive performance observed trancos modified bone marrow mbm datasets all using shared counting model <eos> <eop> hydranets specialized dynamic architectures efficient inference <eos> there growing interest improving design deep network architectures both accurate low cost <eos> paper explores semantic specialization mechanism improving computational efficiency accuracy per unit cost inference context image classification <eos> specifically propose network architecture template called hydranet enables state art architectures image classification transformed into dynamic architectures exploit conditional execution efficient inference <eos> hydranets wide network containing distinct components specialized compute feature visually similar classes but they retain efficiency dynamically selecting only small number components evaluate any one input image <eos> design made possible soft gating mechanism encourages component specialization during training accurately performs component selection during inference <eos> evaluate hydranet approach both cifar imagenet classification tasks <eos> cifar applying hydranet template resnet densenet family models reduces inference cost while retaining accuracy baseline architectures <eos> imagenet applying hydranet template improves accuracy up <eos> when compared efficient baseline architecture similar inference cost <eos> <eop> sketchmate deep hashing million scale human sketch retrieval <eos> propose deep hashing framework sketch retrieval first time works multi million scale human sketch dataset <eos> leveraging large dataset explore few sketch specific traits were otherwise under studied prior literature <eos> instead following conventional sketch recognition task introduce novel problem sketch hashing retrieval only more challenging but also offers better testbed large scale sketch analysis since more fine grained sketch feature learning required accommodate large variations style abstraction ii compact binary code needs learned same time enable efficient retrieval <eos> key network design embedding unique characteristics human sketch two branch cnn rnn architecture adapted explore temporal ordering strokes ii novel hashing loss specifically designed accommodate both temporal abstract traits sketches <eos> sketch dataset show state art hashing models specifically engineered static image fail perform well temporal sketch data <eos> network other hand only offers best retrieval performance various code sizes but also yields best generalization performance under zero shot setting when re purposed sketch recognition <eos> such superior performances effectively demonstrate benefit sketch specific design <eos> <eop> source target back symmetric bi directional adaptive gan <eos> effectiveness gans producing image according specific visual domain shown potential unsupervised domain adaptation <eos> source labeled image modified mimic target sample training classifiers target domain inverse mappings target source domain also evaluated without new image generation <eos> paper aim getting best both worlds introducing symmetric mapping among domains <eos> jointly optimize bi directional image transformations combining them target self labeling <eos> define new class consistency loss aligns generators two directions imposing preserve class identity image passing through both domain mappings <eos> detailed analysis reconstructed image thorough ablation study extensive experiments six different settings confirm power approach <eos> <eop> ol orthogonal low rank embedding plug play geometric loss deep learning <eos> deep neural network trained using softmax layer top cross entropy loss ubiquitous tools image classification <eos> yet naturally enforce intra class similarity nor inter class margin learned deep representations <eos> simultaneously achieve two goals different solutions proposed literature such pairwise triplet losses <eos> however carry extra task selecting pairs triplets extra computational burden computing learning many combinations them <eos> paper propose plug play loss term deep network explicitly reduces intra class variance enforces inter class margin simultaneously simple elegant geometric manner <eos> each class deep feature collapsed into learned linear subspace union them inter class subspaces pushed orthogonal possible <eos> proposed orthogonal low rank embedding ole require carefully crafting pairs triplets sample training works standalone classification loss being first reported deep metric learning framework its kind <eos> because improved margin between feature different classes resulting deep network generalize better more discriminative more robust <eos> demonstrate improved classification performance general object recognition plugging proposed loss term into existing off shelf architectures <eos> particular show advantage proposed loss small data model scenario significantly advance state art stanford stl benchmark <eos> <eop> efficient parametrization multi domain deep neural network <eos> practical limitation deep neural network their high degree specialization single task visual domain <eos> complex applications such mobile platforms requires juggling several large models detrimental effect speed battery life <eos> recently inspired successes transfer learning several authors proposed learn instead universal fixed feature extractors used first stage any deep network work well all tasks domains simultaneously <eos> nevertheless such universal feature still somewhat inferior specialized network <eos> overcome limitation paper propose consider instead universal parametric families neural network still contain specialized problem specific models but differ only small number parameters <eos> study different designs such parametrizations including series parallel residual adapters regularization strategies parameter allocations empirically identify ones yield highest compression <eos> show order maximize performance necessary adapt both shallow deep layer deep network but required changes very small <eos> also show universal parametrization very effective transfer learning they outperform traditional fine tuning techniques <eos> <eop> deep density clustering unconstrained faces <eos> paper consider problem grouping collection unconstrained face image number subjects known <eos> propose unsupervised clustering algorithm called deep density clustering ddc based measuring density affinities between local neighborhoods feature space <eos> learning minimal covering sphere each neighborhood information about underlying structure encapsulated <eos> encapsulation also capable locating high density region neighborhood aids measuring neighborhood similarity <eos> theoretically show encapsulation asymptotically converges parzen window density estimator <eos> experiments show ddc superior candidate clustering unconstrained faces when number subjects unknown <eos> unlike conventional linkage density based method sensitive selection operating point ddc attains more consistent improved performance <eos> furthermore density aware property reduces difficulty finding appropriate operating point <eos> <eop> geometric multi model fitting convex relaxation algorithm <eos> propose novel method fitting multiple geometric models multi structural data via convex relaxation <eos> unlike greedy method maximise number inliers approach efficiently searches soft assignment point geometric models minimising energy overall assignment <eos> inherently parallel nature approach compared sequential approach found state art energy minimisation techniques allows elegant treatment scaling factor occurs number feature data increases <eos> result energy minimisation per iteration much two orders magnitude faster comparable architectures thus bringing real time robust performance wider set geometric multi model fitting problems <eos> demonstrate versatility approach two canonical problems estimating structure image plane extraction rgb image homography estimation pairs image <eos> approach seamlessly adapts different metrics brought forth distinct problems <eos> both cases report result publicly available data set most instances outperform state art while simultaneously presenting run times much order magnitude faster <eos> <eop> fast robust estimation unit norm constrained linear fitting problems <eos> estimator using iteratively reweighted least squares irls one best known method robust estimation <eos> however irls ineffective robust unit norm constrained linear fitting uclf problems such fundamental matrix estimation because poor initial solution <eos> overcome problem developing novel objective function its optimization named iteratively reweighted eigenvalues minimization irem <eos> irem guaranteed decrease objective function achieves fast convergence high robustness <eos> robust fundamental matrix estimation irem performs approximately times faster than random sampling consensus ransac while preserving comparable superior robustness <eos> <eop> importance weighted adversarial nets partial domain adaptation <eos> paper proposes importance weighted adversarial nets based method unsupervised domain adaptation specific partial domain adaptation target domain less number classes compared source domain <eos> previous domain adaptation method generally assume identical label spaces such reducing distribution divergence leads feasible knowledge transfer <eos> however such assumption no longer valid more realistic scenario requires adaptation larger more diverse source domain smaller target domain less number classes <eos> paper extends adversarial nets based domain adaptation proposes novel adversarial nets based partial domain adaptation method identify source sample potentially outlier classes same time reduce shift shared classes between domains <eos> <eop> efficient subpixel refinement symbolic linear predictors <eos> present efficient subpixel refinement method using learning based approach called linear predictors <eos> firstly present novel technique called symbolic linear predictors makes learning step efficient subpixel refinement <eos> makes approach feasible online applications without compromising accuracy while taking advantage run time efficiency learning based approaches <eos> secondly show how linear predictors used predict expected alignment error allowing use only best keypoints resource constrained applications <eos> show efficiency accuracy method through extensive experiments <eos> <eop> scale recurrent network deep image deblurring <eos> single image deblurring coarse fine scheme <eos> gradually restoring sharp image different resolutions pyramid very successful both traditional optimization based method recent neural network based approaches <eos> paper investigate strategy propose scale recurrent network srn deblurnet deblurring task <eos> compared many recent learning based approaches simpler network structure smaller number parameters easier train <eos> evaluate method large scale deblurring datasets complex motion <eos> result show method produce better quality result than state arts both quantitatively qualitatively <eos> <eop> deblurgan blind motion deblurring using conditional adversarial network <eos> present deblurgan end end learned method motion deblurring <eos> learning based conditional gan content loss <eos> deblurgan achieves state art performance both structural similarity measure visual appearance <eos> quality deblurring model also evaluated novel way real world problem object detection de blurred image <eos> method times faster than closest competitor deepdeblur <eos> also introduce novel method generating synthetic motion blurred image sharp ones allowing realistic dataset augmentation <eos> model code dataset available github <eos> com kupynorest deblurgan <eop> rl aesthetics aware reinforcement learning image cropping <eos> image cropping aims improving aesthetic quality image adjusting their composition <eos> most weakly supervised cropping method without bounding box supervision rely sliding window mechanism <eos> sliding window mechanism requires fixed aspect ratios limits cropping region arbitrary size <eos> moreover sliding window method usually produces tens thousands windows input image very time consuming <eos> motivated challenges firstly formulate aesthetic image cropping sequential decision making process propose weakly supervised aesthetics aware reinforcement learning rl framework address problem <eos> particularly proposed method develops aesthetics aware reward function especially benefits image cropping <eos> similar human decision making use comprehensive state representation including both current observation historical experience <eos> train agent using actor critic architecture end end manner <eos> agent evaluated several popular unseen cropping datasets <eos> experiment result show method achieves state art performance much fewer candidate windows much less time compared previous weakly supervised method <eos> <eop> single image dehazing via conditional generative adversarial network <eos> paper present algorithm directly restore clear image hazy image <eos> problem highly ill posed most existing algorithms often use hand crafted feature <eos> dark channel color disparity maximum contrast estimate transmission maps then atmospheric lights <eos> contrast solve problem based conditional generative adversarial network cgan clear image estimated end end trainable neural network <eos> different generative network basic cgan propose encoder decoder architecture so generate better result <eos> generate realistic clear image further modify basic cgan formulation introducing vgg feature regularized gradient prior <eos> also synthesize hazy dataset including indoor outdoor scenes train evaluate proposed algorithm <eos> extensive experimental result demonstrate proposed method performs favorably against state art method both synthetic dataset real world hazy image <eos> <eop> duality between retinex image dehazing <eos> image dehazing deals removal undesired loss visibility outdoor image due presence fog <eos> retinex color vision model mimicking ability human visual system robustly discount varying illuminations when observing scene under different spectral lighting conditions <eos> retinex widely explored computer vision literature image enhancement other related tasks <eos> while two problems apparently unrelated goal work show they connected simple linear relationship <eos> specifically most retinex based algorithms characteristic feature always increasing image brightness turns them into ideal candidates effective image dehazing directly applying retinex hazy image whose intensities inverted <eos> paper give theoretical proof retinex inverted intensities solution image dehazing problem <eos> comprehensive qualitative quantitative result indicate several classical modern implementations retinex transformed into competing image dehazing algorithms performing pair more complex fog removal method overcome some main challenges associated problem <eos> <eop> arbitrary style transfer deep feature reshuffle <eos> paper introduces novel method reshuffling deep feature <eos> permuting spacial locations feature map style image arbitrary style transfer <eos> theoretically prove new style loss based reshuffle connects both global local style losses respectively used most parametric non parametric neural style transfer method <eos> simple idea effectively address challenging issues existing style transfer method <eos> one hand avoid distortions local style patterns allow semantic level transfer compared neural parametric method <eos> other hand preserve globally similar appearance style image avoid wash out artifacts compared neural non parametric method <eos> based proposed loss also present progressive feature domain optimization approach <eos> experiments show method widely applicable various styles produces better quality than existing method <eos> <eop> nonlocal low rank tensor factor analysis image restoration <eos> low rank signal modeling widely leveraged capture non local correlation image processing applications <eos> propose new method employs low rank tensor factor analysis tensors generated grouped image patches <eos> low rank tensors fed into alternative direction multiplier method admm further improve image reconstruction <eos> motivating application compressive sensing cs deep convolutional architecture adopted approximate expensive matrix inversion cs applications <eos> iterative algorithm based low rank tensor factorization strategy called nlr tfa presented detail <eos> experimental result noiseless noisy cs measurements demonstrate superiority proposed approach especially low cs sampling rates <eos> <eop> avatar net multi scale zero shot style transfer feature decoration <eos> zero shot artistic style transfer important image synthesis problem aiming transferring arbitrary style into content image <eos> however trade off between generalization efficiency existing method impedes high quality zero shot style transfer real time <eos> paper resolve dilemma propose efficient yet effective avatar net enables visually plausible multi scale transfer arbitrary style <eos> key ingredient method style decorator makes up content feature semantically aligned style feature arbitrary style image only holistically match their feature distributions but also preserve detailed style patterns decorated feature <eos> embedding module into image reconstruction network fuses multi scale style abstractions avatar net renders multi scale stylization any style image one feed forward pass <eos> demonstrate state art effectiveness efficiency proposed method generating high quality stylized image series successive applications include multiple style integration video stylization etc <eos> <eop> missing slice recovery tensors using low rank model embedded space <eos> let consider case all elements some continuous slices missing tensor data <eos> case nuclear norm total variation regularization method usually fail recover missing elements <eos> key problem capturing some delay shift invariant structure <eos> study consider low rank model embedded space tensor <eos> purpose extend delay embedding time series multi way delay embedding transform tensor takes given incomplete tensor input outputs higher order incomplete hankel tensor <eos> higher order tensor then recovered tucker based low rank tensor factorization <eos> finally estimated tensor obtained using inverse multi way delay embedding transform recovered higher order tensor <eos> experiments showed proposed method successfully recovered missing slices some color image functional magnetic resonance image <eos> <eop> deep semantic face deblurring <eos> paper present effective efficient face deblurring algorithm exploiting semantic cues via deep convolutional neural network cnn <eos> face image highly structured share several key semantic components <eos> eyes mouths semantic information face provides strong prior restoration <eos> such propose incorporate global semantic priors input impose local structure losses regularize output within multi scale deep cnn <eos> train network perceptual adversarial losses generate photo realistic result develop incremental training strategy handle random blur kernels wild <eos> quantitative qualitative evaluations demonstrate proposed face deblurring algorithm restores sharp image more facial details performs favorably against state art method terms restoration quality face recognition execution speed <eos> <eop> graphbit bitwise interaction mining via deep reinforcement learning <eos> paper propose graphbit method learn deep binary descriptors directed acyclic graph unsupervisedly representing bitwise interactions edges between nodes bits <eos> conventional binary representation learning method enforce each element binarized into zero one <eos> however there elements lying boundary suffer doubtful binarization ambiguous bits <eos> ambiguous bits fail collect effective information confident binarization unreliable sensitive noise <eos> argue there implicit inner relationships between bits binary descriptors related bits provide extra instruction prior knowledge ambiguity elimination <eos> specifically design deep reinforcement learning model learn structure graph bitwise interaction mining reducing uncertainty binary codes maximizing mutual information inputs related bits so ambiguous bits receive additional instruction graph confident binarization <eos> due reliability proposed binary codes bitwise interaction obtain average improvement <eos> cifar brown hpatches datasets respectively compared state art unsupervised binary descriptors <eos> <eop> recurrent saliency transformation network incorporating multi stage visual cues small organ segmentation <eos> aim segmenting small organs <eos> pancreas abdominal ct scans <eos> target often occupies relatively small region input image deep neural network easily confused complex variable background <eos> alleviate researchers proposed coarse fine approach used prediction first coarse stage indicate smaller input region second fine stage <eos> despite its effectiveness algorithm dealt two stages individually lacked optimizing global energy function limited its ability incorporate multi stage visual cues <eos> missing contextual information led unsatisfying convergence iterations fine stage sometimes produced even lower segmentation accuracy than coarse stage <eos> paper presents recurrent saliency transformation network <eos> key innovation saliency transformation module repeatedly converts segmentation probability map previous iteration spatial weights applies weights current iteration <eos> brings two fold benefits <eos> training allows joint optimization over deep network dealing different input scales <eos> testing propagates multi stage visual information throughout iterations improve segmentation accuracy <eos> experiments nih pancreas segmentation dataset demonstrate state art accuracy outperforms previous best average over <eos> much higher accuracies also reported several small organs larger dataset collected ourselves <eos> addition approach enjoys better convergence properties making more efficient reliable practice <eos> <eop> thoracic disease identification localization limited supervision <eos> accurate identification localization abnormalities radiology image play integral part clinical diagnosis treatment planning <eos> building highly accurate prediction model tasks usually requires large number image manually annotated labels finding sites abnormalities <eos> reality however such annotated data expensive acquire especially ones location annotations <eos> need method work well only small amount location annotations <eos> address challenge present unified approach simultaneously performs disease identification localization through same underlying model all image <eos> demonstrate approach effectively leverage both class information well limited location annotation significantly outperforms comparative reference baseline both classification localization tasks <eos> <eop> quantization fully convolutional network accurate biomedical image segmentation <eos> pervasive applications medical imaging healthcare biomedical image segmentation plays central role quantitative analysis clinical diagnosis medical intervention <eos> since manual annotation suffers limited reproducibility arduous efforts excessive time automatic segmentation desired process increasingly larger scale histopathological data <eos> recently deep neural network dnns particularly fully convolutional network fcns widely applied biomedical image segmentation attaining much improved performance <eos> same time quantization dnns become active research topic aims represent weights less memory precision considerably reduce memory computation requirements dnns while maintaining acceptable accuracy <eos> paper apply quantization techniques fcns accurate biomedical image segmentation <eos> unlike existing literature quantization primarily targets memory computation complexity reduction apply quantization method reduce overfitting fcns better accuracy <eos> specifically focus state art segmentation framework suggestive annotation judiciously extracts representative annotation sample original training dataset obtaining effective small sized balanced training dataset <eos> develop two new quantization processes framework suggestive annotation quantization highly representative training sample network training quantization high accuracy <eos> extensive experiments miccai gland dataset show both quantization processes improve segmentation performance proposed method exceeds current state art performance up <eos> addition method reduction up <eos> <eop> visual feature attribution using wasserstein gans <eos> attributing pixels input image certain category important well studied problem computer vision applications ranging weakly supervised localisation understanding hidden effects data <eos> recent years approaches based interpreting previously trained neural network classifier become de facto state art commonly used medical well natural image datasets <eos> paper discuss limitation approaches may lead only subset category specific feature being detected <eos> address problem develop novel feature attribution technique based wasserstein generative adversarial network wgan suffer limitation <eos> show proposed method performs substantially better than state art visual attribution synthetic dataset real three dimensional neuroimaging data patients mild cognitive impairment mci alzheimer disease ad <eos> ad patients method produces compellingly realistic disease effect maps very close observed effects <eos> <eop> total capture three dimensional deformation model tracking faces hands bodies <eos> present unified deformation model markerless capture multiple scales human movement including facial expressions body motion hand gestures <eos> initial model generated locally stitching together models individual parts human body refer frankenstein model <eos> model enables full expression part movements including face hands single seamless model <eos> using large scale capture people wearing everyday clothes optimize frankenstein model create adam <eos> adam model shares same skeleton hierarchy initial model but express hair clothing geometry making directly usable fitting people they normally appear everyday life <eos> finally demonstrate use models total motion tracking method simultaneously capturing large scale body movements subtle face hand motion social group people <eos> <eop> augmented skeleton space transfer depth based hand pose estimation <eos> crucial success training depth based three dimensional hand pose estimator hpe availability comprehensive datasets covering diverse camera perspectives shapes pose variations <eos> however collecting such annotated datasets challenging <eos> propose complete existing databases generating new database entries <eos> key idea synthesize data skeleton space instead doing so depth map space enables easy intuitive way manipulating data entries <eos> since skeleton entries generated way corresponding depth map entries exploit them training separate hand pose generator hpg synthesizes depth map skeleton entries <eos> training hpg hpe single unified optimization framework enforcing hpe agrees paired depth skeleton entries hpg hpe combination satisfies cyclic consistency both input output hpg hpe skeletons observed via newly generated unpaired skeletons algorithm constructs hpe robust variations go beyond coverage existing database <eos> training algorithm adopts generative adversarial network gan training process <eos> product obtain hand pose discriminator hpd capable picking out realistic hand poses <eos> algorithm exploits capability refine initial skeleton estimates testing further improving accuracy <eos> test algorithm four challenging benchmark datasets icvl msra nyu big hand <eos> datasets demonstrate approach outperforms par state art method quantitatively qualitatively <eos> <eop> synthesizing image humans unseen poses <eos> address computational problem novel human pose synthesis <eos> given image person desired pose produce depiction person pose retaining appearance both person background <eos> present modular generative neural network synthesizes unseen poses using training pairs image poses taken human action video <eos> network separates scene into different body part background layer moves body parts new locations refines their appearances composites new foreground hole filled background <eos> subtasks implemented separate modules trained jointly using only single target image supervised label <eos> use adversarial discriminator force network synthesize realistic details conditioned pose <eos> demonstrate image synthesis result three action classes golf yoga workouts tennis show method produces accurate result within action classes well across action classes <eos> given sequence desired poses also produce coherent video actions <eos> <eop> ssnet scale selection network online three dimensional action prediction <eos> action prediction early action recognition goal predict class label ongoing action using its observed part so far <eos> paper focus online action prediction streaming three dimensional skeleton sequences <eos> dilated convolutional network introduced model motion dynamics temporal dimension via sliding window over time axis <eos> there significant temporal scale variations observed part ongoing action different progress levels propose novel window scale selection scheme make network focus performed part ongoing action try suppress noise previous actions each time step <eos> furthermore activation sharing scheme proposed deal overlapping computations among adjacent steps allows model run more efficiently <eos> extensive experiments two challenging datasets show effectiveness proposed action prediction framework <eos> <eop> detecting recognizing human object interactions <eos> understand visual world machine must only recognize individual object instances but also how they interact <eos> humans often center such interactions detecting human object interactions important practical scientific problem <eos> paper address task detecting human verb object triplets challenging everyday photos <eos> propose novel model driven human centric approach <eos> hypothesis appearance person their pose clothing action powerful cue localizing object they interacting <eos> exploit cue model learns predict action specific density over target object locations based appearance detected person <eos> model also jointly learns detect people object fusing predictions efficiently infers interaction triplets clean jointly trained end end system call interactnet <eos> validate approach recently introduced verbs coco coco hico det datasets show quantitatively compelling result <eos> <eop> unsupervised learning segmentation complex activities video <eos> paper presents new method unsupervised segmentation complex activities video into multiple steps sub activities without any textual input <eos> propose iterative discriminative generative approach alternates between discriminatively learning appearance sub activities video visual feature sub activity labels generatively modelling temporal structure sub activities using generalized mallows model <eos> addition introduce model background account frames unrelated actual activities <eos> approach validated challenging breakfast actions inria instructional video datasets outperforms both unsupervised weakly supervised state art <eos> <eop> unsupervised training three dimensional morphable model regression <eos> present method training regression network image pixels three dimensional morphable model coordinates using only unlabeled photographs <eos> training loss based feature facial recognition network computed fly rendering predicted faces differentiable renderer <eos> make training feature feasible avoid network fooling effects introduce three objectives batch distribution loss encourages output distribution match distribution morphable model loopback loss ensures network correctly reinterpret its own output multi view identity loss compares feature predicted three dimensional face input photograph multiple viewing angles <eos> train regression network using objectives set unlabeled photographs morphable model itself demonstrate state art result <eos> <eop> video based reconstruction three dimensional people models <eos> paper describes how obtain accurate three dimensional body models texture arbitrary people single monocular video person moving <eos> based parametric body model present robust processing pipeline achieving three dimensional model fits mm accuracy also clothed people <eos> main contribution method nonrigidly deform silhouette cones corresponding dynamic human silhouettes resulting visual hull common reference frame enables surface reconstruction <eos> enables efficient estimation consensus three dimensional shape texture implanted animation skeleton based large number frames <eos> present evaluation result number test subjects analyze overall performance <eos> requiring only smartphone webcam method enables everyone create their own fully animatable digital double <eos> social vr applications virtual try online fashion shopping <eos> <eop> pose guided photorealistic face rotation <eos> face rotation provides effective cheap way data augmentation representation learning face recognition <eos> challenging generative learning problem due large pose discrepancy between two face image <eos> work focuses flexible face rotation arbitrary head poses including extreme profile views <eos> propose novel couple agent pose guided generative adversarial network capg gan generate both neutral profile head pose face image <eos> head pose information encoded facial landmark heatmaps <eos> only forms mask image guide generator learning process but also provides flexible controllable condition during inference <eos> couple agent discriminator introduced reinforce realism synthetic arbitrary view faces <eos> besides generator conditional adversarial loss capg gan further employs identity preserving loss total variation regularization preserve identity information refine local textures respectively <eos> quantitative qualitative experimental result multi pie lfw databases consistently show superiority face rotation method over state art <eos> <eop> mesoscopic facial geometry inference using deep neural network <eos> present learning based approach synthesizing facial geometry medium fine scales diffusely lit facial texture maps <eos> when applied image sequence synthesized detail temporally coherent <eos> unlike current state art method assume dark deep model trained measured facial detail collected using polarized gradient illumination light stage <eos> enables produce plausible facial detail across entire face including previous approaches may incorrectly interpret dark feature concavities such moles hair stubble occluded pores <eos> instead directly inferring three dimensional geometry propose encode fine details high resolution displacement maps learned through hybrid network adopting state art image image translation network super resolution network <eos> effectively capture geometric detail both mid high frequencies factorize learning into two separate sub network enabling full range facial detail modeled <eos> result learning based approach compare favorably high quality active facial scanning technique require only single passive lighting condition without complex scanning setup <eos> <eop> hand pointnet three dimensional hand pose estimation using point set <eos> convolutional neural network cnn shown promising result three dimensional hand pose estimation depth image <eos> different existing cnn based hand pose estimation method take either image three dimensional volumes input proposed hand pointnet directly processes three dimensional point cloud models visible surface hand pose regression <eos> taking normalized point cloud input proposed hand pose regression network able capture complex hand structures accurately regress low dimensional representation three dimensional hand pose <eos> order further improve accuracy fingertips design fingertip refinement network directly takes neighboring point estimated fingertip location input refine fingertip location <eos> experiments three challenging hand pose datasets show proposed method outperforms state art method <eos> <eop> seeing voices hearing faces cross modal biometric matching <eos> introduce seemingly impossible task given only audio clip someone speaking decide two face image speaker <eos> paper study number related cross modal tasks aimed answering question how much infer voice about face vice versa study task wild employing datasets now publicly available face recognition static image vggface speaker identification audio voxceleb <eos> provide training testing scenarios both static dynamic testing cross modal matching <eos> make following contributions introduce cnn architectures both binary multi way cross modal face audio matching ii compare dynamic testing video information available but audio same video static testing only single still image available iii use hu man testing baseline calibrate difficulty task <eos> show cnn indeed trained solve task both static dynamic scenarios even well above chance way classification face given voice <eos> cnn matches human performance easy examples <eos> different gender across faces but exceeds human performance more challenging examples <eos> faces same gender age nationality <eos> <eop> learning monocular three dimensional human pose estimation multi view image <eos> accurate three dimensional human pose estimation single image possible sophisticated deep net architectures trained very large datasets <eos> however still leaves open problem capturing motions no such database exists <eos> manual annotation tedious slow error prone <eos> paper propose replace most annotations use multiple views training time only <eos> specifically train system predict same pose all views <eos> such consistency constraint necessary but sufficient predict accurate poses <eos> therefore complement supervised loss aiming predict correct pose small set labeled image regularization term penalizes drift initial predictions <eos> furthermore propose method estimate camera pose jointly human pose lets utilize multi view footage calibration difficult <eos> pan tilt moving handheld cameras <eos> demonstrate effectiveness approach established benchmarks well new ski dataset rotating cameras expert ski motion annotations truly hard obtain <eos> <eop> separating style content generalized style transfer <eos> neural style transfer drawn broad attention recent years <eos> however most existing method aim explicitly model transformation between different styles learned model thus generalizable new styles <eos> here attempt separate representations styles contents propose generalized style transfer network consisting style encoder content encoder mixer decoder <eos> style encoder content encoder used extract style content factors style reference image content reference image respectively <eos> mixer employs bilinear model integrate above two factors finally feeds into decoder generate image target style content <eos> separate style feature content feature leverage conditional dependence styles contents given image <eos> during training encoder network learns extract styles contents two set reference image limited size one shared style other shared content <eos> learning framework allows simultaneous style transfer among multiple styles deemed special multi task learning scenario <eos> encoders expected capture underlying feature different styles contents generalizable new styles contents <eos> validation applied proposed algorithm chinese typeface transfer problem <eos> extensive experiment result character generation demonstrated effectiveness robustness method <eos> <eop> texturegan controlling deep image synthesis texture patches <eos> paper investigate deep image synthesis guided sketch color texture <eos> previous image synthesis method controlled sketch color strokes but first examine texture control <eos> allow user place texture patch sketch arbitrary locations scales control desired output texture <eos> generative network learns synthesize object consistent texture suggestions <eos> achieve develop local texture loss addition adversarial content loss train generative network <eos> conduct experiments using sketches generated real image textures sampled separate texture database result show proposed algorithm able generate plausible image faithful user controls <eos> ablation studies show proposed pipeline generate more realistic image than adapting existing method directly <eos> <eop> connecting pixels privacy utility automatic redaction private information image <eos> image convey broad spectrum personal information <eos> if such image shared social media platforms personal information leaked conflicts privacy depicted persons <eos> therefore aim automated approaches redact such private information thereby protect privacy individual <eos> conducting user study find obfuscating image region related private information leads privacy while retaining utility image <eos> moreover varying size region different privacy utility trade offs achieved <eos> findings argue redaction segmentation paradigm <eos> hence propose first sizable dataset private image wild annotated pixel instance level labels across broad range privacy classes <eos> present first model automatic redaction diverse private information <eos> effective achieving various privacy utility trade offs within performance redactions based ground truth annotation <eos> <eop> mapnet allocentric spatial memory mapping environments <eos> autonomous agents need reason about world beyond their instantaneous sensory input <eos> integrating information over time however requires switching egocentric representation scene allocentric one expressed world reference frame <eos> must also possible update representation dynamically requires localizing registering sensor respect <eos> paper develop differentiable module satisfies such requirements while being robust efficient suitable integration end end deep network <eos> module contains allocentric spatial memory accessed associatively feeding current sensory input resulting localization then updated using lstm similar mechanism <eos> formulate efficient localization registration sensory information dual pair convolution deconvolution operators memory space <eos> representation environment storing information deep neural network module learns distill rgbd input <eos> result map contains multi task information different classical approaches mapping such structure motion <eos> present result using synthetic mazes dataset hours recorded gameplay classic game doom very recent active vision dataset real image captured robot <eos> <eop> accurate diverse sampling sequences based best many sample objective <eos> autonomous agents successfully operate real world anticipation future events states their environment key competence <eos> problem formalized sequence extrapolation problem number observations used predict sequence into future <eos> real world scenarios demand model uncertainty such predictions predictions become increasingly uncertain particular long time horizons <eos> while impressive result shown point estimates scenarios induce multi modal distributions over future sequences remain challenging <eos> work addresses challenges gaussian latent variable model sequence prediction <eos> core contribution best many sample objective leads more accurate more diverse predictions better capture true variations real world sequence data <eos> beyond analysis improved model fit models also empirically outperform prior work three diverse tasks ranging traffic scenes weather data <eos> <eop> virtualhome simulating household activities via programs <eos> paper interested modeling complex activities occur typical household <eos> propose use programs <eos> sequences atomic actions interactions high level representation complex tasks <eos> programs interesting because they provide non ambiguous representation task allow agents execute them <eos> however nowadays there no database providing type information <eos> towards goal first crowd source programs variety activities happen people homes via game like interface used teaching kids how code <eos> using collected dataset show how learn extract programs directly natural language descriptions video <eos> then implement most common atomic inter actions unity game engine use programs drive artificial agent execute tasks simulated household environment <eos> virtualhome simulator allows create large activity video dataset rich ground truth enabling training testing video understanding models <eos> further showcase examples agent performing tasks virtualhome based language <eop> generate adapt aligning domains using generative adversarial network <eos> domain adaptation actively researched problem computer vision <eos> work propose approach leverages unsupervised data bring source target distributions closer learned joint feature space <eos> accomplish inducing symbiotic relationship between learned embedding generative adversarial network <eos> contrast method use adversarial framework realistic data generation retraining deep models such data <eos> demonstrate strength generality approach performing experiments three different tasks varying levels difficulty digit classification mnist svhn usps datasets object recognition using office dataset domain adaptation synthetic real data <eos> method achieves state art performance most experimental settings far only gan based method shown work well across different datasets such office digits <eos> <eop> multi agent diverse generative adversarial network <eos> propose mad gan intuitive generalization generative adversarial network gans its conditional variants address well known problem mode collapse <eos> first mad gan multi agent gan architecture incorporating multiple generators one discriminator <eos> second enforce different generators capture diverse high probability modes discriminator mad gan designed such along finding real fake sample also required identify generator generated given fake sample <eos> intuitively succeed task discriminator must learn push different generators towards different identifiable modes <eos> perform extensive experiments synthetic real datasets compare mad gan different variants gan <eos> show high quality diverse sample generations challenging tasks such image image translation face generation <eos> addition also show mad gan able disentangle different modalities when trained using highly challenging diverse class dataset <eos> dataset image forests icebergs bedrooms <eos> end show its efficacy unsupervised feature representation task <eos> <eop> pid controller approach stochastic optimization deep network <eos> deep neural network demonstrated their power many computer vision applications <eos> state art deep architectures such vgg resnet densenet mostly optimized sgd momentum algorithm updates weights considering their past current gradients <eos> nonetheless sgd momentum suffers overshoot problem hinders convergence network training <eos> inspired prominent success proportional integral derivative pid controller automatic control propose pid approach accelerating deep network optimization <eos> first reveal intrinsic connections between sgd momentum pid based controller then present optimization algorithm exploits past current change gradients update network parameters <eos> proposed pid method reduces much overshoot phenomena sgd momentum achieves up acceleration popular deep network architectures competitive accuracy verified experiments benchmark datasets including cifar cifar tiny imagenet <eos> <eop> learning compression algorithms neural net pruning <eos> pruning neural net consists removing weights without degrading its performance <eos> old problem renewed interest because need compress ever larger nets so they run mobile devices <eos> pruning traditionally done ranking penalizing weights according some criterion such magnitude removing low ranked weights retraining remaining ones <eos> formulate pruning optimization problem finding weights minimize loss while satisfying pruning cost condition <eos> give generic algorithm solve alternates learning steps optimize regularized data dependent loss compression steps mark weights pruning data independent way <eos> magnitude thresholding arises naturally compression step but unlike existing magnitude pruning approaches algorithm explores subsets weights rather than committing irrevocably specific subset beginning <eos> also able learn automatically best number weights prune each layer net without incurring exponentially costly model selection <eos> using single pruning level user parameter achieve state art pruning nets various sizes <eos> <eop> large scale distance metric learning uncertainty <eos> distance metric learning dml studied extensively past decades its superior performance distance based algorithms <eos> most existing method propose learn distance metric pairwise triplet constraints <eos> however number constraints quadratic even cubic number original examples makes challenging dml handle large scale data set <eos> besides real world data may contain various uncertainty especially image data <eos> uncertainty mislead learning procedure cause performance degradation <eos> investigating image data find original data observed small set clean latent examples different distortions <eos> work propose margin preserving metric learning framework learn distance metric latent examples simultaneously <eos> leveraging ideal properties latent examples training efficiency improved significantly while learned metric also becomes robust uncertainty original data <eos> furthermore show metric learned latent examples only but preserve large margin property even original data <eos> empirical study benchmark image data set demonstrates efficacy efficiency proposed method <eos> <eop> guide me interacting deep network <eos> interaction collaboration between humans intelligent machines become increasingly important machine learning method move into real world applications involve end users <eos> while much prior work lies intersection natural language vision such image captioning image generation text descriptions less focus placed use language guide improve performance learned visual processing algorithm <eos> paper explore method flexibly guide trained convolutional neural network through user input improve its performance during inference <eos> so inserting layer acts spatio semantic guide into network <eos> guide trained modify network activations either directly via energy minimization scheme indirectly through recurrent model translates human language queries interaction weights <eos> learning verbal interaction fully automatic require manual text annotations <eos> evaluate method two datasets showing guiding pre trained network improve performance provide extensive insights into interaction between guide cnn <eos> <eop> art singular vectors universal adversarial perturbations <eos> vulnerability deep neural network dnns adversarial attacks attracting lot attention recent studies <eos> shown many state art dnns performing image classification there exist universal adversarial perturbations image agnostic perturbations mere addition natural image high probability leads their misclassification <eos> work propose new algorithm constructing such universal perturbations <eos> approach based computing so called singular vectors jacobian matrices hidden layer network <eos> resulting perturbations present interesting visual patterns using only image were able construct universal perturbations more than fooling rate dataset consisting image <eos> also investigate correlation between maximal singular value jacobian matrix fooling rate corresponding singular vector show constructed perturbations generalize across network <eos> <eop> deflecting adversarial attacks pixel deflection <eos> cnn poised become integral parts many critical systems <eos> despite their robustness natural variations image pixel values manipulated via small carefully crafted imperceptible perturbations cause model misclassify image <eos> present algorithm process image so classification accuracy significantly preserved presence such adversarial manipulations <eos> image classifiers tend robust natural noise adversarial attacks tend agnostic object location <eos> observations motivate strategy leverages model robustness defend against adversarial perturbations forcing image match natural image statistics <eos> algorithm locally corrupts image redistributing pixel values via process term pixel deflection <eos> subsequent wavelet based denoising operation softens corruption well some adversarial changes <eos> demonstrate experimentally combination techniques enables effective recovery true class against variety robust attacks <eos> result compare favorably current state art defenses without requiring retraining modifying cnn <eos> <eop> moviegraphs towards understanding human centric situations video <eos> there growing interest artificial intelligence build socially intelligent robots <eos> requires machines ability read people emotions motivations other factors affect behavior <eos> towards goal introduce novel dataset called moviegraphs provides detailed graph based annotations social situations depicted movie clips <eos> each graph consists several types nodes capture who present clip their emotional physical attributes their relationships <eos> parent child interactions between them <eos> most interactions associated topics provide additional details reasons give motivations actions <eos> addition most interactions many attributes grounded video time stamps <eos> provide thorough analysis dataset showing interesting common sense correlations between different social aspects scenes well across scenes over time <eos> propose method querying video text graphs show graphs contain rich sufficient information summarize localize each scene subgraphs allow describe situations abstract level retrieve multiple semantically relevant situations <eos> also propose method interaction understanding via ordering reason understanding <eos> moviegraphs first benchmark focus inferred properties human centric situations opens up exciting avenue towards socially intelligent ai agents <eos> <eop> semstyle learning generate stylised image captions using unaligned text <eos> linguistic style essential part written communication power affect both clarity attractiveness <eos> recent advances vision language start tackle problem generating image captions both visually grounded appropriately styled <eos> existing approaches either require styled training captions aligned image generate captions low relevance <eos> develop model learns generate visually relevant styled captions large corpus styled text without aligned image <eos> core idea model called semstyle separate semantics style <eos> one key component novel concise semantic term representation generated using natural language processing techniques frame semantics <eos> addition develop unified language model decodes sentences diverse word choices syntax different styles <eos> evaluations both automatic manual show captions semstyle preserve image semantics descriptive style shifted <eos> more broadly work provides possibilities learn richer image descriptions plethora linguistic data available web <eos> <eop> benchmarking dof outdoor visual localization changing conditions <eos> visual localization enables autonomous vehicles navigate their surroundings augmented reality applications link virtual real worlds <eos> practical visual localization approaches need robust wide variety viewing condition including day night changes well weather seasonal variations while providing highly accurate degree freedom dof camera pose estimates <eos> paper introduce first benchmark datasets specifically designed analyzing impact such factors visual localization <eos> using carefully created ground truth poses query image taken under wide variety conditions evaluate impact various factors dof camera pose estimation accuracy through extensive experiments state art localization approaches <eos> based result draw conclusions about difficulty different conditions showing long term localization far solved propose promising avenues future work including sequence based localization approaches need better local feature <eos> benchmark available visuallocalization <eos> <eop> ivqa inverse visual question answering <eos> propose inverse problem visual question answering ivqa explore its suitability benchmark visuo linguistic understanding <eos> ivqa task generate question corresponds given image answer pair <eos> since answers less informative than questions questions less learnable bias ivqa model needs better understand image successful than vqa model <eos> pose question generation multi modal dynamic inference process propose ivqa model gradually adjust its focus attention guided both partially generated question answer <eos> evaluation apart existing linguistic metrics propose new ranking metric <eos> metric compares ground truth question rank among list distractors allows drawbacks different algorithms sources error studied <eos> experimental result show model generate diverse grammatically correct content correlated questions match given answer <eos> <eop> unsupervised person image synthesis arbitrary poses <eos> present novel approach synthesizing photo realistic image people arbitrary poses using generative adversarial learning <eos> given input image person desired pose represented skeleton model renders image same person under new pose synthesizing novel views parts visible input image hallucinating seen <eos> problem recently addressed supervised manner <eos> during training ground truth image under new poses given network <eos> go beyond approaches proposing fully unsupervised strategy <eos> tackle challenging scenario splitting problem into two principal subtasks <eos> first consider pose conditioned bidirectional generator maps back initially rendered image original pose hence being directly comparable input image without need resort any training image <eos> second devise novel loss function incorporates content style terms aims producing image high perceptual quality <eos> extensive experiments conducted deepfashion dataset demonstrate image rendered model very close appearance obtained fully supervised approaches <eos> <eop> learning descriptor network three dimensional shape synthesis analysis <eos> paper proposes three dimensional shape descriptor network deep convolutional energy based model modeling volumetric shape patterns <eos> maximum likelihood training model follows analysis synthesis scheme interpreted mode seeking mode shifting process <eos> model synthesize three dimensional shape patterns sampling probability distribution via mcmc such langevin dynamics <eos> model used train three dimensional generator network via mcmc teaching <eos> conditional version three dimensional shape descriptor net used three dimensional object recovery three dimensional object super resolution <eos> experiments demonstrate proposed model generate realistic three dimensional shape patterns useful three dimensional shape analysis <eos> <eop> neural kinematic network unsupervised motion retargetting <eos> propose recurrent neural network architecture forward kinematics layer cycle consistency based adversarial training objective unsupervised motion retargetting <eos> network captures high level properties input motion forward kinematics layer adapts them target character different skeleton bone lengths <eos> shorter longer arms etc <eos> collecting paired motion training sequences different characters expensive <eos> instead network utilizes cycle consistency learn solve inverse kinematics problem unsupervised manner <eos> method works online <eos> adapts motion sequence fly new frames received <eos> experiments use mixamo animation data test method variety motions characters achieve state art result <eos> also demonstrate motion retargetting monocular human video three dimensional characters using off shelf three dimensional pose estimator <eos> <eop> group consistent similarity learning via deep crf person re identification <eos> person re identification benefits greatly deep neural network dnn learn accurate similarity metrics robust feature embeddings <eos> however most current method impose only local constraints similarity learning <eos> paper incorporate constraints large image groups combining crf deep neural network <eos> proposed method aims learn local similarity metrics image pairs while taking into account dependencies all image group forming group similarities <eos> method involves multiple image model relationships among local global similarities unified crf during training while combines multi scale local similarities predicted similarity testing <eos> adopt approximate inference scheme estimating group similarity enabling end end training <eos> extensive experiments demonstrate effectiveness model combines dnn crf learning robust multi scale local similarities <eos> overall result outperform state arts considerable margins three widely used benchmarks <eos> <eop> learning compositional visual concepts mutual consistency <eos> compositionality semantic concepts image synthesis analysis appealing help decomposing known generatively recomposing unknown data <eos> instance may learn concepts changing illumination geometry albedo scene try recombine them generate physically meaningful but unseen data training testing <eos> practice however often sample joint concept space available may data illumination change one data set geometric change another one without complete overlap <eos> pose following question how learn two more concepts jointly different data set mutual consistency sample full joint space present novel answer paper based cyclic consistency over multiple concepts represented individually generative adversarial network gans <eos> method conceptgan understood drop data augmentation improve resilience real world applications <eos> qualitative quantitative evaluations demonstrate its efficacy generating semantically meaningful image well one shot face verification example application <eos> <eop> nestednet learning nested sparse structures deep neural network <eos> recently there increasing demands construct compact deep architectures remove unnecessary redundancy improve inference speed <eos> while many recent works focus reducing redundancy eliminating unneeded weight parameters possible apply single deep network multiple devices different resources <eos> when new device circumstantial condition requires new deep architecture necessary construct train new network scratch <eos> work propose novel deep learning framework called nested sparse network exploits type nested structure neural network <eos> nested sparse network consists multiple levels network different sparsity ratio associated each level higher level network share parameters lower level network enable stable nested learning <eos> proposed framework realizes resource aware versatile architecture same network meet diverse resource requirements <eos> moreover proposed nested network learn different forms knowledge its internal network different levels enabling multiple tasks using single network such coarse fine hierarchical classification <eos> order train proposed nested network propose efficient weight connection learning channel layer scheduling strategies <eos> evaluate network multiple tasks including adaptive deep compression knowledge distillation learning class hierarchy demonstrate nested sparse network perform competitively but more efficiently compared existing method <eos> <eop> context embedding network <eos> low dimensional embeddings capture main variations interest collections data important many applications <eos> one way construct embeddings acquire estimates similarity crowd <eos> similarity multi dimensional concept varies individual individual <eos> however existing models learning crowd embeddings typically make simplifying assumptions such all individuals estimate similarity using same criteria list criteria known advance crowd workers influenced data they see <eos> overcome limitations introduce context embedding network cens <eos> addition learning interpretable embeddings image cens also model worker biases different attributes along visual context <eos> attributes highlighted set image <eos> experiments three noisy crowd annotated datasets show modeling both worker bias visual context result more interpretable embeddings compared existing approaches <eos> <eop> iterative learning open set noisy labels <eos> large scale datasets possessing clean label annotations crucial training convolutional neural network cnn <eos> however labeling large scale data very costly error prone even high quality datasets likely contain noisy incorrect labels <eos> existing works usually employ closed set assumption whereby sample associated noisy labels possess true class contained within set known classes training data <eos> however such assumption too restrictive many applications since sample associated noisy labels might fact possess true class present training data <eos> refer more complex scenario open set noisy label problem show nontrivial order make accurate predictions <eos> address problem propose novel iterative learning framework training cnn datasets open set noisy labels <eos> approach detects noisy labels learns deep discriminative feature iterative fashion <eos> benefit noisy label detection design siamese network encourage clean labels noisy labels dissimilar <eos> reweighting module also applied simultaneously emphasize learning clean labels reduce effect caused noisy labels <eos> experiments cifar imagenet real world noisy web search datasets demonstrate proposed model robustly train cnn presence high proportion open set well closed set noisy labels <eos> <eop> learning transferable architectures scalable image recognition <eos> developing neural network image classification models often requires significant architecture engineering <eos> paper study method learn model architectures directly dataset interest <eos> approach expensive when dataset large propose search architectural building block small dataset then transfer block larger dataset <eos> key contribution work design new search space call nasnet search space enables transferability <eos> experiments search best convolutional layer cell cifar dataset then apply cell imagenet dataset stacking together more copies cell each their own parameters design convolutional architecture name nasnet architecture <eos> also introduce new regularization technique called scheduleddroppath significantly improves generalization nasnet models <eos> cifar itself nasnet found method achieves <eos> error rate state art <eos> although cell searched directly imagenet nasnet constructed best cell achieves among published works state art accuracy <eos> better top accuracy than best human invented architectures while having billion fewer flops reduction computational demand previous state art model <eos> when evaluated different levels computational cost accuracies nasnets exceed state art human designed models <eos> instance small version nasnet also achieves top accuracy <eos> better than equivalently sized state art models mobile platforms <eos> finally image feature learned image classification generically useful transferred other computer vision problems <eos> task object detection learned feature nasnet used faster rcnn framework surpass state art <eos> map coco dataset <eos> <eop> sbnet sparse blocks network fast inference <eos> conventional deep convolutional neural network cnn apply convolution operators uniformly space across all feature maps hundreds layer incurs high computational cost real time applications <eos> many problems such object detection semantic segmentation able obtain low cost computation mask either priori problem knowledge low resolution segmentation network <eos> show such computation masks used reduce computation high resolution main network <eos> variants sparse activation cnn previously explored small scale tasks showed no degradation terms object classification accuracy but often measured gains terms theoretical flops without realizing practical speed up when compared highly optimized dense convolution implementations <eos> work leverage sparsity structure computation masks propose novel tiling based sparse convolution algorithm <eos> verified effectiveness sparse cnn lidar based three dimensional object detection report significant wall clock speed ups compared dense convolution without noticeable loss accuracy <eos> <eop> language based image editing recurrent attentive models <eos> investigate problem language based image editing lbie <eos> given source image natural language description want generate target image editing source image based description <eos> propose generic modeling framework two sub tasks lbie language based image segmentation image colorization <eos> framework uses recurrent attentive models fuse image language feature <eos> instead using fixed step size introduce each region image termination gate dynamically determine after each inference step whether continue extrapolating additional information textual description <eos> effectiveness framework validated three datasets <eos> first introduce synthetic dataset called cosal evaluate end end performance lbie system <eos> second show framework leads state art performance image segmentation referit dataset <eos> third present first language based colorization result oxford flowers dataset <eos> <eop> net vec quantifying explaining how concepts encoded filters deep neural network <eos> effort understand meaning intermediate representations captured deep network recent papers tried associate specific semantic concepts individual neural network filter responses interesting correlations often found largely focusing extremal filter responses <eos> paper show approach favor easy interpret cases necessarily representative average behavior representation <eos> more realistic but harder study hypothesis semantic representations distributed thus filters must studied conjunction <eos> order investigate idea while enabling systematic visualization quantification multiple filter responses introduce net vec framework semantic concepts mapped vectorial embeddings based corresponding filter responses <eos> studying such embeddings able show <eos> most cases multiple filters required code concept <eos> often filters concept specific help encode multiple concepts <eos> compared single filter activations filter embeddings able better characterize meaning representation its relationship other concepts <eos> <eop> end end dense video captioning masked transformer <eos> dense video captioning aims generate text descriptions all events untrimmed video <eos> involves both detecting describing events <eos> therefore all previous method dense video captioning tackle problem building two models <eos> event proposal captioning model two sub problems <eos> models either trained separately alternation <eos> prevents direct influence language description event proposal important generating accurate descriptions <eos> address problem propose end end transformer model dense video captioning <eos> encoder encodes video into appropriate representations <eos> proposal decoder decodes encoding different anchors form video event proposals <eos> captioning decoder employs masking network restrict its attention proposal event over encoding feature <eos> masking network converts event proposal differentiable mask ensures consistency between proposal captioning during training <eos> addition model employs self attention mechanism enables use efficient non recurrent structure during encoding leads performance improvements <eos> demonstrate effectiveness end end model activitynet captions youcookii datasets achieved <eos> meteor score respectively <eos> <eop> neural multi sequence alignment technique neumatch <eos> alignment heterogeneous sequential data video text important challenging problem <eos> standard techniques task including dynamic time warping dtw conditional random fields crfs suffer inherent drawbacks <eos> mainly markov assumption implies given immediate past future alignment decisions independent further history <eos> separation between similarity computation alignment decision also prevents end end training <eos> paper propose end end neural architecture alignment actions implemented moving data between stacks long short term memory lstm blocks <eos> flexible architecture supports large variety alignment tasks including one one one many skipping unmatched elements extensions non monotonic alignment <eos> extensive experiments semi synthetic real datasets show algorithm outperforms state art baselines <eos> <eop> path aggregation network instance segmentation <eos> way information propagates neural network great importance <eos> paper propose path aggregation network panet aiming boosting information flow proposal based instance segmentation framework <eos> specifically enhance entire feature hierarchy accurate localization signals lower layer bottom up path augmentation shortens information path between lower layer topmost feature <eos> present adaptive feature pooling links feature grid all feature levels make useful information each level propagate directly following proposal subnetworks <eos> complementary branch capturing different views each proposal created further improve mask prediction <eos> improvements simple implement subtle extra computational overhead <eos> yet they useful make panet reach st place coco challenge instance segmentation task nd place object detection task without large batch training <eos> panet also state art mvd cityscapes <eos> <eop> inaturalist species classification detection dataset <eos> existing image classification datasets used computer vision tend uniform distribution image across object categories <eos> contrast natural world heavily imbalanced some species more abundant easier photograph than others <eos> encourage further progress challenging real world conditions present inaturalist species classification detection dataset consisting image over different species plants animals <eos> feature visually similar species captured wide variety situations all over world <eos> image were collected different camera types varying image quality feature large class imbalance verified multiple citizen scientists <eos> discuss collection dataset present extensive baseline experiments using state art computer vision classification detection models <eos> result show current non ensemble based method achieve only top one classification accuracy illustrating difficulty dataset <eos> specifically observe poor result classes small numbers training examples suggesting more attention needed low shot learning <eos> <eop> multimodal explanations justifying decisions pointing evidence <eos> deep models both effective explainable desirable many settings prior explainable models unimodal offering either image based visualization attention weights text based generation post hoc justifications <eos> propose multimodal approach explanation argue two modalities provide complementary explanatory strengths <eos> collect two new datasets define evaluate task propose novel model provide joint textual rationale generation attention visualization <eos> datasets define visual textual justifications classification decision activity recognition tasks act visual question answering tasks vqa <eos> quantitatively show training textual explanations only yields better textual justification models but also better localizes evidence supports decision <eos> also qualitatively show cases visual explanation more insightful than textual explanation vice versa supporting thesis multimodal explanation models offer significant benefits over unimodal approaches <eos> <eop> stargan unified generative adversarial network multi domain image image translation <eos> recent studies shown remarkable success image image translation two domains <eos> however existing approaches limited scalability robustness handling more than two domains since different models should built independently every pair image domains <eos> address limitation propose stargan novel scalable approach perform image image translations multiple domains using only single model <eos> such unified model architecture stargan allows simultaneous training multiple datasets different domains within single network <eos> leads stargan superior quality translated image compared existing models well novel capability flexibly translating input image any desired target domain <eos> empirically demonstrate effectiveness approach facial attribute transfer facial expression synthesis tasks <eos> <eop> high resolution image synthesis semantic manipulation conditional gans <eos> present new method synthesizing high resolution photo realistic image semantic label maps using conditional generative adversarial network conditional gans <eos> conditional gans enabled variety applications but result often limited low resolution still far realistic <eos> work generate visually appealing result novel adversarial loss well new multi scale generator discriminator architectures <eos> furthermore extend framework interactive visual manipulation two additional feature <eos> first incorporate object instance segmentation information enables object manipulations such removing adding object changing object category <eos> second propose method generate diverse result given same input allowing users edit object appearance interactively <eos> human opinion studies demonstrate method significantly outperforms existing method advancing both quality resolution deep image synthesis editing <eos> <eop> semi parametric image synthesis <eos> present semi parametric approach photographic image synthesis semantic layouts <eos> approach combines complementary strengths parametric nonparametric techniques <eos> nonparametric component memory bank image segments constructed training set image <eos> given novel semantic layout test time memory bank used retrieve photographic references provided source material deep network <eos> synthesis performed deep network draws provided photographic material <eos> experiments multiple semantic segmentation datasets show presented approach yields considerably more realistic image than recent purely parametric techniques <eos> <eop> blockdrop dynamic inference paths residual network <eos> very deep convolutional neural network offer excellent recognition result yet their computational expense limits their impact many real world applications <eos> introduce blockdrop approach learns dynamically choose layer deep network execute during inference so best reduce total computation without degrading prediction accuracy <eos> exploiting robustness residual network resnets layer dropping framework selects fly residual blocks evaluate given novel image <eos> particular given pretrained resnet train policy network associative reinforcement learning setting dual reward utilizing minimal number blocks while preserving recognition accuracy <eos> conduct extensive experiments cifar imagenet <eos> result provide strong quantitative qualitative evidence learned policies only accelerate inference but also encode meaningful visual information <eos> built upon resnet model method achieves speedup average going high some image while maintaining same <eos> top accuracy imagenet <eos> <eop> interpretable convolutional neural network <eos> paper proposes method modify traditional convolutional neural network cnn into interpretable cnn order clarify knowledge representations high conv layer cnn <eos> interpretable cnn each filter high conv layer represents specific object part <eos> interpretable cnn use same training data ordinary cnn without need any annotations object parts textures supervision <eos> interpretable cnn automatically assigns each filter high conv layer object part during learning process <eos> apply method different types cnn various structures <eos> explicit knowledge representation interpretable cnn help people understand logic inside cnn <eos> patterns memorized cnn prediction <eos> experiments shown filters interpretable cnn more semantically meaningful than traditional cnn <eos> code available github <eos> com zqs interpretablecnn <eos> <eop> deep cross media knowledge transfer <eos> cross media retrieval research hotspot multimedia area aims perform retrieval across different media types such image text <eos> performance existing method usually relies labeled data model training <eos> however cross media data very labor consuming collect label so how transfer valuable knowledge existing data new data key problem towards application <eos> achieving goal paper proposes deep cross media knowledge transfer dckt approach transfers knowledge large scale cross media dataset promote model training another small scale cross media dataset <eos> main contributions dckt two level transfer architecture proposed jointly minimize media level correlation level domain discrepancies allows two important complementary aspects knowledge transferred intra media semantic inter media correlation knowledge <eos> enrich training information boost retrieval accuracy <eos> progressive transfer mechanism proposed iteratively select training sample ascending transfer difficulties via metric cross media domain consistency adaptive feedback <eos> drive transfer process gradually reduce vast cross media domain discrepancy so enhance robustness model training <eos> verifying effectiveness dckt take large scale dataset xmedianet source domain widely used datasets target domain cross media retrieval <eos> experimental result show dckt achieves promising improvement retrieval accuracy <eos> <eop> interleaved structured sparse convolutional neural network <eos> paper study problem designing efficient convolutional neural network architectures interest eliminating redundancy convolution kernels <eos> addition structured sparse kernels low rank kernels product low rank kernels product structured sparse kernels framework interpreting recently developed interleaved group convolutions igc its variants <eos> xception attracting increasing interests <eos> motivated observation convolutions contained group convolution igc further decomposed same manner present modularized building block igc interleaved structured sparse convolutions <eos> generalizes interleaved group convolutions composed two structured sparse kernels product more structured sparse kernels further eliminating redundancy <eos> present complementary condition balance condition guide design structured sparse kernels obtaining balance between three aspects model size computation complexity classification accuracy <eos> experimental result demonstrate advantage balance between three aspects compared interleaved group convolutions xception competitive performance other state art architecture design method <eos> <eop> variational net conditional appearance shape generation <eos> deep generative models demonstrated great performance image synthesis <eos> however result deteriorate case spatial deformations since they generate image object directly rather than modeling intricate interplay their inherent shape appearance <eos> present conditional net shape guided image generation conditioned output variational autoencoder appearance <eos> approach trained end end image without requiring sample same object varying pose appearance <eos> experiments show model enables conditional image generation transfer <eos> therefore either shape appearance retained query image while freely altering other <eos> moreover appearance sampled due its stochastic latent representation while preserving shape <eos> quantitative qualitative experiments coco deepfashion shoes market handbags approach demonstrates significant improvements over state art <eos> <eop> detach adapt learning cross domain disentangled deep representation <eos> while representation learning aims derive interpretable feature describing visual data representation disentanglement further result such feature so particular image attributes identified manipulated <eos> however one cannot easily address task without observing ground truth annotation training data <eos> address problem propose novel deep learning model cross domain representation disentangler cdrd <eos> observing fully annotated source domain data unlabeled target domain data interest model bridges information across data domains transfers attribute information accordingly <eos> thus cross domain joint feature disentanglement adaptation jointly performed <eos> experiments provide qualitative result verify disentanglement capability <eos> moreover further confirm model applied solving classification tasks unsupervised domain adaptation performs favorably against state art image disentanglement translation method <eos> <eop> learning deep structured active contours end end <eos> world covered millions buildings precisely knowing each instance position extents vital multitude applications <eos> recently automated building footprint segmentation models shown superior detection accuracy thanks usage convolutional neural network cnn <eos> however even latest evolutions struggle precisely delineating borders often leads geometric distortions inadvertent fusion adjacent building instances <eos> propose overcome issue exploiting distinct geometric properties buildings <eos> end present deep structured active contours dsac novel framework integrates priors constraints into segmentation process such continuous boundaries smooth edges sharp corners <eos> so dsac employs active contour models acm family constraint prior based polygonal models <eos> learn acm parameterizations per instance using cnn show how incorporate all components structured output model making dsac trainable end end <eos> evaluate dsac three challenging building instance segmentation datasets compares favorably against state art <eos> code will made available <eos> <eop> deep learning under privileged information using heteroscedastic dropout <eos> unlike machines humans learn through rapid abstract model building <eos> role teacher simply hammer home right wrong answers but rather provide intuitive comments comparisons explanations pupil <eos> learning under privileged information lupi paradigm endeavors model utilizing extra knowledge only available during training <eos> propose new lupi algorithm specifically designed convolutional neural network cnn recurrent neural network rnns <eos> propose use heteroscedastic dropout ie <eos> dropout varying variance make variance dropout function privileged information <eos> intuitively corresponds using privileged information control uncertainty model output <eos> perform experiments using cnn rnns tasks image classification machine translation <eos> method significantly increases sample efficiency during learning resulting higher accuracy large margin when number training examples limited <eos> also theoretically justify gains sample efficiency providing generalization error bound decreasing number training examples oracle case <eos> <eop> smooth neighbors teacher graphs semi supervised learning <eos> recently proposed self ensembling method achieved promising result deep semi supervised learning penalize inconsistent predictions unlabeled data under different perturbations <eos> however they only consider adding perturbations each single data point while ignoring connections between data sample <eos> paper propose novel method called smooth neighbors teacher graphs sntg <eos> sntg graph constructed based predictions teacher model <eos> implicit self ensemble models <eos> then graph serves similarity measure respect representations similar neighboring point learned smooth low dimensional manifold <eos> achieve state art result semi supervised learning benchmarks <eos> cifar labels svhn labels respectively <eos> particular improvements significant when labels fewer <eos> non augmented mnist only labels error rate reduced previous <eos> method also shows robustness noisy labels <eos> <eop> interpret neural network identifying critical data routing paths <eos> interpretability deep neural network aims explain rationale behind its decisions enable users understand intelligent agents become important issue due its importance practical applications <eos> address issue develop distillation guided routing method flexible framework interpret deep neural network identifying critical data routing paths analyzing functional processing behavior corresponding layer <eos> specifically propose discover critical nodes data routing paths during network inferring prediction individual input sample learning associated control gates each layer output channel <eos> routing paths therefore represented based responses concatenated control gates all layer reflect network semantic selectivity regarding input patterns more detailed functional process across different layer levels <eos> based discoveries propose adversarial sample detection algorithm learning classifier discriminate whether critical data routing paths real adversarial sample <eos> experiments demonstrate algorithm effectively achieve high defense rate minor training overhead <eos> <eop> deep spatio temporal random fields efficient video segmentation <eos> work introduce time memory efficient method structured prediction couples neuron decisions across both space time <eos> show able perform exact efficient inference densely connected spatio temporal graph capitalizing recent advances deep gaussian conditional random fields gcrfs <eos> method called videogcrf efficient unique global minimum trained end end alongside contemporary deep network video understanding <eos> experiment multiple connectivity patterns temporal domain present empirical improvements over strong baselines tasks both semantic instance segmentation video <eos> implementation based caffe framework will available github <eos> com siddharthachandra gcrf <eos> <eop> customized image narrative generation via interactive visual question generation answering <eos> image description task invariably examined static manner qualitative presumptions held universally applicable regardless scope target description <eos> practice however different viewers may pay attention different aspects image yield different descriptions interpretations under various contexts <eos> such diversity perspectives difficult derive conventional image description techniques <eos> paper propose customized image narrative generation task users interactively engaged generation process providing answers questions <eos> further attempt learn user interest via repeating such interactive stages automatically reflect interest descriptions new image <eos> experimental result demonstrate model generate variety descriptions single image cover wider range topics than conventional models while being customizable target user interaction <eos> <eop> pwc net cnn optical flow using pyramid warping cost volume <eos> present compact but effective cnn model optical flow called pwc net <eos> pwc net designed according simple well established principles pyramidal processing warping use cost volume <eos> cast learnable feature pyramid pwc net uses current optical flow estimate warp cnn feature second image <eos> then uses warped feature feature first image construct cost volume processed cnn estimate optical flow <eos> pwc net times smaller size easier train than recent flownet model <eos> moreover outperforms all published optical flow method mpi sintel final pass kitti benchmarks running about fps sintel resolution image <eos> models available project website <eos> <eop> revisiting deep intrinsic image decompositions <eos> while invaluable many computer vision applications decomposing natural image into intrinsic reflectance shading layer represents challenging underdetermined inverse problem <eos> opposed strict reliance conventional optimization filtering solutions strong prior assumptions deep learning based approaches also proposed compute intrinsic image decompositions when granted access sufficient labeled training data <eos> downside current data sources quite limited broadly speaking fall into one two categories either dense fully labeled image synthetic narrow settings weakly labeled data relatively diverse natural scenes <eos> contrast many previous learning based approaches often tailored structure particular dataset may work well others adopt core network structures universally reflect loose prior knowledge regarding intrinsic image formation process largely shared across datasets <eos> then apply flexibly supervised loss layer customized each source ground truth labels <eos> resulting deep architecture achieves state art result all major intrinsic image benchmarks runs considerably faster than most test time <eos> <eop> multi cell detection classification using generative convolutional model <eos> detecting counting classifying various cell types image human blood important many biomedical applications <eos> however tasks very difficult due wide range biological variability resolution limitations many imaging modalities <eos> paper proposes new approach detecting counting classifying white blood cell populations holographic image capitalizes fact variability mixture blood cells constrained physiology <eos> proposed approach based probabilistic generative model describes image population cells sum atoms convolutional dictionary cell templates <eos> class each template drawn prior distribution captures statistical information about blood cell mixtures <eos> parameters prior distribution learned database complete blood count result obtained patients cell templates learned image purified cells single cell class using extension convolutional dictionary learning <eos> cell detection counting classification then done using extension convolutional sparse coding accounts class proportion priors <eos> method successfully used detect count classify white blood cell populations holographic image lysed blood obtained normal blood donors abnormal clinical blood discard sample <eos> error method under <eos> all class populations compared errors over <eos> all other method tested <eos> <eop> learning spatial aware regressions visual tracking <eos> paper analyze spatial information deep feature propose two complementary regressions robust visual tracking <eos> first propose kernelized ridge regression model wherein kernel value defined weighted sum similarity scores all pairs patches between two sample <eos> show model formulated neural network thus efficiently solved <eos> second propose fully convolutional neural network spatially regularized kernels through filter kernel corresponding each output channel forced focus specific region target <eos> distance transform pooling further exploited determine effectiveness each output channel convolution layer <eos> outputs kernelized ridge regression model fully convolutional neural network combined obtain ultimate response <eos> experimental result two benchmark datasets validate effectiveness proposed method <eos> <eop> high performance visual tracking siamese region proposal network <eos> visual object tracking fundamental topic recent years many deep learning based trackers achieved state art performance multiple benchmarks <eos> however most trackers hardly get top performance real time speed <eos> paper propose siamese region proposal network siamese rpn end end trained off line large scale image pairs <eos> specifically consists siamese subnetwork feature extraction region proposal subnetwork including classification branch regression branch <eos> inference phase proposed framework formulated local one shot detection task <eos> pre compute template branch siamese subnetwork formulate correlation layer trivial convolution layer perform online tracking <eos> benefit proposal refinement traditional multi scale test online fine tuning discarded <eos> siamese rpn runs fps while achieving leading performance vot vot vot real time challenges <eos> <eop> liteflownet lightweight convolutional neural network optical flow estimation <eos> flownet state art convolutional neural network cnn optical flow estimation requires over parameters achieve accurate flow estimation <eos> paper present alternative network attains performance par flownet challenging sintel final pass kitti benchmarks while being times smaller model size <eos> times faster running speed <eos> made possible drilling down architectural details might missed current frameworks present more effective flow inference approach each pyramid level through lightweight cascaded network <eos> only improves flow estimation accuracy through early correction but also permits seamless incorporation descriptor matching network <eos> present novel flow regularization layer ameliorate issue outliers vague flow boundaries using feature driven local convolution <eos> network owns effective structure pyramidal feature extraction embraces feature warping rather than image warping practiced flownet <eos> code trained models available github <eos> com twhui liteflownet <eos> <eop> vital visual tracking via adversarial learning <eos> tracking detection framework consists two stages <eos> drawing sample around target object first stage classifying each sample target object background second stage <eos> performance existing tracking detection trackers using deep classification network limited two aspects <eos> first positive sample each frame highly spatially overlapped they fail capture rich appearance variations <eos> second there exists severe class imbalance between positive negative sample <eos> paper presents vital algorithm address two problems via adversarial learning <eos> augment positive sample use generative network randomly generate masks applied input feature capture variety appearance changes <eos> use adversarial learning network identifies mask maintains most robust feature target object over long temporal span <eos> addition handle issue class imbalance propose high order cost sensitive loss decrease effect easy negative sample facilitate training classification network <eos> extensive experiments benchmark datasets demonstrate proposed tracker performs favorably against state art approaches <eos> <eop> super slomo high quality estimation multiple intermediate frames video interpolation <eos> given two consecutive frames video interpolation aims generating intermediate frame form both spatially temporally coherent video sequences <eos> while most existing method focus single frame interpolation propose end end convolutional neural network variable length multi frame video interpolation motion interpretation occlusion reasoning jointly modeled <eos> start computing bi directional optical flow between input image using net architecture <eos> flows then linearly combined each time step approximate intermediate bi directional optical flows <eos> approximate flows however only work well locally smooth region produce artifacts around motion boundaries <eos> address shortcoming employ another net refine approximated flow also predict soft visibility maps <eos> finally two input image warped linearly fused form each intermediate frame <eos> applying visibility maps warped image before fusion exclude contribution occluded pixels interpolated intermediate frame avoid artifacts <eos> since none learned network parameters time dependent approach able produce many intermediate frames needed <eos> train network use fps video clips containing individual video frames <eos> experimental result several datasets predicting different numbers interpolated frames demonstrate approach performs consistently better than existing method <eos> <eop> real world repetition estimation div grad curl <eos> consider problem estimating repetition video such performing push ups cutting melon playing violin <eos> existing work shows good result under assumption static stationary periodicity <eos> realistic video rarely perfectly static stationary often preferred fourier based measurements inapt <eos> instead adopt wavelet transform better handle non static non stationary video dynamics <eos> flow field its differentials derive three fundamental motion types three motion continuities intrinsic periodicity three dimensional top perception three dimensional periodicity considers two extreme viewpoints <eos> follows fundamental cases recurrent perception <eos> practice deal variety repetitive appearance theory implies measuring time varying flow its differentials gradient divergence curl over segmented foreground motion <eos> experiments introduce new quva repetition dataset reflecting reality including non static non stationary video <eos> task counting repetitions video obtain favorable result compared deep learning alternative <eos> <eop> recurrent pixel embedding instance grouping <eos> introduce differentiable end end trainable framework solving pixel level grouping problems such instance segmentation consisting two novel components <eos> first regress pixels into hyper spherical embedding space so pixels same group high cosine similarity while different groups similarity below specified margin <eos> analyze choice embedding dimension margin relating them theoretical result problem distributing point uniformly sphere <eos> second group instances utilize variant mean shift clustering implemented recurrent neural network parameterized kernel bandwidth <eos> recurrent grouping module differentiable enjoys convergent dynamics probabilistic interpretability <eos> backpropagating group weighted loss through module allows learning focus correcting embedding errors won resolved during subsequent clustering <eos> framework while conceptually simple theoretically abundant also practically effective computationally efficient <eos> demonstrate substantial improvements over state art instance segmentation object proposal generation well demonstrating benefits grouping loss classification tasks such boundary detection semantic segmentation <eos> <eop> deep unsupervised saliency detection multiple noisy labeling perspective <eos> success current deep saliency detection method heavily depends availability large scale supervision form per pixel labeling <eos> such supervision while labor intensive always possible tends hinder generalization ability learned models <eos> contrast traditional handcrafted feature based unsupervised saliency detection method even though surpassed deep supervised method generally dataset independent could applied wild <eos> raises natural question possible learn saliency maps without using labeled data while improving generalization ability <eos> end present novel perspective unsupervised saliency detection through learning multiple noisy labeling generated weak noisy unsupervised handcrafted saliency method <eos> end end deep learning framework unsupervised saliency detection consists latent saliency prediction module noise modeling module work collaboratively optimized jointly <eos> explicit noise modeling enables deal noisy saliency maps probabilistic way <eos> extensive experimental result various benchmarking datasets show model only outperforms all unsupervised saliency method large margin but also achieves comparable performance recent state art supervised deep saliency method <eos> <eop> learning intrinsic image decomposition watching world <eos> single view intrinsic image decomposition highly ill posed problem making learning large amounts data attractive approach <eos> however difficult collect ground truth training data scale intrinsic image <eos> paper explore different approach learning intrinsic image observing image sequences over time depicting same scene under changing illumination learning single view decompositions consistent changes <eos> approach allows learn without ground truth decompositions instead exploit information available multiple image <eos> trained model then applied test time single views <eos> describe new learning framework based idea including new loss functions efficiently evaluated over entire sequences <eos> while prior learning based intrinsic image method achieve good performance specific benchmarks show approach generalizes well several diverse datasets including mit intrinsic image intrinsic image wild shading annotations wild <eos> <eop> tienet text image embedding network common thorax disease classification reporting chest rays <eos> chest rays one most common radiological examinations daily clinical routines <eos> reporting thorax diseases using chest rays often entry level task radiologist trainees <eos> yet reading chest ray image remains challenging job learning oriented machine intelligence due shortage large scale machine learnable medical image datasets lack techniques mimic high level reasoning human radiologists requires years knowledge accumulation professional training <eos> paper show clinical free text radiological reports utilized priori knowledge tackling two key problems <eos> propose novel text image embedding network tienet extracting distinctive image text representations <eos> multi level attention models integrated into end end trainable cnn rnn architecture highlighting meaningful text words image region <eos> first apply tienet classify chest rays using both image feature text embeddings extracted associated reports <eos> proposed auto annotation framework achieves high accuracy over <eos> average aucs assigning disease labels hand label evaluation dataset <eos> furthermore transform tienet into chest ray reporting system <eos> simulates reporting process output disease classification preliminary report together <eos> classification result significantly improved increase average aucs compared state art baseline unseen hand labeled dataset openi <eos> <eop> generating synthetic ray image person surface geometry <eos> present novel framework learns predict human anatomy body surface <eos> specifically approach generates synthetic ray image person only person surface geometry <eos> furthermore synthetic ray image parametrized manipulated adjusting set body markers also generated during ray image prediction <eos> proposed framework multiple synthetic ray image easily generated varying surface geometry <eos> perturbing parameters several additional synthetic ray image generated same surface geometry <eos> result approach offers potential overcome training data barrier medical domain <eos> capability achieved learning pair network one learns generate full image partial image set parameters other learns estimate parameters given full image <eos> during training two network trained iteratively such they would converge solution predicted parameters full image consistent each other <eos> addition medical data enrichment framework also used image completion well anomaly detection <eos> <eop> gibson env real world perception embodied agents <eos> perception being active having certain level motion freedom closely tied <eos> learning active perception sensorimotor control physical world cumbersome existing algorithms too slow efficiently learn real time robots fragile costly <eos> given rise learning simulation consequently casts question transferring real world <eos> paper investigate learning real world perception active agents propose gibson virtual environment purpose showcase set learned complex locomotion abilities <eos> primary characteristics learning environments transfer into trained agents being real world reflecting its semantic complexity ii having mechanism ensure no need further domain adaptation prior deployment result real world iii embodiment agent making subject constraints space physics <eos> <eop> reinforcement cutting agent learning video object segmentation <eos> video object segmentation fundamental yet challenging task computer vision community <eos> paper formulate problem markov decision process agents learned segment object region under deep reinforcement learning framework <eos> essentially learning agents segmentation nontrivial segmentation nearly continuous decision making process number involved agents pixels superpixels action steps seed super pixels whole object mask might incredibly huge <eos> overcome difficulty paper simplifies learning segmentation agents learning cutting agent only limited number action units converge just few action steps <eos> basic assumption object segmentation mainly relies interaction between object region their context <eos> thus optimal object box region context box region obtain desirable segmentation mask through further inference <eos> based assumption establish novel reinforcement cutting agent learning framework cutting agent consists cutting policy network cutting execution network <eos> former learns policies deciding optimal object context box pair while latter executing cutting function based inferred object context box pair <eos> collaborative interaction between two network method achieve outperforming vos performance two public benchmarks demonstrates rationality assumption well effectiveness proposed learning framework <eos> <eop> feature space transfer data augmentation <eos> problem data augmentation feature space considered <eos> new architecture denoted feature transfer network fatten proposed modeling feature trajectories induced variations object pose <eos> architecture exploits parametrization pose manifold terms pose appearance <eos> leads deep encoder decoder network architecture encoder factors into appearance pose predictor <eos> unlike previous attempts trajectory transfer fatten efficiently trained end end no need train separate feature transfer functions <eos> realized supplying decoder information about target pose use multi task loss penalizes category pose mismatches <eos> result fatten discourages discontinuous non smooth trajectories fail capture structure pose manifold generalizes well object recognition tasks involving large pose variation <eos> experimental result artificial modelnet database show successfully learn map source feature target feature desired pose while preserving class identity <eos> most notably using feature space transfer data augmentation <eos> pose depth sun rgbd object demonstrate considerable performance improvements one few shot object recognition transfer learning setup compared current state art method <eos> <eop> analytic expressions probabilistic moments pl dnn gaussian input <eos> outstanding performance deep neural network dnns visual recognition task particular demonstrated several large scale benchmarks <eos> performance immensely strengthened line re search aims understand analyze driving reasons behind effectiveness network <eos> one important aspect analysis recently gained much attention namely reaction dnn noisy input <eos> spawned research developing adversarial input attacks well training strategies make dnns more robust against attacks <eos> end derive pa per exact analytic expressions first second moments mean variance small piecewise linear pl network affine relu affine subject general gaussian input <eos> experimentally show expressions tight under simple linearizations deeper pl dnns especially popular architectures literature <eos> extensive experiments image classification show expressions used study behaviour output mean logits each class interclass confusion pixel level spatial noise sensitivity network <eos> moreover show how expressions used systematically construct targeted non targeted adversarial attacks <eos> <eop> detail preserving pooling deep network <eos> most convolutional neural network use some method gradually downscaling size hidden layer <eos> commonly referred pooling applied reduce number parameters improve invariance certain distortions increase receptive field size <eos> since pooling nature lossy process crucial each such layer maintains portion activations most important network discriminability <eos> yet simple maximization averaging over blocks max average pooling plain downsampling form strided convolutions standard <eos> paper aim leverage recent result image downscaling purposes deep learning <eos> inspired human visual system focuses local spatial changes propose detail preserving pooling dpp adaptive pooling method magnifies spatial changes preserves important structural detail <eos> importantly its parameters learned jointly rest network <eos> analyze some its theoretical properties show its empirical benefits several datasets network dpp consistently outperforms previous pooling approaches <eos> <eop> rethinking feature distribution loss functions image classification <eos> propose large margin gaussian mixture gm loss deep neural network classification tasks <eos> different softmax cross entropy loss proposal established assumption deep feature training set follow gaussian mixture distribution <eos> involving classification margin likelihood regularization gm loss facilitates both high classification performance accurate modeling training feature distribution <eos> such gm loss superior softmax loss its major variants sense besides classification readily used distinguish abnormal inputs such adversarial examples based their feature likelihood training feature distribution <eos> extensive experiments various recognition benchmarks like mnist cifar imagenet lfw well adversarial examples demonstrate effectiveness proposal <eos> <eop> shift zero flop zero parameter alternative spatial convolutions <eos> neural network rely convolutions aggregate spatial information <eos> however spatial convolutions expensive terms model size computation both grow quadratically respect kernel size <eos> paper present parameter free flop free shift operation alternative spatial convolutions <eos> fuse shifts point wise convolutions construct end end trainable shift based modules hyperparameter characterizing tradeoff between accuracy efficiency <eos> demonstrate operation efficacy replace resnet convolutions shift based modules improved cifar cifar accuracy using fewer parameters additionally demonstrate operation resilience parameter reduction imagenet outperforming resnet family members despite having millions fewer parameters <eos> further design family neural network called shiftnet achieve strong performance classification face verification style transfer while demanding many fewer parameters <eos> <eop> sketch classifier sketch based photo classifier generation <eos> contemporary deep learning techniques made image recognition reasonably reliable technology <eos> however training effective photo classifiers typically takes numerous examples limits image recognition scalability applicability scenarios image may available <eos> motivated investigation into zero shot learning addresses issue via knowledge transfer other modalities such text <eos> paper investigate alternative approach synthesizing image classifiers almost directly user imagination via free hand sketch <eos> approach doesn require category nameable describable via attributes per zero shot learning <eos> achieve via training model regression network map free hand sketch space space photo classifiers <eos> turns out mapping learned category agnostic way allowing photo classifiers new categories synthesized user no need annotated training photos <eos> also demonstrate modality classifier generation also used enhance granularity existing photo classifier complement name based zero shot learning <eos> <eop> light field intrinsics deep encoder decoder network <eos> present fully convolutional autoencoder light fields jointly encodes stacks horizontal vertical epipolar plane image through deep network residual layer <eos> complex structure light field thus reduced comparatively low dimensional representation decoded variety ways <eos> different pathways upconvolution currently support disparity estimation separation lightfield into diffuse specular intrinsic components <eos> key idea jointly perform unsupervised training autoencoder path network supervised training other decoders <eos> way find feature both tailored respective tasks generalize well datasets only example light fields available <eos> provide extensive evaluation synthetic light field data show network yields good result previously unseen real world data captured lytro illum camera various gantries <eos> <eop> learning generative convnets via multi grid modeling sampling <eos> paper proposes multi grid method learning energy based generative convnet models image <eos> each grid learn energy based probabilistic model energy function defined bottom up convolutional neural network convnet cnn <eos> learning such model requires generating synthesized examples model <eos> within each iteration learning algorithm each observed training image generate synthesized image multiple grids initializing finite step mcmc sampling minimal version training image <eos> synthesized image each subsequent grid obtained finite step mcmc initialized synthesized image generated previous coarser grid <eos> after obtaining synthesized examples parameters models multiple grids updated separately simultaneously based differences between synthesized observed examples <eos> show multi grid method learn realistic energy based generative convnet models outperforms original contrastive divergence cd persistent cd <eos> <eop> manifold learning quotient spaces <eos> when learning three dimensional shapes usually interested their intrinsic geometry rather than their orientation <eos> deal orientation variations usual trick consists augmenting data exhibit all possible variability thus let model learn both geometry well rotations <eos> paper introduce new autoencoder model encoding synthesis three dimensional shapes <eos> get rid undesirable input variability model learns manifold quotient space input space <eos> typically propose quotient space three dimensional models action rotations <eos> thus quotient autoencoder allows directly learn space interest ignoring side information <eos> reflected better performances reconstruction interpolation tasks experiments show model outperforms vanilla autoencoder well known shapenet dataset <eos> moreover model learns rotation invariant representation leading interesting result shapes co alignment <eos> finally extend quotient autoencoder quotient non rigid transformations <eos> <eop> learning intelligent dialogs bounding box annotation <eos> introduce intelligent annotation dialogs bounding box annotation <eos> train agent automatically choose sequence actions human annotator produce bounding box minimal amount time <eos> specifically consider two actions box verification annotator verifies box generated object detector manual box drawing <eos> explore two kinds agents one based predicting probability box will positively verified other based reinforcement learning <eos> demonstrate agents able learn efficient annotation strategies several scenarios automatically adapting image difficulty desired quality boxes detector strength all scenarios resulting annotation dialogs speed up annotation compared manual box drawing alone box verification alone while also outperforming any fixed combination verification drawing most scenarios realistic scenario detector iteratively re trained agents evolve series strategies reflect shifting trade off between verification drawing detector grows stronger <eos> <eop> boosting adversarial attacks momentum <eos> deep neural network vulnerable adversarial examples poses security concerns algorithms due potentially severe consequences <eos> adversarial attacks serve important surrogate evaluate robustness deep learning models before they deployed <eos> however most existing adversarial attacks only fool black box model low success rate <eos> address issue propose broad class momentum based iterative algorithms boost adversarial attacks <eos> integrating momentum term into iterative process attacks method stabilize update directions escape poor local maxima during iterations resulting more transferable adversarial examples <eos> further improve success rates black box attacks apply momentum iterative algorithms ensemble models show adversarially trained models strong defense ability also vulnerable black box attacks <eos> hope proposed method will serve benchmark evaluating robustness various deep models defense method <eos> method won first places nips non targeted adversarial attack targeted adversarial attack competitions <eos> <eop> nisp pruning network using neuron importance score propagation <eos> reduce significant redundancy deep convolutional neural network cnn most existing method prune neurons only considering statistics individual layer two consecutive layer <eos> prune one layer minimize reconstruction error next layer ignoring effect error propagation deep network <eos> contrast argue pruned network retain its predictive power essential prune neurons entire neuron network jointly based unified goal minimizing reconstruction error important responses final response layer frl second last layer before classification <eos> specifically apply feature ranking techniques measure importance each neuron frl formulate network pruning binary integer optimization problem derive closed form solution pruning neurons earlier layer <eos> based theoretical analysis propose neuron importance score propagation nisp algorithm propagate importance scores final responses every neuron network <eos> cnn pruned removing neurons least importance then fine tuned recover its predictive power <eos> nisp evaluated several datasets multiple cnn models demonstrated achieve significant acceleration compression negligible accuracy loss <eos> <eop> pointgrid deep network three dimensional shape understanding <eos> paper presents new deep learning architecture called pointgrid designed three dimensional model recognition unorganized point clouds <eos> new architecture embeds input point cloud into three dimensional grid simple yet effective sampling strategy directly learns transformations feature their raw coordinates <eos> proposed method integration point grid hybrid model leverages simplicity grid based approaches such voxelnet while avoid its information loss <eos> pointgrid learns better global information compared pointnet much simpler than pointnet kd net oct net cnn yet provides comparable recognition accuracy <eos> experiments popular shape recognition benchmarks pointgrid demonstrates competitive performance over existing deep learning method both classification segmentation <eos> <eop> tell me look guided attention inference network <eos> weakly supervised learning only coarse labels obtain visual explanations deep neural network such attention maps back propagating gradients <eos> attention maps then available priors tasks such object localization semantic segmentation <eos> one common framework address three shortcomings previous approaches modeling such attention maps make attention maps explicit natural component end end training first time provide self guidance directly maps exploring supervision network itself improve them seamlessly bridge gap between using weak extra supervision if available <eos> despite its simplicity experiments semantic segmentation task demonstrate effectiveness method <eos> clearly surpass state art pascal voc test val <eos> besides proposed framework provides way only explaining focus learner but also feeding back direct guidance towards specific tasks <eos> under mild assumptions method also understood plug existing weakly supervised learners improve their generalization performance <eos> <eop> semantic segmentation submanifold sparse convolutional network <eos> convolutional network de facto standard analyzing spatio temporal data such image video three dimensional shapes <eos> whilst some data naturally dense <eos> photos many other data sources inherently sparse <eos> examples include three dimensional point clouds were obtained using lidar scanner rgb camera <eos> standard dense implementations convolutional network very inefficient when applied such sparse data <eos> introduce new sparse convolutional operations designed process spatially sparse data more efficiently use them develop spatially sparse convolutional network <eos> demonstrate strong performance resulting models called submanifold sparse convolutional network sscns two tasks involving semantic segmentation three dimensional point clouds <eos> particular models outperform all prior state art test set recent semantic segmentation competition <eos> <eop> tom net learning transparent object matting single image <eos> paper addresses problem transparent object matting <eos> existing image matting approaches transparent object often require tedious capturing procedures long processing time limit their practical use <eos> paper first formulate transparent object matting refractive flow estimation problem <eos> then propose deep learning framework called tom net learning refractive flow <eos> framework comprises two parts namely multi scale encoder decoder network producing coarse prediction residual network refinement <eos> test time tom net takes single image input outputs matte consisting object mask attenuation mask refractive flow field fast feed forward pass <eos> no off shelf dataset available transparent object matting create large scale synthetic dataset consisting image transparent object rendered front image sampled microsoft coco dataset <eos> also collect real dataset consisting sample using transparent object background image <eos> promising experimental result achieved both synthetic real data clearly demonstrate effectiveness approach <eos> <eop> translating segmenting multimodal medical volumes cycle shape consistency generative adversarial network <eos> synthesized medical image several important applications <eos> intermedium cross modality image registration supplementary training sample boost generalization capability classifier <eos> especially synthesized ct data provide ray attenuation map radiation therapy planning <eos> work propose generic cross modality synthesis approach following targets synthesizing realistic looking three dimensional image using unpaired training data ensuring consistent anatomical structures could changed geometric distortion cross modality synthesis improving volume segmentation using synthetic data modalities limited training sample <eos> show goals achieved end end three dimensional convolutional neural network cnn composed mutually beneficial generators segmentors image synthesis segmentation tasks <eos> generators trained adversarial loss cycle consistency loss also shape consistency loss supervised segmentors reduce geometric distortion <eos> segmentation view segmentors boosted synthetic data generators online manner <eos> generators segmentors prompt each other alternatively end end training fashion <eos> extensive experiments dataset including total ct mri cardiovascular volumes show both tasks beneficial each other coupling two tasks result better performance than solving them exclusively <eos> <eop> unsupervised learning model deformable medical image registration <eos> present fast learning based algorithm deformable pairwise three dimensional medical image registration <eos> current registration method optimize objective function independently each pair image time consuming large data <eos> define registration parametric function optimize its parameters given set image collection interest <eos> given new pair scans quickly compute registration field directly evaluating function using learned parameters <eos> model function using cnn use spatial transform layer reconstruct one image another while imposing smoothness constraints registration field <eos> proposed method require supervised information such ground truth registration fields anatomical landmarks <eos> demonstrate registration accuracy comparable state art three dimensional image registration while operating orders magnitude faster practice <eos> method promises significantly speed up medical image analysis processing pipelines while facilitating novel directions learning based registration its applications <eos> code available github <eos> com balakg voxelmorph <eos> <eop> deep lesion graphs wild relationship learning organization significant radiology image findings diverse large scale lesion database <eos> radiologists their daily work routinely find annotate significant abnormalities large number radiology image <eos> such abnormalities lesions collected over years stored hospitals picture archiving communication systems <eos> however they basically unsorted lack semantic annotations like type location <eos> paper aim organize explore them learning deep feature representation each lesion <eos> large scale comprehensive dataset deeplesion introduced task <eos> deeplesion contains bounding boxes size measurements over lesions <eos> model their similarity relationship leverage multiple supervision information including types self supervised location coordinates sizes <eos> they require little manual annotation effort but describe useful attributes lesions <eos> then triplet network utilized learn lesion embeddings sequential sampling strategy depict their hierarchical similarity structure <eos> experiments show promising qualitative quantitative result lesion retrieval clustering classification <eos> learned embeddings further employed build lesion graph various clinically useful applications <eos> algorithm intra patient lesion matching proposed validated experiments <eos> <eop> learning distributions shape trajectories longitudinal datasets hierarchical model manifold diffeomorphisms <eos> propose method learn distribution shape trajectories longitudinal data <eos> collection individual object repeatedly observed multiple time point <eos> method allows compute average spatiotemporal trajectory shape changes group level individual variations trajectory both terms geometry time dynamics <eos> first formulate non linear mixed effects statistical model combination generic statistical model manifold valued longitudinal data deformation model defining shape trajectories via action finite dimensional set diffeomorphisms manifold structure efficient numerical scheme compute parallel transport manifold <eos> second introduce mcmc saem algorithm specific approach shape sampling adaptive scheme proposal variances log likelihood tempering strategy estimate model <eos> third validate algorithm simulated data then estimate scenario alteration shape hippocampus three dimensional brain structure during course alzheimer disease <eos> method shows instance hippocampal atrophy progresses more quickly female subjects occurs earlier apoe mutation carriers <eos> finally illustrate potential method classifying pathological trajectories versus normal ageing <eos> <eop> cnn driven sparse multi level spline image registration <eos> traditional single grid pyramidal spline parameterizations used deformable image registration require users specify control point spacing configurations capable accurately capturing both global complex local deformations <eos> many cases such grid configurations non obvious largely selected based user experience <eos> recent regularization method imposing sparsity upon spline coefficients throughout simultaneous multi grid optimization however provided promising means determining suitable configurations automatically <eos> unfortunately imposing sparsity over parameterized spline models computationally expensive introduces additional difficulties such undesirable local minima spline coefficient optimization process <eos> overcome difficulties determining spline grid configurations paper investigates use convolutional neural network cnn learn infer expressive sparse multi grid configurations prior spline coefficient optimization <eos> experimental result show multi grid configurations produced fashion using cnn based approach provide registration quality comparable norm constrained over parameterizations terms exactness while exhibiting significantly reduced computational requirements <eos> <eop> anatomical priors convolutional network unsupervised biomedical segmentation <eos> consider problem segmenting biomedical image into anatomical region interest <eos> specifically address frequent scenario no paired training data contains image their manual segmentations <eos> instead employ unpaired segmentation image use build anatomical prior <eos> critically segmentations derived imaging data different dataset imaging modality than current task <eos> introduce generative probabilistic model employs learned prior through convolutional neural network compute segmentations unsupervised setting <eos> conducted empirical analysis proposed approach context structural brain mri segmentation using multi study dataset more than scans <eos> result show anatomical prior enables fast unsupervised segmentation typically possible using standard convolutional network <eos> integration anatomical priors facilitate cnn based anatomical segmentation range novel clinical problems few no annotations available thus standard network trainable <eos> code model definitions model weights freely available github <eos> com adalca neuron <eop> registration curves surfaces using local differential information <eos> article presents first time global method registering three dimensional curves three dimensional surfaces without requiring initialization <eos> algorithm works tuples point vector consist pairs point augmented information their tangents normals <eos> closed form solution determining alignment transformation pair matching tuples proposed <eos> addition set necessary conditions two tuples match derived <eos> allows fast search correspondences used hypothesise test framework accomplishing global registration <eos> comparative experiments demonstrate proposed algorithm first effective solution curve vs surface registration method achieving accurate alignment situations small overlap large percentage outliers fraction second <eos> proposed framework extended cases curve vs curve surface vs surface registration former being particularly relevant since also largely unsolved problem <eos> <eop> weakly supervised learning single cell feature embeddings <eos> many new applications drug discovery functional genomics require capturing morphology individual imaged cells comprehensively possible rather than measuring one particular feature <eos> so called profiling experiments goal compare populations cells treated different chemicals genetic perturbations order identify biomedically important similarities <eos> deep convolutional neural network cnn often make excellent feature extractors but require ground truth training rarely available biomedical profiling experiments <eos> therefore propose train cnn based weakly supervised approach network aims classify each treatment against all others <eos> using network feature extractor performed comparably network trained non biological natural image chemical screen benchmark task improved result significantly more challenging genetic benchmark presented first time <eos> <eop> guided proofreading automatic segmentations connectomics <eos> automatic cell image segmentation method connectomics produce merge split errors require correction through proofreading <eos> previous research identified visual search errors bottleneck interactive proofreading <eos> aid error correction develop two classifiers automatically recommend candidate merges splits user <eos> classifiers use convolutional neural network cnn trained errors automatic segmentations against expert labeled ground truth <eos> classifiers detect potentially erroneous region considering large context region around segmentation boundary <eos> corrections then performed user yes no decisions reduces variation information <eos> faster than previous proofreading method <eos> also present fully automatic mode uses probability threshold make merge split decisions <eos> extensive experiments using automatic approach comparing performance novice expert users demonstrate method performs favorably against state art proofreading method different connectomics datasets <eos> <eop> wide compression tensor ring nets <eos> deep neural network demonstrated state art performance variety real world applications <eos> order obtain performance gains network grown larger deeper containing millions even billions parameters over thousand layer <eos> trade off large architectures require enormous amount memory storage computation thus limiting their usability <eos> inspired recent tensor ring factorization introduce tensor ring network tr nets significantly compress both fully connected layer convolutional layer deep network <eos> result show tr nets approach able compress lenet without losing accuracy compress state art wide resnet only <eos> degradation cifar image classification <eos> overall compression scheme shows promise scientific computing deep learning especially emerging resource constrained devices such smartphones wearables iot devices <eos> <eop> improvements context based self supervised learning <eos> develop set method improve result self supervised learning using context <eos> start baseline patch based arrangement context learning go there <eos> method address some overt problems such chromatic aberration well other potential problems such spatial skew mid level feature neglect <eos> prevent problems testing generalization common self supervised benchmark tests using different datasets during development <eos> result method combined yield top scores all standard self supervised benchmarks including classification detection pascal voc segmentation pascal voc linear tests imagenet csail places datasets <eos> obtain improvement over baseline method between <eos> percentage point transfer learning classification tests <eos> also show result different standard network architectures demonstrate generalization well portability <eos> all data models programs available gdo datasci <eos> <eop> learning structure strength cnn filters small sample size training <eos> convolutional neural network provided state art result several computer vision problems <eos> however due large number parameters cnn they require large number training sample limiting factor small sample size problems <eos> address limitation paper propose ssf cnn focuses learning structure strength filters <eos> structure filter initialized using dictionary based filter learning algorithm strength filter learned using small sample training data <eos> architecture provides flexibility training both small large training databases yields good accuracies even small size training data <eos> effectiveness algorithm demonstrated mnist cifar norb omniglot newborn face image databases varying number training sample <eos> result show ssf cnn significantly reduces number parameters required training while providing high accuracies test database <eos> small problems such newborn face recognition result demonstrate improvement rank identification accuracy least <eos> <eop> boosting self supervised learning via knowledge transfer <eos> self supervised learning one trains model solve so called pretext task dataset without need human annotation <eos> main objective however transfer model target domain task <eos> currently most effective transfer strategy fine tuning restricts one use same model parts thereof both pretext target tasks <eos> paper present novel framework self supervised learning overcomes limitations designing comparing different tasks models data domains <eos> particular framework decouples structure self supervised model final task specific fine tuned model <eos> allows quantitatively assess previously incompatible models including handcrafted feature show deeper neural network models learn better representations same pretext task transfer knowledge learned deep model shallower one thus boost its learning <eos> use framework design novel self supervised task achieves state art performance common benchmarks pascal voc ilsvrc places significant margin <eos> surprising result learned feature shrink map gap between models trained via self supervised learning supervised learning <eos> object detection pascal voc <eos> <eop> power ensembles active learning image classification <eos> deep learning method become de facto standard challenging image processing tasks such image classification <eos> one major hurdle deep learning approaches large set labeled data necessary prohibitively costly obtain particularly medical image diagnosis applications <eos> active learning techniques alleviate labeling effort <eos> paper investigate some recently proposed method active learning high dimensional data convolutional neural network classifiers <eos> compare ensemble based method against monte carlo dropout geometric approaches <eos> find ensembles perform better lead more calibrated predictive uncertainties basis many active learning algorithms <eos> investigate why monte carlo dropout uncertainties perform worse explore potential differences isolation series experiments <eos> show result mnist cifar achieve test set accuracy roughly labeled image initial result imagenet <eos> additionally show result large highly class imbalanced diabetic retinopathy dataset <eos> observe ensemble based active learning effectively counteracts imbalance during acquisition <eos> <eop> learning compact recurrent neural network block term tensor decomposition <eos> recurrent neural network rnns powerful sequence modeling tools <eos> however when dealing high dimensional inputs training rnns becomes computational expensive due large number model parameters <eos> hinders rnns solving many important computer vision tasks such action recognition video image captioning <eos> overcome problem propose compact flexible structure namely block term tensor decomposition greatly reduces parameters rnns improves their training efficiency <eos> compared alternative low rank approximations such tensor train rnn tt rnn method block term rnn bt rnn only more concise when using same rank but also able attain better approximation original rnns much fewer parameters <eos> three challenging tasks including action recognition video image captioning image generation bt rnn outperforms tt rnn standard rnn terms both prediction accuracy convergence rate <eos> specifically bt lstm utilizes times fewer parameters than standard lstm achieve accuracy improvement over <eos> action recognition task ucf dataset <eos> <eop> spatially adaptive filter units deep neural network <eos> classical deep convolutional network increase receptive field size either gradual resolution reduction application hand crafted dilated convolutions prevent increase number parameters <eos> paper propose novel displaced aggregation unit dau require hand crafting <eos> contrast classical filters units pixels placed fixed regular grid displacement daus learned enables filters spatially adapt their receptive field given problem <eos> extensively demonstrate strength daus classification semantic segmentation tasks <eos> compared convnets regular filter convnets daus achieve comparable performance faster convergence up times reduction parameters <eos> furthermore daus allow study deep network novel perspectives <eos> study spatial distributions dau filters analyze number parameters allocated spatial coverage filter <eos> <eop> so net self organizing network point cloud analysis <eos> paper presents so net permutation invariant architecture deep learning orderless point clouds <eos> so net models spatial distribution point cloud building self organizing map som <eos> based som so net performs hierarchical feature extraction individual point som nodes ultimately represents input point cloud single feature vector <eos> receptive field network systematically adjusted conducting point node nearest neighbor search <eos> recognition tasks such point cloud reconstruction classification object part segmentation shape retrieval proposed network demonstrates performance similar better than state art approaches <eos> addition training speed significantly faster than existing point cloud recognition network because parallelizability simplicity proposed architecture <eos> code available project website <eos> <eop> sgan alternative training generative adversarial network <eos> generative adversarial network gans demonstrated impressive performance data synthesis now used wide range computer vision tasks <eos> spite success they gained reputation being difficult train result time consuming human involved development process use them <eos> consider alternative training process named sgan several adversarial local pairs network trained independently so global supervising pair network trained against them <eos> goal train global pair corresponding ensemble opponent improved performances terms mode coverage <eos> approach aims increasing chances learning will stop global pair preventing both trapped unsatisfactory local minimum face oscillations often observed practice <eos> guarantee latter global pair never affects local ones <eos> rules sgan training thus follows global generator discriminator trained using local discriminators generators respectively whereas local network trained their fixed local opponent <eos> experimental result both toy real world problems demonstrate approach outperforms standard training terms better mitigating mode collapse stability while converging surprisingly increases convergence speed well <eos> <eop> sketchygan towards diverse realistic sketch image synthesis <eos> synthesizing realistic image human drawn sketches challenging problem computer graphics vision <eos> existing approaches either need exact edge maps rely retrieval existing photographs <eos> work propose novel generative adversarial network gan approach synthesizes plausible image categories including motorcycles horses couches <eos> demonstrate data augmentation technique sketches fully automatic show augmented data helpful task <eos> introduce new network building block suitable both generator discriminator improves information flow injecting input image multiple scales <eos> compared state art image translation method approach generates more realistic image achieves significantly higher inception scores <eos> <eop> explicit loss error aware quantization low bit deep neural network <eos> benefiting tens millions hierarchically stacked learnable parameters deep neural network dnns demonstrated overwhelming accuracy variety artificial intelligence tasks <eos> however reversely large size dnn models lays heavy burden storage computation power consumption prohibits their deployments embedded mobile systems <eos> paper propose explicit loss error aware quantization elq new method train dnn models very low bit parameter values such ternary binary ones approximate bit floating point counterparts without noticeable loss predication accuracy <eos> unlike existing method usually pose problem straightforward approximation layer wise weights outputs original full precision model specifically minimizing error layer wise weights inner products weights inputs between original respective quantized models elq elaborately bridges loss perturbation weight quantization incremental quantization strategy address dnn quantization <eos> through explicitly regularizing loss perturbation weight approximation error incremental way show such new optimization method theoretically reasonable practically effective <eos> validated two mainstream convolutional neural network families <eos> fully convolutional non fully convolutional elq shows better result than state art quantization method large scale imagenet classification dataset <eos> code will made publicly available <eos> <eop> towards universal representation unseen action recognition <eos> unseen action recognition uar aims recognise novel action categories without training examples <eos> while previous method focus inner dataset seen unseen splits paper proposes pipeline using large scale training source achieve universal representation ur generalise more realistic cross dataset uar cd uar scenario <eos> first address uar generalised multiple instance learning gmil problem discover building blocks large scale activitynet dataset using distribution kernels <eos> essential visual semantic components preserved shared space achieve ur efficiently generalise new datasets <eos> predicted ur exemplars improved simple semantic adaptation then unseen action directly recognised using ur during test <eos> without further training extensive experiments manifest significant improvements over ucf hmdb benchmarks <eos> <eop> deep image prior <eos> deep convolutional network become popular tool image generation restoration <eos> generally their excellent performance imputed their ability learn realistic image priors large number example image <eos> paper show contrary structure generator network sufficient capture great deal low level image statistics prior any learning <eos> order so show randomly initialized neural network used handcrafted prior excellent result standard inverse problems such denoising super resolution inpainting <eos> furthermore same prior used invert deep neural representations diagnose them restore image based flash no flash input pairs <eos> apart its diverse applications approach highlights inductive bias captured standard generator network architectures <eos> also bridges gap between two very popular families image restoration method learning based method using deep convolutional network learning free method based handcrafted image priors such self similarity <eos> <eop> st gan spatial transformer generative adversarial network image compositing <eos> address problem finding realistic geometric corrections foreground object such appears natural when composited into background image <eos> achieve propose novel generative adversarial network gan architecture utilizes spatial transformer network stns generator call spatial transformer gans st gans <eos> st gans seek image realism operating geometric warp parameter space <eos> particular exploit iterative stn warping scheme propose sequential training strategy achieves better result compared naive training single generator <eos> one key advantages st gan its applicability high resolution image indirectly since predicted warp parameters transferable between reference frames <eos> demonstrate approach two applications visualizing how indoor furniture <eos> product image might perceived room hallucinating how accessories like glasses would look when matched real portraits <eos> <eop> cartoongan generative adversarial network photo cartoonization <eos> paper propose solution transforming photos real world scenes into cartoon style image valuable challenging computer vision computer graphics <eos> solution belongs learning based method recently become popular stylize image artistic forms such painting <eos> however existing method produce satisfactory result cartoonization due fact cartoon styles unique characteristics high level simplification abstraction cartoon image tend clear edges smooth color shading relatively simple textures exhibit significant challenges texture descriptor based loss functions used existing method <eos> paper propose cartoongan generative adversarial network gan framework cartoon stylization <eos> method takes unpaired photos cartoon image training easy use <eos> two novel losses suitable cartoonization proposed semantic content loss formulated sparse regularization high level feature maps vgg network cope substantial style variation between photos cartoons edge promoting adversarial loss preserving clear edges <eos> further introduce initialization phase improve convergence network target manifold <eos> method also much more efficient train than existing method <eos> experimental result show method able generate high quality cartoon image real world photos <eos> following specific artists styles clear edges smooth shading outperforms state art method <eos> <eop> 