ask your neurons neural based approach answering questions about image <eos> address question answering task real world image set up visual turing test <eos> combining latest advances image representation natural language processing propose neural image qa end end formulation problem all parts trained jointly <eos> contrast previous efforts facing multi modal problem language output answer conditioned visual natural language input image question <eos> approach neural image qa doubles performance previous best approach problem <eos> provide additional insights into problem analyzing how much information contained only language part provide new human baseline <eos> study human consensus related ambiguities inherent challenging task propose two novel metrics collect additional answers extends original daquar dataset daquar consensus <eos> <eop> segment phrase table semantic segmentation visual entailment paraphrasing <eos> introduce segment phrase table spt large collection bijective associations between textual phrases their corresponding segmentations <eos> leveraging recent progress object recognition natural language semantics show how successfully build high quality segment phrase table using minimal human supervision <eos> more importantly demonstrate unique value unleashed rich bimodal resource both vision well natural language understanding <eos> first show fine grained textual labels facilitate contextual reasoning helps satisfying semantic constraints across image segments <eos> feature enables achieve state art segmentation result benchmark datasets <eos> next show association high quality segmentations textual phrases aids richer semantic understanding reasoning textual phrases <eos> leveraging feature motivate problem visual entailment visual paraphrasing demonstrate its utility large dataset <eos> <eop> aligning books movies towards story like visual explanations watching movies reading books <eos> books rich source both fine grained information how character object scene looks like well high level semantics someone thinking feeling how states evolve through story <eos> paper aims align books their movie releases order provide rich descriptive explanations visual content go semantically far beyond captions available current datasets <eos> align movies books propose neural sentence embedding trained unsupervised way large corpus books well video text neural embedding computing similarities between movie clips sentences book <eos> propose context aware cnn combine information multiple sources <eos> demonstrate good quantitative performance movie book alignment show several qualitative examples showcase diversity tasks model used <eos> <eop> learning query image similarities ranking canonical correlation analysis <eos> one fundamental problems image search learn ranking functions <eos> similarity between query image <eos> research topic evolved through two paradigms feature based vector model image ranker learning <eos> former relies image surrounding texts while latter learns ranker based human labeled query image pairs <eos> each paradigms its own limitation <eos> vector model sensitive quality text descriptions learning paradigm difficult scaled up human labeling always too expensive obtain <eos> demonstrate paper above two limitations well mitigated jointly exploring subspace learning use click through data <eos> specifically propose novel ranking canonical correlation analysis rcca learning query image similarities <eos> rcca initially finds common subspace between query image views maximizing their correlations further simultaneously learns bilinear query image similarity function adjusts subspace preserve preference relations implicit click through data <eos> once subspace finalized query image similarity computed bilinear similarity function their mappings subspace <eos> large scale click based image dataset <eos> million queries one million image rcca shown powerful image search superior performance over several state art method both keyword based query example tasks <eos> <eop> learning see moving <eos> current dominant paradigm feature learning computer vision relies training neural network task object recognition using millions hand labelled image <eos> also possible learn feature diverse set visual tasks using any other form supervision biology living organisms developed ability visual perception purpose moving acting world <eos> drawing inspiration observation work investigated if awareness egomotion <eos> self motion used supervisory signal feature learning <eos> opposed knowledge class labels information about egomotion freely available mobile agents <eos> found using same number training image feature learnt using egomotion supervision compare favourably feature learnt using class label supervision tasks scene recognition object recognition visual odometry keypoint matching <eos> <eop> object detection using generalization efficiency balanced co occurrence feature <eos> paper propose high accuracy object detector based co occurrence feature <eos> firstly introduce three kinds local co occurrence feature constructed traditional haar lbp hog respectively <eos> then boosted detectors learned each weak classifier corresponds local image region co occurrence feature <eos> addition propose generalization efficiency balanced geb framework boosting training <eos> feature selection procedure discrimination ability generalization power computation cost candidate feature all evaluated decision <eos> result boosted detector achieves both high accuracy good efficiency <eos> also shows performance competitive state art method pedestrian detection general object detection tasks <eos> <eop> mining graphs graph matching object discovery <eos> paper reformulates theory graph mining technical basis graph matching extends its scope applications computer vision <eos> given set attributed relational graphs args propose use hierarchical graph aog model pattern maximal size common subgraphs embedded args develop general method mine aog model unlabeled args <eos> method provides general solution problem mining hierarchical models unannotated visual data without exhaustive search object <eos> apply method rgb rgb image video demonstrate its generality wide range applicability <eos> code will available sites <eos> com site quanshizhang mining graphs <eos> <eop> pose induction novel object categories <eos> address task predicting pose object unannotated object categories small seed set annotated object classes <eos> present generalized classifier reliably induce pose given single instance novel category <eos> case availability large collection novel instances approach then jointly reasons over all instances improve initial estimates <eos> empirically validate various components algorithm quantitatively show method produces reliable pose estimates <eos> also show qualitative result diverse set classes further demonstrate applicability system learning shape models novel object classes <eos> <eop> dynamic texture recognition via orthogonal tensor dictionary learning <eos> dynamic textures dts video sequences stationary properties exhibit repetitive patterns over space time <eos> paper aims investigating sparse coding based approach characterizing local dt patterns recognition <eos> owing high dimensionality dt sequences existing dictionary learning algorithms suitable purpose due their high computational costs well poor scalability <eos> overcome obstacles proposed structured tensor dictionary learning method sparse coding learns dictionary structured orthogonality separability <eos> proposed method very fast more scalable high dimensional data than existing ones <eos> addition based proposed dictionary learning method dt descriptor developed better adaptivity discriminability scalability than existing approaches <eos> advantages demonstrated experiments multiple datasets <eos> <eop> convolutional channel feature <eos> deep learning method powerful tools but often suffer expensive computation limited flexibility <eos> alternative combine light weight models deep representations <eos> successful cases exist several visual problems unified framework absent <eos> paper revisit two widely used approaches computer vision namely filtered channel feature convolutional neural network cnn absorb merits both proposing integrated method called convolutional channel feature ccf <eos> ccf transfers low level feature pre trained cnn models feed boosting forest model <eos> combination cnn feature boosting forest ccf benefits richer capacity feature representation compared channel feature well lower cost computation storage compared end end cnn method <eos> show ccf serves good way tailoring pre trained cnn models diverse tasks without fine tuning whole network each task achieving state art performances pedestrian detection face detection edge detection object proposal generation <eos> <eop> local convolutional feature unsupervised training image retrieval <eos> patch level descriptors underlie several important computer vision tasks such stereo matching content based image retrieval <eos> introduce deep convolutional architecture yields patch level descriptors alternative popular sift descriptor image retrieval <eos> proposed family descriptors called patch ckn adapt recently introduced convolutional kernel network ckn unsupervised framework learn convolutional architectures <eos> present comparison framework benchmark current deep convolutional approaches along patch ckn both patch image retrieval including novel romepatches dataset <eos> patch ckn descriptors yield competitive result compared supervised cnn alternatives patch image retrieval <eos> <eop> ride reversal invariant descriptor enhancement <eos> many fine grained object recognition datasets image orientation left right might vary sample sample <eos> since handcrafted descriptors such sift reversal invariant stability image representation based them consequently limited <eos> popular solution augment datasets adding left right reversed copy each original image <eos> strategy improves recognition accuracy some extent but also brings price almost doubled time memory consumptions <eos> paper present ride reversal invariant descriptor enhancement fine grained object recognition <eos> ride generalized algorithm cancels out impact image reversal estimating orientation local descriptors guarantees produce identical representation image its left right reversed copy <eos> experimental result reveal consistent accuracy gain ride various types descriptors <eos> also provide insightful discussions working mechanism ride its generalization other applications <eos> <eop> discrete tabu search graph matching <eos> graph matching fundamental problem computer vision <eos> paper propose novel graph matching algorithm based tabu search <eos> proposed method solves graph matching problem casting into equivalent weighted maximum clique problem corresponding association graph further penalize through introducing negative weights <eos> subsequent tabu search optimization allows overcoming convention using positive weights <eos> method distinct feature utilizes history search make more strategic decisions while looking optimal solution thus effectively escaping local optima practice achieving superior result <eos> proposed method unlike existing algorithms enables direct optimization original discrete space while encouraging rather than artificially enforcing hard one one constraint thus resulting better solution <eos> experiments demonstrate robustness algorithm variety settings presenting state art result <eos> code available cv <eos> kr research dtsgm <eop> discriminative learning deep convolutional feature point descriptors <eos> deep learning revolutionalized image level tasks such classification but patch level tasks such correspondence still rely hand crafted feature <eos> paper use convolutional neural network cnn learn discriminant patch representations particular train siamese network pairs non corresponding patches <eos> deal large number potential pairs combination stochastic sampling training set aggressive mining strategy biased towards patches hard classify <eos> using distance during both training testing develop descriptors whose euclidean distances reflect patch similarity used drop replacement any task involving sift <eos> demonstrate consistent performance gains over state art generalize well against scaling rotation perspective transformation non rigid deformation illumination changes <eos> descriptors efficient compute amenable modern gpus publicly available <eos> <eop> amodal completion size constancy natural scenes <eos> consider problem enriching current object detection systems veridical object sizes relative depth estimates single image <eos> there several technical challenges such occlusions lack calibration data scale ambiguity between object size distance <eos> addressed full generality previous work <eos> here propose tackle issues building upon advances object recognition using recently created large scale datasets <eos> first introduce task amodal bounding box completion aims infer full extent object instances image <eos> then propose probabilistic framework learning category specific object size distributions available annotations leverage conjunction amodal completions infer veridical sizes object novel image <eos> finally introduce focal length prediction approach exploits scene recognition overcome inherent scale ambiguities demonstrate qualitative result challenging real world scenes <eos> <eop> learning position parts <eos> common issue deformable object detection finding good way position parts <eos> issue even more outspoken when considering detection pose estimation three dimensional object parts should placed three dimensional space <eos> some method extract three dimensional shape object three dimensional cad models <eos> limits their applicability categories such models available <eos> others represent object predefined simple shape <eos> extends applicability model but many cases pre defined shape too simple properly represent object three dimensional paper propose new method detection pose estimation three dimensional object use any three dimensional cad model other three dimensional information <eos> starting simple general three dimensional shape learn weakly supervised manner three dimensional part locations best fit training data <eos> method builds iterative estimation part locations introduce several speedups make method fast enough practical experiments <eos> evaluate model detection pose estimation faces cars <eos> method obtains result comparable state art faster than most other approaches need any additional three dimensional information <eos> <eop> query adaptive similarity measure rgb object recognition <eos> paper studies problem improving top accuracy rgb object recognition <eos> despite impressive top accuracies achieved existing method their top accuracies very satisfactory <eos> reasons two fold existing similarity measures sensitive object pose scale changes well intra class variations effectively fusing rgb depth cues still open problem <eos> address problems paper first proposes new similarity measure based dense matching through object comparison warped aligned better tolerate variations <eos> towards rgb depth fusion argue constant golden weight doesn exist <eos> two modalities varying contributions when comparing object different categories <eos> capture such dynamic characteristic group matchers equipped various fusion weights constructed explore responses dense matching under different fusion configurations <eos> all response scores finally merged following learning combination way provides quite good generalization ability practice <eos> proposed approach win best result several public benchmarks <eos> top test accuracy washington rgb object dataset <eos> improvement over state art <eos> <eop> listening your eyes towards practical visual speech recognition system using deep boltzmann machines <eos> paper presents novel feature learning method visual speech recognition using deep boltzmann machines dbm <eos> unlike all existing visual feature extraction techniques solely extracts feature video sequences method able explore both acoustic information visual information learn better visual feature representation training stage <eos> during test stage instead using both audio visual signals only video used generating missing audio feature both given visual given audio feature used obtain joint representation <eos> carried out experiments large scale audio visual data corpus experimental result show proposed techniques outperforms performance hadncrafted feature feature learned other commonly used deep learning techniques <eos> <eop> cluster based point set saliency <eos> propose cluster based approach point set saliency detection challenge since point set lack topological information <eos> point set first decomposed into small clusters using fuzzy clustering <eos> evaluate cluster uniqueness spatial distribution each cluster combine values into cluster saliency function <eos> finally probabilities point belonging each cluster used assign saliency each point <eos> approach detects fine scale salient feature uninteresting region consistently lower saliency values <eos> evaluate proposed saliency model testing saliency based keypoint detection against three dimensional interest point detection benchmark <eos> evaluation shows method achieves good balance between false positive false negative error rates without using any topological information <eos> <eop> comprehensive multi illuminant dataset benchmarking intrinsic image algorithms <eos> paper provide new real photo dataset precise ground truth intrinsic image research <eos> prior ground truth datasets restricted rather simple illumination conditions scene geometries enhanced using image synthesis method <eos> dataset provided paper based complex multi illuminant scenarios under multi colored illumination conditions challenging cast shadows <eos> provide full per pixel intrinsic ground truth data scenarios <eos> reflectance specularity shading illumination scenes well preliminary depth information <eos> furthermore evaluate state art intrinsic image recovery method using dataset <eos> <eop> patchmatch based automatic lattice detection near regular textures <eos> work investigate problem automatically inferring lattice structure near regular textures nrt real world image <eos> technique leverages patchmatch algorithm finding nearest neighbor knn correspondences image <eos> use knns recover initial estimate wallpaper basis vectors seed vertices texture lattice <eos> iteratively expand lattice solving mrf optimization problem <eos> show discretize space good solutions mrf using knns allowing efficiently accurately optimize mrf energy function using particle belief propagation algorithm <eos> demonstrate technique benchmark nrt dataset containing wide range image geometric photometric variations show method clearly outperforms state art terms both texel detection rate texel localization score <eos> <eop> data driven metric comprehensive evaluation saliency models <eos> past decades hundreds saliency models proposed fixation prediction along dozens evaluation metrics <eos> however existing metrics often heuristically designed may draw conflict conclusions comparing saliency models <eos> consequence becomes somehow confusing selection metrics comparing new models state arts <eos> address problem propose data driven metric comprehensive evaluation saliency models <eos> instead heuristically designing such metric first conduct extensive subjective tests find how saliency maps assessed human being <eos> based user data collected tests nine representative evaluation metrics directly compared quantizing their performances assessing saliency maps <eos> moreover propose learn data driven metric using convolutional neural network <eos> compared existing metrics experimental result show data driven metric performs most consistently human being evaluating saliency maps well saliency models <eos> <eop> matrix decomposition perspective multiple graph matching <eos> graph matching wide spectrum real world applications general known np hard <eos> many vision tasks one realistic problem arises finding global node mappings across batch corrupted weighted graphs <eos> paper attempt connect graph matching especially multi graph matching matrix decomposition model its relevant shelf convex optimization algorithms <eos> method aims extract common inliers their synchronized permutations disordered weighted graphs presence deformation outliers <eos> under proposed framework several variants derived hope accommodating other types noises <eos> experimental result both synthetic data real image empirically show proposed paradigm exhibits several interesting behaviors many cases performs competitively state arts <eos> <eop> fast effective gradient minimization region fusion <eos> gradient minimization applied input signal control number non zero gradients <eos> useful reducing small gradients generally associated signal noise while preserving important signal feature <eos> computer vision gradient minimization found applications image denoising three dimensional mesh denoising image enhancement <eos> minimizing norm however np hard problem because its non convex property <eos> result existing method rely approximation strategies perform minimization <eos> paper present new method perform gradient minimization fast effective <eos> method uses descent approach based region fusion converges faster than other method while providing better approximation optimal norm <eos> addition method applied both image three dimensional mesh topologies <eos> effectiveness approach demonstrated number examples <eos> <eop> generic promotion diffusion based salient object detection <eos> work propose generic scheme promote any diffusion based salient object detection algorithm original ways re synthesize diffusion matrix construct seed vector <eos> first make novel analysis working mechanism diffusion matrix reveals close relationship between saliency diffusion spectral clustering <eos> following analysis propose re synthesize diffusion matrix most discriminative eigenvectors after adaptive re weighting <eos> further propose generate seed vector based readily available diffusion maps avoiding extra computation color based seed search <eos> particular instance use inverse normalized laplacian matrix original diffusion matrix promote corresponding salient object detection algorithm leads superior performance experimentally demonstrated <eos> <eop> nighttime haze removal glow multiple light colors <eos> paper focuses dehazing nighttime image <eos> most existing dehazing method use models formulated describe haze daytime <eos> daytime models assume single uniform light color attributed light source directly visible scene <eos> nighttime scenes however commonly include visible lights sources varying colors <eos> light sources also often introduce noticeable amounts glow present daytime haze <eos> address effects introduce new nighttime haze model accounts varying light sources their glow <eos> model linear combination three terms direct transmission airlight glow <eos> glow term represents light light sources scattered around before reaching camera <eos> based model propose framework first reduces effect glow image resulting nighttime image consists direct transmission airlight only <eos> then compute spatially varying atmospheric light map encodes light colors locally <eos> atmospheric map used predict transmission use obtain nighttime scene reflection image <eos> demonstrate effectiveness nighttime haze model correction method number examples compare result existing daytime nighttime dehazing method result <eos> <eop> conformal low rank sparse representation image restoration <eos> obtaining appropriate dictionary key point when sparse representation applied computer vision image processing problems such image restoration <eos> expected preserving data structure during sparse coding dictionary learning enhance recovery performance <eos> however many existing dictionary learning method handle training sample individually while missing relationships between sample result dictionaries redundant atoms but poor representation ability <eos> paper propose novel sparse representation approach called conformal low rank sparse representation clrsr image restoration problems <eos> achieve more compact representative dictionary conformal property introduced preserving angles local geometry formed neighboring sample feature space <eos> furthermore imposing low rank constraint coefficient matrix lead more faithful subspaces capture global structure data <eos> apply clrsr model several image restoration tasks demonstrate effectiveness <eos> <eop> patch group based nonlocal self similarity prior learning image denoising <eos> patch based image modeling achieved great success low level vision such image denoising <eos> particular use image nonlocal self similarity nss prior refers fact local patch often many nonlocal similar patches across image significantly enhanced denoising performance <eos> however most existing method only nss input degraded image exploited while how utilize nss clean natural image still open problem <eos> paper propose patch group pg based nss prior learning scheme learn explicit nss models natural image high performance denoising <eos> pgs extracted training image putting nonlocal similar patches into groups pg based gaussian mixture model pg gmm learning algorithm developed learn nss prior <eos> demonstrate owe learned pg gmm simple weighted sparse coding model closed form solution used perform image denoising effectively resulting high psnr measure fast speed particularly best visual quality among all competing method <eos> <eop> automatic thumbnail generation based visual representativeness foreground recognizability <eos> present automatic thumbnail generation technique based two essential considerations how well they visually represent original photograph how well foreground recognized after cropping downsizing steps thumbnailing <eos> factors while important image indexing purpose thumbnails largely ignored previous method instead designed highlight salient content while disregarding effects downsizing <eos> propose set image feature modeling two considerations thumbnails learn how balance their relative effects thumbnail generation through training image pairs composed photographs their corresponding thumbnails created expert photographer <eos> experiments show effectiveness approach variety image well its advantages over related techniques <eos> <eop> salicon reducing semantic gap saliency prediction adapting deep neural network <eos> saliency context salicon ongoing effort aims understanding predicting visual attention <eos> conventional saliency models typically rely low level image statistics predict human fixations <eos> while models perform significantly better than chance there still large gap between model prediction human behavior <eos> gap largely due limited capability models predicting eye fixations strong semantic content so called semantic gap <eos> paper presents focused study narrow semantic gap architecture based deep neural network dnn <eos> leverages representational power high level semantics encoded dnns pretrained object recognition <eos> two key components fine tuning dnns fully convolutionally objective function based saliency evaluation metrics integrating information different image scales <eos> compare method saliency models public eye tracking benchmark datasets <eos> result demonstrate dnns automatically learn feature particularly saliency prediction surpass big margin state art <eos> addition model ranks top date under all seven metrics mit challenge set <eos> <eop> novel sparsity measure tensor recovery <eos> paper propose new sparsity regularizer measuring low rank structure underneath tensor <eos> proposed sparsity measure natural physical meaning intrinsically size fundamental kronecker basis express tensor <eos> embedding sparsity measure into tensor completion tensor robust pca frameworks formulate new models enhance their capability tensor recovery <eos> through introducing relaxation forms proposed sparsity measure also adopt alternating direction method multipliers admm solving proposed models <eos> experiments implemented synthetic multispectral image data set substantiate effectiveness proposed method <eos> <eop> oriented object proposals <eos> paper propose new approach generate oriented object proposals oops reduce detection error caused various orientations object <eos> end propose efficiently locate object region according pixelwise object probability rather than measuring objectness set sampled windows <eos> formulate proposal generation problem generative probabilistic model such object proposals different shapes <eos> sizes orientations produced locating local maximum likelihoods <eos> new approach three main advantages <eos> first helps object detector handle object different orientations <eos> second shapes proposals may vary fit object resulting proposals tighter than sampling windows fixed sizes <eos> third avoids massive window sampling thereby reducing number proposals while maintaining high recall <eos> experiments pascal voc dataset show proposed oop outperforms state art fast method <eos> further experiments show rotation invariant property helps class specific object detector achieve better performance than state art proposal generation method either object rotation scenarios general scenarios <eos> generating oops very fast takes only <eos> <eop> learning nonlinear spectral filters color image reconstruction <eos> paper presents idea learning optimal filters color image reconstruction based novel concept nonlinear spectral image decompositions recently proposed guy gilboa <eos> use multiscale image decomposition approach based total variation regularization bregman iterations represent input data sum image layer containing feature different scales <eos> filtered image obtained weighted linear combinations different frequency layer <eos> introduce idea learning optimal filters task image denoising propose idea mixing high frequency components different color channels <eos> numerical experiments demonstrate learning optimal weights significantly improve result comparison standard variational approach achieves state art image denoising result <eos> <eop> beyond white ground truth colors color constancy correction <eos> limitation color constancy research inability establish ground truth colors evaluating corrected image <eos> many existing datasets contain image scenes color chart included however only chart neutral colors grayscale patches used provide ground truth illumination estimation correction <eos> because corrected neutral colors known lie along achromatic line camera color space <eos> correct rgb values other color patches known <eos> result most method estimate diagonal matrix ensures only neutral colors correct <eos> paper describe how overcome limitation <eos> specifically show under certain illuminations diagonal matrix capable correcting only neutral colors but all colors scene <eos> finding allows find ground truth rgb values color chart camera color space <eos> show how use information correct all image existing datasets correct colors <eos> working new color corrected datasets describe how modify existing color constancy algorithms perform better image correction <eos> <eop> rgb guided hyperspectral image upsampling <eos> hyperspectral imaging usually lack spatial resolution due limitations hardware design imaging sensors <eos> contrary latest imaging sensors capture rgb image resolution multiple times larger than hyperspectral image <eos> paper present algorithm enhance upsample resolution hyperspectral image <eos> algorithm consists two stages spatial upsampling stage spectrum substitution stage <eos> spatial upsampling stage guided high resolution rgb image same scene spectrum substitution stage utilizes sparse coding locally refine upsampled hyperspectral image through dictionary substitution <eos> experiments show algorithm highly effective outperformed state art matrix factorization based approaches <eos> <eop> projection onto manifold elongated structures accurate extraction <eos> detection elongated structures image three dimensional image stacks critical prerequisite many applications machine learning based approaches recently shown deliver superior performance <eos> however method essentially classify individual locations explicitly model strong relationship exists between neighboring ones <eos> result isolated erroneous responses discontinuities topological errors present resulting score maps <eos> solve problem projecting patches score map their nearest neighbors set ground truth training patches <eos> algorithm induces global spatial consistency classifier score map returns result provably geometrically consistent <eos> apply algorithm challenging datasets four different domains show compares favorably state art method <eos> <eop> naive bayes super resolution forest <eos> paper presents fast high performance method super resolution external learning <eos> first contribution leading excellent performance bimodal tree clustering successfully exploits antipodal invariance coarse high res mapping natural image patches provides scalability finer partitions underlying coarse patch space <eos> during training ensemble such bimodal trees computed providing different linearizations mapping <eos> second main contribution fast inference algorithm selects most suitable mapping function within tree ensemble each patch adopting local naive bayes formulation <eos> experimental validation shows promising scalability properties reflect suitability proposed model may also generalized other tasks <eos> resulting method beyond one order magnitude faster performs objectively subjectively better than current state art <eos> <eop> pop image fusion derivative domain image fusion without reintegration <eos> there many applications multiple image fused form single summary greyscale colour output including computational photography <eos> rgb nir diffusion tensor imaging medical remote sensing <eos> often intuitively image fusion carried out derivative domain <eos> here new composite fused derivative found best accounts detail across all image then resulting gradient field reintegrated <eos> however reintegration step generally hallucinates new detail appearing any input image bands including halo bending artifacts <eos> paper avoid hallucinated details avoiding reintegration step <eos> work builds directly work socolinsky wolff who derive their equivalent gradient field per pixel di zenzo structure tensor defined inner product image jacobian <eos> show derivatives projection original image onto principal characteristic vector outer product pop jacobian generates same equivalent gradient field <eos> so doing derived fused image derivative structure seek <eos> course projection will meaningful only jacobian non zero derivatives so diffuse projection directions using bilateral filter before calculate fused image <eos> resulting pop fused image maximal fused detail but avoids hallucinated artifacts <eos> experiments demonstrate method delivers state art image fusion performance <eos> <eop> adaptive spatial spectral dictionary learning hyperspectral image denoising <eos> hyperspectral imaging beneficial diverse range applications diagnostic medicine agriculture surveillance name few <eos> however hyperspectral image often times suffer degradation due limited light introduces noise into imaging process <eos> paper propose effective model hyperspectral image hsi denoising considers underlying characteristics hsis sparsity across spatial spectral domain high correlation across spectra non local self similarity over space <eos> first exploit high correlation across spectra non local self similarity over space noisy hsi learn adaptive spatial spectral dictionary <eos> then employ local non local sparsity hsi under learned spatial spectral dictionary design hsi denoising model effectively solved iterative numerical algorithm parameters adaptively adjusted different clusters different noise levels <eos> experimental result hsi denoising show proposed method provide substantial improvements over current state art hsi denoising method terms both objective metric subjective visual quality <eos> <eop> fully connected guided image filtering <eos> paper presents linear time fully connected guided filter introducing minimum spanning tree mst guided filter gf <eos> since intensity based filtering kernel gf apt overly smooth edges fixed shape local box support region adopted gf geometric adaptive filter introduces extra spatial term tree similarity filtering kernel gf substitutes box window implicit support region establishing all pairs connections among pixels image assigning spatial intensity aware similarity connections <eos> adaptive implicit support region composed pixels large kernel weights entire image domain big advantage over predefined local box window presenting structure image reason mst efficiently present structure image kernel weight filter considers tree distance defined mst <eos> due reasons filter achieves better edge preserving result <eos> demonstrate strength proposed filter several applications <eos> experimental result show method produces better result than state art method <eos> <eop> segment graph based image filtering fast structure preserving smoothing <eos> paper design new edge aware structure named segment graph represent image further develop novel double weighted average image filter sgf based segment graph <eos> sgf use tree distance segment graph define internal weight function filtering kernel enables filter smooth out high contrast details textures while preserving major image structures very well <eos> while external weight function introduce user specified smoothing window balance smoothing effects each node segment graph <eos> moreover also set threshold adjust edge preserving performance <eos> advantages make sgf more flexible various applications overcome halo leak problems appearing most state art approaches <eos> finally importantly develop linear algorithm implementation sgf time complexity both gray scale high dimensional image regardless kernel size intensity range <eos> typically one fastest edge preserving filters cpu implementation achieves <eos> per megapixel when performing filtering channel color image <eos> strength proposed filter demonstrated various applications including stereo matching optical flow joint depth map upsampling edge preserving smoothing edges detection image abstraction texture editing <eos> <eop> deep network image super resolution sparse prior <eos> deep learning techniques successfully applied many areas computer vision including low level image restoration problems <eos> image super resolution several models based deep neural network recently proposed attained superior performance overshadows all previous handcrafted models <eos> question then arises whether large capacity data driven models become dominant solution ill posed super resolution problem <eos> paper argue domain expertise represented conventional sparse coding model still valuable combined key ingredients deep learning achieve further improved result <eos> show sparse coding model particularly designed super resolution incarnated neural network trained cascaded structure end end <eos> interpretation network based sparse coding leads much more efficient effective training well reduced model size <eos> model evaluated wide range image shows clear advantage over existing state art method terms both restoration accuracy human subjective quality <eos> <eop> convolutional color constancy <eos> color constancy problem inferring color light illuminated scene usually so illumination color removed <eos> because problem underconstrained often solved modeling statistical regularities colors natural object illumination <eos> contrast paper reformulate problem color constancy spatial localization task log chrominance space thereby allowing apply techniques object detection structured prediction color constancy problem <eos> directly learning how discriminate between correctly white balanced image poorly white balanced image model able improve performance standard benchmarks nearly <eos> <eop> thin structure estimation curvature regularization <eos> many applications vision require estimation thin structures such boundary edges surfaces roads blood vessels neurons etc <eos> unlike most previous approaches simultaneously detect delineate thin structures sub pixel localization real valued orientation estimation <eos> ill posed problem requires regularization <eos> propose objective function combining detection likelihoods prior minimizing curvature center lines surfaces <eos> unlike simple block coordinate descent develop novel algorithm able perform joint optimization location detection variables more effectively <eos> lower bound optimization algorithm applies quadratic absolute curvature <eos> proposed early vision framework sufficiently general used many higher level applications <eos> illustrate advantage approach range three dimensional examples <eos> <eop> harf hierarchy associated rich feature salient object detection <eos> state art salient object detection models able perform well relatively simple scenes yet more complex ones they still difficulties highlighting salient object completely background largely due lack sufficiently robust feature saliency prediction <eos> address such issue paper proposes novel hierarchy associated feature construction framework salient object detection based integrating elementary feature multi level region hierarchy <eos> furthermore multi layered deep learning feature introduced incorporated elementary feature into framework through compact integration scheme <eos> leads rich feature representation able represent context whole object background much more discriminative well robust salient object detection <eos> extensive experiments most widely used challenging benchmark datasets demonstrate proposed approach substantially outperforms state art salient object detection <eos> <eop> deep colorization <eos> paper investigates into colorization problem converts grayscale image colorful version <eos> very difficult problem normally requires manual adjustment achieve artifact free quality <eos> instance normally requires human labelled color scribbles grayscale target image careful selection colorful reference image <eos> capturing same scene grayscale target image <eos> unlike previous method paper aims high quality fully automatic colorization method <eos> assumption perfect patch matching technique use extremely large scale reference database contains sufficient color image most reliable solution colorization problem <eos> however patch matching noise will increase respect size reference database practice <eos> inspired recent success deep learning techniques provide amazing modeling large scale data paper re formulates colorization problem so deep learning techniques directly employed <eos> ensure artifact free quality joint bilateral filtering based post processing step proposed <eos> numerous experiments demonstrate method outperforms state art algorithms both terms quality speed <eos> <eop> image matting kl divergence based sparse sampling <eos> previous sampling based image matting method typically rely certain heuristics collecting representative sample known region thus their performance deteriorates if underlying assumptions satisfied <eos> alleviate paper take entirely new approach formulate sampling sparse subset selection problem propose pick small set candidate sample best explains unknown pixels <eos> moreover describe new distance measure comparing two sample based kl divergence between distributions feature extracted vicinity sample <eos> using standard benchmark dataset image matting demonstrate approach provides more accurate result compared state art method <eos> <eop> intrinsic decomposition image sequences local temporal variations <eos> present method intrinsic image decomposition aims decompose image into reflectance shading layer <eos> input sequence image varying illumination acquired static camera <eos> indoor scene moving light source outdoor timelapse <eos> leverage local color variations observed over time infer constraints reflectance solve ill posed image decomposition problem <eos> particular derive adaptive local energy observations each local neighborhood over time integrate distant pairwise constraints enforce coherent decomposition across all surfaces consistent shading changes <eos> method solely based multiple observations lambertian scene under varying illumination require user interaction scene geometry explicit lighting model <eos> compare result several intrinsic decomposition method number synthetic captured datasets <eos> <eop> low rank tensor approximation laplacian scale mixture modeling multiframe image denoising <eos> patch based low rank models shown effective exploiting spatial redundancy natural image especially application image denoising <eos> however two dimensional low rank model fully exploit spatio temporal correlation larger data set such multispectral image three dimensional mris <eos> work propose novel low rank tensor approximation framework laplacian scale mixture lsm modeling multi frame image denoising <eos> first similar three dimensional patches grouped form tensor order high order singular value decomposition hosvd applied grouped tensor <eos> then task multiframe image denoising formulated maximum posterior map estimation problem lsm prior tensor coefficients <eos> both unknown sparse coefficients hidden lsm parameters efficiently estimated method alternating optimization <eos> specifically derived closed form solutions both subproblems <eos> experimental result spectral dynamic mri image show proposed algorithm better preserve sharpness important image structures outperform several existing state art multiframe denoising method <eos> bm tensor dictionary learning <eos> <eop> learning parametric distributions image super resolution patch matching meets sparse coding <eos> existing approaches toward image super resolution sr often either data driven <eos> based internet scale matching web image retrieval model based <eos> formulated maximizing posterior estimation problem <eos> former conceptually simple yet heuristic while latter constrained fundamental limit frequency aliasing <eos> paper propose develop hybrid approach toward sr combining two lines ideas <eos> more specifically parameters underlying sparse distributions desirable hr image patches learned pair lr image retrieved hr image <eos> hybrid approach interpreted first attempt reconciling difference between parametric nonparametric models low level vision tasks <eos> experimental result show proposed hybrid sr method performs much better than existing state art method terms both subjective objective image qualities <eos> <eop> improving image restoration soft rounding <eos> several important classes image such text barcode pattern image property pixels only take distinct subset values <eos> knowledge benefit restoration such image but widely considered current restoration method <eos> work describe effective efficient approach incorporate knowledge distinct pixel values pristine image into general regularized least squares restoration framework <eos> introduce new regularizer attains zero designated pixel values becomes quadratic penalty function intervals between them <eos> when incorporated into regularized least squares restoration framework regularizer leads simple efficient step resembles extends rounding operation term soft rounding <eos> apply soft rounding enhanced solution restoration binary text barcode image pattern image multiple distinct pixel values <eos> experimental result show soft rounding enhanced restoration method achieve significant improvement both visual quality quantitative measures psnr ssim <eos> furthermore show regularizer also benefit restoration general natural image <eos> <eop> see difference direct pre image reconstruction pose estimation differentiating hog <eos> histogram oriented gradient hog descriptor led many advances computer vision over last decade still part many state art approaches <eos> realize associated feature computation piecewise differentiable therefore many pipelines build hog made differentiable <eos> lends advanced introspection well opportunities end end optimization <eos> present implementation hog based auto differentiation toolbox chumpy show applications pre image visualization pose estimation extends existing differentiable renderer opendr pipeline <eos> both applications improve respective state art hog approaches <eos> <eop> efficient statistical method image noise level estimation <eos> paper address problem estimating noise level single image contaminated additive zero mean gaussian noise <eos> first provide rigorous analysis statistical relationship between noise variance eigenvalues covariance matrix patches within image shows many state art noise estimation method underestimate noise level image <eos> end derive new nonparametric algorithm efficient noise level estimation based observation patches decomposed clean image often lie around low dimensional subspace <eos> performance method guaranteed both theoretically empirically <eos> specifically method outperforms existing state art algorithms estimating noise level least executing time experiments <eos> further demonstrate denoising algorithm bm algorithm achieves optimal performance using noise variance estimated algorithm <eos> <eop> contour detection characterization asynchronous event sensors <eos> bio inspired asynchronous event based dynamic vision sensor records temporal changes luminance scene high temporal resolution <eos> since events only triggered significant luminance changes most events occur boundary object their parts <eos> detection contours essential step further interpretation scene <eos> paper presents approach learn location contours their border ownership using structured random forests event based feature encode motion timing texture spatial orientations <eos> classifier integrates elegantly information over time utilizing classification result previously computed <eos> finally contour detection boundary assignment demonstrated layer segmentation scene <eos> experimental result demonstrate good performance boundary detection segmentation <eos> <eop> class specific image deblurring <eos> image deblurring fundamental problem blur kernel suppresses number spatial frequencies difficult recover reliably <eos> paper explore potential class specific image prior recovering spatial frequencies attenuated blurring process <eos> specifically devise prior based class specific subspace image intensity responses band pass filters <eos> learn aggregation subspaces across all frequency bands serves good class specific prior restoration frequencies cannot recovered generic image priors <eos> extensive validation method equipped above prior yields greater image quality than many state art method up db terms image psnr across various image categories including portraits cars cats pedestrians household object <eos> <eop> high low low high efficient boundary detection deep object feature its applications high level vision <eos> most current boundary detection systems rely exclusively low level feature such color texture <eos> however perception studies suggest humans employ object level reasoning when judging if particular pixel boundary <eos> inspired observation work show how predict boundaries exploiting object level feature pretrained object classification network <eos> method viewed high low approach high level object feature inform low level boundary detection process <eos> model achieves state art performance established boundary detection benchmark efficient run <eos> additionally show due semantic nature boundaries use them aid number high level vision tasks <eos> demonstrate using boundaries improve performance state art method problems semantic boundary labeling semantic segmentation object proposal generation <eos> view process low high scheme low level boundaries aid high level vision tasks <eos> thus contributions include boundary detection system accurate efficient generalizes well multiple datasets also shown improve existing state art high level vision method three distinct tasks <eos> <eop> variational depth superresolution using example based edge representations <eos> paper propose novel method depth image superresolution combines recent advances example based upsampling variational superresolution based known blur kernel <eos> most traditional depth superresolution approaches try use additional high resolution intensity image guidance superresolution <eos> method learn dictionary edge priors external database high low resolution examples <eos> novel variational sparse coding approach dictionary used infer strong edge priors <eos> additionally traditional sparse coding constraints difference overlap neighboring edge patches minimized optimization <eos> edge priors used novel variational superresolution anisotropic guidance higher order regularization <eos> both sparse coding variational superresolution depth solved based primal dual formulation <eos> exhaustive numerical visual evaluation show method clearly outperforms existing approaches multiple real synthetic datasets <eos> <eop> conditioned regression models non blind single image super resolution <eos> single image super resolution important task field computer vision finds many practical applications <eos> current state art method typically rely machine learning algorithms infer mapping low high resolution image <eos> method use single fixed blur kernel during training consequently assume exact same kernel underlying image formation process all test image <eos> however setting realistic practical applications because blur typically different each test image <eos> paper loosen restrictive constraint propose conditioned regression models including convolutional neural network random forests effectively exploit additional kernel information during both training inference <eos> allows training single model while previous method need re trained every blur kernel individually achieve good result demonstrate evaluations <eos> also empirically show proposed conditioned regression models effectively handle scenarios blur kernel different each image ii outperform related approaches trained only single kernel <eos> <eop> video super resolution via deep draft ensemble learning <eos> propose new direction fast video super resolution videosr via sr draft ensemble defined set high resolution patch candidates before final image deconvolution <eos> method contains two main components <eos> sr draft ensemble generation its optimal reconstruction <eos> first component renovate traditional feedforward reconstruction pipeline greatly enhance its ability compute different super resolution result considering large motion variation possible errors arising process <eos> then combine sr drafts through nonlinear process deep convolutional neural network cnn <eos> analyze why framework proposed explain its unique advantages compared previous iterative method update different modules passes <eos> promising experimental result shown natural video sequences <eos> <eop> pan sharpening hyper laplacian penalty <eos> pan sharpening task fusing spectral information low resolution multispectral image spatial information corresponding high resolution panchromatic image <eos> such approaches there trade off between spectral spatial quality well computational efficiency <eos> present method pan sharpening sparsity promoting objective function preserves both spatial spectral content efficient optimize <eos> objective incorporates norm way leverage recent computationally efficient method alternating direction method multipliers used <eos> additionally objective penalizes image gradients enforce high resolution fidelity exploits fourier domain further computational efficiency <eos> visual quality metrics demonstrate proposed objective function achieve higher spatial spectral resolution than several previous well known method competitive computational efficiency <eos> <eop> video restoration against yin yang phasing <eos> common video degradation problem largely untreated literature call yin yang phasing yyp <eos> yyp characterized involuntary dramatic flip flop intensity possibly chromaticity object video plays <eos> such temporal artifacts occur under ill illumination conditions triggered object camera motions mislead settings camera auto exposure white point <eos> paper investigate problem propose video restoration technique suppress yyp artifacts retain temporal consistency object appearance via inter frame spatially adaptive optimal tone mapping <eos> video quality further improved novel image enhancer designed weber perception principle exploiting second order statistics scene <eos> experimental result encouraging pointing effective practical solution common but surprisingly understudied problem <eos> <eop> rolling shutter super resolution <eos> classical multi image super resolution sr algorithms designed ccd cameras assume motion among image global <eos> but cmos sensors increasingly started replace their more expensive ccd counterparts many applications respect assumption if there motion camera relative scene during exposure duration image because row wise acquisition mechanism <eos> paper study hitherto unexplored topic multi image sr cmos cameras <eos> initially develop sr observation model accounts row wise distortions called rolling shutter rs effect observed image captured using non stationary cmos cameras <eos> then propose unified rs sr framework obtain rs free high resolution image row wise motion distorted low resolution image <eos> demonstrate efficacy proposed scheme using synthetic data well real image captured using hand held cmos camera <eos> quantitative qualitative assessments reveal method significantly advances state art <eos> <eop> learning large scale automatic image colorization <eos> describe automated method image colorization learns colorize examples <eos> method exploits learch framework train quadratic objective function chromaticity maps comparable gaussian random field <eos> coefficients objective function conditioned image feature using random forest <eos> objective function admits correlations long spatial scales control spatial error colorization image <eos> image then colorized minimizing objective function <eos> demonstrate method strongly outperforms natural baseline large scale experiments image real scenes using demanding loss function <eos> demonstrate learning model conditioned scene produces improved result <eos> show how incorporate desired color histogram into objective function doing so lead further improvements result <eos> <eop> compression artifacts reduction deep convolutional network <eos> lossy compression introduces complex compression artifacts particularly blocking artifacts ringing effects blurring <eos> existing algorithms either focus removing blocking artifacts produce blurred output restores sharpened image accompanied ringing effects <eos> inspired deep convolutional network dcn super resolution formulate compact efficient network seamless attenuation different compression artifacts <eos> also demonstrate deeper model effectively trained feature learned shallow network <eos> following similar easy hard idea systematically investigate several practical transfer settings show effectiveness transfer learning low level vision problems <eos> method shows superior performance than state arts both benchmark datasets real world use cases <eos> <eop> multiple hypothesis affine region estimation anisotropic log filters <eos> propose method estimating multiple hypothesis affine region keypoint using anisotropic laplacian gaussian log filter <eos> although conventional affine region detectors such hessian harris affine iterate find affine region fits given image patch such iterative searching adversely affected initial point <eos> avoid problem allow multiple detections single keypoint <eos> demonstrate responses all possible anisotropic log filters efficiently computed factorizing them similar manner spectral sift <eos> large number log filters densely sampled parameter space reconstructed weighted combination limited number representative filters called eigenfilters using singular value decomposition <eos> also reconstructed filter responses sampled parameters interpolated continuous representation using series proper functions <eos> result efficient multiple extrema searching continuous space <eos> experiments revealed method higher repeatability than conventional method <eos> <eop> self paced multiple instance learning framework co saliency detection <eos> interesting emerging topic co saliency detection aims simultaneously extracting common salient object group image <eos> traditional co saliency detection approaches rely heavily human knowledge designing hand crafted metrics explore intrinsic patterns underlying co salient object <eos> such strategies however always suffer poor generalization capability flexibly adapt various scenarios real applications especially due their lack insightful understanding biological mechanisms human visual co attention <eos> alleviate problem propose novel framework task naturally reformulating multiple instance learning mil problem further integrating into self paced learning spl regime <eos> proposed framework one hand capable fitting insightful metric measurements discovering common patterns under co salient region self learning way mil other hand tends promise learning reliability stability simulating human learning process through spl <eos> experiments benchmark datasets demonstrated effectiveness proposed framework compared state arts <eos> <eop> external patch prior guided internal clustering image denoising <eos> natural image modeling plays key role many vision problems such image denoising <eos> image priors widely used regularize denoising process illposed inverse problem <eos> one category denoising method exploit priors <eos> tv sparsity learned external clean image reconstruct given noisy image while another category method exploit internal prior <eos> self similarity reconstruct latent image <eos> though internal prior based method achieved impressive denoising result improvement visual quality will become very difficult increase noise level <eos> paper propose exploit image external patch prior internal self similarity prior jointly develop external patch prior guided internal clustering algorithm image denoising <eos> known natural image patches form multiple subspaces <eos> utilizing gaussian mixture models gmms learning image similar patches clustered subspaces learned <eos> learned gmms clean image then used guide clustering noisypatches input noisy image followed low rank approximation process estimate latent subspace image recovery <eos> numerical experiments show proposed method outperforms many state art denoising algorithms such bm wnnm <eos> <eop> self calibration optical lenses <eos> even high quality lenses suffer optical aberrations especially when used full aperture <eos> furthermore there significant lens lens deviations due manufacturing tolerances often rendering current software solutions like dxo lightroom ptlens insufficient they don adapt only include generic lens blur models <eos> propose method enables self calibration lenses natural image set such image <eos> end develop machine learning framework able exploit several recorded image distills available information into accurate model considered lens <eos> <eop> illumination robust color naming via label propagation <eos> color composition important property many computer vision tasks like image retrieval object classification <eos> paper address problem inferring color composition intrinsic reflectance object shadows highlights may change observed color dramatically <eos> achieve through color label propagation without recovering intrinsic reflectance beforehand <eos> specifically color labels propagated between region sharing same reflectance direction propagation promoted region under full illumination normal view angles abnormal region <eos> detect shadowed highlighted region well pairs region similar reflectance <eos> joint inference process adopted trim inconsistent identities connections <eos> evaluation collect three datasets image under noticeable highlights shadows <eos> experimental result show model effectively describe color composition real world image <eos> <eop> unsupervised cross modal synthesis subject specific scans <eos> recently cross modal synthesis subject specific scans receiving significant attention medical imaging community <eos> though various synthesis approaches introduced recent past most them either tailored specific application proposed supervised setting <eos> they assume availability training data same set subjects both source target modalities <eos> but collecting multiple scans each subject undesirable <eos> hence address issue propose general unsupervised cross modal medical image synthesis approach works without paired training data <eos> given source modality image subject first generate multiple target modality candidate values each voxel independently using cross modal nearest neighbor search <eos> then select best candidate values jointly all voxels simultaneously maximizing global mutual information cost function local spatial consistency cost function <eos> finally use coupled sparse representation further refinement synthesized image <eos> experiments generating mri brain scans mri vice versa demonstrate synthesis capability proposed unsupervised approach comparable various state art supervised approaches literature <eos> <eop> learning boost filamentary structure segmentation <eos> challenging problem filamentary structure segmentation broad range applications biological medical fields <eos> critical yet challenging issue remains how detect restore small filamentary fragments backgrounds small fragments diverse shapes appearances meanwhile backgrounds could cluttered ambiguous <eos> focusing issue paper proposes iterative two step learning based approach boost performance based base segmenter arbitrarily chosen number existing segmenters start initial partial segmentation filamentary structure obtained high confidence based existing segmenter <eos> also define scanning horizon epsilon balls centred around partial segmentation result <eos> step one approach centers data driven latent classification tree model detect filamentary fragments <eos> model learned via training process large number distinct local figure background separation scenarios established geometrically organized into tree structure <eos> step two spatially restores isolated fragments back current partial segmentation accomplished means completion fields matting <eos> both steps then alternated growth partial segmentation result until input image space entirely explored <eos> approach rather generic easily augmented wide range existing supervised unsupervised segmenters produce improved result <eos> empirically verified specific filamentary structure segmentation tasks retinal blood vessel segmentation well neuronal segmentations noticeable improvement shown over original state arts <eos> <eop> weakly supervised structured output learning flexible latent graphs using high order loss functions <eos> introduce two new structured output models use latent graph flexible terms number nodes structure training process minimises high order loss function using weakly annotated training set <eos> models developed context microscopy imaging malignant tumours estimation number proportion classes microcirculatory supply units mcsu important assessment efficacy common cancer treatments mcsu region tumour tissue supplied microvessel <eos> proposed methodologies take input multimodal microscopy image tumour estimate number proportion mcsu classes <eos> estimation facilitated use underlying latent graph present manual annotations each mcsu represented node graph labelled mcsu class image location <eos> training process uses manual weak annotations available consisting number mcsu classes per training image training objective minimisation high order loss function based norm error between manual estimated annotations <eos> one models proposed based new flexible latent structure support vector machine flssvm other based deep convolutional neural network dcnn model <eos> using dataset weakly annotated pairs multimodal image eight tumours show quantitative result dcnn superior but qualitative result flssvm better both display high correlation values regarding number proportion mcsu classes compared manual annotations <eos> <eop> efficient classifier training minimize false merges electron microscopy segmentation <eos> prospect neural reconstruction electron microscopy em image elucidated automatic segmentation algorithms <eos> although segmentation algorithms eliminate necessity tracing neurons hand significant manual effort still essential correcting mistakes they make <eos> considerable amount human labor also required annotating groundtruth volumes training classifiers segmentation framework <eos> critically important diminish dependence human interaction overall reconstruction system <eos> study proposes novel classifier training algorithm em segmentation aimed reduce amount manual effort demanded groundtruth annotation error refinement tasks <eos> instead using exhaustive pixel level groundtruth active learning algorithm proposed sparse labeling pixel boundaries superpixels <eos> because over segmentation errors general more tolerable easier correct than under segmentation errors algorithm designed prioritize minimization false merges over false split mistakes <eos> experiments both three dimensional data suggest proposed method yields segmentation outputs more amenable neural reconstruction than existing method <eos> <eop> statistical analysis neuroimages imperfect registration <eos> variety studies neuroscience neuroimaging seek perform statistical inference acquired brain image scans diagnosis well understanding pathological manifestation diseases <eos> so important first step register co register all image data into common coordinate system <eos> permits meaningful comparison intensities each voxel across groups <eos> diseased versus healthy evaluate effects disease use machine learning algorithms subsequent step <eos> but errors underlying registration make problematic they either decrease statistical power make follow up inference tasks less effective accurate <eos> paper derive novel algorithm offers immunity local errors underlying deformation field obtained registration procedures <eos> deriving deformation invariant representation image downstream analysis made more robust if one had access hypothetical far superior registration procedure <eos> algorithm based recent work scattering coefficients <eos> using starting point show how result harmonic analysis especially non euclidean wavelets yields strategies designing deformation additive noise invariant representations large brain image volumes <eos> present set result synthetic real brain image achieve robust statistical analysis even presence substantial deformation errors here standard analysis procedures significantly under perform fail identify true signal <eos> <eop> convex optimization abstract linear operators <eos> introduce convex optimization modeling framework transforms convex optimization problem expressed form natural convenient user into equivalent cone program way preserves fast linear transforms original problem <eos> representing linear functions transformation process matrices but graphs encode composition abstract linear operators arrive matrix free cone program <eos> one whose data matrix represented abstract linear operator its adjoint <eos> cone program then solved matrix free cone solver <eos> combining matrix free modeling framework cone solver obtain general method efficiently solving convex optimization problems involving fast linear transforms <eos> <eop> building dynamic cloud maps ground up <eos> satellite imagery cloud cover extremely important understanding predicting weather <eos> demonstrate how imagery constructed ground up without requiring expensive geo stationary satellites <eos> accomplished through novel approach approximate continental scale cloud maps using only ground level imagery publicly available webcams <eos> collected year worth satellite data simultaneously captured geo located outdoor webcam image sparsely distributed cameras across continental usa <eos> satellite data used train dynamic model cloud motion alongside regression models one each camera relate ground level webcam data satellite data camera location <eos> novel application large scale computer vision meteorology remote sensing enabled smoothed hierarchically regularized dynamic texture model whose system dynamics driven remain consistent measurements geo located webcams <eos> show hierarchical model better able incorporate sparse webcam measurements resulting more accurate cloud maps comparison standard dynamic textures implementation <eos> finally demonstrate model successfully applied other natural image sequences dyntex database suggesting broader applicability method <eos> <eop> versatile learning based three dimensional temporal tracker scalable robust online <eos> paper proposes temporal tracking algorithm based random forest uses depth image estimate track three dimensional pose rigid object real time <eos> compared state art aimed same goal algorithm holds important attributes such high robustness against holes occlusion low computational cost both learning tracking stages low memory consumption <eos> obtained novel formulation learning strategy based dense sampling camera viewpoints learning independent trees single image each camera view well insightful occlusion handling strategy enforces forest recognize object local global structures <eos> due attributes report state art tracking accuracy benchmark datasets accomplish remarkable scalability number targets being able simultaneously track pose over hundred object fps off shelf cpu <eos> addition fast learning time enables extend algorithm robust online tracker model free three dimensional object under different viewpoints appearance changes demonstrated experiments <eos> <eop> realtime edge based visual odometry monocular camera <eos> work present novel algorithm realtime visual odometry monocular camera <eos> main idea develop approach between classical feature based visual odometry systems modern direct dense semi dense method trying benefit best attributes both <eos> similar feature based systems extract information image instead working raw image intensities direct method <eos> particular information extracted edges present image while rest algorithm designed take advantage structural information provided when pixels treated edges <eos> edge extraction efficient higly parallelizable operation <eos> edge depth information extracted dense enough allow acceptable surface fitting similar modern semi dense method <eos> valuable attribute feature based odometry lacks <eos> experimental result show proposed method similar drift than state art feature based direct method simple algorithm runs realtime parallelized <eos> finally also developed inertial aided version successfully stabilizes unmanned air vehicle complex indoor environments using only frontal camera while running complete solution embedded hardware board vehicle <eos> <eop> fill transfer simple physics based approach containability reasoning <eos> visual perception object affordances emerged useful ingredient building powerful computer vision robotic applications <eos> paper introduce novel approach reason about liquid containability affordance containing liquid <eos> approach analyzes container object based two simple physical processes fill transfer liquid <eos> first reasons about whether given three dimensional object liquid container its best filling direction <eos> second proposes directions transfer its contained liquid outside while avoiding spillage <eos> compare simplified model common fluid dynamics simulation demonstrate algorithm makes human like choices about best directions fill containers transfer liquid them <eos> apply approach reason about containability several real world object acquired using consumer grade depth camera <eos> <eop> linear structure motion light field cameras <eos> present novel approach relative pose estimation tailored light field cameras <eos> relationships between scene geometry light field structure analysis light field projection terms pluecker ray coordinates deduce set linear constraints ray space correspondences between light field camera pair <eos> applied infer relative pose light field cameras thus obtain point cloud reconstruction scene <eos> while proposed method interesting relationships pose estimation generalized cameras based ray ray correspondence experiments demonstrate approach both more accurate computationally more efficient <eos> also compares favourably direct linear pose estimation based aligning three dimensional point clouds obtained reconstructing depth each individual light field <eos> further validate method employ pose estimates merge light fields captured hand held consumer light field cameras into refocusable panoramas <eos> <eop> object reconstruction hand object interactions <eos> recent advances enabled object reconstruction approaches using single off shelf rgb camera <eos> although approaches successful wide range object classes they rely stable distinctive geometric texture feature <eos> many object like mechanical parts toys household decorative articles however textureless characterized minimalistic shapes simple symmetric <eos> existing hand scanning systems reconstruction techniques fail such symmetric object absence highly distinctive feature <eos> work show extracting hand motion hand scanning effectively facilitates reconstruction even featureless highly symmetric object present approach fuses rich additional information hands into reconstruction pipeline significantly contributing state art hand scanning <eos> <eop> minimal solvers three dimensional geometry satellite imagery <eos> propose two novel minimal solvers advance state art satellite imagery processing <eos> method efficient rely prior existence complex inverse mapping functions correlate image coordinates three dimensional terrain <eos> first solver improves stereo correspondence problem satellite imagery provide exact image object space mapping prior method were inaccurate <eos> second solver provides novel mechanism three dimensional point triangulation improved robustness accuracy over prior techniques <eos> given usefulness ubiquity satellite imagery proposed method allow improved result variety existing future applications <eos> <eop> efficient minimal solution multi camera motion <eos> propose efficient method estimating motion multi camera rig minimal set feature correspondences <eos> existing method solving multi camera relative pose problem require extra correspondences slow compute produce multitude solutions <eos> solution uses first order approximation relative pose order simplify problem produce accurate estimate quickly <eos> solver applicable sequential multi camera motion estimation fast enough real time implementation random sampling framework <eos> experiments show approach both stable efficient challenging test sequences <eos> <eop> learning shape motion elastic models force space <eos> paper address problem simultaneously recovering three dimensional shape pose deformable potentially elastic object motion <eos> highly ambiguous problem typically tackled using low rank shape trajectory constraints <eos> show formulating problem terms low rank force space induces deformation allows better physical interpretation resulting priors more accurate representation actual object behavior <eos> however comes price besides force pose having estimate elastic model object <eos> use expectation maximization strategy each parameters successively learned within partial steps while robustly dealing missing observations <eos> thoroughly validate approach both mocap real sequences showing more accurate three dimensional reconstructions than state art additionally providing estimate full elastic model no priori information <eos> <eop> versatile scene model differentiable visibility applied generative pose estimation <eos> generative reconstruction method compute three dimensional configuration such pose geometry shape optimizing overlap projected three dimensional shape model image <eos> proper handling occlusions big challenge since visibility function indicates if surface point seen camera often formulated closed form general discrete non differentiable occlusion boundaries <eos> present new scene representation enables analytically differentiable closed form formulation surface visibility <eos> contrast previous method yields smooth analytically differentiable efficient optimize pose similarity energies rigorous occlusion handling fewer local minima experimentally verified improved convergence numerical optimization <eos> underlying idea new image formation model represents opaque object translucent medium smooth gaussian density distribution turns visibility into smooth phenomenon <eos> demonstrate advantages versatile scene model several generative pose estimation problems namely marker less multi object pose estimation marker less human motion capture few cameras image based three dimensional geometry estimation <eos> <eop> semantic pose using deep network trained synthetic rgb <eos> work address problem indoor scene understanding rgb image <eos> specifically propose find instances common furniture classes their spatial extent their pose respect generalized class models <eos> accomplish use deep wide multi output convolutional neural network cnn predicts class pose location possible object simultaneously <eos> overcome lack large annotated rgb training set especially pose use fly rendering pipeline generates realistic cluttered room scenes parallel training <eos> then perform transfer learning relatively small amount publicly available annotated rgb data find model able successfully annotate even highly challenging real scenes <eos> importantly trained network able understand noisy sparse observations highly cluttered scenes remarkable degree accuracy inferring class pose very limited set cues <eos> additionally neural network only moderately deep computes class pose position tandem so overall run time significantly faster than existing method estimating all output parameters simultaneously parallel <eos> <eop> exploiting high level scene cues stereo reconstruction <eos> present novel approach three dimensional reconstruction inspired human visual system <eos> system unifies standard appearance matching triangulation techniques higher level reasoning scene understanding order resolve ambiguities between different interpretations scene <eos> types reasoning integrated approach includes recognising common configurations surface normals semantic edges <eos> convex concave occlusion boundaries <eos> also recognise coplanar collinear symmetric structures especially common man made environments <eos> <eop> point triangulation through polyhedron collapse using norm <eos> multi camera triangulation feature point based minimisation overall reprojection error get stuck suboptimal local minima require slow global optimisation <eos> reason researchers proposed optimising infinity norm single view reprojection errors avoids problem local minima entirely <eos> paper present novel method infinity triangulation minimizes infinity norm infinity reprojection errors apparently small difference leads much faster but equally accurate solution related mle under assumption uniform noise <eos> proposed method adopts new optimisation strategy based solving simple quadratic equations <eos> stands contrast fastest existing method solve sequence more complex auxiliary linear programming second order cone problems <eos> proposed algorithm performs well triangulation achieves same accuracy existing techniques while executing faster being straightforward implement <eos> <eop> optimizing viewing graph structure motion <eos> viewing graph represents set views related pairwise relative geometries <eos> context structure motion sfm viewing graph input incremental global estimation pipeline <eos> much effort put towards developing robust algorithms overcome potentially inaccurate relative geometries viewing graph during sfm <eos> paper take fundamentally different approach sfm instead focus improving quality viewing graph before applying sfm <eos> main contribution novel optimization improves quality relative geometries viewing graph enforcing loop consistency constraints epipolar point transfer <eos> show optimization greatly improves accuracy relative poses viewing graph removes need filtering steps robust algorithms typically used global sfm method <eos> addition optimized viewing graph used efficiently calibrate cameras scale <eos> combine viewing graph optimization focal length calibration into global sfm pipeline more efficient than existing approaches <eos> knowledge ours first global sfm pipeline capable handling uncalibrated image set <eos> <eop> intrinsic scene decomposition rgb image <eos> paper address problem computing intrinsic decomposition colors surface into albedo shading term <eos> surface reconstructed single multiple rgb image static scene obtained different views <eos> thereby extend improve existing works area intrinsic image decomposition <eos> variational framework formulate problem minimization energy composed two terms data term regularity term <eos> first term related image formation process expresses relation between albedo surface normals incident illumination <eos> use affine shading model combination lambertian model ambient lighting term <eos> model relevant lambertian surfaces <eos> when available multiple views used handle view dependent non lambertian reflections <eos> second term contains efficient combination regularizers illumination vector field albedo respectively <eos> unlike most previous approaches especially retinex like techniques terms depend image gradient texture thus reducing mixing shading reflectance artifacts leading better result <eos> obtained non linear optimization problem efficiently solved using cyclic block coordinate descent algorithm <eos> method outperforms range state art algorithms popular benchmark dataset <eos> <eop> hand pose estimation using randomized decision forest segmentation index point <eos> paper propose real time three dimensional hand pose estimation algorithm using randomized decision forest framework <eos> algorithm takes depth image input generates set skeletal joints output <eos> previous decision forest based method often give labels all point point cloud very early stage vote joint locations <eos> contrast algorithm only tracks set more flexible virtual landmark point named segmentation index point sips before reaching final decision leaf node <eos> roughly speaking sip represents centroid subset skeletal joints located leaves branch expanded sip <eos> inspired recent latent regression forest based hand pose estimation framework tang <eos> integrate sip into framework several important improvements first devise new forest growing strategy whose decision made using randomized feature guided sips <eos> second speed up training procedure since only sips skeletal joints estimated non leaf nodes <eos> third experimental result public benchmark datasets show clearly advantage proposed algorithm over previous state art method algorithm runs <eos> fps normal cpu without parallelism <eos> <eop> accurate camera calibration robust defocus using smartphone <eos> propose novel camera calibration method defocused image using smartphone under assumption defocus blur modeled convolution sharp image gaussian point spread function psf <eos> contrast existing calibration approaches require well focused image proposed method achieves accurate camera calibration severely defocused image <eos> robustness defocus due proposed set unidirectional binary patterns simplifies gaussian deconvolution gaussian deconvolution problem multiple observations <eos> capturing set patterns consecutively displayed smartphone formulate feature extraction deconvolution problem estimate feature point locations sub pixel accuracy blur kernel each location <eos> also compensate error camera parameters due refraction glass panel display device <eos> evaluate performance proposed method synthetic real data <eos> even under severe defocus method shows accurate camera calibration result <eos> <eop> high quality structure small motion rolling shutter cameras <eos> present practical three dimensional reconstruction method obtain high quality dense depth map narrow baseline image sequences captured commercial digital cameras such dslrs mobile phones <eos> depth estimation small motion gained interest means various photographic editing but important limitations present themselves form depth uncertainty due narrow baseline rolling shutter <eos> address problems introduce novel three dimensional reconstruction method narrow baseline image sequences effectively handles effects rolling shutter occur most commercial digital cameras <eos> additionally present depth propagation method fill holes associated unknown pixels based novel geometric guidance model <eos> both qualitative quantitative experimental result show new algorithm consistently generates better three dimensional depth maps than state art method <eos> <eop> photogeometric scene flow high detail dynamic three dimensional reconstruction <eos> photometric stereo ps established technique high detail reconstruction three dimensional geometry appearance <eos> correct surface integration errors ps often combined multiview stereo mvs <eos> dynamic object ps reconstruction also faces problem computing optical flow image alignment under rapid changes illumination <eos> current ps method typically compute optical flow mvs independent stages each one its own limitations errors introduced early regularization <eos> contrast scene flow method estimate geometry motion but lack fine detail ps <eos> paper proposes photogeometric scene flow pgsf high quality dynamic three dimensional reconstruction <eos> pgsf performs ps mvs simultaneously <eos> based two key observations while image alignment improves ps ps allows surfaces relit improve alignment ii ps provides surface gradients render smoothness term mvs unnecessary leading truly data driven continuous depth estimates <eos> synergy demonstrated quality resulting rgb appearance three dimensional geometry three dimensional motion <eos> <eop> blur aware disparity estimation defocus stereo image <eos> defocus blur usually causes performance degradation establishing visual correspondence between stereo image <eos> propose blur aware disparity estimation method robust mismatch focus stereo image <eos> relative blur resulting mismatch focus between stereo image approximated difference square diameters blur kernels <eos> based defocus stereo model propose relative blur versus disparity rbd model characterizes relative blur second order polynomial function disparity <eos> method alternates between rbd model update disparity update each iteration <eos> rbd model return refines disparity estimation updating matching cost aggregation weight compensate mismatch focus <eos> experiments using both synthesized real datasets demonstrate effectiveness proposed algorithm <eos> <eop> global structure motion similarity averaging <eos> global structure motion sfm method solve all cameras simultaneously all available relative motions <eos> better potential both reconstruction accuracy computation efficiency than incremental method <eos> however global sfm challenging mainly because two reasons <eos> firstly translation averaging difficult since essential matrix only tells direction relative translation <eos> secondly also hard filter out bad essential matrices due feature matching failures <eos> propose compute sparse depth image each camera solve both problems <eos> depth image help upgrade essential matrix similarity transformation determine scale relative translation <eos> thus camera registration formulated well posed similarity averaging problem <eos> depth image also make filtering essential matrices simple effective <eos> way translation averaging solved robustly two convex optimization problems reach global optimum rapidly <eos> demonstrate method various examples including sequential data internet data ambiguous data repetitive scene structures <eos> <eop> massively parallel multiview stereopsis surface normal diffusion <eos> present new massively parallel method high quality multiview matching <eos> work builds patchmatch idea starting randomly generated three dimensional planes scene space best fitting planes iteratively propagated refined obtain three dimensional depth normal field per view such robust photo consistency measure over all image maximized <eos> main novelties one hand formulate patchmatch scene space makes possible aggregate image similarity across multiple views obtain more accurate depth maps <eos> other hand modified diffusion like propagation scheme massively parallelized delivers dense multiview correspondence over ten <eos> megapixel image seconds consumer grade gpu <eos> method uses slanted support window thus no fronto parallel bias completely local parallel such computation time scales linearly image size inversely proportional number parallel threads <eos> furthermore low memory footprint four values per pixel independent depth range <eos> therefore scales exceptionally well handle multiple large image high depth resolution <eos> experiments dtu middlebury multiview datasets well oblique aerial image show method achieves very competitive result high accuracy completeness across range different scenarios <eos> <eop> variational patchmatch multiview reconstruction refinement <eos> work propose novel approach problem multi view stereo reconstruction <eos> building upon previously proposed patchmatch stereo pm huber algorithm introduce extension multi view scenario employs iterative refinement scheme <eos> proposed approach uses extended robustified volumetric truncated signed distance function representation advantageous fusion refined depth maps also raycasting current reconstruction estimation together estimated depth normals into arbitrary camera views <eos> formulate combined multi view stereo reconstruction refinement variational optimization problem <eos> newly introduced plane based smoothing term energy formulation guided current reconstruction confidence image contents <eos> further propose extension patchmatch scheme additional klt step avoid unnecessary sampling iterations <eos> improper camera poses corrected direct image aligment step performs robust outlier compensation means recently proposed kernel lifting framework <eos> speed up optimization variational formulation adapted scheme used faster convergence <eos> <eop> rigid possible volumetric shape template <eos> objective shape template sft infer object shape single image three dimensional object tem plate <eos> existing method called thin shell sft they represent object its outer surface <eos> may open surface thin object such piece paper closed surface thicker object such ball <eos> pro pose volumetric sft specifically handles object latter kind <eos> volumetric sft uses object full volume express deformation constraints reconstructs object surface interior deformation <eos> chal lenging problem because opaque object only part outer surface visible image <eos> inspired mesh editing techniques use rigid possible arap deformation model softly imposes local rigidity <eos> formalise arap isometric sft constrained variational optimisation problem solve using iterative opti misation <eos> present strategies find initial solution based thin shell sft volume propagation <eos> experi ments synthetic real data show method typical maximum relative error reconstruct ing deformation entire object including its back interior no visual data available <eos> <eop> general dynamic scene reconstruction multiple view video <eos> paper introduces general approach dynamic scene reconstruction multiple moving cameras without prior knowledge limiting constraints scene structure appearance illumination <eos> existing techniques dynamic scene reconstruction multiple wide baseline camera views primarily focus accurate reconstruction controlled environments cameras fixed calibrated background known <eos> approaches robust general dynamic scenes captured sparse moving cameras <eos> previous approaches outdoor dynamic scene reconstruction assume prior knowledge static background appearance structure <eos> primary contributions paper twofold automatic method initial coarse dynamic scene segmentation reconstruction without prior knowledge background appearance structure general robust approach joint segmentation refinement dense reconstruction dynamic scenes multiple wide baseline static moving cameras <eos> evaluation performed variety indoor outdoor scenes cluttered backgrounds multiple dynamic non rigid object such people <eos> comparison state art approaches demonstrates improved accuracy both multiple view segmentation dense reconstruction <eos> proposed approach also eliminates requirement prior knowledge scene structure appearance <eos> <eop> joint image handbook <eos> given multiple perspective photographs point correspondences form joint image effectively replica three dimensional space distributed across its two dimensional projections <eos> set characterized multilinear equations over image coordinates such epipolar trifocal constraints <eos> revisit paper geometric algebraic properties joint image address fundamental questions such how many multilinearities necessary sufficient determine camera geometry image correspondences <eos> new theoretical result paper answer questions very general setting turn intended serve handbook reference about multilinearities practitioners <eos> <eop> direct dense deformable template based non rigid three dimensional reconstruction rgb video <eos> paper tackle problem capturing dense detailed three dimensional geometry generic complex non rigid meshes using single rgb only commodity video camera direct approach <eos> while robust even real time solutions exist problem if observed scene static non rigid dense shape capture current systems typically restricted use complex multi camera rigs take advantage additional depth channel available rgb cameras deal specific shapes such faces planar surfaces <eos> contrast method makes use single rgb video input capture deformations generic shapes depth estimation dense per pixel direct <eos> first compute dense three dimensional template shape object using short rigid sequence subsequently perform online reconstruction non rigid mesh evolves over time <eos> energy optimization approach minimizes robust photometric cost simultaneously estimates temporal correspondences three dimensional deformations respect template mesh <eos> experimental evaluation show range qualitative result novel datasets compare against existing method requires multi frame optical flow perform quantitative evaluation against other template based approaches ground truth dataset <eos> <eop> single image pop up discriminatively learned parts <eos> introduce new approach estimating fine grained three dimensional shape continuous pose object single image <eos> given training set view exemplars learn select appearance based discriminative parts mapped onto three dimensional model through facility location optimization <eos> training set three dimensional models summarized into set basis shapes generalize linear combination <eos> given test image detect hypotheses each part <eos> main challenge select hypotheses compute three dimensional pose shape coefficients same time <eos> achieve optimize function considers simultaneously appearance matching parts well geometric reprojection error <eos> apply alternating direction method multipliers admm minimize resulting convex function <eos> main novel contribution simultaneous solution part localization detailed three dimensional geometry estimation maximizing both appearance geometric compatibility convex relaxation <eos> <eop> learning informative edge maps indoor scene layout prediction <eos> paper introduce new edge based feature task recovering three dimensional layout indoor scene single image <eos> indoor scenes certain edges very informative about spatial layout room namely edges formed pairwise intersections room faces two walls wall ceiling wall floor <eos> contrast previous approaches rely area based feature like geometric context orientation maps method attempts directly detect informative edges <eos> learn predict informative edge probability maps using two recent method exploit local global context respectively structured edge detection forests fully convolutional network pixelwise labeling <eos> show fully convolutional network quite successful predicting informative edges even when they lack contrast occluded accuracy further improved training network jointly predict edges geometric context <eos> using feature derived informative edge maps learn maximum margin structured classifier achieves state art performance layout prediction <eos> <eop> multi view convolutional neural network three dimensional shape recognition <eos> longstanding question computer vision concerns representation three dimensional shapes recognition should three dimensional shapes represented descriptors operating their native three dimensional formats such voxel grid polygon mesh they effectively represented view based descriptors address question context learning recognize three dimensional shapes collection their rendered views image <eos> first present standard cnn architecture trained recognize shapes rendered views independently each other show three dimensional shape recognized even single view accuracy far higher than using state art three dimensional shape descriptors <eos> recognition rates further increase when multiple views shapes provided <eos> addition present novel cnn architecture combines information multiple views three dimensional shape into single compact shape descriptor offering even better recognition performance <eos> same architecture applied accurately recognize human hand drawn sketches shapes <eos> conclude collection views highly informative three dimensional shape recognition amenable emerging cnn architectures their derivatives <eos> <eop> learning analysis synthesis pose estimation rgb image <eos> analysis synthesis successful approach many tasks computer vision such pose estimation object rgb image topic work <eos> idea compare observation output forward process such rendered image object interest particular pose <eos> due occlusion complicated sensor noise difficult perform comparison meaningful way <eos> propose approach learns compare while taking difficulties into account <eos> done describing posterior density particular object pose convolutional neural network cnn compares observed rendered image <eos> network trained maximum likelihood paradigm <eos> observe empirically cnn specialize geometry appearance specific object <eos> used object vastly different shapes appearances different backgrounds <eos> compared state art demonstrate significant improvement two different datasets include total eleven object cluttered background heavy occlusion <eos> <eop> surface profilometry using phase shifting de bruijn pattern <eos> novel structured light method color three dimensional surface profilometry proposed <eos> proposed method require color calibration camera projector pair may used reconstruction both dynamic static scenes <eos> method uses structured light pattern combination de bruijn color sequence sinusoidal fringe <eos> dynamic scenes hessian ridge detector gaussian mixture model combined extract stripe centers identify color <eos> stripes then uniquely identified using dynamic programming based smith waterman algorithm de bruijn window property <eos> static scenes phase shifting de bruijn window property combined obtain high accuracy reconstruction <eos> tested proposed method multiple object challenging surfaces different albedos demonstrate usability robustness method <eos> <eop> deep visual correspondence embedding model stereo matching costs <eos> paper presents data driven matching cost stereo matching <eos> novel deep visual correspondence embedding model trained via convolutional neural network large set stereo image ground truth disparities <eos> deep embedding model leverages appearance data learn visual similarity relationships between corresponding image patches explicitly maps intensity values into embedding feature space measure pixel dissimilarities <eos> experimental result kitti middlebury data set demonstrate effectiveness model <eos> first prove new measure pixel dissimilarity outperforms traditional matching costs <eos> furthermore when integrated global stereo framework method ranks top among all two frame algorithms kitti benchmark <eos> finally cross validation result show model able make correct predictions unseen data outside its labeled training set <eos> <eop> learning concept embeddings combined human machine expertise <eos> paper presents work snack low dimensional concept embedding algorithm combines human expertise automatic machine similarity kernels <eos> both parts complimentary human insight capture relationships apparent object visual similarity machine help relieve human having exhaustively specify many constraints <eos> show snack embeddings useful several tasks distinguishing prime nonprime numbers mnist discovering labeling mistakes caltech ucsd birds cub dataset help deep learned feature creating training datasets bird classifiers capturing subjective human taste new dataset foods qualitatively exploring unstructured set pictographic characters <eos> comparisons state art tasks show snack produces better concept embeddings require less human supervision than leading method <eos> <eop> deep multi patch aggregation network image style aesthetics quality estimation <eos> paper investigates problems image style aesthetics quality estimation require fine grained details high resolution image utilizing deep neural network training approach <eos> existing deep convolutional neural network mostly extracted one patch such down sized crop each image training example <eos> however one patch may always well represent entire image may cause ambiguity during training <eos> propose deep multi patch aggregation network training approach allows train models using multiple patches generated one image <eos> achieve constructing multiple shared columns neural network feeding multiple patches each columns <eos> more importantly propose two novel network layer statistics sorting support aggregation patches <eos> proposed deep multi patch aggregation network integrates shared feature learning aggregation function learning into unified framework <eos> demonstrate effectiveness deep multi patch aggregation network three problems <eos> image style recognition aesthetic quality categorization image quality estimation <eos> models trained using proposed network significantly outperformed state art all three applications <eos> <eop> towards computational baby learning weakly supervised approach object detection <eos> intuitive observations show baby may inherently possess capability recognizing new visual concept <eos> chair dog learning only very few positive instances taught parent others recognition capability gradually further improved exploring interacting real instances physical world <eos> inspired observations propose computational model weakly supervised object detection based prior knowledge modelling exemplar learning learning video contexts <eos> prior knowledge modeled pre trained convolutional neural network cnn <eos> when very few instances new concept given initial concept detector built exemplar learning over deep feature pre trained cnn <eos> well designed tracking solution then used discover more diverse instances massive online weakly labeled video <eos> once positive instance detected identified high score each video more instances possibly different view angles different distances tracked accumulated <eos> then concept detector fine tuned based new instances <eos> process repeated again again till obtain very mature concept detector <eos> extensive experiments pascal voc object detection datasets well demonstrate effectiveness framework <eos> beat state art full training based performances learning very few sample each object category along about weakly labeled video <eos> <eop> improving image classification location context <eos> widespread availability cellphones cameras gps capabilities common image being uploaded internet today gps coordinates associated them <eos> addition research tries predict gps coordinates visual feature also opens up door problems conditioned availability gps coordinates <eos> work tackle problem performing image classification location context given gps coordinates image both train test phases <eos> explore different ways encoding extracting feature gps coordinates show how naturally incorporate feature into convolutional neural network cnn current state art most image classification recognition problems <eos> also show how possible simultaneously learn optimal pooling radii subset feature within cnn framework <eos> evaluate model help promote research area identify set location sensitive concepts annotate subset yahoo flickr creative commons dataset gps coordinates concepts make publicly available <eos> leveraging location context able achieve almost gain mean average precision <eos> <eop> hico benchmark recognizing human object interactions image <eos> introduce new benchmark humans interacting common object hico recognizing human object interactions hoi <eos> demonstrate key feature hico diverse set interactions common object categories list well defined sense based hoi categories exhaustive labeling co occurring interactions object category each image <eos> perform depth analysis representative current approaches show dnns enjoy significant edge <eos> addition show semantic knowledge significantly improve hoi recognition especially uncommon categories <eos> <eop> delving deep into rectifiers surpassing human level performance imagenet classification <eos> rectified activation units rectifiers essential state art neural network <eos> work study rectifier neural network image classification two aspects <eos> first propose parametric rectified linear unit prelu generalizes traditional rectified unit <eos> prelu improves model fitting nearly zero extra computational cost little overfitting risk <eos> second derive robust initialization method particularly considers rectifier nonlinearities <eos> method enables train extremely deep rectified models directly scratch investigate deeper wider network architectures <eos> based learnable activation advanced initialization achieve <eos> top test error imagenet classification dataset <eos> relative improvement over ilsvrc winner googlenet <eos> knowledge result first surpass reported human level performance <eos> <eop> continuous pose estimation spatial ensemble fisher regressors <eos> paper treat problem continuous pose estimation object categories regression problem basis only training information <eos> while regression natural framework continuous problems regression method so far achieved inferior result respect three dimensional based based classification refinement approaches <eos> may attributed their weakness high intra class variability well noisy matching procedures lack geometrical constraints <eos> propose apply regression fisher encoded vectors computed large cells learning array fisher regressors <eos> fisher encoding makes algorithm flexible variations class appearance while array structure permits indirectly introduce spatial context information approach <eos> formulate problem map inference problem likelihood function composed generative term based prediction error generated ensemble fisher regressors well discriminative term based svm classifiers <eos> test algorithm three publicly available datasets envisage several difficulties such high intra class variability truncations occlusions motion blur obtaining state art result <eos> <eop> adaptive hashing fast similarity search <eos> staggering growth image video datasets algorithms provide fast similarity search compact storage crucial <eos> hashing method map data into hamming space shown promise however many method employ batch learning strategy computational cost memory requirements may become intractable infeasible larger larger datasets <eos> overcome challenges propose online learning algorithm based stochastic gradient descent hash functions updated iteratively streaming data <eos> experiments three image retrieval benchmarks online algorithm attains retrieval accuracy comparable competing state art batch learning solutions while formulation orders magnitude faster being online adaptable variations data <eos> moreover formulation yields improved retrieval performance over recently reported online hashing technique online kernel hashing <eos> <eop> single image three dimensional without single three dimensional image <eos> really need three dimensional labels order learn how predict three dimensional paper show one learn mapping appearance three dimensional properties without ever seeing single explicit three dimensional label <eos> rather than use explicit supervision use regularity indoor scenes learn mapping completely unsupervised manner <eos> demonstrate both standard three dimensional scene understanding dataset well internet image three dimensional unavailable precluding supervised learning <eos> despite never seeing three dimensional label method produces competitive result <eos> <eop> cross domain image retrieval dual attribute aware ranking network <eos> address problem cross domain image retrieval considering following practical application given user photo depicting clothing image goal retrieve same attribute similar clothing items online shopping stores <eos> challenging problem due large discrepancy between online shopping image usually taken ideal lighting pose background conditions user photos captured uncontrolled conditions <eos> address problem propose dual attribute aware ranking network darn retrieval feature learning <eos> more specifically darn consists two sub network one each domain whose retrieval feature representations driven semantic attribute learning <eos> show attribute guided learning key factor retrieval accuracy improvement <eos> addition further align nature retrieval problem impose triplet visual similarity constraint learning rank across two subnetworks <eos> another contribution work large scale dataset makes network learning feasible <eos> exploit customer review websites crawl large set online shopping image corresponding offline user photos fine grained clothing attributes <eos> around online shopping image about exact offline counterpart image online ones <eos> all image collected real world consumer websites reflecting diversity data modality makes dataset unique rare academic community <eos> extensively evaluate retrieval performance network different configurations <eos> top retrieval accuracy doubled when using proposed darn other than current popular solution using pre trained cnn feature only <eos> <eop> attribute graph graph based approach image ranking <eos> propose novel image representation termed attribute graph rank image their semantic similarity given query image <eos> attribute graph undirected fully connected graph incorporating both local global image characteristics <eos> graph nodes characterise object well overall scene context using mid level semantic attributes while edges capture object topology <eos> demonstrate effectiveness attribute graphs applying them problem image ranking <eos> benchmark performance algorithm rpascal rimagenet datasets created order evaluate ranking performance complex queries containing multiple object <eos> experimental evaluation shows modelling image attribute graphs result improved ranking performance over existing techniques <eos> <eop> contextual action recognition cnn <eos> there multiple cues image reveal action person performing <eos> example jogger pose characteristic jogging but scene <eos> road trail presence other joggers additional source information <eos> work exploit simple observation actions accompanied contextual cues build strong action recognition system <eos> adapt rcnn use more than one region classification while still maintaining ability localize action <eos> call system cnn <eos> action specific models feature maps trained jointly allowing action specific representations emerge <eos> mean ap pasal voc action dataset outperforming all other approaches field significant margin <eos> last show cnn limited action recognition <eos> particular cnn also used tackle fine grained tasks such attribute classification <eos> validate claim reporting state art performance berkeley attributes people dataset <eos> <eop> makes object memorable <eos> recent studies image memorability shed light distinguishes memorability different image intrinsic extrinsic properties make image memorable <eos> however clear understanding memorability specific object inside image remains elusive <eos> paper provide first attempt answer question exactly remembered about image augment both image object segmentations pascal dataset ground truth memorability scores shed light various factors properties make object memorable forgettable humans <eos> analyze various visual factors may influence object memorability <eos> color visual saliency object categories <eos> also study correlation between object image memorability find image memorability greatly affected memorability its most memorable object <eos> lastly explore effectiveness deep learning other computational approaches predicting object memorability image <eos> efforts offer deeper understanding memorability general thereby opening up avenues wide variety applications <eos> <eop> knn hashing factorized neighborhood representation <eos> hashing very effective many tasks reducing processing time compressing massive databases <eos> although lots approaches developed learn data dependent hash functions recent years how learn hash functions yield good performance acceptable computational memory cost still challenging problem <eos> based observation retrieval precision highly related knn classification accuracy paper proposes novel knn based supervised hashing method learns hash functions directly maximizing knn accuracy hamming embedded training data <eos> make scalable well large problem propose factorized neighborhood representation parsimoniously model neighborhood relationships inherent training data <eos> considering real world data often linearly inseparable further kernelize basic model improve its performance <eos> result proposed method able learn accurate hashing functions tolerable computation storage cost <eos> experiments four benchmarks demonstrate method outperforms state arts <eos> <eop> multi view complementary hash tables nearest neighbor search <eos> recent years witnessed success hashing techniques fast nearest neighbor search <eos> practice many applications <eos> visual search object detection image matching etc <eos> enjoyed benefits complementary hash tables information fusion over multiple views <eos> however most prior research mainly focused compact hash code cleaning rare work studies how build multiple complementary hash tables much less adaptively integrate information stemming multiple views <eos> paper first present novel multi view complementary hash table method learns complementarity hash tables data multiple views <eos> single multi view table using exemplar based feature fusion approximate inherent data similarities low rank matrix learn discriminative hash functions efficient way <eos> build complementary tables meanwhile maintain scalable training fast out sample extension exemplar reweighting scheme introduced update induced low rank similarity sequential table construction framework indeed brings mutual benefits between tables placing greater importance exemplars shared mis separated neighbors <eos> extensive experiments three large scale image datasets demonstrate proposed method significantly outperforms various naive solutions state art multi table method <eos> <eop> scalable person re identification benchmark <eos> paper contributes new high quality dataset person re identification named market <eos> generally current datasets limited scale consist hand drawn bboxes unavailable under realistic settings only one ground truth one query image each identity close environment <eos> tackle problems proposed market dataset featured three aspects <eos> first contains over annotated bboxes plus distractor set over image making largest person re id dataset date <eos> second image market dataset produced using deformable part model dpm pedestrian detector <eos> third dataset collected open system each identity multiple image under each camera <eos> minor contribution inspired recent advances large scale image search paper proposes unsupervised bag words descriptor <eos> view person re identification special task image search <eos> experiment show proposed descriptor yields competitive accuracy viper cuhk market datasets scalable large scale dataset <eos> <eop> mmss multi modal sharable specific feature learning rgb object recognition <eos> most feature learning method rgb object recognition either learn feature color depth modalities separately simply treat rgb undifferentiated four channel data cannot adequately exploit relationship between different modalities <eos> motivated intuition different modalities should contain only some modal specific patterns but also some shared common patterns propose multi modal feature learning framework rgb object recognition <eos> first construct deep cnn layer color depth separately then connect them carefully designed multi modal layer fuse color depth information enforcing common part shared feature different modalities <eos> way obtain feature reflecting shared properties well modal specific properties different modalities <eos> information multi modal learning frameworks back propagated early cnn layer <eos> experimental result show proposed multi modal feature learning method outperforms state art approaches two widely used rgb object benchmark datasets <eos> <eop> object detection via multi region semantic segmentation aware cnn model <eos> propose object detection system relies multi region deep convolutional neural network cnn also encodes semantic segmentation aware feature <eos> resulting cnn based representation aims capturing diverse set discriminative appearance factors exhibits localization sensitivity essential accurate object localization <eos> exploit above properties recognition module integrating iterative localization mechanism alternates between scoring box proposal refining its location deep cnn regression model <eos> thanks efficient use modules detect object very high localization accuracy <eos> detection challenges pascal voc pascal voc achieve map <eos> correspondingly surpassing any other published work significant margin <eos> <eop> neural activation constellations unsupervised part model discovery convolutional network <eos> part models object categories essential challenging recognition tasks differences categories subtle only reflected appearances small parts object <eos> present approach able learn part models completely unsupervised manner without part annotations even without given bounding boxes during learning <eos> key idea find constellations neural activation patterns computed using convolutional neural network <eos> experiments outperform existing approaches fine grained recognition cub oxford pets oxford flowers dataset case no part bounding box annotations available achieve state art performance stanford dog dataset <eos> also show benefits neural constellation models data augmentation technique fine tuning <eos> furthermore paper unites areas generic fine grained classification since approach suitable both scenarios <eos> <eop> cascaded sparse spatial bins efficient effective generic object detection <eos> novel efficient method extraction object proposals introduced <eos> its objectness function exploits deep spatial pyramid feature novel fast compute hog based edge statistic edgeboxes score <eos> efficiency achieved use spatial bins novel combination sparsity inducing group normalized svm <eos> state art recall performance achieved pascal voc significantly outperforming method comparable speed <eos> interestingly when only proposals per image considered method attains recall voc <eos> method improves map rcnn class specific detector increasing point when only proposals used each image <eos> system trained twenty classes performs well two hundred class ilsvrc set confirming generalization capability <eos> <eop> probabilistic label relation graphs ising models <eos> consider classification problems label space structure <eos> common example hierarchical label spaces corresponding case one label subsumes another <eos> animal subsumes dog <eos> but labels also mutually exclusive <eos> dog vs cat unrelated <eos> jointly model hierarchy exclusion relations notion hex hierarchy exclusion graph was introduced <eos> combined conditional random field crf deep neural network dnn resulting state art result when applied visual object classification problems training labels were drawn different levels imagenet hierarchy <eos> image might labeled basic level category dog rather than more specific label husky <eos> paper extend hex model allow soft probabilistic relations between labels useful when there uncertainty about relationship between two labels <eos> antelope sort furry but same degree grizzly bear <eos> call new model phex probabilistic hex <eos> show phex graph converted ising model allows use existing off shelf inference method contrast hex method needed specialized inference algorithms <eos> experimental result show significant improvements number large scale visual object classification tasks outperforming previous hex model <eos> <eop> predicting good feature image geo localization using per bundle vlad <eos> address problem recognizing place depicted query image using large database geo tagged image city scale <eos> particular discover feature useful recognizing place data driven manner use knowledge predict useful feature query image prior geo localization process <eos> allows achieve better performance while reducing number feature <eos> also both learning predict feature retrieving geo tagged image database propose per bundle vector locally aggregated descriptors pbvlad each maximally stable region described vector locally aggregated descriptors vlad multiple scale invariant feature detected within region <eos> experimental result show proposed approach achieves significant improvement over other baseline method <eos> <eop> task driven feature pooling image classification <eos> feature pooling important strategy achieve high performance image classification <eos> however most pooling method unsupervised heuristic <eos> paper propose novel task driven pooling tdp model directly learn pooled representation data discriminative manner <eos> different traditional method <eos> average max pooling tdp implicit pooling method elegantly integrates learning representations into given classification task <eos> optimization tdp equalize similarities between descriptors learned representation maximize classification accuracy <eos> tdp combined traditional bow models coding vectors recent state art cnn models feature maps achieve much better pooled representation <eos> furthermore self training mechanism used generate tdp representation new test image <eos> multi task extension tdp also proposed further improve performance <eos> experiments three databases flower indoor caltech well validate effectiveness models <eos> <eop> cutting edge soft correspondences multimodal scene parsing <eos> exploiting multiple modalities semantic scene parsing shown improve accuracy over single modality scenario <eos> existing method however assume corresponding region two modalities same label <eos> paper address problem data misalignment label inconsistencies <eos> due moving object semantic labeling violate assumption existing techniques <eos> end formulate multimodal semantic labeling inference crf introduce latent nodes explicitly model inconsistencies between two domains <eos> latent nodes allow only leverage information both domains improve their labeling but also cut edges between inconsistent region <eos> eliminate need hand tuning parameters model propose learn intra domain inter domain potential functions training data <eos> demonstrate benefits approach two publicly available datasets containing imagery three dimensional point clouds <eos> thanks latent nodes learning strategy method outperforms state art both cases <eos> <eop> one shot learning via compositions meaningful patches <eos> task discriminating one object another almost trivial human being <eos> however task computationally taxing most modern machine learning method whereas perform task ease given very few examples learning <eos> proposed quick grasp concept may come shared knowledge between new example examples previously learned <eos> believe key one shot learning sharing common parts each part holds immense amounts information how visual concept constructed <eos> propose unsupervised method learning compact dictionary image patches representing meaningful components object <eos> using patches feature build compositional model outperforms number popular algorithms one shot learning task <eos> demonstrate effectiveness approach hand written digits show model generalizes multiple datasets <eos> <eop> fastext efficient unconstrained scene text detector <eos> propose novel easy implement stroke detector based efficient pixel intensity comparison surrounding pixels <eos> stroke specific keypoints efficiently detected text fragments subsequently extracted local thresholding guided keypoint properties <eos> classification based effectively calculated feature then eliminates non text region <eos> stroke specific keypoints produce times less region segmentations still detect more characters than commonly exploited mser detector process times faster <eos> after novel efficient classification step number region reduced times less than standard method still almost times faster <eos> all stages proposed pipeline scale rotation invariant support wide variety scripts latin hebrew chinese etc <eos> when proposed detector plugged into scene text localization recognition pipeline state art text localization accuracy maintained whilst processing time significantly reduced <eos> <eop> multi scale recognition dag cnn <eos> explore multi scale convolutional neural nets cnn image classification <eos> contemporary approaches extract feature single output layer <eos> extracting feature multiple layer one simultaneously reason about high mid low level feature during classification <eos> resulting multi scale architecture itself seen feed forward model structured directed acyclic graph dag cnn <eos> use dag cnn learn set multi scale feature effectively shared between coarse fine grained classification tasks <eos> while fine tuning such models helps performance show even off self multi scale feature perform quite well <eos> present extensive analysis demonstrate state art classification performance three standard scene benchmarks sun mit scene <eos> terms heavily benchmarked mit scene datasets result reduce lowest previously reported error <eos> <eop> relaxed multiple instance svm application object discovery <eos> multiple instance learning mil served important tool wide range vision applications instance image classification object detection visual tracking <eos> paper propose novel method solve classical mil problem named relaxed multiple instance svm rmi svm <eos> treat positiveness instance continuous variable use noisy model enforce mil constraints optimize them jointly unified framework <eos> optimization problem efficiently solved using stochastic gradient decent <eos> extensive experiments demonstrate rmi svm consistently achieves superior performance various benchmarks mil <eos> moreover simply applied rmi svm challenging vision task common object discovery <eos> state arts result object discovery pascal voc datasets further confirm advantages proposed method <eos> <eop> im calories towards automated mobile vision food diary <eos> present system recognize contents your meal single image then predict its nutritional contents such calories <eos> simplest version assumes user eating restaurant know menu <eos> case collect image offline train multi label classifier <eos> run time apply classifier running your phone predict foods present your meal lookup corresponding nutritional facts <eos> apply method new dataset image different restaurants using cnn based classifier significantly outperforming previous work <eos> more challenging setting works outside restaurants <eos> case need estimate size foods well their labels <eos> requires solving segmentation depth volume estimation single image <eos> present cnn based approaches problems promising preliminary result <eos> <eop> lewis latent embeddings word image their semantics <eos> goal work bring semantics into tasks text recognition retrieval natural image <eos> although text recognition retrieval received lot attention recent years previous works focused recognizing retrieving exactly same word used query without taking paper ask following question predict semantic concepts directly word image without explicitly trying transcribe word image its characters any point goal propose convolutional neural network cnn weighted ranking loss objective ensures concepts relevant query image ranked ahead relevant <eos> also interpreted learning euclidean space word image concepts jointly embedded <eos> model learned end end manner image pixels semantic concepts using dataset synthetically generated word image concepts mined lexical database wordnet <eos> result show despite complexity task word image concepts indeed associated high degree accuracy <eos> <eop> per sample kernel adaptation visual recognition grouping <eos> object action scene representations corrupted noise significantly impair performance visual recognition <eos> typically partial occlusion clutter excessive articulation affects only subset all feature dimensions most importantly different dimensions corrupted different sample <eos> nevertheless common approach problem feature selection kernel method down weight eliminate entire training sample same dimensions all sample <eos> thus valuable signal lost resulting suboptimal classification <eos> goal therefore adjust contribution individual feature dimensions when comparing any two sample computing their similarity <eos> consequently per sample selection informative dimensions directly integrated into kernel computation <eos> interrelated problems learning parameters kernel classifier determining informative components each sample then addressed joint objective function <eos> approach integrated into learning stage any kernel based visual recognition problem affect computational performance retrieval phase <eos> experiments diverse challenges action recognition video indoor scene classification show general applicability approach its ability improve learning visual representations <eos> <eop> fine grained change detection misaligned scenes varied illuminations <eos> detecting fine grained subtle changes among scene critically important practice <eos> previous change detection method focusing detecting large scale significant changes cannot well <eos> paper proposes feasible end end approach challenging problem <eos> start active camera relocation quickly relocates camera nearly same pose position last time observation <eos> guarantee detection sensitivity accuracy minute changes observation capture group image under multiple illuminations need only roughly aligned last time lighting conditions <eos> given two times observations formulate fine grained change detection joint optimization problem three related factors <eos> normal aware lighting difference camera geometry correction flow real scene change mask <eos> solve three factors coarse fine manner achieve reliable change decision rank minimization <eos> build three real world datasets benchmark fine grained change detection misaligned scenes under varied multiple lighting conditions <eos> extensive experiments show superior performance approach over state art change detection method its ability distinguish real scene changes false ones caused lighting variations <eos> <eop> aggregating local deep feature image retrieval <eos> several recent works shown image descriptors produced deep convolutional neural network provide state art performance image classification retrieval problems <eos> also shown activations convolutional layer interpreted local feature describing particular image region <eos> local feature aggregated using aggregating method developed local feature <eos> fisher vectors thus providing new powerful global descriptor <eos> paper investigate possible ways aggregate local deep feature produce compact descriptors image retrieval <eos> first show deep feature traditional hand engineered feature quite different distributions pairwise similarities hence existing aggregation method carefully re evaluated <eos> such re evaluation reveals contrast shallow feature simple aggregation method based sum pooling provides best performance deep convolutional feature <eos> method efficient few parameters bears little risk overfitting when <eos> learning pca matrix <eos> addition suggest simple yet efficient query expansion scheme suitable proposed aggregation method <eos> overall new compact global descriptor improves state art four common benchmarks considerably <eos> <eop> learning deep object detectors three dimensional models <eos> crowdsourced three dimensional cad models easily accessible online potentially generate infinite number training image almost any object category <eos> show augmenting training data contemporary deep convolutional neural net dcnn models such synthetic data effective especially when real training data limited well matched target domain <eos> most freely available cad models capture three dimensional shape but often missing other low level cues such realistic object texture pose background <eos> detailed analysis use synthetic cad image probe ability dcnn learn without cues surprising findings <eos> particular show when dcnn fine tuned target detection task exhibits large degree invariance missing low level cues but when pretrained generic imagenet classification learns better when low level cues simulated <eos> show synthetic dcnn training approach significantly outperforms previous method benchmark pascal voc dataset when learning few shot scenario improves performance domain shift scenario office benchmark <eos> <eop> harvesting discriminative meta object deep cnn feature scene classification <eos> recent work scene classification still makes use generic cnn feature rudimentary manner <eos> paper present novel pipeline built upon deep cnn feature harvest discriminative visual object parts scene classification <eos> first use region proposal technique generate set high quality patches potentially containing object apply pre trained cnn extract generic deep feature patches <eos> then perform both unsupervised weakly supervised learning screen patches discover discriminative ones representing category specific object parts <eos> further apply discriminative clustering enhanced local cnn fine tuning aggregate similar object parts into groups called meta object <eos> scene image representation constructed pooling feature response maps all learned meta object multiple spatial scales <eos> confirmed scene image representation obtained using new pipeline capable delivering state art performance two popular scene benchmark datasets mit indoor sun <eos> <eop> scalable nonlinear embeddings semantic category based image retrieval <eos> propose novel algorithm task supervised discriminative distance learning nonlinearly embedding vectors into low dimensional euclidean space <eos> work challenging setting supervision constraints similar dissimilar pairs while training <eos> proposed method derived approximate kernelization linear mahalanobis like distance metric learning algorithm also seen kernel neural network <eos> number model parameters test time evaluation complexity proposed method dd dimensionality input feature dimension projection space contrast usual kernelization method unlike them complexity scale linearly number training examples <eos> propose stochastic gradient based learning algorithm makes method scalable <eos> number training examples while being nonlinear <eos> train method up half million training pairs dimensional cnn feature <eos> give empirical comparisons relevant baselines seven challenging datasets task low dimensional semantic category based image retrieval <eos> <eop> person re identification ranking optimisation discriminant context information analysis <eos> person re identification open challenging problem computer vision <eos> existing re identification approaches focus optimal method feature matching <eos> metric learning approaches study inter camera transformations such feature <eos> method hardly ever pay attention problem visual ambiguities shared between first ranks <eos> paper focus such problem introduce unsupervised ranking optimization approach based discriminant context information analysis <eos> proposed approach refines given initial ranking removing visual ambiguities common first ranks <eos> achieved analyzing their content context information <eos> extensive experiments three publicly available benchmark datasets different baseline method conducted <eos> result demonstrate remarkable improvement first positions ranking <eos> regardless selected dataset state art method strongly outperformed method <eos> <eop> unsupervised generation viewpoint annotated car dataset video <eos> object recognition approaches recently extended yield aside object class output also viewpoint pose <eos> training such approaches typically requires additional viewpoint keypoint annotation training data alternatively synthetic cad models <eos> paper present approach creates dataset image annotated bounding boxes viewpoint labels fully automated manner video <eos> assume scene static order reconstruct three dimensional surfaces via structure motion <eos> automatically detect when reconstruction fails normalize viewpoint three dimensional models aligning reconstructed point clouds <eos> exemplarily cars show expand large dataset annotated single image obtain improved performance when training viewpoint regressor joined dataset <eos> <eop> structured indoor modeling <eos> paper presents novel three dimensional modeling framework reconstructs indoor scene structured model panorama rgbd image <eos> scene geometry represented graph nodes correspond structural elements such rooms walls object <eos> approach devises structure grammar defines how scene graph manipulated <eos> grammar then drives principled new reconstruction algorithm grammar rules sequentially applied recover structured model <eos> paper also proposes new room segmentation algorithm offset map reconstruction algorithm used framework enforce architectural shape priors far beyond existing state art <eos> structured scene representation enables variety novel applications ranging indoor scene visualization automated floorplan generation inverse cad more <eos> tested framework algorithms six synthetic five real datasets qualitative quantitative evaluations <eos> <eop> time lapse reconstruction internet photos <eos> given internet photo collection landmark compute three dimensional time lapse video sequence virtual camera moves continuously time space <eos> while previous work assumed static camera addition camera motion during time lapse creates very compelling impression parallax <eos> achieving goal however requires addressing multiple technical challenges including solving time varying depth maps regularizing three dimensional point color profiles over time reconstructing high quality hole free image every frame projected profiles <eos> result show photorealistic time lapses skylines natural scenes over many years dramatic parallax effects <eos> <eop> global dense multiscale reconstruction billion point <eos> present variational approach surface reconstruction set oriented point scale information <eos> focus particularly scenarios non uniform point densities due image taken different distances <eos> contrast previous method integrate scale information objective globally optimize signed distance function surface balanced octree grid <eos> use finite element discretization dual structure octree minimizing number variables <eos> tetrahedral mesh generated efficiently dual structure also memory efficiency optimized such robust data terms used even very large scenes <eos> surface normals explicitly optimized used surface extraction improve reconstruction edges corners <eos> <eop> visibility point clouds <eos> possible determine visible subset point directly given point cloud interestingly was shown indeed case despite fact point cannot occlude each other task performed without surface reconstruction normal estimation <eos> operator very simple first transforms point new domain then constructs convex hull domain <eos> point lie convex hull transformed set point image visible point <eos> operator found numerous applications computer vision including face reconstruction keypoint detection finding best viewpoints reduction point many more <eos> current paper addresses fundamental question properties should transformation function satisfy order utilized operator show three such properties sufficient sign function monotonicity condition regarding function parameter <eos> correctness algorithm satisfies three properties proved <eos> finally show interesting application operator assignment visibility confidence score <eos> feature missing previous approaches binary yes no visibility determined <eos> score utilized various applications illustrate its use view dependent curvature estimation <eos> <eop> weakly supervised graph based semantic segmentation learning communities image parts <eos> present weakly supervised approach semantic segmentation <eos> goal assign pixel level labels given only partial information example image level labels <eos> important problem many application scenarios difficult get accurate segmentation feasible obtain detailed annotations <eos> proposed approach starts initial coarse segmentation followed spectral clustering approach groups related image parts into communities <eos> community driven graph then constructed captures spatial feature relationships between communities while label graph captures correlations between image labels <eos> finally mapping image level labels appropriate communities formulated convex optimization problem <eos> proposed approach require location information image level labels trained using partially labeled datasets <eos> compared state art weakly supervised approaches achieve significant performance improvement msrc dataset labelme dataset while being more than times faster <eos> <eop> piecewise flat embedding image segmentation <eos> image segmentation critical step many computer vision tasks including high level visual recognition scene understanding well low level photo video processing <eos> paper propose new nonlinear embedding called piecewise flat embedding image segmentation <eos> based theory sparse signal recovery piecewise flat embedding attempts identify segment boundaries while significantly suppressing variations within segments <eos> adopt regularized energy term formulation promote sparse solutions <eos> further devise effective two stage numerical algorithm based bregman iterations solve proposed embedding <eos> piecewise flat embedding easily integrated into existing image segmentation frameworks including segmentation based spectral clustering hierarchical segmentation based contour detection <eos> experiments bsds indicate segmentation algorithms incorporating embedding achieve significantly improved result both frameworks <eos> <eop> semantic image segmentation via deep parsing network <eos> paper addresses semantic image segmentation incorporating rich information into markov random field mrf including high order relations mixture label contexts <eos> unlike previous works optimized mrfs using iterative algorithm solve mrf proposing convolutional neural network cnn namely deep parsing network dpn enables deterministic end end computation single forward pass <eos> specifically dpn extends contemporary cnn architecture model unary terms additional layer carefully devised approximate mean field algorithm mf pairwise terms <eos> several appealing properties <eos> first different recent works combined cnn mrf many iterations mf were required each training image during back propagation dpn able achieve high performance approximating one iteration mf <eos> second dpn represents various types pairwise terms making many existing works its special cases <eos> third dpn makes mf easier parallelized speeded up graphical processing unit gpu <eos> dpn thoroughly evaluated pascal voc dataset single dpn model yields new state art segmentation accuracy <eos> <eop> human parsing contextualized convolutional neural network <eos> work address human parsing task novel contextualized convolutional neural network co cnn architecture well integrates cross layer context global image level context within super pixel context cross super pixel neighborhood context into unified network <eos> given input human image co cnn produces pixel wise categorization end end way <eos> first cross layer context captured basic local global local structure hierarchically combines global semantic structure local fine details within cross layer <eos> second global image level label prediction used auxiliary objective intermediate layer co cnn its outputs further used guiding feature learning subsequent convolutional layer leverage global image level context <eos> finally further utilize local super pixel contexts within super pixel smoothing cross super pixel neighbourhood voting formulated natural sub components co cnn achieve local label consistency both training testing process <eos> comprehensive evaluations two public datasets well demonstrate significant superiority co cnn architecture over other state arts human parsing <eos> particular score large dataset reaches <eos> co cnn significantly higher than <eos> state art algorithms cnn atr respectively <eos> <eop> holistically nested edge detection <eos> develop new edge detection algorithm addresses two critical issues long standing vision problem holistic image training multi scale feature learning <eos> proposed method holistically nested edge detection hed turns pixel wise edge classification into image image prediction means deep learning model leverages fully convolutional neural network deeply supervised nets <eos> hed automatically learns rich hierarchical representations guided deep supervision side responses crucially important order approach human ability resolve challenging ambiguity edge object boundary detection <eos> significantly advance state art bsd dataset ods score <eos> nyu depth dataset ods score <eos> so improved speed <eos> second per image orders magnitude faster than recent cnn based edge detection algorithms <eos> <eop> minimum barrier salient object detection fps <eos> propose highly efficient yet powerful salient object detection method based minimum barrier distance mbd transform <eos> mbd transform robust pixel value fluctuation thus effectively applied raw pixels without region abstraction <eos> present approximate mbd transform algorithm speedup over exact algorithm <eos> error bound analysis also provided <eos> powered fast mbd transform algorithm proposed salient object detection method runs fps significantly outperforms previous method similar speed four large benchmark datasets achieves comparable better performance than state art method <eos> furthermore technique based color whitening proposed extend method leverage appearance based backgroundness cue <eos> extended version further improves performance while still being one order magnitude faster than all other leading method <eos> <eop> learning image representations tied ego motion <eos> understanding how image object scenes behave response specific ego motions crucial aspect proper visual development yet existing visual learning method conspicuously disconnected physical source their image <eos> propose exploit proprioceptive motor signals provide unsupervised regularization convolutional neural network learn visual representations egocentric video <eos> specifically enforce learned feature exhibit equivariance <eos> they respond predictably transformations associated distinct ego motions <eos> three datasets show unsupervised feature learning approach significantly outperforms previous approaches visual recognition next best view prediction tasks <eos> most challenging test show feature learned video captured autonomous driving platform improve large scale scene recognition static image disjoint domain <eos> <eop> unsupervised visual representation learning context prediction <eos> work explores use spatial context source free plentiful supervisory signal training rich visual representation <eos> given only large unlabeled image collection extract random pairs patches each image train convolutional neural net predict position second patch relative first <eos> argue doing well task requires model learn recognize object their parts <eos> demonstrate feature representation learned using within image context indeed captures visual similarity across image <eos> example representation allows perform unsupervised visual discovery object like cats people even birds pascal voc detection dataset <eos> furthermore show learned convnet used cnn framework provides significant boost over randomly initialized convnet resulting state art performance among algorithms use only pascal provided training set annotations <eos> <eop> webly supervised learning convolutional network <eos> present approach utilize large amounts web data learning cnn <eos> specifically inspired curriculum learning present two step approach cnn training <eos> first use easy image train initial visual representation <eos> then use initial cnn adapt harder more realistic image leveraging structure data categories <eos> demonstrate two stage cnn outperforms fine tuned cnn trained imagenet pascal voc <eos> also demonstrate strength webly supervised learning localizing object web image training cnn style detector <eos> achieves best performance voc no voc training data used <eos> finally show approach quite robust noise performs comparably even when use image search result march pre cnn image search era <eos> <eop> fast cnn <eos> paper proposes fast region based convolutional network method fast cnn object detection <eos> fast cnn builds previous work efficiently classify object proposals using deep convolutional network <eos> compared previous work fast cnn employs several innovations improve training testing speed while also increasing detection accuracy <eos> fast cnn trains very deep vgg network faster than cnn faster test time achieves higher map pascal voc <eos> compared sppnet fast cnn trains vgg faster tests faster more accurate <eos> fast cnn implemented python using caffe available under open source mit license github <eos> com rbgirshick fast rcnn <eos> <eop> bilinear cnn models fine grained visual recognition <eos> propose bilinear models recognition architecture consists two feature extractors whose outputs multiplied using outer product each location image pooled obtain image descriptor <eos> architecture model local pairwise feature interactions translationally invariant manner particularly useful fine grained categorization <eos> also generalizes various orderless texture descriptors such fisher vector vlad <eos> present experiments bilinear models feature extractors based convolutional neural network <eos> bilinear form simplifies gradient computation allows end end training both network using image labels only <eos> using network initialized imagenet dataset followed domain specific fine tuning obtain <eos> accuracy cub dataset requiring only category labels training time <eos> present experiments visualizations analyze effects fine tuning choice two network speed accuracy models <eos> result show architecture compares favorably existing state art number fine grained datasets while being substantially simpler easier train <eos> moreover most accurate model fairly efficient running frames sec nvidia tesla gpu <eos> source code complete system will made available vis www <eos> edu bcnn <eop> discovering spatial extent relative attributes <eos> present weakly supervised approach discovers spatial extent relative attributes given only pairs ordered image <eos> contrast traditional approaches use global appearance feature rely keypoint detectors goal automatically discover image region relevant attribute even when attribute appearance changes drastically across its attribute spectrum <eos> accomplish first develop novel formulation combines detector local smoothness discover set coherent visual chains across image collection <eos> then introduce efficient way generate additional chains anchored initial discovered ones <eos> finally automatically identify most relevant visual chains create ensemble image representation model attribute <eos> through extensive experiments demonstrate method promise relative several baselines modeling relative attributes <eos> <eop> deep neural decision forests <eos> present deep neural decision forests novel approach unifies classification trees representation learning functionality known deep convolutional network training them end end manner <eos> combine two worlds introduce stochastic differentiable decision tree model steers representation learning usually conducted initial layer deep convolutional network <eos> model differs conventional deep network because decision forest provides final predictions differs conventional decision forests since propose principled joint global optimization split leaf node parameters <eos> show experimental result benchmark machine learning datasets like mnist imagenet find par superior result when compared state art deep models <eos> most remarkably obtain top errors only <eos> imagenet validation data when integrating forests single crop single seven model googlenet architecture respectively <eos> thus even without any form training data set augmentation improving <eos> error obtained best googlenet architecture models crops <eos> <eop> deep fried convnets <eos> fully connected layer deep convolutional neural network typically contain over network parameters consume majority memory required store network <eos> reducing number parameters while preserving predictive performance critically important deploying deep neural network memory constrained environments such gpus embedded devices <eos> paper show how kernel method particular single fastfood layer used replace fully connected layer deep convolutional neural network <eos> deep fried network end end trainable conjunction convolutional layer <eos> new architecture substantially reduces memory footprint convolutional network trained mnist imagenet no drop predictive performance <eop> semantic component analysis <eos> unsupervised weakly supervised visual learning large image collections critical order avoid time consuming error prone process manual labeling <eos> standard approaches rely method like multiple instance learning graphical models computationally intensive sensitive initialization <eos> other hand simpler component analysis clustering method usually cannot achieve meaningful invariances semantic interpretability <eos> address issues previous work present simple but effective method called semantic component analysis sca provides decomposition image into semantic components <eos> unsupervised sca decomposes additive image representations into spatially meaningful visual components naturally correspond object categories <eos> using overcomplete representation allows rich instance level constraints spatial priors sca gives improved result more interpretable components comparison traditional matrix factorization techniques <eos> if weakly supervised information available form image level tags sca factorizes set image into semantic groups superpixels <eos> also provide qualitative connections traditional method component analysis <eos> grassmann averages pca nmf <eos> effectiveness approach validated through synthetic data msrc sift flow datasets demonstrating competitive result unsupervised weakly supervised semantic segmentation <eos> <eop> low rank matrix factorization under general mixture noise distributions <eos> many computer vision problems posed learning low dimensional subspace high dimensional data <eos> low rank matrix factorization lrmf represents commonly utilized subspace learning strategy <eos> most current lrmf techniques constructed optimization problem using norm norm mainly deal laplacian gaussian noise respectively <eos> make lrmf capable adapting more complex noise paper proposes new lrmf model assuming noise mixture exponential power moep distributions proposes penalized moep model combining penalized likelihood method moep distributions <eos> such setting facilitates learned lrmf model capable automatically fitting real noise through moep distributions <eos> each component mixture adapted series preliminary super sub gaussian candidates <eos> expectation maximization em algorithm also designed infer parameters involved proposed pmoep model <eos> advantage method demonstrated extensive experiments synthetic data face modeling hyperspectral image restoration <eos> <eop> web scale image clustering revisited <eos> large scale duplicate detection clustering mining documents image conventionally treated seed detection via hashing followed seed growing heuristics using fast search <eos> principled clustering method especially kernelized spectral ones higher complexity difficult scale above millions <eos> under assumption documents image embedded euclidean space revisit recent advances approximate means variants borrow their best ingredients introduce new one inverted quantized means iq means <eos> key underlying concepts quantization data point multi index based inverted search centroids cells <eos> its quantization form hashing analogous seed detection while its updates analogous seed growing yet principled sense distortion minimization <eos> further design dynamic variant able determine number clusters single run nearly zero additional cost <eos> combined powerful deep learned representations achieve clustering million image collection single machine less than one hour <eos> <eop> learning discriminative reconstructions unsupervised outlier removal <eos> study problem automatically removing outliers noisy data application removing outlier image image collection <eos> address problem utilizing reconstruction errors autoencoder <eos> observe when data reconstructed low dimensional representations inliers outliers well separated according their reconstruction errors <eos> based basic observation gradually inject discriminative information learning process autoencoder make inliers outliers more separable <eos> experiments variety image datasets validate approach <eos> <eop> learning deconvolution network semantic segmentation <eos> propose novel semantic segmentation algorithm learning deep deconvolution network <eos> learn network top convolutional layer adopted vgg layer net <eos> deconvolution network composed deconvolution unpooling layer identify pixelwise class labels predict segmentation masks <eos> apply trained network each proposal input image construct final semantic segmentation map combining result all proposals simple manner <eos> proposed algorithm mitigates limitations existing method based fully convolutional network integrating deep deconvolution network proposal wise prediction segmentation method typically identifies detailed structures handles object multiple scales naturally <eos> network demonstrates outstanding performance pascal voc dataset achieve best accuracy <eos> among method trained without using microsoft coco dataset through ensemble fully convolutional network <eos> <eop> conditional random fields recurrent neural network <eos> pixel level labelling tasks such semantic segmentation play central role image understanding <eos> recent approaches attempted harness capabilities deep learning techniques image recognition tackle pixel level labelling tasks <eos> one central issue methodology limited capacity deep learning techniques delineate visual object <eos> solve problem introduce new form convolutional neural network combines strengths convolutional neural network cnn conditional random fields crfs based probabilistic graphical modelling <eos> end formulate conditional random fields gaussian pairwise potentials mean field approximate inference recurrent neural network <eos> network called crf rnn then plugged part cnn obtain deep network desirable properties both cnn crfs <eos> importantly system fully integrates crf modelling cnn making possible train whole deep network end end usual back propagation algorithm avoiding offline post processing method object delineation <eos> apply proposed method problem semantic image segmentation obtaining top result challenging pascal voc segmentation benchmark <eos> <eop> one triangle three parallelograms sampling strategy its application shape regression <eos> purpose paper threefold <eos> firstly paper introduces one triangle three parallelograms ottp sampling strategy viewed way index pixels given shape image <eos> secondly framework cascaded shape regression including ottp sampling presented <eos> short framework involves binary pixel tests appearance feature combined shape feature followed large linear system each regression stage cascade <eos> proposed solution found produce state art result task facial landmark estimation <eos> thirdly dependence accuracy landmark predictions placement mean shape within detection box discussed method visualize presented <eos> <eop> boosting object proposals pascal coco <eos> computer vision general object proposals particular nowadays strongly influenced databases researchers evaluate performance their algorithms <eos> paper studies transition pascal visual object challenge dataset benchmark reference last years updated bigger more challenging microsoft common object context <eos> first review deeply analyze new challenges opportunities database presents <eos> then survey current state art object proposals evaluate focusing how generalizes new dataset <eos> sight result propose various lines research take advantage new benchmark improve techniques <eos> explore one lines leads improvement over state art <eos> <eop> secrets grabcut kernel means <eos> log likelihood energy term popular model fitting segmentation method <eos> zhu yuille chan vese grabcut presented generalized probabilistic means energy color space clustering <eos> interpretation reveals some limitations <eos> propose alternative approach color clustering using kernel means energy well known properties such non linear separation scalability higher dimensional feature spaces <eos> bound formulation kernel means allows combine general pair wise feature clustering method image grid regularization using graph cuts similarly standard color model fitting techniques segmentation <eos> unlike histogram gmm fitting approach closely related average association normalized cut <eos> but contrast previous pairwise clustering algorithms approach incorporate any standard geometric regularization image domain <eos> analyze extreme cases kernel bandwidth <eos> gini bias demonstrate effectiveness knn based adaptive bandwidth strategies <eos> kernel means approach segmentation benefits higher dimensional feature standard model fitting fails <eos> <eop> video matting via sparse low rank representation <eos> introduce novel method video matting via sparse low rank representation <eos> previous matting method introduced nonlocal prior estimate alpha matte achieved impressive result some data <eos> however one hand searching inadequate excessive sample may miss good sample introduce noise other hand difficult construct consistent nonlocal structures pixels similar feature yielding spatially temporally inconsistent video mattes <eos> paper proposed novel video matting method achieve spatially temporally consistent matting result <eos> toward end sparse low rank representation model introduced pursue consistent nonlocal structures pixels similar feature <eos> sparse representation used adaptively select best sample accurately construct nonlocal structures all pixels while low rank representation used globally ensure consistent nonlocal structures pixels similar feature <eos> two representations combined generate consistent video mattes <eos> experimental result show method achieved high quality result variety challenging examples featuring illumination changes feature ambiguity topology changes transparency variation dis occlusion fast motion motion blur <eos> <eop> joint object part segmentation using deep learned potentials <eos> segmenting semantic object image parsing them into their respective semantic parts fundamental steps towards detailed object understanding computer vision <eos> paper propose joint solution tackles semantic object part segmentation simultaneously higher object level context provided guide part segmentation more detailed part level localization utilized refine object segmentation <eos> specifically first introduce concept semantic compositional parts scp similar semantic parts grouped shared among different object <eos> two stream fully convolutional network fcn then trained provide scp object potentials each pixel <eos> same time compact set segments also obtained scp predictions network <eos> given potentials generated segments order explore long range context finally construct efficient fully connected conditional random field fcrf jointly predict final object part labels <eos> extensive evaluation three different datasets shows approach mutually enhance performance object part segmentation outperforms current state art both tasks <eos> <eop> low rank tensor constrained multiview subspace clustering <eos> paper explore problem multiview subspace clustering <eos> introduce low rank tensor constraint explore complementary information multiple views accordingly establish novel method called low rank tensor constrained multiview subspace clustering lt msc <eos> method regards subspace representation matrices different views tensor captures dexterously high order correlations underlying multiview data <eos> then tensor equipped low rank constraint models elegantly cross information among different views reduces effectually redundancy learned subspace representations improves accuracy clustering well <eos> inference process affinity matrix clustering formulated tensor nuclear norm minimization problem constrained additional norm regularizer some linear equalities <eos> minimization problem convex thus solved efficiently augmented lagrangian alternating direction minimization adm method <eos> extensive experimental result four benchmark datasets show effectiveness proposed lt msc method <eos> <eop> bodyprint pose invariant three dimensional shape matching human bodies <eos> three dimensional human body shape matching large potential many real world applications especially recent advances three dimensional range sensing technology <eos> address problem proposing novel holistic human body shape descriptor called bodyprint <eos> compute bodyprint given body scan fit deformable human body mesh project mesh parameters low dimensional subspace improves discriminability across different persons <eos> experiments carried out three real world human body datasets demonstrate bodyprint robust pose variation well missing information sensor noise <eos> improves matching accuracy significantly compared conventional three dimensional shape matching techniques using local feature <eos> facilitate practical applications shape database may grow over time also extend learning framework handle online updates <eos> <eop> middle child problem revisiting parametric min cut seeds object proposals <eos> object proposals recently fueled progress detection performance <eos> proposals aim provide category agnostic localizations all object image <eos> one way generate proposals perform parametric min cuts over seed locations <eos> paper demonstrates standard parametric cut models ineffective obtaining medium sized object refer middle child problem <eos> propose new energy minimization framework incorporating geodesic distances between segments solves problem <eos> addition introduce new superpixel merging algorithm generate small set seeds reliably cover large number object all sizes <eos> call method poise proposals object improved seeds energies <eos> poise enables parametric min cuts reach their full potential <eos> pascal voc generates segments average overlap <eos> whereas closest competing method require more than proposals reach same accuracy <eos> show detailed quantitative comparisons against state art method pascal voc microsoft coco segmentation challenges <eos> <eop> contour guided hierarchical model shape matching <eos> its simplicity effectiveness star model popular shape matching <eos> however suffers loose geometric connections among parts <eos> paper present novel algorithm reconsiders connections reduces global matching set interrelated local matching <eos> purpose divide shape template into overlapped parts model matching through part based layered structure uses latent variable constrain parts deformation <eos> inference each part used localizing candidates partial matching <eos> thanks contour fragments partial matching solved via modified dynamic programming <eos> overlapped region among parts template then explored make candidates parts meet their shared point <eos> process fulfilled via refined procedure based iterative dynamic programming <eos> result ethz shape inria horse datasets demonstrate benefits proposed algorithm <eos> <eop> robust image segmentation using contour guided color palettes <eos> contour guided color palette ccp proposed robust image segmentation <eos> efficiently integrates contour color cues image <eos> find representative colors image color sample along long contours between region similar spirit machine learning methodology focus sample near decision boundaries collected followed mean shift ms algorithm sampled color space achieve image dependent color palette <eos> color palette provides preliminary segmentation spatial domain further fine tuned post processing techniques such leakage avoidance fake boundary removal small region mergence <eos> segmentation performances ccp ms compared analyzed <eos> while ccp offers acceptable standalone segmentation result further integrated into framework layered spectral segmentation produce more robust segmentation <eos> superior performance ccp based segmentation algorithm demonstrated experiments berkeley segmentation dataset <eos> <eop> joint optimization segmentation color clustering <eos> binary energy optimization popular approach segmenting color image into foreground background region <eos> model appearance region color relatively high dimensional feature should handled effectively <eos> full color histogram usually too sparse reliable <eos> one approach explicitly reduce dimensionality clustering quantizing color space <eos> another popular approach fit gmms soft implicit clustering color space <eos> approaches work well when foreground background sufficiently distinct <eos> cases more subtle difference appearance both approaches may reduce even eliminate foreground background distinction <eos> happens because either color clustering performed completely independently segmentation process preprocessing step clustering independently foreground independently background gmm <eos> propose make clustering integral part segmentation including new clustering term energy function <eos> energy function clustering term favours clusterings make foreground background appearance more distinct <eos> thus energy function jointly optimizes over color clustering foreground background models segmentation <eos> exact optimization feasible therefore develop approximate algorithm <eos> show advantage including color clustering term into energy function camouflage image well standard segmentation datasets <eos> <eop> boxsup exploiting bounding boxes supervise convolutional network semantic segmentation <eos> recent leading approaches semantic segmentation rely deep convolutional network trained human annotated pixel level segmentation masks <eos> such pixel accurate supervision demands expensive labeling effort limits performance deep network usually benefit more training data <eos> paper propose method achieves competitive accuracy but only requires easily obtained bounding box annotations <eos> basic idea iterate between automatically generating region proposals training convolutional network <eos> two steps gradually recover segmentation masks improving network vise versa <eos> method called boxsup produces competitive result <eos> map validation supervised boxes only par strong baselines <eos> map fully supervised masks under same setting <eos> leveraging large amount bounding boxes boxsup further yields state art result pascal voc pascal context <eos> <eop> detection segmentation curved reflection symmetric structures <eos> symmetry one key components gestalt theory provides important mid level cue serves input higher visual processes such segmentation <eos> work propose complete approach links detection curved reflection symmetries produce symmetry constrained segments structures region real image clutter <eos> curved reflection symmetry detection leverage patch based symmetric feature train structured random forest classifier detects multiscaled curved symmetries image <eos> next using curved symmetries modulate novel symmetry constrained foreground background segmentation their symmetry scores so enforce global symmetrical consistency final segmentation <eos> achieved imposing pairwise symmetry prior encourages symmetric pixels same labels over mrf based representation input image edges final segmentation obtained via graph cuts <eos> experimental result over four publicly available datasets containing annotated symmetric structures symmax bsd parts weizmann horse ny roads demonstrate approach applicability different environments state art performance <eos> <eop> unsupervised tube extraction using transductive learning dense trajectories <eos> address problem automatic extraction foreground object video <eos> goal provide method unsupervised collection sample further used object detection training without any human intervention <eos> use well known selective search approach produce initial still image based segmentation video frames <eos> initial set proposals pruned temporally extended using optical flow transductive learning <eos> specifically propose use dense trajectories order robustly match track candidate boxes over different frames <eos> obtained box tracks used collect sample unsupervised training track specific detectors <eos> finally detectors run video extract final tubes <eos> combination appearance based static objectness selective search motion information dense trajectories transductive learning detectors forced overfit unsupervised data used training makes proposed approach extremely robust <eos> outperform state art systems large margin common benchmarks used tube proposal evaluation <eos> <eop> compositional hierarchical representation shape manifolds classification non manifold shapes <eos> address problem statistical learning shape models invariant translation rotation scale compositional hierarchies when data spaces measurements shape spaces topological manifolds <eos> practice problem observed while modeling shapes having multiple disconnected components <eos> partially occluded shapes cluttered scenes <eos> resolve aforementioned problem first reformulating relationship between data shape spaces considering interaction between receptive fields rfs shape manifolds sms compositional hierarchical shape vocabulary <eos> then suggest method model topological structure sms statistical learning geometric transformations shapes defined group actions sms <eos> purpose design disjoint union topology using indexing mechanism formation shape models sms vocabulary recursively <eos> represent topological relationship between shape components using graphs aggregated construct hierarchical graph structure shape vocabulary <eos> end introduce framework implement indexing mechanisms employment vocabulary structural shape classification <eos> proposed approach used construct invariant shape representations <eos> result benchmark shape classification outperform state art method <eos> <eop> shell pca statistical shape modelling shell space <eos> paper describe how perform principal components analysis shell space <eos> thin shells physical model surfaces non zero thickness whose deformation dissipates elastic energy <eos> thin shells their discrete counterparts considered reside shell space notion distance given elastic energy required deform one shape into another <eos> setting show how perform statistical analysis set shapes meshes dense correspondence providing hybrid between physical statistical shape modelling <eos> resulting models better able capture non linear deformations example resulting articulated motion even when training data very sparse compared dimensionality observation space <eos> <eop> learning combine mid level cues object proposal generation <eos> recent years region proposals replaced sliding windows support object recognition offering more discriminating shape appearance information through improved localization <eos> one powerful approach generating region proposals based minimizing parametric energy functions parametric maxflow <eos> paper introduce parametric min loss pml novel structured learning framework parametric energy functions <eos> while pml generally applicable different domains use context region proposals learn combine set mid level grouping cues yield small set object region proposals high recall <eos> learning framework accounts multiple diverse outputs complemented diversification seeds based image location color <eos> approach casts perceptual grouping cue combination novel structured learning framework yields baseline improvements voc coco <eos> <eop> enhancing road maps parsing aerial image around world <eos> recent years contextual models exploit maps shown very effective many recognition localization tasks <eos> paper propose exploit aerial image order enhance freely available world maps <eos> towards goal make use openstreetmap formulate problem one inference markov random field parameterized terms location road segment centerlines well their width <eos> parameterization enables very efficient inference returns only topologically correct roads <eos> particular segment all osm roads whole world single day using small cluster computers <eos> importantly approach generalizes very well trained using only <eos> km aerial imagery produce very accurate result any location across globe <eos> demonstrate effectiveness approach outperforming state art two new benchmarks collect <eos> then show how enhanced maps beneficial semantic segmentation ground image <eos> <eop> probabilistic appearance models segmentation classification <eos> statistical shape appearance models often based accurate identification one one correspondences training data set <eos> same time determination corresponding landmarks most challenging part such method <eos> hufnagel etal developed alternative method using correspondence probabilities statistical shape model <eos> propose use probabilistic correspondences statistical appearance models incorporating appearance information into framework <eos> point based representation employed representing image set vectors assembling position appearances <eos> using probabilistic correspondences between multi dimensional feature vectors eliminates need extensive preprocessing find corresponding landmarks reduces dependence generated model landmark positions <eos> then maximum posteriori approach used derive single global optimization criterion respect model parameters observation dependent parameters directly affects shape appearance information considered structures <eos> model generation fitting expressed optimizing same criterion <eos> developed framework describes modeling process concise flexible mathematical way allows additional constraints topological regularity modeling process <eos> furthermore eliminates demand costly correspondence determination <eos> apply model segmentation landmark identification hand ray image segmentation information modeled further feature vectorial image representation <eos> result demonstrate feasibility model reconstruct contours landmarks unseen test image <eos> furthermore apply model tissue classification model generated healthy brain tissue using mri slices <eos> applying model image stroke patients probabilistic correspondences used classify between healthy pathological structures <eos> result demonstrate ability probabilistic model recognize healthy pathological tissue automatically <eos> <eop> randomized ensemble approach industrial ct segmentation <eos> tuning models parameters common segmentation approaches challenging especially presence noise artifacts <eos> ensemble based techniques attempt compensate randomly varying models parameters create diverse set hypotheses subsequently ranked arrive best solution <eos> however method restricted cases underlying models well established <eos> practice difficult determine suitable base model amount randomization required <eos> furthermore multi object scenes no single hypothesis may perform well all object reducing overall quality result <eos> paper presents new ensemble based segmentation framework industrial ct image demonstrating comparatively simple models randomization strategies significantly improve result over existing techniques <eos> furthermore introduce per object based ranking followed consensus inference outperform even best case scenario existing hypothesis ranking approaches <eos> demonstrate effectiveness approach using set noise artifact rich ct image baggage security show significantly outperforms existing solutions area <eos> <eop> semi supervised normalized cuts image segmentation <eos> since its introduction powerful graph based method image segmentation normalized cuts ncuts algorithm generalized incorporate expert knowledge about how certain pixels region should grouped how resulting segmentation should biased correlated priors <eos> previous approaches incorporate hard must link constraints how certain pixels should grouped well hard cannot link constraints how other pixels should separated into different groups <eos> paper reformulate ncuts allow both set constraints handled soft manner enabling user tune degree constraints satisfied <eos> approximate spectral solution reformulated problem exists without requiring explicit construction large dense matrix hence computation time comparable unconstrained ncuts <eos> using synthetic data real imagery show soft handling constraints yields better result than unconstrained ncuts enables more robust clustering segmentation than possible when constraints strictly enforced <eos> <eop> stereosnakes contour based consistent object extraction stereo image <eos> consistent object extraction plays essential role stereo image editing population stereoscopic three dimensional media <eos> most previous method perform segmentation entire image both views using dense stereo correspondence constraints <eos> find such kind method computation highly redundant since two views near duplicate <eos> besides consistency may violated due imperfectness current stereo matching algorithms <eos> paper propose contour based method searches consistent object contours instead region <eos> integrates both stereo correspondence object boundary constraints into energy minimization framework <eos> proposed method several advantages compared previous works <eos> first searching space restricted object boundaries thus efficiency significantly improved <eos> second discriminative power object contours result more consistent segmentation <eos> furthermore proposed method effortlessly extend existing single image segmentation method work stereo scenarios <eos> experiment adobe benchmark shows superior extraction accuracy significant improvement efficiency method state art <eos> also demonstrate few applications how method used basic tool stereo image editing <eos> <eop> semantic segmentation rgbd image mutex constraints <eos> paper address problem semantic scene segmentation rgb image indoor scenes <eos> propose novel image region labeling method augments crf formulation hard mutual exclusion mutex constraints <eos> way approach make use rich accurate three dimensional geometric structure coming kinect principled manner <eos> final labeling result must satisfy all mutex constraints allows eliminate configurations violate common sense physics laws like placing floor above night stand <eos> three classes mutex constraints proposed global object co occurrence constraint relative height relationship constraint local support relationship constraint <eos> evaluate approach nyu depth dataset consists cluttered indoor scenes also test generalization model trained nyu depth dataset directly recent sun dataset without any new training <eos> experimental result show significantly outperform state art method scene labeling both datasets <eos> <eop> weakly semi supervised learning deep convolutional network semantic image segmentation <eos> deep convolutional neural network dcnns trained large number image strong pixel level annotations recently significantly pushed state art semantic image segmentation <eos> study more challenging problem learning dcnns semantic image segmentation either weakly annotated training data such bounding boxes image level labels combination few strongly labeled many weakly labeled image sourced one multiple datasets <eos> develop expectation maximization em method semantic image segmentation model training under weakly supervised semi supervised settings <eos> extensive experimental evaluation shows proposed techniques learn models delivering competitive result challenging pascal voc image segmentation benchmark while requiring significantly less annotation effort <eos> share source code implementing proposed system bitbucket <eos> org deeplab deeplab public <eos> <eop> efficient decomposition image mesh graphs lifted multicuts <eos> formulations image decomposition problem multicut problem mp <eos> superpixel graph received considerable attention <eos> contrast instances mp <eos> pixel grid graph received little attention firstly because mp np hard instances <eos> pixel grid graph hard solve practice secondly due lack long range terms objective function mp <eos> propose generalization mp long range terms lmp <eos> design implement two efficient algorithms primal feasible heuristics mp lmp allow study instances both problems <eos> pixel grid graphs image bsds benchmark <eos> decompositions obtain differ significantly state art suggesting lmp competitive formulation image decomposition problem <eos> demonstrate generality lmp apply also mesh decomposition problem posed princeton benchmark obtaining state art decompositions <eos> <eop> parsimonious labeling <eos> propose new family discrete energy minimization problems call parsimonious labeling <eos> energy function consists unary potentials high order clique potentials <eos> while unary potentials arbitrary clique potentials proportional diversity set unique labels assigned clique <eos> intuitively energy function encourages labeling parsimonious use few labels possible <eos> turn allows capture useful cues important computer vision applications such stereo correspondence image denoising <eos> furthermore propose efficient graph cuts based algorithm parsimonious labeling problem provides strong theoretical guarantees quality solution <eos> algorithm consists three steps <eos> first approximate given diversity using mixture novel hierarchical pn potts model <eos> second use divide conquer approach each mixture component each subproblem solved using efficient alpha expansion algorithm <eos> provides small number putative labelings one each mixture component <eos> third choose best putative labeling terms energy value <eos> using both synthetic standard real datasets show algorithm significantly outperforms other graph cuts based approaches <eos> <eop> volumetric bias segmentation reconstruction secrets solutions <eos> many standard optimization method segmentation reconstruction compute ml model estimates appearance geometry segments <eos> zhu yuille torr chan vese grabcut delong <eos> observe standard likelihood term formulations corresponds generalized probabilistic means energy <eos> learning well known energy strong bias clusters equal size express penalty kl divergence uniform distribution cardinalities <eos> however volumetric bias mostly ignored computer vision <eos> demonstrate significant artifacts standard segmentation reconstruction method due bias <eos> moreover propose binary multi label optimization techniques either remove bias replace kl divergence term any given target volume distribution <eos> general ideas apply continuous discrete energy formulations segmentation stereo other reconstruction problems <eos> <eop> entropy minimization convex relaxation approaches <eos> despite their enormous success solving hard combinatorial problems convex relaxation approaches often suffer fact computed solutions far binary subsequent heuristic binarization may substantially degrade quality computed solutions <eos> paper propose novel relaxation technique incorporates entropy objective variable measure relaxation tightness <eos> show both theoretically experimentally augmenting objective function entropy term gives rise more binary solutions consequently solutions substantially tighter optimality gap <eos> use difference convex function dc programming efficient provably convergent solver arising convex concave minimization problem <eos> evaluate approach three prominent non convex computer vision challenges multi label inpainting image segmentation spatio temporal multi view reconstruction <eos> experiments show approach consistently yields better solutions respect original integral optimization problem <eop> adaptively unified semi supervised dictionary learning active point <eos> semi supervised dictionary learning aims construct dictionary utilizing both labeled unlabeled data <eos> enhance discriminative capability learned dictionary numerous discriminative terms proposed evaluating either prediction loss class separation criterion coding vectors labeled data but rare consideration power coding vectors corresponding unlabeled data <eos> paper present novel semi supervised dictionary learning method uses informative coding vectors both labeled unlabeled data adaptively emphasizes high confidence coding vectors unlabeled data enhance dictionary discriminative capability simultaneously <eos> doing so integrate discrimination dictionary induction classifier new testing data transduction labels unlabeled data into unified framework <eos> solve proposed problem effective iterative algorithm designed <eos> experimental result series benchmark databases show method outperforms other state art dictionary learning method most cases <eos> <eop> constrained convolutional neural network weakly supervised segmentation <eos> present approach learn dense pixel wise labeling image level tags <eos> each image level tag imposes constraints output labeling convolutional neural network cnn classifier <eos> propose constrained cnn ccnn method uses novel loss function optimize any set linear constraints output space <eos> predicted label distribution cnn <eos> loss formulation easy optimize incorporated directly into standard stochastic gradient descent optimization <eos> key idea phrase training objective biconvex optimization linear models then relax nonlinear deep network <eos> extensive experiments demonstrate generality new learning framework <eos> constrained loss yields state art result weakly supervised semantic image segmentation <eos> further demonstrate adding slightly more supervision greatly improve performance learning algorithm <eos> <eop> multiscale variable grouping framework mrf energy minimization <eos> present multiscale approach minimizing energy associated markov random fields mrfs energy functions include arbitrary pairwise potentials <eos> mrf represented hierarchy successively coarser scales problem each scale itself mrf suitably defined potentials <eos> representations used construct efficient multiscale algorithm seeks minimal energy solution original problem <eos> algorithm iterative feature bidirectional crosstalk between fine coarse representations <eos> use consistency criteria guarantee energy nonincreasing throughout iterative process <eos> algorithm evaluated real world datasets achieving competitive performance relatively short run times <eos> <eop> inferring best diverse labelings single one <eos> consider task finding best diverse solutions graphical model <eos> previous work batra <eos> algorithmic approach finding such solutions was proposed its usefulness was shown numerous applications <eos> contrary previous work propose novel formulation problem form single energy minimization problem specially constructed graphical model <eos> show method batra <eos> considered greedy approximate algorithm model whereas introduce efficient specialized optimization technique based alpha expansion <eos> evaluate method two application scenarios interactive semantic image segmentation binary multiple labels <eos> both cases achieve considerably better error rates than state art diversity method <eos> furthermore empirically discover binary label case were able reach global optimality all test instances <eos> <eop> convolutional sparse coding image super resolution <eos> sparse coding sc plays important role versatile computer vision applications such image super resolution sr <eos> most previous sc based sr method partition image into overlapped patches process each patch separately <eos> method however ignore consistency pixels overlapped patches strong constraint image reconstruction <eos> paper propose convolutional sparse coding csc based sr csc sr method address consistency issue <eos> csc sr involves three groups parameters learned set filters decompose low resolution lr image into lr sparse feature maps ii mapping function predict high resolution hr feature maps lr ones iii set filters reconstruct hr image predicted hr feature maps via simple convolution operations <eos> working directly whole image proposed csc sr algorithm need divide image into overlapped patches exploit image global correlation produce more robust reconstruction image local structures <eos> experimental result clearly validate advantages csc over patch based sc sr application <eos> compared state art sr method proposed csc sr method achieves highly competitive psnr result while demonstrating better edge texture preservation performance <eos> <eop> wavefront marching method solving eikonal equation cartesian grids <eos> paper presents new wavefront propagation method dealing classic eikonal equation <eos> while classic dijkstra like graph based techniques achieve solution log they approximate unique physically relevant solution very well <eos> fast marching method fmm were created efficiently solve continuous problem <eos> proposed approximation tries maintain complexity order make algorithm useful wide range contexts <eos> key idea behind method creation mini wave fronts combined propagate solution <eos> experimental result show improvement accuracy respect state art while average computational speed maintained log similar fmm techniques <eos> <eop> projection free method generalized eigenvalue problem nonsmooth regularizer <eos> eigenvalue problems ubiquitous computer vision covering very broad spectrum applications ranging estimation problems multi view geometry image segmentation <eos> few other linear algebra problems more mature set numerical routines available many computer vision libraries leverage such tools extensively <eos> however ability call underlying solver only black box often become restrictive <eos> many human loop settings vision frequently exploit supervision expert extent user considered subroutine overall system <eos> other cases there additional domain knowledge side even partial information one may want incorporate within formulation <eos> general regularizing generalized eigenvalue problem such side information remains difficult <eos> motivated needs paper presents optimization scheme solve generalized eigenvalue problems gep involving nonsmooth regularizer <eos> start alternative formulation gep feasibility set model involves stiefel manifold <eos> core paper presents end end stochastic optimization scheme resultant problem <eos> show how general algorithm enables improved statistical analysis brain imaging data regularizer derived other views disease pathology involving clinical measurements other image derived representations <eos> <eop> optimizing expected intersection over union candidate constrained crfs <eos> study question how make loss aware predictions image segmentation settings evaluation function intersection over union iou measure used widely evaluating image segmentation systems <eos> currently there two dominant approaches first approximates expected iou eiou score expected intersection over expected union eioeu second approach compute exact eiou but only over small set high quality candidate solutions <eos> begin asking approach should favor two typical image segmentation tasks <eos> studying question leads two new method draw ideas both existing approaches <eos> new method use eioeu approximation paired high quality candidate solutions <eos> experimentally show new approaches lead improved performance both image segmentation tasks <eos> <eop> higher order inference multi class log supermodular models <eos> higher order models shown very useful plethora computer vision tasks <eos> however existing techniques focused mainly map inference <eos> paper present first efficient approach towards approximate bayesian marginal inference general class high order multi label attractive models previous techniques slow down exponentially order clique size <eos> formalize task performing inference log supermodular models under partition constraints present efficient variational inference technique <eos> resulting optimization problems convex yield bounds partition function <eos> also obtain fully factorized approximation posterior used lieu true complicated distribution <eos> empirically demonstrate performance approach comparing traditional inference method challenging high fidelity multi label image segmentation dataset <eos> obtain state art classification accuracy map inference substantially improved roc curves using approximate marginals <eos> <eop> depth based hand pose estimation data method challenges <eos> hand pose estimation matured rapidly recent years <eos> introduction commodity depth sensors multitude practical applications spurred new advances <eos> provide extensive analysis state art focusing hand pose estimation single depth frame <eos> so implemented considerable number systems will release all software evaluation code <eos> summarize important conclusions here pose estimation appears roughly solved scenes isolated hands <eos> however method still struggle analyze cluttered scenes hands may interacting nearby object surfaces <eos> spur further progress introduce challenging new dataset diverse cluttered scenes <eos> many method evaluate themselves disparate criteria making comparisons difficult <eos> define consistent evaluation criteria rigorously motivated human experiments <eos> introduce simple nearest neighbor baseline outperforms most existing systems <eos> implies most systems generalize beyond their training set <eos> also reinforces under appreciated point training data important model itself <eos> conclude directions future progress <eos> <eop> adaptive dither voting robust spatial verification <eos> hough voting geometric transformation space allows realize spatial verification but remains sensitive feature detection errors because inflexible quantization single feature correspondences <eos> handle problem propose new method called adaptive dither voting robust spatial verification <eos> each correspondence instead hard mapping single transformation method augments its description using multiple dithered transformations deterministically generated other correspondences <eos> method reduces probability losing correspondences during transformation quantization provides high robustness regards mismatches imposing three geometric constraints dithering process <eos> also propose exploiting non uniformity hough histogram spatial similarity handle multiple matching surfaces <eos> extensive experiments conducted four datasets show superiority method <eos> method outperforms its state art counterparts both accuracy scalability especially when comes retrieval small rotated object <eos> <eop> alternating co quantization cross modal hashing <eos> paper addresses problem unsupervised learning binary hash codes efficient cross modal retrieval <eos> many unimodal hashing studies proven both similarity preservation data maintenance quantization quality essential improving retrieval performance binary hash codes <eos> however most existing cross modal hashing method mainly focused former latter still remains almost untouched <eos> propose method minimize binary quantization errors tailored cross modal hashing <eos> approach named alternating co quantization acq alternately seeks binary quantizers each modality space help connections other modality data so they give minimal quantization errors while preserving data similarities <eos> acq coupled various existing cross modal dimension reduction method such canonical correlation analysis cca substantially boosts their retrieval performance hamming space <eos> extensive experiments demonstrate acq outperform several state art method even when combined simple cca <eos> <eop> learning deep representation large scale attributes <eos> learning strong feature representations large scale supervision achieved remarkable success computer vision emergence deep learning techniques <eos> driven big visual data rich annotations <eos> paper contributes large scale object attribute database dataset available www <eos> hk xgwang imagenetattribute <eos> html contains rich attribute annotations over attributes sample object classes <eos> based imagenet object detection dataset annotates rotation viewpoint object part location part occlusion part existence common attributes class specific attributes <eos> then use dataset train deep representations extensively evaluate how attributes useful general object detection task <eos> order make better use attribute annotations deep learning scheme proposed modeling relationship attributes hierarchically clustering them into semantically meaningful mixture types <eos> experimental result show attributes helpful learning better feature improving object detection accuracy <eos> map ilsvrc object detection dataset <eos> map pascal voc object detection dataset <eos> such improvement well generalized across datasets <eos> <eop> deep learning strong parts pedestrian detection <eos> recent advances pedestrian detection attained transferring learned feature convolutional neural network convnet pedestrians <eos> convnet typically pre trained massive general object categories <eos> although feature able handle variations such poses viewpoints lightings they may fail when pedestrian image complex occlusions present <eos> occlusion handling one most important problem pedestrian detection <eos> unlike previous deep models directly learned single detector pedestrian detection propose deepparts consists extensive part detectors <eos> deepparts several appealing properties <eos> first deepparts trained weakly labeled data <eos> only pedestrian bounding boxes without part annotations provided <eos> second deepparts able handle low iou positive proposals shift away ground truth <eos> third each part detector deepparts strong detector detect pedestrian observing only part proposal <eos> extensive experiments caltech dataset demonstrate effectiveness deepparts yields new state art miss rate outperforming second best method <eos> <eop> flowing convnets human pose estimation video <eos> objective work human pose estimation video multiple frames available <eos> investigate convnet architecture able benefit temporal context combining information across multiple frames using optical flow <eos> end propose network architecture following novelties deeper network than previously investigated regressing heatmaps ii spatial fusion layer learn implicit spatial model iii optical flow used align heatmap predictions neighbouring frames iv final parametric pooling layer learns combine aligned heatmaps into pooled confidence map <eos> show architecture outperforms number others including one uses optical flow solely input layer one regresses joint coordinates directly one predicts heatmaps without spatial fusion <eos> new architecture outperforms state art large margin three video pose estimation datasets including very challenging poses wild dataset outperforms other deep method don use graphical model single image flic benchmark also chen yuille tompson <eos> high precision region <eos> <eop> top rank supervised binary coding visual search <eos> recent years binary coding techniques becoming increasingly popular because their high efficiency handling large scale computer vision applications <eos> demonstrated supervised binary coding techniques leverage supervised information significantly enhance coding quality hence greatly benefit visual search tasks <eos> typically modern binary coding method seeks learn group coding functions compress data sample into binary codes <eos> however few method pursued coding functions such precision top ranking list according hamming distances generated binary codes optimized <eos> paper propose novel supervised binary coding approach namely top rank supervised binary coding top rsbc explicitly focuses optimizing precision top positions hamming distance ranking list towards preserving supervision information <eos> core idea train disciplined coding functions mistakes top hamming distance ranking list penalized more than bottom <eos> solve such coding functions relax original discrete optimization objective continuous surrogate derive stochastic gradient descent optimize surrogate objective <eos> further reduce training time cost also design online learning algorithm optimize surrogate objective more efficiently <eos> empirical studies based upon three benchmark image datasets demonstrate proposed binary coding approach achieves superior image search accuracy over state arts <eos> <eop> bubblenet foveated imaging visual discovery <eos> propose new method turning internet scale corpus categorized image into small set human interpretable discriminative visual elements using powerful tools based deep learning <eos> key challenge deep learning method generating human interpretable models <eos> address propose new technique uses bubble image image most content obscured identify spatially localized discriminative content each image <eos> modifying model training procedure use both source imagery bubble image arrive final models retain much original classification performance but much more amenable identifying interpretable visual elements <eos> apply algorithm wide variety datasets including two new internet scale datasets people places show applications visual mining discovery <eos> method simple scalable produces visual elements highly representative compared prior work <eos> <eop> pqtable fast exact asymmetric distance neighbor search product quantization using hash tables <eos> propose product quantization table pqtable product quantization based hash table fast requires neither parameter tuning nor training steps <eos> pqtable produces exactly same result linear pq search times faster when tested sift data <eos> addition although state art performance achieved previous inverted indexing based approaches such method require manually designed parameter setting much training whereas method free them <eos> therefore pqtable offers practical useful solution real world problems <eos> <eop> lending hand detecting hands recognizing activities complex egocentric interactions <eos> hands appear very often egocentric video their appearance pose give important cues about people doing they paying attention <eos> but existing work hand detection made strong assumptions work well only simple scenarios such limited interaction other people lab settings <eos> develop method locate distinguish between hands egocentric video using strong appearance models convolutional neural network introduce simple candidate region generation approach outperforms existing techniques fraction computational cost <eos> show how high quality bounding boxes used create accurate pixelwise hand region application investigate extent hand segmentation alone distinguish between different activities <eos> evaluate techniques new dataset first person video along pixel level ground truth over hand instances people interacting realistic environments <eos> <eop> fast accurate head pose estimation via random projection forests <eos> paper consider problem estimating gaze direction person low resolution image <eos> under condition reliably extracting facial feature very difficult <eos> propose novel head pose estimation algorithm based compressive sensing <eos> head image patches mapped large feature space using proposed extensive yet efficient filter bank <eos> filter bank designed generate sparse responses color gradient information compressed using random projection classified random forest <eos> extensive experiments challenging datasets show proposed algorithm performs favorably against state art method head pose estimation low resolution image degraded noise occlusion blurring <eos> <eop> mrf poselets model detecting highly articulated humans <eos> detecting highly articulated object such humans challenging problem <eos> paper proposes novel part based model built upon poselets notion parts markov random field mrf modelling human body structure under variation human poses viewpoints <eos> problem human detection then formulated maximum posteriori map estimation mrf model <eos> variational mean field method robust statistical inference adopted approximate map estimation <eos> proposed method was evaluated compared existing method different test set including pascal voc <eos> experimental result favourbly shown robustness proposed method comparison state art <eos> <eop> beyond tree structure models new occlusion aware graphical model human pose estimation <eos> occlusion main challenge human pose estimation largely ignored popular tree structure models <eos> tree structure model simple convenient exact inference but short modeling occlusion coherence especially case self occlusion <eos> propose occlusion aware graphical model able model both self occlusion occlusion other object simultaneously <eos> proposed model structure encode interactions between human body parts object hence enable learn occlusion coherence data discriminatively <eos> evaluate model several public benchmarks human pose estimation including challenging subsets featuring significant occlusion <eos> experimental result show method obtains comparable accuracy state arts achieves promising performance human pose estimation occlusion <eos> <eop> relaxing vocabulary robust weakly supervised deep learning vocabulary free image tagging <eos> development deep learning empowered machines comparable capability recognizing limited image categories human beings <eos> however most existing approaches heavily rely human curated training data hinders scalability large unlabeled vocabularies image tagging <eos> paper propose weakly supervised deep learning model trained readily available web image relax dependence human labors scale up arbitrary tags categories <eos> specifically based assumption feature true sample category tend similar noises tend variant embed feature map last deep layer into new affinity representation further minimize discrepancy between affinity representation its low rank approximation <eos> discrepancy finally transformed into objective function give relevance feedback back propagation <eos> experiments show achieve performance gain <eos> terms semantic based relevance metric image tagging tags wordnet against typical deep model trained imagenet vocabulary set <eos> <eop> visual phrases exemplar face detection <eos> recently exemplar based approaches successfully applied face detection wild <eos> contrary traditional approaches model face variations large diverse set training examples exemplar based approaches use collection discriminatively trained exemplars detection <eos> paradigm each exemplar casts vote using retrieval framework generalized hough voting locate faces target image <eos> advantage approach having large database covers all possible variations faces challenging conditions detected without having learn explicit models different variations <eos> current schemes however make assumption independence between visual words ignoring their relations process <eos> they also ignore spatial consistency visual words <eos> consequently every exemplar word contributes equally during voting regardless its location <eos> paper propose novel approach incorporates higher order information voting process <eos> discover visual phrases contain semantically related visual words exploit them detection along visual words <eos> spatial consistency estimate spatial distribution visual words phrases entire database then weigh their occurrence exemplars <eos> ensures visual word phrase exemplar makes major contribution only if occurs its semantic location thereby suppressing noise significantly <eos> perform extensive experiments standard fddb afw album datasets show significant improvement over previous exemplar approaches <eos> <eop> spatial semantic regularisation large scale object detection <eos> large scale object detection thousands classes introduces problem many contradicting false positive detections suppressed <eos> class independent non maximum suppression traditionally used step but scale well number classes grows <eos> traditional non maximum suppression consider label instance level relationships nor allow exploitation spatial layout detection proposals <eos> propose new multi class spatial semantic regularisation method based affinity propagation clustering simultaneously optimises across all categories all proposed locations image improve both localisation categorisation selected detection proposals <eos> constraints shared across labels through semantic wordnet hierarchy <eos> approach proves especially useful large scale settings thousands classes spatial semantic interactions very frequent only weakly supervised detectors built due lack bounding box annotations <eos> detection experiments conducted imagenet coco dataset settings thousands detected categories <eos> method provides significant precision improvement reducing false positives while simultaneously improving recall <eos> <eop> human pose estimation video <eos> paper present method estimate sequence human poses unconstrained video <eos> contrast commonly employed graph optimization framework np hard needs approximate solutions formulate problem into unified two stage tree based optimization problem efficient exact solution exists <eos> although proposed method finds exact solution sacrifice ability model spatial temporal constraints between body parts video frames indeed even models symmetric parts better than existing method <eos> proposed method based two main ideas abstraction association enforce intra inter frame body part constraints respectively without inducing extra computational complexity polynomial time solution <eos> using idea abstraction new concept abstract body part introduced model only tree based body part structure similar existing method but also extra constraints between symmetric parts <eos> using idea association optimal tracklets generated each abstract body part order enforce spatiotemporal constraints between body parts adjacent frames <eos> finally sequence best poses inferred abstract body part tracklets through tree based optimization <eos> evaluated proposed method three publicly available video based human pose estimation datasets obtained dramatically improved performance compared state art method <eos> <eop> contour box rejecting object proposals without explicit closed contours <eos> closed contour important objectness indicator <eos> propose new measure subject completeness tightness constraints optimized closed contour should tightly bounded within object proposal <eos> closed contour measure defined using closed path integral solve optimization problem efficiently polar coordinate system global optimum guaranteed <eos> extensive experiments show method reject large number false proposals achieve over improvement object recall challenging overlap threshold <eos> voc test dataset <eos> <eop> registering image untextured geometry using average shading gradients <eos> many existing approaches image geometry registration assume either textured three dimensional model good initial guess three dimensional pose available bootstrap registration process <eos> paper consider registration photographs three dimensional models even when no texture information available <eos> very challenging cannot rely texture gradients even shading gradients hard estimate since lighting conditions unknown <eos> end propose average shading gradients rendering technique estimates average gradient magnitude over all lighting directions under lambertian shading <eos> use gradient representation building block registration pipeline based matching sparse feature <eos> cope inevitable false matches due missing texture information increase robustness pose three dimensional model estimated two stages <eos> coarse pose hypotheses first obtained single correct match each subsequently refined using sift flow finally verified <eos> apply algorithm registering image real world object untextured three dimensional meshes limited accuracy <eos> <eop> robust nonrigid registration convex optimization <eos> present approach nonrigid registration three dimensional surfaces <eos> cast isometric embedding mrf optimization apply efficient global optimization algorithms based linear programming relaxations <eos> markov random field perspective suggests natural connection robust statistics motivates robust forms intrinsic distortion functional <eos> approach outperforms large body prior work significant margin increasing registration precision real data factor <eos> <eop> robust optimal sum squares based point plane registration image set structured scenes <eos> paper deals problem registering known structured three dimensional scene its metric structure motion sfm counterpart <eos> proposed work relies prior plane segmentation three dimensional scene aligns data obtained both modalities solving point plane assignment problem <eos> inliers maximization approach within branch bound bnb search scheme adopted <eos> first time paper sum squares optimization theory framework employed identifying point plane mismatches <eos> allows iteratively build potential inliers set converge solution satisfied largest number point plane assignments <eos> furthermore approach boosted new plane visibility conditions also introduced paper <eos> using framework solve registration problem two cases set putative point plane correspondences possibly overwhelmingly many outliers given input ii no initial correspondences given <eos> both cases approach yields outstanding result terms robustness optimality <eos> <eop> meshstereo global stereo model mesh alignment regularization view interpolation <eos> present novel global stereo model designed view interpolation <eos> unlike existing stereo models only output disparity map model able output three dimensional triangular mesh directly used view interpolation <eos> aim partition input stereo image into triangles shared vertices <eos> lifting triangulation three dimensional naturally generates corresponding mesh <eos> technical difficulty properly split vertices multiple copies when they appear depth discontinuous boundaries <eos> deal problem formulate objective two layer mrf upper layer modeling splitting properties vertices lower layer optimizing region based stereo matching <eos> experiments middlebury herodion datasets demonstrate model able synthesize visually coherent new view angles high psnr well outputting high quality disparity maps rank first place new challenging high resolution middlebury <eos> <eop> cv hazop introducing test data validation computer vision <eos> test data plays important role computer vision cv but plagued two questions situations should covered test data tested enough reach conclusion paper propose new solution answering questions using standard procedure devised safety community validate complex systems hazard operability analysis hazop <eos> designed systematically search identify difficult performance decreasing situations aspects <eos> introduce generic cv model creates basis hazard analysis first time apply extensive hazop cv domain <eos> result publicly available checklist more than identified individual hazards <eos> checklist used evaluate existing test datasets quantifying amount covered hazards <eos> evaluate approach first analyzing annotating popular stereo vision test datasets middlebury kitti <eos> second compare performance six popular stereo matching algorithms identified hazards checklist their average performance show expected clear negative influence hazards <eos> presented approach useful tool evaluate improve test datasets creates common basis future dataset designs <eos> <eop> structure motion using structure less resection <eos> paper proposes new incremental structure motion sfm algorithm based novel structure less camera resection technique <eos> traditional method rely three dimensional correspondences compute pose candidate cameras using pnp <eos> work take collection already reconstructed cameras generalized camera determine absolute pose candidate pinhole camera pure correspondences call semi generalized camera pose problem <eos> present minimal solvers new problem both calibrated partially calibrated unknown focal length pinhole cameras <eos> integrating new algorithms incremental sfm system go beyond state art method capability reconstructing cameras without three dimensional correspondences <eos> large scale real image experiments show new sfm system significantly improves completeness three dimensional reconstruction over standard approach <eos> <eop> joint camera clustering surface segmentation large scale multi view stereo <eos> paper propose optimal decomposition approach large scale multi view stereo initial sparse reconstruction <eos> success approach depends introduction surface segmentation based camera clustering rather than sparse point based camera clustering suffers problems non uniform reconstruction coverage ratio high redundancy <eos> details introduce three criteria camera clustering surface segmentation reconstruction then formulate criteria into energy minimization problem under constraints <eos> solve problem propose joint optimization hierarchical framework obtain final surface segments corresponding optimal camera clusters <eos> each level hierarchical framework camera clustering problem formulated parameter estimation problem probability model solved general expectation maximization algorithm surface segmentation problem formulated markov random field model based probability estimated previous camera clustering process <eos> experiments several internet datasets aerial photo datasets demonstrate proposed approach method generates more uniform complete dense reconstruction less redundancy resulting more efficient multi view stereo algorithm <eos> <eop> higher order crf structural segmentation three dimensional reconstructed surfaces <eos> paper propose structural segmentation algorithm partition multi view stereo reconstructed surfaces large scale urban environments into structural segments <eos> each segment corresponds structural component describable surface primitive up second order <eos> segmentation use subsequent urban object modeling vectorization recognition <eos> overcome high geometrical topological noise levels three dimensional reconstructed urban surfaces formulate structural segmentation higher order conditional random field crf labeling problem <eos> only incorporates classical lower order three dimensional local cues but also encodes contextual geometric regularities disambiguate noisy local cues <eos> general higher order crf difficult solve <eos> develop bottom up progressive approach through patch based surface representation iteratively evolves initial mesh triangles final segmentation <eos> each iteration alternates between performing prior discovery step finds contextual regularities patch based representation inference step leverages regularities higher order priors construct more stable regular segmentation <eos> efficiency robustness proposed method extensively demonstrated real reconstruction models yielding significantly better performance than classical mesh segmentation method <eos> <eop> hyperpoints fine vocabularies large scale location recognition <eos> structure based localization task finding absolute pose given query image <eos> pre computed three dimensional model <eos> while almost trivial small scale special care must taken size three dimensional model grows because straight forward descriptor matching becomes ineffective due large memory footprint model well strictness ratio test three dimensional recently several authors tried overcome problems either smart compression three dimensional model clever sampling strategies geometric verification <eos> here explore orthogonal strategy uses all three dimensional point standard sampling but performs feature matching implicitly quantization into fine vocabulary <eos> show although matching ambiguous gives rise three dimensional hyperpoints when matching each query feature isolation simple voting strategy enforces fact selected three dimensional point shall co visible reliably find locally unique three dimensional point assignment <eos> experiments two large scale datasets demonstrate method achieves state art performance while memory footprint greatly reduced since only visual word labels but no three dimensional point descriptors need stored <eos> <eop> globally optimal three dimensional registration point lines without correspondences <eos> present novel approach three dimensional registration point lines without correspondences <eos> while there exist established solutions case correspondences known there many situations possible reliably extract such correspondences across modalities thus requiring use correspondence free registration algorithm <eos> existing correspondence free method rely local search strategies consequently no guarantee finding optimal solution <eos> contrast present first globally optimal approach three dimensional registration without correspondences achieved branch bound algorithm <eos> furthermore deterministic annealing procedure proposed speed up nested branch bound algorithm used <eos> theoretical practical advantages brings demonstrated range synthetic real data observed proposed approach significantly more robust high proportions outliers compared existing approaches <eos> <eop> hci stereo metrics geometry aware performance analysis stereo algorithms <eos> performance characterization stereo method mandatory decide algorithm useful application <eos> prevalent benchmarks mainly use root mean squared error rms respect ground truth disparity maps quantify algorithm performance <eos> show rms limited expressiveness algorithm selection introduce hci stereo metrics <eos> metrics assess stereo result harnessing three semantic cues depth discontinuities planar surfaces fine geometric structures <eos> each cue extract relevant set pixels existing ground truth <eos> then apply evaluation functions quantify characteristics such edge fattening surface smoothness <eos> demonstrate approach supports practitioners selecting most suitable algorithm their application <eos> using new middlebury dataset show rankings based metrics reveal specific algorithm strengths weaknesses quantified existing metrics <eos> finally show how stacked bar charts radar charts visually support multidimensional performance evaluation <eos> interactive stereo benchmark based proposed metrics visualizations available hci <eos> de stereometrics <eop> merging unmatchable stitching visually disconnected sfm models <eos> recent advances structure motion only enable reconstruction large scale scenes but also able detect ambiguous structures caused repeating elements might result incorrect reconstructions <eos> yet always possible fully reconstruct scene <eos> image required merge different sub models might missing might impossible acquire such image first place due occlusions structure scene <eos> problem aligning multiple reconstructions visual overlap impossible solve general <eos> important variant problem case individual sides building reconstructed but joined due missing visual overlap <eos> paper present combinatorial approach solving variant automatically stitching multiple sides building together <eos> approach exploits symmetries semantic information reason about possible geometric relations between individual models <eos> show approach able reconstruct complete building models traditional sfm ends up disconnected building sides <eos> <eop> fragment reassembly using integrated template guidance fracture region matching <eos> paper studies matching fragmented object recompose their original geometry <eos> solving geometric reassembly problem direct applications archaeology forensic investigation computer aided restoration damaged artifacts evidence <eos> develop new algorithm effectively integrate both guidance template matching adjacent pieces fracture region <eos> first compute partial matchings between fragments template pairwise matchings among fragments <eos> many potential matches obtained then selected refined multi piece matching stage maximize global groupwise matching consistency <eos> pipeline effective composing fragmented thin shell object containing small pieces whose pairwise matching usually unreliable ambiguous hence their reassembly remains challenging existing algorithms <eos> <eop> procedural editing three dimensional building point clouds <eos> thanks recent advances computational photography remote sensing point clouds buildings becoming increasingly available yet their processing poses various challenges <eos> work tackle problem point cloud completion editing approach via inverse procedural modeling <eos> contrary previous work approach operates directly point cloud without intermediate triangulation <eos> approach consists semi automatic segmentation input point cloud segment comparison template matching detect repeating structures consensus based voting schema pattern extraction algorithm discover completed terminal geometry their patterns usage all encoded into context free grammar interactive editing tool user create new point clouds using procedural copy paste operations smart resizing <eos> demonstrate approach editing building models up <eos> implementation preprocessing takes up several minutes single editing operation needs one second one minute depending model size operation type <eos> <eop> semantically aware aerial reconstruction multi modal data <eos> consider methodology integrating multiple sensors along semantic information enhance scene representations <eos> propose probabilistic generative model inferring semantically informed aerial reconstructions multi modal data within consistent mathematical framework <eos> approach called semantically aware aerial reconstruction saar only exploits inferred scene geometry appearance semantic observations obtain meaningful categorization data but also extends previously proposed method imposing structure prior over geometry appearance semantic labels <eos> leads more accurate reconstructions ability fill missing contextual labels via joint sensor semantic information <eos> introduce new multi modal synthetic dataset order provide quantitative performance analysis <eos> additionally apply model real world data exploit openstreetmap source semantic observations <eos> show quantitative improvements reconstruction accuracy large scale urban scenes combination lidar aerial photography semantic data <eos> furthermore demonstrate model ability fill missing sensed data leading more interpretable reconstructions <eos> <eop> guaranteed outlier removal rotation search <eos> rotation search become core routine solving many computer vision problems <eos> aim rotationally align two input point set correspondences <eos> recently there significant interest developing globally optimal rotation search algorithms <eos> notable weakness global algorithms however their relatively high computational cost especially large problem sizes data high proportion outliers <eos> paper propose novel outlier removal technique rotation search <eos> method guarantees any correspondence discards outlier exist inlier set globally optimal rotation original data <eos> based simple geometric operations algorithm deterministic fast <eos> used preprocessor prune large portion outliers input data method enables substantial speed up rotation search algorithms without compromising global optimality <eos> demonstrate efficacy method various synthetic real data experiments <eos> <eop> peeking template matching depth extension <eos> propose method extends given depth image into region three dimensional visible point view camera <eos> algorithm detects repeated three dimensional structures visible scene suggests set three dimensional extension hypotheses then combined together through global three dimensional mrf discrete optimization <eos> recovered global three dimensional surface consistent both input depth map hypotheses <eos> key component work novel three dimensional template matcher used detect repeated three dimensional structure scene suggest hypotheses <eos> unique property matcher handle depth uncertainty <eos> crucial because matcher required peek around corner operates boundaries visible three dimensional scene depth information missing <eos> proposed matcher fast guaranteed find approximation globally optimal solution <eos> demonstrate real world data algorithm capable completing full three dimensional scene single depth image synthesize full depth map novel viewpoint scene <eos> addition report result extensive synthetic set three dimensional shapes allows evaluate method both qualitatively quantitatively <eos> <eop> deformable three dimensional fusion partial dynamic three dimensional observations complete models <eos> capturing three dimensional motion dynamic non rigid object attracted significant attention computer vision <eos> existing method typically require either complete three dimensional volumetric observations shape template <eos> paper introduce template less reconstruction method incrementally fuses highly incomplete three dimensional observations deforming object generates complete temporally coherent shape representation object <eos> end design online algorithm alternatively registers new observations current model estimate updates model <eos> demonstrate effectiveness approach reconstructing non rigidly moving object highly incomplete measurements both sequences partial three dimensional point clouds kinect video <eos> <eop> non parametric structure based calibration radially symmetric cameras <eos> propose novel two step method estimating intrinsic extrinsic calibration any radially symmetric camera including non central systems <eos> first step consists estimating camera pose given structure motion sfm model up translation along optical axis <eos> second step obtain calibration finding translation camera center using ordering constraint <eos> method makes use radial camera model allows effectively handle any radially symmetric camera including non central ones <eos> using ordering constraint show able calibrate several different central non central wide field view wfov cameras including fisheye hyper catadioptric spherical catadioptric cameras well pinhole cameras using single image jointly solving several views <eos> <eop> exploiting object similarity three dimensional reconstruction <eos> despite recent progress reconstructing outdoor scenes three dimensional movable platforms remains highly difficult endeavour <eos> challenges include low frame rates occlusions large distortions difficult lighting conditions <eos> paper leverage fact larger reconstructed area more likely object similar type shape will occur scene <eos> particularly true outdoor scenes buildings vehicles often suffer missing texture reflections but share similarity three dimensional shape <eos> take advantage shape similarity localizing object using detectors jointly reconstructing them while learning volumetric model their shape <eos> allows reduce noise while completing missing surfaces object similar shape benefit all observations respective category <eos> evaluate approach respect lidar ground truth novel challenging suburban dataset show its advantages over state art <eos> <eop> you here mimicking human thinking process reading floor plans <eos> human easily find his her way unfamiliar building walking around reading floor plan <eos> try mimic automate human thinking process <eos> more precisely introduce new useful task locating user floor plan using only camera floor plan without any other prior information <eos> address problem novel matching localization algorithm inspired human logic <eos> demonstrate through experiments method outperforms state art floor plan based localization method large margin while also being highly efficient real time applications <eos> <eop> map disparity estimation using hidden markov trees <eos> new method introduced stereo matching operates minimum spanning trees msts generated image <eos> disparity maps represented collection hidden states msts each mst modeled hidden markov tree <eos> efficient recursive message passing scheme designed operate hidden markov trees known upward downward algorithm used compute maximum posteriori map disparity estimate each pixel <eos> messages processed upward downward algorithm involve two types probabilities probability pixel having particular disparity given set per pixel matching costs probability disparity transition between pair connected pixels given their similarity <eos> distributions probabilities modeled collection image ground truth disparities <eos> performance evaluation using middlebury stereo benchmark version demonstrates proposed method ranks second third terms overall accuracy when evaluated training test image set respectively <eos> <eop> wide baseline stereo matching convex bounded distortion constraints <eos> finding correspondences wide baseline setups challenging problem <eos> existing approaches focused largely developing better feature descriptors correspondence accurate recovery epipolar line constraints <eos> paper focuses challenging problem finding correspondences once approximate epipolar constraints given <eos> introduce novel method integrates deformation model <eos> specifically formulate problem finding largest number corresponding point related bounded distortion map obeys given epipolar constraints <eos> show while set bounded distortion maps convex subset maps obey epipolar line constraints convex allowing introduce efficient algorithm matching <eos> further utilize robust cost function matching employ majorization minimization its optimization <eos> experiments indicate method finds significantly more accurate maps than existing approaches <eos> <eop> interactive visual hull refinement specular transparent object surface reconstruction <eos> paper present method using standard multi view image three dimensional surface reconstruction non lambertian object <eos> extend original visual hull concept incorporate three dimensional cues presented internal occluding contours <eos> occluding contours inside object silhouettes <eos> discovered internal contours result convex parts object surface lead tighter fit than original visual hull <eos> formulated new visual hull refinement scheme locally convex carving completely reconstruct concavity caused two more intersecting convex surfaces <eos> addition develop novel approach contour tracking given labeled contours sparse key frames <eos> designed specifically highly specular transparent object assumptions made traditional contour detection tracking method such highest gradient stationary texture edges no longer valid <eos> formulated energy minimization function several novel terms developed increase robustness <eos> based two core algorithms developed interactive system three dimensional modeling <eos> validated system both quantitatively qualitatively four datasets different object materials <eos> result show able generate visually pleasing models very challenging cases <eos> <eop> hierarchical higher order regression forest fields application three dimensional indoor scene labelling <eos> paper addresses problem semantic segmentation three dimensional indoor scenes reconstructed rgb image <eos> traditionally label prediction three dimensional point tackled employing graphical models capture scene feature complex relations between different class labels <eos> however existing work restricted pairwise conditional random fields insufficient when encoding rich scene context <eos> work propose models higher order potentials describe complex relational information three dimensional scenes <eos> specifically relax labelling problem regression generalize higher order associative potts model new family arbitrary higher order models based regression forests <eos> show models like robust models still decomposed into sum pairwise terms introducing auxiliary variables <eos> moreover proposed higher order models also permit extension hierarchical random fields allows integration scene context feature computed different scales <eos> potential functions constructed based regression forests encoding gaussian densities admit efficient inference <eos> parameters model learned training data using structured learning approach <eos> result two datasets show clear improvements over current state art method <eos> <eop> classical scaling revisited <eos> multidimensional scaling mds information analysis tool <eos> involves evaluation distances between data point quadratic space time problem <eos> then mds procedures find embedding point low dimensional euclidean flat domain optimizing similarity inter point distances <eos> present efficient solver classical scaling specific mds model extending distances measured subset point rest while exploiting smoothness property distance functions <eos> smoothness measured norm laplace beltrami operator applied unknown distance function <eos> laplace beltrami reflects local differential relations between point computed linear time <eos> classical scaling thereby reformulated into quasi linear space time complexities procedure <eos> <eop> dense continuous time tracking mapping rolling shutter rgb cameras <eos> propose dense continuous time tracking mapping method rgb cameras <eos> parametrize camera trajectory using continuous splines optimize trajectory through dense direct image alignment <eos> method also directly models rolling shutter both rgb depth image within optimization improves tracking reconstruction quality low cost cmos sensors <eos> using continuous trajectory representation number advantages over discrete time representation <eos> camera poses frame interval <eos> splines less variables need optimized than discrete representation since trajectory represented fewer control point than frames <eos> splines also naturally include smoothness constraints derivatives trajectory estimate <eos> finally continuous trajectory representation allows compensate rolling shutter effects since pose estimate available any exposure time image <eos> approach demonstrates superior quality tracking reconstruction compared approaches discrete time global shutter assumptions <eos> <eop> dense image registration deformable surface reconstruction presence occlusions minimal texture <eos> deformable surface tracking monocular image well known under constrained <eos> occlusions often make task even more challenging result failure if surface sufficiently textured <eos> work explicitly address problem three dimensional reconstruction poorly textured occluded surfaces proposing framework based template matching approach scales dense robust feature relevancy score <eos> approach extensively compared current method employing both local feature matching dense template alignment <eos> test standard datasets well new dataset will made publicly available sparsely textured occluded surface <eos> framework achieves state art result both well poorly textured occluded surfaces <eos> <eop> likelihood ratio test efficient robust estimation <eos> robust estimation model parameters presence outliers key problem computer vision <eos> ransac inspired techniques widely used context although their application might limited due need priori knowledge inlier noise level <eos> propose new approach jointly optimizing over model parameters inlier noise level based likelihood ratio test <eos> allows control over type error incurred <eos> also propose early bailout strategy efficiency <eos> tests both synthetic real data show method outperforms state art fraction time <eos> <eop> reflection modeling passive stereo <eos> stereo reconstruction presence reality faces many challenges still need addressed <eos> paper considers reflections introduce incorrect matches due observation violating diffuse world assumption underlying majority stereo techniques <eos> unlike most existing work employ regularization robust data terms suppress such errors derive two least squares models first principles generalize diffuse world stereo explicitly take reflections into account <eos> models parametrized depth orientation material properties resulting total up parameters per pixel estimated <eos> additionally large non local interactions between viewed reflected surface taken into account <eos> two properties make inference model appear prohibitive but present evidence inference actually possible using variant patch match stereo <eos> <eop> detailed full body reconstructions moving people monocular rgb sequences <eos> accurately estimate three dimensional geometry appearance human body monocular rgb sequence user moving freely front sensor <eos> range data each frame first brought into alignment multi resolution three dimensional body model coarse fine process <eos> method then uses geometry image texture over time obtain accurate shape pose appearance information despite unconstrained motion partial views varying resolution occlusion soft tissue deformation <eos> novel body model variable shape detail allowing capture faces high resolution deformable head model body shape lower resolution <eos> finally combine range data entire sequence estimate high resolution displacement map captures fine shape details <eos> compare recovered models high resolution scans professional system avatars created commercial product <eos> extract accurate three dimensional avatars challenging motion sequences even capture soft tissue dynamics <eos> <eop> efficient solution epipolar geometry radially distorted cameras <eos> estimation epipolar geometry two cameras image matches fundamental problem computer vision many applications <eos> while closely related problem estimating relative pose two different uncalibrated cameras radial distortion particular importance none previously published method suitable practical applications <eos> solutions either numerically unstable sensitive noise based large number point correspondences simply too slow real time applications <eos> paper present new efficient solution problem uses image correspondences <eos> manipulating ten input polynomial equations derive degree polynomial equation one variable <eos> solutions equation efficiently found using sturm sequences method <eos> experiments show proposed solution stable noise resistant fast such efficiently usable practical structure motion pipeline <eos> <eop> learning descriptor specific three dimensional keypoint detector <eos> keypoint detection represents first stage majority modern computer vision pipelines based automatically established correspondences between local descriptors <eos> however no standard solution emerged yet case three dimensional data such point clouds meshes exhibit high variability level detail noise <eos> more importantly existing proposals three dimensional keypoint detection rely geometric saliency functions attempt maximize repeatability rather than distinctiveness selected region may lead sub optimal performance overall pipeline <eos> overcome shortcomings cast three dimensional keypoint detection binary classification between point whose support correctly matched predefined three dimensional descriptor thereby learning descriptor specific detector adapts seamlessly different scenarios <eos> through experiments several public datasets show novel approach design keypoint detector represents flexible solution nonetheless provide state art descriptor matching performance <eos> <eop> component wise modeling articulated object <eos> introduce novel framework modeling articulated object based aspects their components <eos> decomposing object into components divide problem smaller modeling tasks <eos> after obtaining three dimensional models each component aspect employing shape deformation paradigm merge them together forming object components <eos> final model obtained assembling components using optimization scheme fits respective three dimensional models corresponding apparent contours reference pose <eos> result suggest approach produce realistic three dimensional models articulated object reasonable time <eos> <eop> collaborative filtering approach real time hand pose estimation <eos> collaborative filtering aims predict unknown user ratings recommender system collectively assessing known user preferences <eos> paper first draw analogies between collaborative filtering pose estimation problem <eos> specifically recast hand pose estimation problem cold start problem new user unknown item ratings recommender system <eos> inspired fast accurate matrix factorization techniques collaborative filtering develop real time algorithm estimating hand pose rgb data commercial depth camera <eos> first efficiently identify nearest neighbors using local shape descriptors rgb domain library hand poses known pose parameter values <eos> then use information evaluate unknown pose parameters using joint matrix factorization completion jmfc approach <eos> quantitative qualitative result suggest approach robust variation hand configurations while achieving real time performance fps standard computer <eos> <eop> equivalence moving entrance pupil radial distortion camera calibration <eos> radial distortion ordinary non fisheye camera lenses traditionally modeled infinite series function radial location image pixel image center <eos> while there enough empirical evidence show such model accurate sufficient radial distortion calibration there much analysis geometric physical understanding radial distortion camera calibration perspective <eos> paper show using thick lens imaging model variation entrance pupil location function incident image ray angle directly responsible radial distortion captured image <eos> thus unlike proposed current state art camera calibration radial distortion entrance pupil movement equivalent need modeled together <eos> modeling only entrance pupil motion instead radial distortion achieve two main benefits first obtain comparable if better pixel re projection error than traditional method second more importantly directly back project radially distorted image pixel along true image ray formed <eos> using thick lens setting show such back projection more accurate than two step method undistorting image pixel then back projecting <eos> applied calibration method problem generative depth focus using focal stack get accurate depth estimates <eos> <eop> linear generalized camera calibration three intersecting reference planes <eos> paper presents new generalized ray pixel raxel camera calibration algorithm camera systems involving distortions unknown refraction reflection processes <eos> key idea use intersections calibration planes while conventional method utilized collinearity constraints point planes <eos> show intersections calibration planes realize simple linear algorithm method applied any ray distributions while conventional method require knowing ray distribution class advance <eos> evaluations using synthesized real datasets demonstrate performance method quantitatively qualitatively <eos> <eop> towards pointless structure motion three dimensional reconstruction camera parameters general three dimensional curves <eos> modern structure motion sfm remains dependent point feature recover camera positions meaning reconstruction severely hampered low texture environments example scanning plain coffee cup uncluttered table <eos> show how three dimensional curves used refine camera position estimation challenging low texture scenes <eos> contrast previous work allow curves partially observed all image meaning first time curve based sfm demonstrated realistic scenes <eos> algorithm based bundle adjustment so needs initial estimate but even poor estimate few point correspondences substantially improved including curves suggesting method would benefit many existing systems <eos> <eop> attributed grammars joint estimation human attributes part pose <eos> paper interested developing compositional models explicit representing pose parts attributes tackling tasks attribute recognition pose estimation part localization jointly <eos> different recent trend using cnn based approaches training testing tasks separately large amount data <eos> conventional attribute models typically use large number region based attribute classifiers parts pre trained pose estimator without explicitly detecting object its parts considering correlations between attributes <eos> contrast approach jointly represents both object parts their semantic attributes within unified compositional hierarchy <eos> apply attributed grammar model task human parsing simultaneously performing part localization attribute recognition <eos> show modeling helps performance improvements pose estimation task also outperforms other existing method attribute prediction task <eos> <eop> real time pose estimation piggybacked object detection <eos> present object detector coupled pose estimation directly single compact simple model detector shares extracted image feature pose estimator <eos> output classification each candidate window consists both object score likelihood map poses <eos> extension introduces negligible overhead during detection so detector still capable real time operation <eos> evaluated proposed approach problem vehicle detection <eos> used existing datasets viewpoint pose annotation wcvp three dimensional object kitti <eos> besides collected new traffic surveillance dataset cod fills certain gaps existing datasets make public <eos> experimental result show proposed approach comparable state art approaches terms accuracy but considerably faster easily operating real time matlab code <eos> source codes collected cod dataset made public along paper <eos> <eop> understanding predicting image memorability large scale <eos> progress estimating visual memorability limited small scale lack variety benchmark data <eos> here introduce novel experimental procedure objectively measure human memory building largest annotated image memorability dataset date labeled image diverse array sources <eos> using convolutional neural network cnn show fine tuned deep feature outperform all other feature large margin reaching rank correlation <eos> near human consistency <eos> analysis responses high level cnn layer shows object region positively negatively correlated memorability allowing create memorability maps each image provide concrete method perform image memorability manipulation <eos> work demonstrates one now robustly estimate memorability image many different classes positioning memorability deep memorability feature prime candidates estimate utility information cognitive systems <eos> <eop> multiple granularity descriptors fine grained categorization <eos> fine grained categorization aims distinguish subordinate level categories such bird species dog breeds extremely challenging task <eos> due two main issues how localize discriminative region recognition how learn sophisticated feature representation <eos> neither them easy handle if there insufficient labeled data <eos> leverage fact subordinate level object already other labels its ontology tree <eos> free labels used train series cnn based classifiers each specialized one grain level <eos> internal representations network different region interests allowing construction multi grained descriptors encode informative discriminative feature covering all grain levels <eos> multiple granularity framework learned weakest supervision requiring only image level label avoiding use labor intensive bounding box part annotations <eos> experimental result three challenging fine grained image datasets demonstrate approach outperforms state art algorithms including requiring strong labels <eos> <eop> guiding long short term memory model image caption generation <eos> work focus problem image caption generation <eos> propose extension long short term memory lstm model coin glstm short <eos> particular add semantic information extracted image extra input each unit lstm block aim guiding model towards solutions more tightly coupled image content <eos> additionally explore different length normalization strategies beam search avoid bias towards short sentences <eos> various benchmark datasets such flickr flickr ms coco obtain result par better than current state art <eos> <eop> just noticeable differences visual attributes <eos> explore problem predicting just noticeable differences visual attribute <eos> while some pairs image clear ordering attribute <eos> more sporty than others difference may indistinguishable human observers <eos> however existing relative attribute models unequipped infer partial orders novel data <eos> attempting map relative attribute ranks equality predictions non trivial particularly since span indistinguishable pairs attribute space may vary different parts feature space <eos> develop bayesian local learning strategy infer when image indistinguishable given attribute <eos> ut zap shoes lfw faces datasets outperform variety alternative method <eos> addition show practical impact fine grained visual search <eos> <eop> vqa visual question answering <eos> propose task free form open ended visual question answering vqa <eos> given image natural language question about image task provide accurate natural language answer <eos> mirroring real world scenarios such helping visually impaired both questions answers open ended <eos> visual questions selectively target different areas image including background details underlying context <eos> result system succeeds vqa typically needs more detailed understanding image complex reasoning than system producing generic image captions <eos> moreover vqa amenable automatic evaluation since many open ended answers contain only few words closed set answers provided multiple choice format <eos> provide dataset containing <eos> questions answers www <eos> org discuss information provides <eos> numerous baselines vqa provided compared human performance <eos> <eop> localize me anywhere anytime multi task point retrieval approach <eos> image based localization essential complement gps localization <eos> current image based localization method based either three dimensional three dimensional find correspondences ignore real scene geometric attributes <eos> main contribution paper use three dimensional model reconstructed short video query realize three dimensional three dimensional localization under multi task point retrieval framework <eos> firstly use three dimensional model query enables efficiently select location candidates <eos> furthermore reconstruction three dimensional model exploits correlations among different image based fact image captured different views sfm share information through matching feature <eos> exploring shared information matching feature across multiple related tasks image same scene captured different views visual feature view invariance property improved order get higher point retrieval accuracy <eos> more specifically use multi task point retrieval framework explore relationship between descriptors three dimensional point extracts discriminant point more accurate three dimensional three dimensional correspondences retrieval <eos> further apply multi task learning mtl retrieval approach thermal image prove mtl retrieval framework also provides superior performance thermal domain <eos> application exceptionally helpful cope localization problem environment limited light sources <eos> <eop> dense optical flow prediction static image <eos> given scene going move direction will move such question could considered non semantic form action prediction <eos> work present convolutional neural network cnn based approach motion prediction <eos> given static image cnn predicts future motion each every pixel image terms optical flow <eos> cnn model leverages data tens thousands realistic video train model <eos> method relies absolutely no human labeling able predict motion based context scene <eos> because cnn model makes no assumptions about underlying scene predict future optical flow diverse set scenarios <eos> outperform all previous approaches large margins <eos> <eop> unsupervised domain adaptation zero shot learning <eos> zero shot learning zsl considered special case transfer learning source target domains different tasks label spaces target domain unlabelled providing little guidance knowledge transfer <eos> zsl method typically assumes two domains share common semantic representation space visual feature vector extracted image video projected embedded using projection function <eos> existing approaches learn projection function source domain apply without adaptation target domain <eos> they thus based naive knowledge transfer learned projections prone domain shift problem <eos> paper novel zsl method proposed based unsupervised domain adaptation <eos> specifically formulate novel regularised sparse coding framework uses target domain class labels projections semantic space regularise learned target domain projection thus effectively overcoming projection domain shift problem <eos> extensive experiments four object action recognition benchmark datasets show proposed zsl method significantly outperforms state arts <eos> <eop> visual madlibs fill blank description generation question answering <eos> paper introduce new dataset consisting focused natural language descriptions image <eos> dataset visual madlibs dataset collected using automatically produced fill blank templates designed gather targeted descriptions about people object their appearances activities interactions well inferences about general scene its broader context <eos> provide several analyses visual madlibs dataset demonstrate its applicability two new description generation tasks focused description generation multiple choice question answering image <eos> experiments using joint embedding deep learning method show promising result tasks <eos> <eop> actions attributes wholes parts <eos> investigate importance parts tasks action attribute classification <eos> develop part based approach leveraging convolutional network feature inspired recent advances computer vision <eos> part detectors deep version poselets capture parts human body under distinct set poses <eos> tasks action attribute classification train holistic convolutional neural network show adding parts leads top performing result both tasks <eos> observe deeper network parts less significant <eos> addition demonstrate effectiveness approach when replace oracle person detector default current evaluation protocol both tasks state art person detection system <eos> <eop> deepbox learning objectness convolutional network <eos> existing object proposal approaches use primarily bottom up cues rank proposals while believe objectness fact high level construct <eos> argue data driven semantic approach ranking object proposals <eos> framework call deepbox uses convolutional neural network cnn rerank proposals bottom up method <eos> use novel four layer cnn architecture good much larger network task evaluating objectness while being much faster <eos> show deepbox significantly improves over bottom up ranking achieving same recall proposals achieved bottom up method <eos> improvement generalizes categories cnn never seen before leads <eos> point gain detection map <eos> implementation achieves performance while running ms per image <eos> <eop> active object localization deep reinforcement learning <eos> present active detection model localizing object scenes <eos> model class specific allows agent focus attention candidate region identifying correct location target object <eos> agent learns deform bounding box using simple transformation actions goal determining most specific location target object following top down reasoning <eos> proposed localization agent trained using deep reinforcement learning evaluated pascal voc dataset <eos> show agents guided proposed model able localize single instance object after analyzing only between region image obtain best detection result among systems use object proposals object localization <eos> <eop> scene domain active part models object representation <eos> paper interested enhancing expressivity robustness part based models object representation common scenario training data based image <eos> end propose scene domain active part models sdapm reconstruct characterize three dimensional geometric statistics between object parts three dimensional scene domain using training data image domain alone <eos> top explicitly model handle occlusions sdapm <eos> together developed learning inference algorithms such model provides rich object descriptions including object parts localization three dimensional landmark shape camera viewpoint offers effective representation various image understanding tasks such object parts detection three dimensional landmark shape viewpoint estimation image <eos> experiments above tasks show sdapm outperforms previous part based models thus demonstrates potential proposed technique <eos> <eop> unified multiplicative framework attribute learning <eos> attributes mid level semantic properties object <eos> recent research shown visual attributes benefit many traditional learning problems computer vision community <eos> however attribute learning still challenging problem attributes may always predictable directly input image variation visual attributes sometimes large across categories <eos> paper propose unified multiplicative framework attribute learning tackles key problems <eos> specifically image category information jointly projected into shared feature space latent factors disentangled multiplied attribute prediction <eos> resulting attribute classifier category specific instead being shared all categories <eos> moreover method leverage auxiliary data enhance predictive ability attribute classifiers reducing effort instance level attribute annotation some extent <eos> experimental result show method achieves superior performance both instance level category level attribute prediction <eos> zero shot learning based attributes method significantly improves state art performance awa dataset achieves comparable performance cub dataset <eos> <eop> contractive rectifier network nonlinear maximum margin classification <eos> find optimal nonlinear separating boundary maximum margin input data space paper proposes contractive rectifier network crns wherein hidden layer transformations restricted contraction mappings <eos> contractive constraints ensure achieved separating margin input space larger than equal separating margin output layer <eos> training proposed crns formulated linear support vector machine svm output layer combined two more contractive hidden layer <eos> effective algorithms proposed address optimization challenges arising contraction constraints <eos> experimental result mnist cifar cifar mit datasets demonstrate proposed contractive rectifier network consistently outperform their conventional unconstrained rectifier network counterparts <eos> <eop> augmenting strong supervision using web data fine grained categorization <eos> propose new method fine grained object recognition employs part level annotations deep convolutional neural network cnn unified framework <eos> although both schemes widely used boost recognition performance due difficulty acquiring detailed part annotations strongly supervised fine grained datasets usually too small keep pace rapid evolution cnn architectures <eos> paper solve problem exploiting inexhaustible web data <eos> proposed method improves classification accuracy two ways more discriminative cnn feature representations generated using training set augmented collecting large number part patches weakly supervised web image more robust object classifiers learned using multi instance learning algorithm jointly strong weak datasets <eos> despite its simplicity proposed method delivers remarkable performance improvement cub dataset compared baseline part based cnn method achieves highest accuracy dataset even absence test image annotations <eos> <eop> learning like child fast novel visual concept learning sentence descriptions image <eos> paper address task learning novel visual concepts their interactions other concepts few image sentence descriptions <eos> using linguistic context visual feature method able efficiently hypothesize semantic meaning new words add them its word dictionary so they used describe image contain novel concepts <eos> method image captioning module based rnn model several improvements <eos> particular propose transposed weight sharing scheme only improves performance image captioning but also makes model more suitable novel concept learning task <eos> propose method prevent overfitting new concepts <eos> addition three novel concept datasets constructed new task publicly available project page <eos> experiments show method effectively learns novel visual concepts few examples without disturbing previously learned concepts <eos> project page www <eos> mao projects child learning <eos> html <eop> learning common sense through visual abstraction <eos> common sense essential building intelligent machines <eos> while some commonsense knowledge explicitly stated human generated text learnt mining web much unwritten <eos> often unnecessary even unnatural write about commonsense facts <eos> while unwritten commonsense knowledge unseen visual world around full structure modeled commonsense knowledge <eos> machines learn common sense simply observing visual world unfortunately requires automatic accurate detection object their attributes poses interactions between object remain challenging problems <eos> key insight while visual common sense depicted visual content semantic feature relevant low level pixel information <eos> other words photorealism necessary learn common sense <eos> explore use human generated abstract scenes made clipart learning common sense <eos> particular reason about plausibility interaction relation between pair nouns measuring similarity relation nouns other relations nouns seen abstract scenes <eos> show commonsense knowledge learn complementary learnt sources text <eos> <eop> domain generalization object recognition multi task autoencoders <eos> problem domain generalization take knowledge acquired number related domains training data available then successfully apply previously unseen domains <eos> propose new feature learning algorithm multi task autoencoder mtae provides good generalization performance cross domain object recognition <eos> algorithm extends standard denoising autoencoder framework substituting artificially induced corruption naturally occurring inter domain variability appearance object <eos> instead reconstructing image noisy versions mtae learns transform original image into analogs multiple related domains <eos> thereby learns feature robust variations across domains <eos> learnt feature then used inputs classifier <eos> evaluated performance algorithm benchmark image recognition datasets task learn feature multiple datasets then predict image label unseen datasets <eos> found denoising mtae outperforms alternative autoencoder based models well current state art algorithms domain generalization <eos> <eop> square localization efficient accurate object detection <eos> key contribution paper compact square object localization relaxes exhaustive sliding window testing all windows different combinations aspect ratios <eos> square object localization category scalable <eos> using binary search strategy number scales test further reduced empirically only log minfh wg rounds sliding cnn respectively image height width <eos> training phase square cnn models object co presence priors learned <eos> testing phase sliding cnn models applied produces set response maps effectively filtered learned co presence prior output final bounding boxes localizing object <eos> performed extensive experimental evaluation voc datasets demonstrate while efficient square localization output precise bounding boxes improve final detection result <eos> <eop> box aggregation proposal decimation last mile object detection <eos> region convolutional neural network rcnn now commonly employed object detection pipeline <eos> its main steps <eos> proposal generation convolutional neural network cnn feature extraction intensively investigated <eos> focus last step system aggregate thousands scored box proposals into final object prediction call proposal decimation <eos> show step enhanced very simple box aggregation function considering statistical properties proposals respect ground truth object <eos> method extremely light weight computation while yields improvement <eos> map pascal voc test <eos> explain why works using some statistics paper <eos> <eop> deepproposal hunting object cascading deep convolutional layer <eos> paper evaluate quality activation layer convolutional neural network cnn generation object proposals <eos> generate hypotheses sliding window fashion over different activation layer show final convolutional layer find object interest high recall but poor localization due coarseness feature maps <eos> instead first layer network better localize object interest but reduced recall <eos> based observation design method proposing object locations based cnn feature combines best both worlds <eos> build inverse cascade going final initial convolutional layer cnn selects most promising object locations refines their boxes coarse fine manner <eos> method efficient because uses same feature extracted detection ii aggregates feature using integral image iii avoids dense evaluation proposals due inverse coarse fine cascade <eos> method also accurate outperforms most previously proposed object proposals approaches when plugged into cnn based detector produces state art detection performance <eos> <eop> semantic segmentation object clique potential <eos> paper propose object clique potential semantic segmentation <eos> object clique potential addresses misclassified object part issues arising solutions based fully connected network <eos> object clique set compared yielded segment proposal based approaches significantly small size making method consume notably less computation <eos> regarding system design model formation object clique potential regarded functionally complement local appearance based crf models works synergy effective approaches further performance improvement <eos> extensive experiments verify method <eos> <eop> automatic concept discovery parallel text visual corpora <eos> humans connect language vision perceive world <eos> how build similar connection computers one possible way via visual concepts text terms relate visually discriminative entities <eos> propose automatic visual concept discovery algorithm using parallel text visual corpora filters text terms based visual discriminative power associated image groups them into concepts using visual semantic similarities <eos> illustrate applications discovered concepts using bidirectional image sentence retrieval task image tagging task show discovered concepts only outperform several large set manually selected concepts significantly but also achieves state art performance retrieval task <eos> <eop> simpler non parametric method provide good better result multiple instance learning <eos> multiple instance learning mil unique learning problem training data labels available only collections object called bags instead individual object called instances <eos> plethora approaches developed solve problem past years <eos> popular method include diverse density milis dd svm <eos> while having widely used method particularly computer vision attempted fairly sophisticated solutions solve certain unique particular configurations mil space <eos> paper analyze mil feature space using modified versions traditional non parametric techniques like parzen window nearest neighbour develop learning approach employing distances nearest neighbours point feature space <eos> show method work well if better than most recently published method benchmark datasets <eos> compare contrast analysis well established diverse density approach its variants recent literature using benchmark datasets including musk andrews corel datasets along diabetic retinopathy pathology diagnosis dataset <eos> experimental result demonstrate while enjoying intuitive interpretation supporting fast learning method potential delivering improved performance even complex data arising real world applications <eos> <eop> monocular object instance segmentation depth ordering cnn <eos> paper tackle problem instance level segmentation depth ordering single monocular image <eos> towards goal take advantage convolutional neural nets train them directly predict instance level segmentations instance id encodes depth ordering within image patches <eos> provide coherent single explanation image develop markov random field takes input predictions convolutional neural nets applied overlapping patches different resolutions well output connected component algorithm <eos> aims predict accurate instance level segmentation depth ordering <eos> demonstrate effectiveness approach challenging kitti benchmark show good performance both tasks <eos> <eop> multimodal convolutional neural network matching image sentence <eos> paper propose multimodal convolutional neural network cnn matching image sentence <eos> cnn provides end end framework convolutional architectures exploit image representation word composition matching relations between two modalities <eos> more specifically consists one image cnn encoding image content one matching cnn modeling joint representation image sentence <eos> matching cnn composes different semantic fragments words learns inter modal relations between image composed fragments different levels thus fully exploit matching relations between image sentence <eos> experimental result demonstrate proposed cnn effectively capture information necessary image sentence matching <eos> more specifically proposed cnn significantly outperform state art approaches bidirectional image sentence retrieval flickr flickr datasets <eos> <eop> structural kernel learning large scale multiclass object co detection <eos> exploiting contextual relationships across image recently proven key improve object detection <eos> resulting object co detection algorithms however fail exploit correlations between multiple classes scalability reasons limited modeling object instance similarity relatively low dimensional hand crafted feature <eos> here address problem multiclass object co detection large scale datasets <eos> end formulate co detection joint multiclass labeling object candidates obtained class independent manner <eos> exploit correlations between object build fully connected crf candidates explicitly incorporates both geometric layout relations across object classes similarity relations across multiple image <eos> then introduce structural boosting algorithm lets exploits rich high dimensional deep network feature learn object similarity within fully connected crf <eos> experiments pascal voc evidences benefits approach over object detection rcnn single image crf method state art co detection algorithms <eos> <eop> flickr entities collecting region phrase correspondences richer image sentence models <eos> flickr dataset become standard benchmark sentence based image description <eos> paper presents flickr entities augments captions flickr coreference chains linking mentions same entities image well manually annotated bounding boxes corresponding each entity <eos> such annotation essential continued progress automatic image description grounded language understanding <eos> present experiments demonstrating usefulness annotations text image reference resolution task localizing textual entity mentions image bidirectional image sentence retrieval <eos> experiments confirm further improve accuracy state art retrieval method training explicit region phrase correspondence but same time they show accurately inferring correspondence given image caption remains really challenging <eos> <eop> predicting depth surface normals semantic labels common multi scale convolutional architecture <eos> paper address three different computer vision tasks using single basic architecture depth prediction surface normal estimation semantic labeling <eos> use multiscale convolutional network able adapt easily each task using only small modifications regressing input image output map directly <eos> method progressively refines predictions using sequence scales captures many image details without any superpixels low level segmentation <eos> achieve state art performance benchmarks all three tasks <eos> <eop> attentionnet aggregating weak directions accurate object detection <eos> present novel detection method using deep convolutional neural network cnn named attentionnet <eos> cast object detection problem iterative classification problem most suitable form cnn <eos> attentionnet provides quantized weak directions pointing target object ensemble iterative predictions attentionnet converges accurate object boundary box <eos> since attentionnet unified network object detection detects object without any separated models object proposal post bounding box regression <eos> evaluate attentionnet human detection task achieve state art performance ap pascal voc layered architecture only <eos> <eop> common subspace model similarity phrase learning caption generation image <eos> generating captions describe image fundamental problem combines computer vision natural language processing <eos> recent works focus descriptive phrases such white dog explain visual composites input image <eos> phrases only express object attributes events their relations but also reduce visual complexity <eos> caption input image generated connecting estimated phrases using grammar model <eos> however because phrases combinations various words number phrases much larger than number single words <eos> consequently accuracy phrase estimation suffers too few training sample per phrase <eos> paper propose novel phrase learning method common subspace model similarity cosmos <eos> order overcome shortage training sample cosmos obtains subspace all feature vectors associated same phrase mapped mutually close classifiers each phrase learned training sample shared among co occurring phrases <eos> experimental result demonstrate system more accurate than earlier work accuracy increases when dataset web increases <eos> <eop> assisted feature synthesis novel views object <eos> comparing two image different views long standing challenging problem computer vision visual feature stable under large view point changes <eos> paper given single input image object synthesize its feature other views leveraging existing modestly sized three dimensional model collection related but identical object <eos> accomplish study relationship image patches between different views same object seeking call surrogate patches patches one view whose feature content predicts well feature patch another view <eos> based upon surrogate relationships create feature set all views latent object per patch basis providing augmented multi view representation object <eos> provide theoretical empirical analysis feature synthesis process evaluate augmented feature fine grained image retrieval recognition instance retrieval tasks <eos> experimental result show synthesized feature enable view independent comparison between image perform significantly better than other traditional approaches respect <eos> <eop> render cnn viewpoint estimation image using cnn trained rendered three dimensional model views <eos> object viewpoint estimation image essential task computer vision <eos> however two issues hinder its progress scarcity training data viewpoint annotations lack powerful feature <eos> inspired growing availability three dimensional models propose framework address both issues combining render based image synthesis cnn convolutional neural network <eos> believe three dimensional models potential generating large number image high variation well exploited deep cnn high learning capacity <eos> towards goal propose scalable overfit resistant image synthesis pipeline together novel cnn specifically tailored viewpoint estimation task <eos> experimentally show viewpoint estimation pipeline significantly outperform state art method pascal three dimensional benchmark <eos> <eop> lost shopping monocular localization large indoor spaces <eos> paper propose novel approach localization very large indoor spaces <eos> store shopping malls takes single image floor plan environment input <eos> formulate localization problem inference markov random field jointly reasons about text detection localizing shop names image precise bounding boxes shop facade segmentation well camera rotation translation within entire shopping mall <eos> power approach use any prior information about appearance instead exploits text detections corresponding shop names <eos> makes method applicable variety domains robust store appearance variation across countries seasons illumination conditions <eos> demonstrate performance approach new dataset collected two very large shopping malls show power holistic reasoning <eos> <eop> camera pose voting large scale image based localization <eos> image based localization approaches aim determine camera pose image was taken <eos> finding correct three dimensional correspondences between query image feature three dimensional point scene model becomes harder size model increases <eos> current state art method therefore combine elaborate matching schemes camera pose estimation techniques able handle large fractions wrong matches <eos> work study benefits limitations spatial verification compared appearance based filtering <eos> propose voting based pose estimation strategy exhibits complexity number matches thus facilitates consider much more matches than previous approaches whose complexity grows least quadratically <eos> new outlier rejection formulation enables evaluate pose estimation many matches surpass state art <eos> same time show using more matches automatically lead better performance <eos> <eop> mantra minimum maximum latent structural svm image classification ranking <eos> work propose novel weakly supervised learning wsl framework dedicated learn discriminative part detectors image annotated global label <eos> wsl method encompasses three main contributions <eos> firstly introduce new structured output latent variable model minimum maximum latent structural svm mantra prediction relies pair latent variables resp <eos> provides positive resp <eos> negative evidence given output <eos> secondly instantiate mantra two different visual recognition tasks multi class classification ranking <eos> ranking propose efficient solutions exactly solve inference loss augmented problems <eos> finally extensive experiments highlight relevance proposed method mantra outperforms state art result five different datasets <eos> <eop> deepdriving learning affordance direct perception autonomous driving <eos> today there two major paradigms vision based autonomous driving systems mediated perception approaches parse entire scene make driving decision behavior reflex approaches directly map input image driving action regressor <eos> paper propose third paradigm direct perception approach estimate affordance driving <eos> propose map input image small number key perception indicators directly relate affordance road traffic state driving <eos> representation provides set compact yet complete descriptions scene enable simple controller drive autonomously <eos> falling between two extremes mediated perception behavior reflex argue direct perception representation provides right level abstraction <eos> demonstrate train deep convolutional neural network using recording hours human driving video game show model work well drive car very diverse set virtual environments <eos> also train model car distance estimation kitti dataset <eos> result show direct perception approach generalize well real driving image <eos> source code data available project website <eos> <eop> active transfer learning zero shot priors reusing past datasets future tasks <eos> how reuse existing knowledge form available datasets when solving new apparently unrelated target task set unlabeled data work make first contribution answer question context image classification <eos> frame quest active learning problem use zero shot classifiers guide learning process linking new task existing classifiers <eos> revisiting dual formulation adaptive svm reveal two basic conditions choose greedily only most relevant sample annotated <eos> basis propose effective active learning algorithm learns best possible target classification model minimum human labeling effort <eos> extensive experiments two challenging datasets show value approach compared state art active learning methodologies well its potential reuse past datasets minimal effort future tasks <eos> <eop> hd cnn hierarchical deep convolutional neural network large scale visual recognition <eos> image classification visual separability between different object categories highly uneven some categories more difficult distinguish than others <eos> such difficult categories demand more dedicated classifiers <eos> however existing deep convolutional neural network cnn trained flat way classifiers few efforts made leverage hierarchical structure categories <eos> paper introduce hierarchical deep cnn hd cnn embedding deep cnn into category hierarchy <eos> hd cnn separates easy classes using coarse category classifier while distinguishing difficult classes using fine category classifiers <eos> during hd cnn training component wise pretraining followed global finetuning multinomial logistic loss regularized coarse category consistency term <eos> addition conditional executions fine category classifiers layer parameter compression make hd cnn scalable large scale visual recognition <eos> achieve state art result both cifar large scale imagenet class benchmark datasets <eos> experiments build up three different hd cnn they lower top error standard cnn <eos> respectively <eop> learning structure deep convolutional network <eos> work develop novel method automatically learning aspects structure deep model order improve its performance especially when labeled training data scarce <eos> propose new convolutional neural network model indian buffet process ibp prior termed ibpcnn <eos> ibpcnn automatically adapts its structure provided training data achieves optimal balance among model complexity data fidelity training loss thus offers better generalization performance <eos> proposed ibpcnn captures complicated data distribution unsupervised generative way <eos> therefore ibpcnn exploit unlabeled data collected low cost learn its structure <eos> after determining structure ibpcnn further learns its parameters according specified tasks end end fashion produces discriminative yet compact representations <eos> evaluate performance ibpcnn fully semi supervised image classification tasks ibpcnn surpasses standard cnn models benchmark datasets much smaller size higher efficiency <eos> <eop> flownet learning optical flow convolutional network <eos> convolutional neural network cnn recently very successful variety computer vision tasks especially linked recognition <eos> optical flow estimation among tasks cnn succeeded <eos> paper construct cnn capable solving optical flow estimation problem supervised learning task <eos> propose compare two architectures generic architecture another one including layer correlates feature vectors different image locations <eos> since existing ground truth data set sufficiently large train cnn generate large synthetic flying chairs dataset <eos> show network trained unrealistic data still generalize very well existing datasets such sintel kitti achieving competitive accuracy frame rates fps <eos> <eop> learning semi supervised representation towards unified optimization framework semi supervised learning <eos> state art approaches semi supervised learning ssl usually follow two stage framework constructing affinity matrix data then propagating partial labels affinity matrix infer unknown labels <eos> while such two stage framework successful many applications solving two subproblems separately only once still suboptimal because fully exploit correlation between affinity labels <eos> paper formulate two stages ssl into unified optimization framework learns both affinity matrix unknown labels simultaneously <eos> unified framework both given labels estimated labels used learn affinity matrix infer unknown labels <eos> solve unified optimization problem via alternating direction method multipliers combined label propagation <eos> extensive experiments synthetic data set several benchmark data set demonstrate effectiveness approach <eos> <eop> context guided diffusion label propagation graphs <eos> existing approaches diffusion graphs <eos> label propagation mainly focused isotropic diffusion induced commonly used graph laplacian regularizer <eos> inspired success diffusivity tensors anisotropic diffusion image processing presents anisotropic diffusion graphs corresponding label propagation algorithm <eos> develop positive definite diffusivity operators vector bundles riemannian manifolds discretize them diffusivity operators graphs <eos> enables easily define new robust diffusivity operators significantly improve semi supervised learning performance over existing diffusion algorithms <eos> <eop> learning rank based subsequences <eos> present supervised learning rank algorithm effectively orders image exploiting structure image sequences <eos> most often supervised learning rank literature ranking approached either analysing pairs image optimizing list wise surrogate loss function full sequences <eos> work propose midrank learns moderately sized sub sequences instead <eos> sub sequences contain useful structural ranking information leads better learnability during training better generalization during testing <eos> exploiting sub sequences proposed midrank improves ranking accuracy considerably extensive array image ranking applications datasets <eos> <eop> unsupervised learning visual representations using video <eos> strong supervision necessary learning good visual representation really need millions semantically labeled image train convolutional neural network cnn paper present simple yet surprisingly powerful approach unsupervised learning cnn <eos> specifically use hundreds thousands unlabeled video web learn visual representations <eos> key idea visual tracking provides supervision <eos> two patches connected track should similar visual representation deep feature space since they probably belong same object object part <eos> design siamese triplet network ranking loss function train cnn representation <eos> without using single image imagenet just using unlabeled video voc dataset train ensemble unsupervised network achieves map no bounding box regression <eos> performance comes tantalizingly close its imagenet supervised counterpart ensemble achieves map <eos> also show unsupervised network perform competitively other tasks such surface normal estimation <eos> <eop> nonparametric bayesian approach toward stacked convolutional independent component analysis <eos> unsupervised feature learning algorithms based convolutional formulations independent components analysis ica demonstrated yield state art result several action recognition benchmarks <eos> however existing approaches allow number latent components feature automatically inferred data unsupervised manner <eos> significant disadvantage state art result considerable burden imposed researchers practitioners who must resort tedious cross validation procedures obtain optimal number latent feature <eos> resolve issues paper introduce convolutional nonparametric bayesian sparse ica architecture overcomplete feature learning high dimensional data <eos> method utilizes indian buffet process prior facilitate inference appropriate number latent feature under hybrid variational inference algorithm scalable massive datasets <eos> show model naturally used obtain deep unsupervised hierarchical feature extractors greedily stacking successive model layer similar existing approaches <eos> addition inference model completely heuristics free thus obviates need tedious parameter tuning major challenge most deep learning approaches faced <eos> evaluate method several action recognition benchmarks exhibit its advantages over state art <eos> <eop> robust principal component analysis graphs <eos> principal component analysis pca most widely used tool linear dimensionality reduction clustering <eos> still highly sensitive outliers scale well respect number data sample <eos> robust pca solves first issue sparse penalty term <eos> second issue handled matrix factorization model however non convex <eos> besides pca based clustering also enhanced using graph data similarity <eos> article introduce new model called robust pca graphs incorporates spectral graph regularization into robust pca framework <eos> proposed model benefits robustness principal components occlusions missing values enhanced low rank recovery improved clustering property due graph smoothness assumption low rank matrix convexity resulting optimization problem <eos> extensive experiments benchmark video artificial datasets corruptions clearly reveal model outperforms other state art models its clustering low rank recovery tasks <eos> <eop> projection bank high dimensional data medium length binary codes <eos> recently very high dimensional feature representations <eos> fisher vector achieved excellent performance visual recognition retrieval <eos> however lengthy representations always cause extremely heavy computational storage costs even become unfeasible some large scale applications <eos> few existing techniques transfer very high dimensional data into binary codes but they still require reduced code length relatively long maintain acceptable accuracies <eos> target better balance between computational efficiency accuracies paper propose novel embedding method called binary projection bank bpb effectively reduce very high dimensional representations medium dimensional binary codes without sacrificing accuracies <eos> instead using conventional single linear bilinear projections proposed method learns bank small projections via max margin constraint optimally preserve intrinsic data similarity <eos> systematically evaluated proposed method three datasets flickr ilsvr ucf showing competitive retrieval recognition accuracies compared state art approaches but significantly smaller memory footprint lower coding complexity <eos> <eop> robust optimization deep regression <eos> convolutional neural network convnets successfully contributed improve accuracy regression based method computer vision tasks such human pose estimation landmark localization object detection <eos> network optimization usually performed loss without considering impact outliers training process outlier context defined sample estimation lies abnormal distance other training sample estimations objective space <eos> work propose regression model convnets achieves robustness such outliers minimizing tukey biweight function estimator robust outliers loss function convnet <eos> addition robust loss introduce coarse fine model processes input image progressively higher resolutions improving accuracy regressed values <eos> experiments demonstrate faster convergence better generalization robust loss function tasks human pose estimation age estimation face image <eos> also show combination robust loss function coarse fine model produces comparable better result than current state art approaches four publicly available human pose estimation datasets <eos> <eop> multi class multi annotator active learning robust gaussian process visual recognition <eos> active learning effective way relieve tedious work manual annotation many applications visual recognition <eos> however less research attention focused multi class active learning <eos> paper propose novel gaussian process classifier model multiple annotators multi class visual recognition <eos> expectation propagation ep adopted efficient approximate bayesian inference probabilistic model classification <eos> based ep approximation inference generalized expectation maximization gem algorithm derived estimate both parameters instances quality each individual annotator <eos> also incorporate idea reinforcement learning actively select both informative sample high quality annotators better explores trade off between exploitation exploration <eos> experiments clearly demonstrate efficacy proposed model <eos> <eop> maximum margin structured learning deep network three dimensional human pose estimation <eos> paper focuses structured output learning using deep neural network three dimensional human pose estimation monocular image <eos> network takes image three dimensional pose inputs outputs score value high when image pose pair matches low otherwise <eos> network structure consists convolutional neural network image feature extraction followed two sub network transforming image feature pose into joint embedding <eos> score function then dot product between image pose embeddings <eos> image pose embedding score function jointly trained using maximum margin cost function <eos> proposed framework interpreted special form structured support vector machines joint feature space discriminatively learned using deep neural network <eos> test framework human <eos> dataset obtain state art result compared other recent method <eos> finally present visualizations image pose embedding space demonstrating network learned high level embedding body orientation pose configuration <eos> <eop> exploration parameter redundancy deep network circulant projections <eos> explore redundancy parameters deep neural network replacing conventional linear projection fully connected layer circulant projection <eos> circulant structure substantially reduces memory footprint enables use fast fourier transform speed up computation <eos> considering fully connected neural network layer input nodes output nodes method improves time complexity dlogd space complexity <eos> space savings particularly important modern deep convolutional neural network architectures fully connected layer typically contain more than network parameters <eos> further show gradient computation optimization circulant projections performed very efficiently <eos> experiments three standard datasets show proposed approach achieves significant gain storage efficiency minimal increase error rate compared neural network unstructured projections <eos> <eop> additive nearest neighbor feature maps <eos> paper present concise framework approximately construct feature maps nonlinear additive kernels such intersection hellinger chi kernels <eos> core idea construct each individual feature set anchor point assign every query feature map its nearest neighbor weighted combination its nearest neighbors anchors <eos> resultant feature maps compactly stored group nearest neighbor binary indication vectors along anchor feature maps <eos> approximation error such anchored feature mapping approach analyzed <eos> evaluate performance approach large scale nonlinear support vector machines svms learning tasks context visual object classification <eos> experimental result several benchmark data set show superiority method over existing feature mapping method achieving reasonable trade off between training time testing accuracy <eos> <eop> understanding deep feature computer generated imagery <eos> introduce approach analyzing variation feature generated convolutional neural network cnn trained large image datasets respect scene factors occur natural image <eos> such factors may include object style three dimensional viewpoint color scene lighting configuration <eos> approach analyzes cnn feature responses respect different scene factors controlling them via rendering using large database three dimensional cad models <eos> rendered image presented trained cnn responses different layer studied respect input scene factors <eos> perform linear decomposition responses based knowledge input scene factors analyze resulting components <eos> particular quantify their relative importance cnn responses visualize them using principal component analysis <eos> show qualitative quantitative result study three trained cnn alexnet places oxford vgg <eos> observe important differences across different network cnn layer respect different scene factors object categories <eos> finally demonstrate analysis based computer generated imagery translates network representation natural image <eos> <eop> interpolation manifold component gmms <eos> probability density functions pdfs fundamental object mathematics numerous applications computer vision machine learning medical imaging <eos> feasibility basic operations such computing distance between two pdfs estimating mean set pdfs direct function representation choose work <eos> paper study gaussian mixture model gmm representation pdfs motivated its numerous attractive feature <eos> gmms arguably more interpretable than say square root parameterizations model complexity explicitly controlled number components they already widely used many applications <eos> main contributions paper numerical algorithms enable basic operations such object strictly respect their underlying geometry <eos> instance when operating set component gmms first order expectation result simple operations like interpolation averaging should provide object also component gmm <eos> literature provides very little guidance enforcing such requirements systematically <eos> turns out tasks important internal modules analysis processing field ensemble average propagators eaps common diffusion weighted magnetic resonance imaging <eos> provide proof principle experiments showing how proposed algorithms interpolation facilitate statistical analysis such data essential many neuroimaging studies <eos> separately also derive interesting connections algorithm functional spaces gaussians may independent interest <eos> <eop> context aware cnn person head detection <eos> person detection key problem many computer vision tasks <eos> while face detection reached maturity detecting people under full variation camera view point human poses lighting conditions occlusions still difficult challenge <eos> work focus detecting human heads natural scenes <eos> starting recent cnn object detector extend two ways <eos> first leverage person scene relations propose global cnn model trained predict positions scales heads directly full image <eos> second explicitly model pairwise relations among object via energy based model potentials computed cnn framework <eos> full combined model complements cnn contextual cues derived scene <eos> train test model introduce large dataset human heads annotated movie frames <eos> evaluate method demonstrate improvements person head detection compared several recent baselines three datasets <eos> also show improvements detection speed provided model <eos> <eop> mode seeking hypergraphs robust geometric model fitting <eos> paper propose novel geometric model fitting method called mode seeking hypergraphs msh deal multi structure data even presence severe outliers <eos> proposed method formulates geometric model fitting mode seeking problem hypergraph vertices represent model hypotheses hyperedges denote data point <eos> msh intuitively detects model instances simple effective mode seeking algorithm <eos> addition mode seeking algorithm msh includes similarity measure between vertices hypergraph weight aware sampling technique <eos> proposed method only alleviates sensitivity data distribution but also scalable large scale problems <eos> experimental result further demonstrate proposed method significant superiority over state art fitting method both synthetic data real image <eos> <eop> highly expressive spaces well behaved transformations keeping simple <eos> propose novel finite dimensional spaces transformations derived continuously defined parametric stationary velocity fields <eos> particularly obtain transformations diffeomorphisms fast highly accurate integration continuous piecewise affine velocity fields also provide exact solution <eos> simple yet highly expressive proposed representation handles optional constraints <eos> volume preservation easily supports convenient modeling choices rapid likelihood evaluations facilitating tractable inference over latent transformations <eos> its applications include but limited unconstrained optimization over monotonic functions modeling cumulative distribution functions histograms time warping image registration landmark based warping real time diffeomorphic image editing <eos> code available github <eos> com freifeld cpabdiffeo <eop> entropy based latent structured output prediction <eos> recently several generalizations popular latent structural svm framework proposed literature <eos> broadly speaking generalizations divided into two categories predict output variables while either marginalizing latent variables estimating their most likely values ii predict output variables minimizing entropy based uncertainty measure over latent space <eos> order aid their application computer vision study generalizations aim identifying their strengths weaknesses <eos> end propose novel prediction criterion includes special cases all previous prediction criteria used literature <eos> specifically framework prediction criterion minimizes aczel daroczy entropy output <eos> turn allows design learning objective provides unified framework uf latent structured prediction <eos> develop single optimization algorithm empirically show effective more complex approaches previously employed latent structured prediction <eos> using algorithm provide empirical evidence lends support prediction via minimization latent space uncertainty <eos> <eop> fast orthogonal projection based kronecker product <eos> propose family structured matrices speed up orthogonal projections high dimensional data commonly seen computer vision applications <eos> structured matrix formed kronecker product series smaller orthogonal matrices <eos> achieves dlogd computational complexity logd space complexity dimensional data drastic improvement over standard unstructured projections whose computational space complexities both <eos> proposed structured matrices applicable number application domains faster more compact than other structured matrices used past <eos> also introduce efficient learning procedure optimizing such matrices data dependent fashion <eos> demonstrate significant advantages proposed approach solving approximate nearest neighbor ann image search problem both binary embedding quantization <eos> find orthogonality plays very important role solving ann problem since random orthogonal kronecker projection already provided promising performance <eos> comprehensive experiments show proposed approach achieve similar better accuracy existing state art but significantly less time memory <eos> <eop> posenet convolutional network real time dof camera relocalization <eos> present robust real time monocular six degree freedom relocalization system <eos> system trains convolutional neural network regress dof camera pose single rgb image end end manner no need additional engineering graph optimisation <eos> algorithm operate indoors outdoors real time taking ms per frame compute <eos> obtains approximately degrees accuracy large scale outdoor scenes <eos> degrees accuracy indoors <eos> achieved using efficient layer deep convnet demonstrating convnets used solve complicated out image plane regression problems <eos> was made possible leveraging transfer learning large scale classification data <eos> show posenet localizes high level feature robust difficult lighting motion blur different camera intrinsics point based sift registration fails <eos> furthermore show how pose feature produced generalizes other scenes allowing regress pose only few dozen training examples <eos> <eop> predicting multiple structured visual interpretations <eos> present simple approach producing small number structured visual outputs high recall variety tasks including monocular pose estimation semantic scene segmentation <eos> current state art approaches learn single model modify inference procedures produce small number diverse predictions <eos> take alternate route modifying learning procedure directly optimize good high recall sequences structured output predictors <eos> approach introduces no new parameters naturally learns diverse predictions tied any specific structured learning inference procedure <eos> leverage recent advances contextual submodular maximization literature learn sequence predictors empirically demonstrate simplicity performance approach multiple challenging vision tasks including achieving state art result multiple predictions monocular pose estimation image foreground background segmentation <eos> <eop> look think twice capturing top down visual attention feedback convolutional neural network <eos> while feedforward deep convolutional neural network cnn great success computer vision important remember human visual contex contains generally more feedback connections than foward connections <eos> paper will briefly introduce background feedbacks human visual cortex motivates develop computational feedback mechanism deep neural network <eos> proposed network perform inference image feature bottom up manner traditional convolutional network while during feedback loops set up high level semantic labels agoala infer activation status hidden layer neurons <eos> feedback network help better visualize understand how deep neural network work well capture visual attention expected object even image cluttered background multiple object <eos> <eop> matrix backpropagation deep network structured layer <eos> deep neural network architectures recently produced excellent result variety areas artificial intelligence visual recognition well surpassing traditional shallow architectures trained using hand designed feature <eos> power deep network stems both their ability perform local computations followed pointwise non linearities over increasingly larger receptive fields simplicity scalability gradient descent training procedure based backpropagation <eos> open problem inclusion layer perform global structured matrix computations like segmentation <eos> normalized cuts higher order pooling <eos> log tangent space metrics defined over manifold symmetric positive definite matrices while preserving validity efficiency end end deep training framework <eos> paper propose sound mathematical apparatus formally integrate global structured computation into deep computation architectures <eos> heart methodology development theory practice backpropagation generalizes calculus adjoint matrix variations <eos> perform segmentation experiments using bsds mscoco benchmarks demonstrate deep network relying second order pooling normalized cuts layer trained end end using matrix backpropagation outperform counterparts take advantage such global layer <eos> <eop> introducing geometry active learning image segmentation <eos> propose active learning approach training segmentation classifier exploits geometric priors streamline annotation process three dimensional image volumes <eos> end use priors only select voxels most need annotation but guarantee they lie planar patch makes much easier annotate than if they were randomly distributed volume <eos> simplified version approach effective natural image <eos> evaluated approach electron microscopy magnetic resonance image volumes well natural image <eos> comparing approach against several accepted baselines demonstrates marked performance increase <eos> <eop> joint fine tuning deep neural network facial expression recognition <eos> temporal information useful feature recognizing facial expressions <eos> however manually design useful feature requires lot effort <eos> paper reduce effort deep learning technique regarded tool automatically extract useful feature raw data adopted <eos> deep network based two different models <eos> first deep network extracts temporal appearance feature image sequences while other deep network extracts temporal geometry feature temporal facial landmark point <eos> two models combined using new integration method order boost performance facial expression recognition <eos> through several experiments show two models cooperate each other <eos> result achieve superior performance other state art method ck oulu casia databases <eos> furthermore show new integration method gives more accurate result than traditional method such weighted summation feature concatenation method <eos> <eop> direct intrinsics learning albedo shading decomposition convolutional regression <eos> introduce new approach intrinsic image decomposition task decomposing single image into albedo shading components <eos> strategy term direct intrinsics learn convolutional neural network cnn directly predicts output albedo shading channels input rgb image patch <eos> direct intrinsics departure classical techniques intrinsic image decomposition typically rely physically motivated priors graph based inference algorithms <eos> large scale synthetic ground truth mpi sintel dataset plays key role training direct intrinsics <eos> demonstrate result both synthetic image sintel real image classic mit intrinsic image dataset <eos> sintel direct intrinsics using only rgb input outperforms all prior work including method rely rgb depth input <eos> direct intrinsics also generalizes across modalities sintel trained cnn produces quite reasonable decompositions real image mit dataset <eos> result indicate marriage cnn synthetic training data may powerful new technique tackling classic problems computer vision <eos> <eop> face flow <eos> paper propose method robust efficient computation multi frame optical flow expressive sequence facial image <eos> formulate novel energy minimisation problem establishing dense correspondences between neutral template every frame sequence <eos> exploit highly correlated nature human expressions representing dense facial motion using deformation basis <eos> furthermore exploit even higher correlation between deformations given input sequence imposing low rank prior coefficients deformation basis yielding temporally consistent optical flow <eos> proposed model based formulation conjunction inverse compositional strategy low rank matrix optimisation adopt leads highly efficient algorithm calculating facial flow <eos> experimental evaluation show quantitative experiments challenging novel benchmark face sequences dense ground truth optical flow provided motion capture data <eos> also provide qualitative result real sequence displaying fast motion occlusions <eos> extensive quantitative qualitative comparisons demonstrate proposed method outperforms state art optical flow dense non rigid registration techniques whilst running order magnitude faster <eos> <eop> discriminative low rank tracking <eos> good tracking performance general attributed accurate representation over previously obtained targets reliable discrimination between target surrounding background <eos> work exploit advantages both approaches achieve robust tracker <eos> construct subspace represent target neighboring background simultaneously propagate their class labels via learned subspace <eos> moreover propose novel criterion identify target numerous target candidates each frame takes into account both discrimination reliability representation accuracy <eos> addition proposed criterion ambiguity class labels neighboring background sample often influences reliability discriminative tracking model effectively alleviated while training set still kept small <eos> extensive experiments demonstrate tracker performs favourably against many other state art trackers <eos> <eop> sowp spatially ordered weighted patch descriptor visual tracking <eos> simple yet effective object descriptor visual tracking proposed paper <eos> first decompose bounding box target object into multiple patches described color gradient histograms <eos> then concatenate feature spatially ordered patches represent object appearance <eos> moreover alleviate impacts background information possibly included bounding box determine patch weights using random walk restart rwr simulations <eos> patch weights represent importance each patch description foreground information used construct object descriptor called spatially ordered weighted patch sowp descriptor <eos> incorporate proposed sowp descriptor into structured output tracking framework <eos> experimental result demonstrate proposed algorithm yields significantly better performance than state art trackers recent benchmark dataset also excels another recent benchmark dataset <eos> <eop> live repetition counting <eos> task counting number repetitions approximately same action input video sequence addressed <eos> proposed method runs online complete pre captured video <eos> analyzes sequentially blocks non consecutive frames <eos> cycle length within each block evaluated using convolutional neural network information then integrated over time <eos> entropy network predictions used order automatically start stop repetition counter select appropriate time scale <eos> coupled region interest detection mechanism method robust enough handle real world video even when camera moving <eos> unique property method shown successfully train entirely unrealistic data created synthesizing moving random patches <eos> <eop> near online multi target tracking aggregated local flow descriptor <eos> paper tackle two key aspects multiple target tracking problem designing accurate affinity measure associate detections implementing efficient accurate near online multiple target tracking algorithm <eos> first contribution introduce novel aggregated local flow descriptor alfd encodes relative motion pattern between pair temporally distant detections using long term interest point trajectories ipts <eos> leveraging ipts alfd provides robust affinity measure estimating likelihood matching detections regardless application scenarios <eos> another contribution present near online multi target tracking nomt algorithm <eos> tracking problem formulated data association between targets detections temporal window performed repeatedly every frame <eos> while being efficient nomt achieves robustness via integrating multiple cues including alfd metric target dynamics appearance similarity long term trajectory regularization into model <eos> ablative analysis verifies superiority alfd metric over other conventional affinity metrics <eos> run comprehensive experimental evaluation two challenging tracking datasets kitti mot datasets <eos> nomt method combined alfd metric achieves best accuracy both datasets significant margins about higher mota over state art <eos> <eop> multi kernel correlation filter visual tracking <eos> correlation filter based trackers ranked top terms performances <eos> nevertheless they only employ single kernel time <eos> paper will derive multi kernel correlation filter mkcf based tracker fully takes advantage invariance discriminative power spectrums various feature further improve performance <eos> moreover may easily introduce location representation errors search several discrete scales proper one object bounding box because normally discrete candidate scales determined corresponding feature pyramid generated ahead searching <eos> paper will propose novel efficient scale estimation method based optimal bisection search fast evaluation feature <eos> scale estimation method first one uses truly minimal number layer feature pyramid avoids constructing pyramid before searching proper scales <eos> <eop> joint probabilistic data association revisited <eos> paper revisit joint probabilistic data association jpda technique propose novel solution based recent developments finding best solutions integer linear program <eos> key advantage approach makes jpda computationally tractable applications high target clutter density such spot tracking fluorescence microscopy sequences pedestrian tracking surveillance footage <eos> also show jpda algorithm embedded simple tracking framework surprisingly competitive state art global tracking method two applications while needing considerably less processing time <eos> <eop> tracking segmentation online gradient boosting decision tree <eos> propose online tracking algorithm adaptively models target appearances based online gradient boosting decision tree <eos> algorithm particularly useful non rigid articulated object since handles various deformations target effectively integrating classifier operating individual patches provides segmentation masks target final result <eos> posterior target state propagated over time particle filtering likelihood computed based mainly patch level confidence map associated latent target state corresponding each sample <eos> once tracking completed each frame gradient boosting decision tree updated adapt new data recursive manner <eos> effective evaluation segmentation based tracking algorithms construct new ground truth contains pixel level annotation segmentation mask <eos> evaluate performance tracking algorithm based measures segmentation masks algorithm illustrates superior accuracy compared state art segmentation based tracking method <eos> <eop> exploring causal relationships visual object tracking <eos> causal relationships often found visual object tracking between motions camera tracked object <eos> object motion may effect camera motion <eos> unsteady handheld camera <eos> but may also cause <eos> cameraman framing ob ject <eos> paper explore relationships pro vide statistical tools detect quantify them based transfer entropy stem information ory <eos> relationships then exploited make predic tions about object location <eos> approach shown excellent measure describing such relationships <eos> vot dataset prediction accuracy increased over best non causal predictor <eos> show location predictions robust camera shake sud den motion invaluable any tracking algorithm demonstrate applying causal prediction two state art trackers <eos> both them benefit struck gain ing accuracy robustness increase vtb <eos> benchmark becoming new state art <eos> <eop> hierarchical convolutional feature visual tracking <eos> visual object tracking challenging target object often undergo significant appearance changes caused deformation abrupt motion background clutter occlusion <eos> paper exploit feature extracted deep convolutional neural network trained object recognition datasets improve tracking accuracy robustness <eos> outputs last convolutional layer encode semantic information targets such representations robust significant appearance variations <eos> however their spatial resolution too coarse precisely localize targets <eos> contrast earlier convolutional layer provide more precise localization but less invariant appearance changes <eos> interpret hierarchies convolutional layer nonlinear counterpart image pyramid representation exploit multiple levels abstraction visual tracking <eos> specifically adaptively learn correlation filters each convolutional layer encode target appearance <eos> hierarchically infer maximum response each layer locate targets <eos> extensive experimental result largescale benchmark dataset show proposed algorithm performs favorably against state art method <eos> <eop> robust non rigid motion tracking surface reconstruction using regularization <eos> present new motion tracking method robustly reconstruct non rigid geometries motions single view depth inputs captured consumer depth sensor <eos> idea comes observation existence intrinsic articulated subspace most non rigid motions <eos> take advantage characteristic propose novel based motion regularizer iterative optimization solver implicitly constrain local deformation only joints articulated motions leading reduced solution space physical plausible deformations <eos> strategy integrated into available non rigid motion tracking pipeline forming proposed non rigid motion tracking method adaptively stop tracking error propagation <eos> extensive experiments over complex human body motions occlusions face hand motions demonstrate approach substantially improves tracking robustness surface reconstruction accuracy <eos> <eop> online object tracking proposal selection <eos> tracking detection approaches some most successful object trackers recent years <eos> their success largely determined detector model they learn initially then update over time <eos> however under challenging conditions object undergo transformations <eos> severe rotation method found lacking <eos> paper address problem formulating proposal selection task making two contributions <eos> first one introducing novel proposals estimated geometric transformations undergone object building rich candidate set predicting object location <eos> second one devising novel selection strategy using multiple cues <eos> detection score edgeness score computed state art object edges motion boundaries <eos> extensively evaluate approach visual object tracking challenge online tracking benchmark datasets show best performance <eos> <eop> understanding diagnosing visual tracking systems <eos> several benchmark datasets visual tracking research created recent years <eos> despite their usefulness whether they sufficient understanding diagnosing strengths weaknesses different trackers remains questionable <eos> address issue propose framework breaking tracker down into five constituent parts namely motion model feature extractor observation model model updater ensemble post processor <eos> then conduct ablative experiments each component study how affects overall result <eos> surprisingly findings discrepant some common beliefs visual tracking research community <eos> find feature extractor plays most important role tracker <eos> other hand although observation model focus many studies find often brings no significant improvement <eos> moreover motion model model updater contain many details could affect result <eos> also ensemble post processor improve result substantially when constituent trackers high diversity <eos> based findings put together some very elementary building blocks give basic tracker competitive performance state art trackers <eos> believe framework provide solid baseline when conducting controlled experiments visual tracking research <eos> <eop> integrating dashcam views through inter video mapping <eos> paper inter video mapping approach proposed integrate video footages two dashcams installed preceding its following vehicle provide illusion driver following vehicle see through preceding one <eos> key challenge adapt perspectives two video based small number common feature since large portion common region video captured following vehicle occluded preceding one <eos> inspired observation image most similar viewpoints yield dense high quality matches proposed inter video mapping estimates spatially varying motions across two video utilizing image very similar contents <eos> specifically estimate frame frame motions each two consecutive image incrementally add new views into merged representation <eos> way long rang motion estimation achieved observed perspective discrepancy between two video well approximated motion estimation <eos> once inter video mapping established correspondences updated incrementally so proposed method suitable line applications <eos> experiments demonstrate effectiveness approach real world challenging video <eos> <eop> visual tracking fully convolutional network <eos> propose new approach general object tracking fully convolutional neural network <eos> instead treating convolutional neural network cnn black box feature extractor conduct depth study properties cnn feature offline pre trained massive image data classification task imagenet <eos> discoveries motivate design tracking system <eos> found convolutional layer different levels characterize target different perspectives <eos> top layer encodes more semantic feature serves category detector while lower layer carries more discriminative information better separate target distracters similar appearance <eos> both layer jointly used switch mechanism during tracking <eos> also found tracking target only subset neurons relevant <eos> feature map selection method developed remove noisy irrelevant feature maps reduce computation redundancy improve tracking accuracy <eos> extensive evaluation widely used tracking benchmark shows proposed tacker outperforms state art significantly <eos> <eop> multiple feature fusion via weighted entropy visual tracking <eos> desirable combine multiple feature descriptors improve visual tracking performance because different feature provide complementary information describe object interest <eos> however how effectively fuse multiple feature remains challenging problem visual tracking especially data driven manner <eos> paper propose new data adaptive visual tracking approach using multiple feature fusion via weighted entropy <eos> unlike existing visual trackers simply concatenate multiple feature vectors together object representation employ weighted entropy evaluate dissimilarity between object state background state seek optimal feature combination minimizing weighted entropy so more complementary information exploited object representation <eos> experimental result demonstrate effectiveness approach tackling various challenges visual object tracking <eos> <eop> pedestrian travel time estimation crowded scenes <eos> paper target problem estimating statistic pedestrian travel time within period entrance destination crowded scene <eos> such estimation based global distributions crowd densities velocities instead complete trajectories pedestrians cannot obtained crowded scenes <eos> proposed method motivated statistical investigation into correlations between travel time global properties crowded scenes <eos> active region created each source destination pair model probable walking region over corresponding source destination traffic flow <eos> two set scene feature specially designed modeling moving stationary persons inside active region their influences pedestrian travel time <eos> estimation pedestrian travel time provides valuable information both crowd scene understanding pedestrian behavior analysis but was sufficiently studied literature <eos> effectiveness proposed pedestrian travel time estimation model demonstrated through several surveillance applications including dynamic scene monitoring localization region blocking traffics detection abnormal pedestrian behaviors <eos> many more valuable applications based method explored future <eos> <eop> unsupervised synchrony discovery human interaction <eos> people inherently social <eos> social interaction plays important natural role human behavior <eos> most computational method focus individuals alone rather than social context <eos> they also require labelled training data <eos> present unsupervised approach discover interpersonal synchrony referred two more persons preforming common actions overlapping video frames segments <eos> computational efficiency develop branch bound approach affords exhaustive search while guaranteeing globally optimal solution <eos> proposed method entirely general <eos> takes two more video any multi dimensional signal represented histogram <eos> derive three novel bounding functions provide efficient extensions including multi synchrony detection accelerated search using warm start strategy parallelism <eos> evaluate effectiveness approach multiple databases including human actions using cmu mocap dataset spontaneous facial behaviors using group formation task dataset parent infant interaction dataset <eos> <eop> efficient video segmentation using parametric graph partitioning <eos> video segmentation task grouping similar pixels spatio temporal domain become important preprocessing step subsequent video analysis <eos> most video segmentation supervoxel method output hierarchy segmentations but while provides useful multiscale information also adds difficulty selecting appropriate level task <eos> work propose efficient robust video segmentation framework based parametric graph partitioning pgp fast almost parameter free graph partitioning method identifies removes between cluster edges form node clusters <eos> apart its computational efficiency pgp performs clustering spatio temporal volume without requiring pre specified cluster number bandwidth parameters thus making video segmentation more practical use applications <eos> pgp framework also allows processing sub volumes further improves performance contrary other streaming video segmentation method sub volume processing reduces performance <eos> evaluate pgp method using segtrack chen xiph <eos> org datasets show outperforms related state art algorithms three dimensional segmentation metrics running time <eos> <eop> learning track spatio temporal action localization <eos> propose effective approach spatio temporal action localization realistic video <eos> approach first detects proposals frame level scores them combination static motion cnn feature <eos> then tracks high scoring proposals throughout video using tracking detection approach <eos> tracker relies simultaneously instance level class level detectors <eos> tracks scored using spatio temporal motion histogram descriptor track level combination cnn feature <eos> finally perform temporal localization action using sliding window approach track level <eos> present experimental result spatio temporal localization ucf sports hmdb ucf action localization datasets approach outperforms state art margin respectively map <eos> <eop> unsupervised object discovery tracking video collections <eos> paper addresses problem automatically localizing dominant object spatio temporal tubes noisy collection video minimal even no supervision <eos> formulate problem combination two complementary processes discovery tracking <eos> first one establishes correspondences between prominent region across video second one associates similar object region within same video <eos> interestingly algorithm also discovers implicit topology frames associated instances same object class across different video role normally left supervisory information form class labels conventional image video understanding method <eos> indeed demonstrated experiments method handle video collections featuring multiple object classes substantially outperforms state art colocalization even though tackles broader problem much less supervision <eos> <eop> car knows before you anticipating maneuvers via learning temporal driving models <eos> advanced driver assistance systems adas made driving safer over last decade <eos> they prepare vehicles unsafe road conditions alert drivers if they perform dangerous maneuver <eos> however many accidents unavoidable because time drivers alerted already too late <eos> anticipating maneuvers beforehand alert drivers before they perform maneuver also give adas more time avoid prepare danger <eos> work anticipate driving maneuvers few seconds before they occur <eos> purpose equip car cameras computing device capture driving context both inside outside car <eos> propose autoregressive input output hmm model contextual information alongwith maneuvers <eos> evaluate approach diverse data set miles natural freeway city driving show anticipate maneuvers <eos> seconds before they occur over score real time <eos> <eop> activity auto completion predicting human activities partial video <eos> paper propose activity auto completion aac model human activity prediction formulating activity prediction query auto completion qac problem information retrieval <eos> first extract discriminative patches frames video <eos> video represented based patches divided into collection segments each regarded character typed search box <eos> then partially observed video considered activity prefix consisting one more characters <eos> finally missing observation activity predicted activity candidates provided auto completion model <eos> candidates matched against activity prefix fly ranked learning rank algorithm <eos> validate method ut interaction set set <eos> experimental result show proposed activity auto completion model achieves promising performance <eos> <eop> person re identification correspondence structure learning <eos> paper addresses problem handling spatial misalignments due camera view changes human pose variations person re identification <eos> first introduce boosting based approach learn correspondence structure indicates patch wise matching probabilities between image target camera pair <eos> learned correspondence structure only capture spatial correspondence pattern between cameras but also handle viewpoint human pose variation individual image <eos> further introduce global based matching process <eos> integrates global matching constraint over learned correspondence structure exclude cross view misalignments during image patch matching process hence achieving more reliable matching score between image <eos> experimental result various datasets demonstrate effectiveness approach <eos> <eop> adaptive exponential smoothing online filtering pixel prediction maps <eos> propose efficient online video filtering method called adaptive exponential filtering aes refine pixel prediction maps <eos> assuming each pixel associated discriminative prediction score proposed aes applies exponentially decreasing weights over time smooth prediction score each pixel similar classic exponential smoothing <eos> however instead fixing spatial pixel location perform temporal filtering trace each pixel past frames finding optimal path bring maximum exponential smoothing score thus performing adaptive non linear filtering <eos> thanks pixel tracing aes better address object movements avoid over smoothing <eos> enable real time filtering propose linear complexity dynamic programming scheme trace all pixels simultaneously <eos> apply proposed filtering method improve both saliency detection maps scene parsing maps <eos> comparisons average exponential filtering well state art method validate aes effectively refine pixel prediction maps without using original video again <eos> <eop> cnn pose based cnn feature action recognition <eos> work targets human action recognition video <eos> while recent method typically represent actions statistics local video feature here argue importance representation derived human pose <eos> end propose new pose based convolutional neural network descriptor cnn action recognition <eos> descriptor aggregates motion appearance information along tracks human body parts <eos> investigate different schemes temporal aggregation experiment cnn feature obtained both automatically estimated manually annotated human poses <eos> evaluate method recent challenging jhmdb mpii cooking datasets <eos> both datasets method shows consistent improvement over state art <eos> <eop> fully connected object proposals video segmentation <eos> present novel approach video segmentation using multiple object proposals <eos> problem formulated minimization novel energy function defined over fully connected graph object proposals <eos> model combines appearance long range point tracks key ensure robustness respect fast motion occlusions over longer video sequences <eos> opposed previous approaches based object proposals seek best per frame object hypotheses perform segmentation <eos> instead combine multiple potentially imperfect proposals improve overall segmentation accuracy ensure robustness outliers <eos> overall basic algorithm consists three steps <eos> first generate very large number object proposals each video frame using existing techniques <eos> next perform svm based pruning step retain only high quality proposals sufficiently discriminative power <eos> finally determine fore background classification solving maximum posteriori fully connected conditional random field defined using novel energy function <eos> experimental result well established dataset demonstrate method compares favorably several recent state art approaches <eos> <eop> video segmentation just few strokes <eos> use video becoming more popular computer vision need annotated video datasets increases <eos> such datasets required either training data simply ground truth benchmark datasets <eos> particular challenge video segmentation due disocclusions hamper frame frame propagation conjunction non moving object <eos> show combination motion point trajectories known motion segmentation along minimal supervision largely help solve problem <eos> moreover integrate new constraint enforces consistency color distribution successive frames <eos> quantify user interaction effort respect segmentation quality challenging ego motion video <eos> compare approach diverse set algorithms terms user effort terms performance common video segmentation benchmarks <eos> <eop> actionness assisted recognition actions <eos> elicit fundamental definition action low level attributes reveal agency intentionality <eos> descriptors mainly trajectory based measuring sudden changes temporal synchrony repetitiveness <eos> actionness map used localize actions way generic across action agent types <eos> furthermore also groups interacting region into useful unit analysis crucial recognition actions involving interactions <eos> then implement actionness driven pooling scheme improve action recognition performance <eos> experimental result three datasets show advantages method both action detection action recognition comparing other state art method <eos> <eop> count forest co voting uncertain number targets using random forest crowd density estimation <eos> paper presents patch based approach crowd density estimation public scenes <eos> formulate problem estimating density structured learning framework applied random decision forests <eos> approach learns mapping between patch feature relative locations all object inside each patch contribute generate patch density map through gaussian kernel density estimation <eos> build forest coarse fine manner two split node layer further propose crowdedness prior effective forest reduction method improve estimation accuracy speed <eos> moreover introduce semi automatic training method learn estimator specific scene <eos> achieved state art result public mall dataset ucsd dataset also proposed two potential applications traffic counts scene understanding promising result <eos> <eop> multi cue structure preserving mrf unconstrained video segmentation <eos> video segmentation stepping stone understanding video context <eos> video segmentation enables one represent video decomposing into coherent region comprise whole parts object <eos> however challenge originates fact most video segmentation algorithms based unsupervised learning due expensive cost pixelwise video annotation intra class variability within similar unconstrained video classes <eos> propose markov random field model unconstrained video segmentation relies tight integration multiple cues vertices defined contour based superpixels unary potentials temporally smooth label likelihood pairwise potentials global structure video <eos> multi cue structure breakthrough extracting coherent object region unconstrained video absence supervision <eos> experiments vsb dataset show proposed model significantly outperforms competing state art algorithms <eos> qualitative analysis illustrates video segmentation result proposed model consistent human perception object <eos> <eop> motion trajectory segmentation via minimum cost multicuts <eos> segmentation moving object video analysis long term point trajectories very popular recently <eos> paper formulate segmentation video sequence based point trajectories minimum cost multicut problem <eos> unlike commonly used spectral clustering formulation minimum cost multicut formulation gives natural rise optimize only cluster assignment but also number clusters while allowing varying cluster sizes <eos> setup provide method create long term point trajectory graph attractive repulsive binary terms outperform state art method based spectral clustering fbms dataset motion subtask vsb dataset <eos> <eop> action localization video through context walk <eos> paper presents efficient approach localizing actions learning contextual relations form relative locations between different video region <eos> begin over segmenting video into supervoxels ability preserve action boundaries also reduce complexity problem <eos> context relations learned during training capture displacements all supervoxels video belonging foreground actions <eos> then given testing video select supervoxel randomly use context information acquired during training estimate probability each supervoxel belonging foreground action <eos> walk proceeds new supervoxel process repeated few steps <eos> context walk generates conditional distribution action over all supervoxels <eos> conditional random field then used find action proposals video whose confidences obtained using svms <eos> validated proposed approach several datasets show context form relative displacements between supervoxels extremely useful action localization <eos> also result significantly fewer evaluations classifier sharp contrast alternate sliding window approaches <eos> <eop> rgb when vision meets wireless <eos> inspired recent success rgb cameras propose enrichment rgb data additional quasi free modality namely wireless signal <eos> wifi bluetooth emitted individuals cell phones referred rgb <eos> received signal strength acts rough proxy depth reliable cue their identity <eos> although measured signals highly noisy more than average localization error demonstrate combination visual wireless data significantly improves localization accuracy <eos> introduce novel image driven representation wireless data embeds all received signals onto single image <eos> then indicate ability additional data locate persons within sparsity driven framework ii track individuals new confidence measure data association problem <eos> solution outperforms existing localization method significant margin <eos> applied millions currently installed rgb cameras better analyze human behavior offer next generation high accuracy location based services <eos> <eop> action detection implicit intentional motion clustering <eos> explicitly using human detection pose estimation found limited success action recognition problems <eos> may due complexity articulated motion human exhibit <eos> yet know action requires actor intention <eos> paper hence seeks understand spatiotemporal properties intentional movement how capture such intentional movement without relying challenging human detection tracking <eos> conduct quantitative analysis intentional movement findings motivate new approach implicit intentional movement extraction based spatiotemporal trajectory clustering leveraging properties intentional movement <eos> intentional movement clusters then used action proposals detection <eos> result three action detection benchmarks indicate relevance focusing intentional movement action detection method significantly outperforms state art challenging msr ii multi action video benchmark <eos> <eop> simultaneous foreground detection classification hybrid feature <eos> paper propose hybrid background model relies edge non edge feature image produce model <eos> encode feature into coding scheme called local hybrid pattern lhp selectively models edges non edges feature each pixel <eos> furthermore model each pixel adaptive code dictionary represent background dynamism update adding stable codes discarding unstable ones <eos> weight each code dictionary enhance its description pixel models <eos> foreground detected incoming codes deviate dictionary <eos> detect foreground background classify edge inner region each pixel simultaneously <eos> tested proposed method existing databases promising result <eos> <eop> training feedback loop hand pose estimation <eos> propose entirely data driven approach estimating three dimensional pose hand given depth image <eos> show correct mistakes made convolutional neural network trained predict estimate three dimensional pose using feedback loop <eos> components feedback loop also deep network optimized using training data <eos> they remove need fitting three dimensional model input data requires both carefully designed fitting function algorithm <eos> show approach outperforms state art method efficient implementation runs over fps single gpu <eos> <eop> opening black box hierarchical sampling optimization estimating human hand pose <eos> address problem hand pose estimation formulated inverse problem <eos> typical approaches optimize energy function over pose parameters using black box image generation procedure <eos> procedure knows little about either relationships between parameters form energy function <eos> paper show significantly improving upon black box optimization exploiting high level knowledge structure parameters using local surrogate energy function <eos> new framework hierarchical sampling optimization consists sequence predictors organized into kinematic hierarchy <eos> each predictor conditioned its ancestors generates set sample over subset pose parameters <eos> highly efficient surrogate energy used select among sample <eos> having evaluated full hierarchy partial pose sample concatenated generate full pose hypothesis <eos> several hypotheses generated using same procedure finally original full energy function selects best result <eos> experimental evaluation three publically available datasets show method particularly impressive low compute scenarios significantly outperforms all other state art method <eos> <eop> panoptic studio massively multiview system social motion capture <eos> present approach capture three dimensional structure motion group people engaged social interaction <eos> core challenges capturing social interactions occlusion functional frequent subtle motion needs measured over space large enough host social group human appearance configuration variation immense <eos> panoptic studio system organized around thesis social interactions should measured through perceptual integration large variety view point <eos> present modularized system designed around principle consisting integrated structural hardware software innovations <eos> system takes input synchronized video streams multiple people engaged social activities produces output labeled time varying three dimensional structure anatomical landmarks individuals space <eos> algorithmic contributions include hierarchical approach generating skeletal trajectory proposals optimization framework skeletal reconstruction trajectory re association <eos> <eop> buy matching street clothing photos online shops <eos> paper define new task exact street shop goal match real world example garment item same item online shop <eos> extremely challenging task due visual differences between street photos pictures people wearing clothing everyday uncontrolled settings online shop photos pictures clothing items people mannequins isolation captured professionals more controlled settings <eos> collect new dataset application containing shop photos collected different online retailers street photos providing total clothing item matches between street shop photos <eos> develop three different method exact street shop retrieval including two deep learning baseline method method learn similarity measure between street shop domains <eos> experiments demonstrate learned similarity significantly outperforms baselines use existing deep learning based representations <eos> <eop> multi task recurrent neural network immediacy prediction <eos> paper propose predict immediacy interacting persons still image <eos> complete immediacy set includes interactions relative distance body leaning direction standing orientation <eos> measures found related attitude social relationship social interaction action nationality religion communicators <eos> large scale dataset image constructed all immediacy measures human poses annotated <eos> propose rich set immediacy representations help predict immediacy imperfect person person pose estimation result <eos> multi task deep recurrent neural network constructed take proposed rich immediacy representation input learn complex relationship among immediacy predictions multiple steps refinement <eos> effectiveness proposed approach proved through extensive experiments large scale dataset <eos> <eop> learning complexity aware cascades deep pedestrian detection <eos> design complexity aware cascaded detectors combining feature very different complexities considered <eos> new cascade design procedure introduced formulating cascade learning lagrangian optimization risk accounts both accuracy complexity <eos> boosting algorithm denoted complexity aware cascade training compact then derived solve optimization <eos> compact cascades shown seek optimal trade off between accuracy complexity pushing feature higher complexity later cascade stages only few difficult candidate patches remain classified <eos> enables use feature vastly different complexities single detector <eos> result feature pool expanded feature previously impractical cascade design such responses deep convolutional neural network cnn <eos> demonstrated through design pedestrian detector pool feature whose complexities span orders magnitude <eos> resulting cascade generalizes combination cnn object proposal mechanism rather than pre processing stage compact cascades seamlessly integrate cnn their stages <eos> enables state art performance caltech kitti datasets fairly fast speeds <eos> <eop> polarized three dimensional high quality depth sensing polarization cues <eos> coarse depth maps enhanced using shape information polarization cues <eos> propose framework combine surface normals polarization hereafter polarization normals aligned depth map <eos> polarization normals used depth enhancement before <eos> because polarization normals suffer physics based artifacts such azimuthal ambiguity refractive distortion fronto parallel signal degradation <eos> propose framework overcome key challenges allowing benefits polarization used enhance depth maps <eos> result demonstrate improvement respect state art three dimensional reconstruction techniques <eos> <eop> airborne three dimensional cloud tomography <eos> seek sense three dimensional three dimensional volumetric distribution scatterers heterogenous medium <eos> important case study such medium atmosphere <eos> atmospheric contents their role earth radiation balance significant uncertainties regards scattering components aerosols clouds <eos> clouds made water droplets also lead local effects precipitation shadows <eos> sensing approach computational tomography using passive multi angular imagery <eos> light matter interaction accounts multiple scattering use three dimensional radiative transfer equation forward model <eos> volumetric recovery inverting model suffers computational bottleneck large scales include many unknowns <eos> steps taken make tomography tractable without approximating scattering order angle range <eos> <eop> leave one out kernel optimization shadow detection <eos> objective work detect shadows image <eos> pose problem labeling image region each region corresponds group superpixels <eos> predict label each region train kernel least squares svm separating shadow non shadow region <eos> parameters kernel classifier jointly learned minimize leave one out cross validation error <eos> optimizing leave one out cross validation error typically difficult but done efficiently framework <eos> experiments two challenging shadow datasets ucf uiuc show region classifier outperforms more complex method <eos> further enhance performance region classifier embedding mrf framework adding pairwise contextual cues <eos> leads method significantly outperforms state art <eos> <eop> removing rain single image via discriminative sparse coding <eos> visual distortions image caused bad weather conditions negative impact performance many outdoor vision systems <eos> one often seen bad weather rain causes significant yet complex local intensity fluctuations image <eos> paper aims developing effective algorithm remove visual effects rain single rainy image <eos> separate rain layer de rained image layer rainy image <eos> built upon non linear generative model rainy image namely screen blend mode proposed dictionary learning based algorithm single image de raining <eos> basic idea sparsely approximate patches two layer very high discriminative codes over learned dictionary strong mutual exclusivity property <eos> such discriminative sparse codes lead accurate separation two layer their non linear composite <eos> experiments showed proposed method outperformed existing single image de raining method tested rain image <eos> <eop> mutual structure joint filtering <eos> previous joint guided filters directly transfer structural information reference image target one <eos> paper first analyze its major drawback there may completely different edges two image <eos> simply passing all patterns target could introduce significant errors <eos> address issue propose concept mutual structure refers structural information contained both image thus safely enhanced joint filtering untraditional objective function efficiently optimized yield mutual structure <eos> method result necessary important edge preserving greatly benefits depth completion optical flow estimation image enhancement stereo matching name few <eos> <eop> photometric stereo scattering medium <eos> photometric stereo widely used three dimensional reconstruction <eos> however its use scattering media such water biological tissue fog limited until now because forward scattered light both source object well light scattered back medium backscatter <eos> here make three contributions address key modes light propagation under common single scattering assumption dilute media <eos> first show through extensive simulations single scattered light source approximated point light source single direction <eos> alleviates need handle light source blur explicitly <eos> next model blur due scattering light object <eos> measure object point spread function introduce simple deconvolution method <eos> finally show how imaging fluorescence emission available eliminates backscatter component increases signal noise ratio <eos> experimental result water tank different concentrations scattering media added show deconvolution produces higher quality three dimensional reconstructions than previous techniques when combined fluorescence produce result similar clear water even highly turbid media <eos> <eop> resolving scale ambiguity via xslit aspect ratio analysis <eos> perspective cameras image frontal parallel three dimensional object preserve its aspect ratio invariant its depth <eos> such invariance useful photography but unique perspective projection <eos> paper show alternative non perspective cameras such crossed slit xslit cameras exhibit different depth dependent aspect ratio ddar property used three dimensional recovery <eos> first conduct comprehensive analysis characterize ddar infer object depth its ar model recoverable depth range sensitivity error <eos> show repeated shape patterns real manhattan world scenes used three dimensional reconstruction using single xslit image <eos> also extend analysis model slopes lines <eos> specifically parallel three dimensional lines exhibit depth dependent slopes dds their image also used infer their depths <eos> validate analyses using real xslit cameras xslit panoramas catadioptric mirrors <eos> experiments show ddar dds provide important depth cues enable effective single image scene reconstruction <eos> <eop> single shot specular surface reconstruction gonio plenoptic imaging <eos> present gonio plenoptic imaging system realizes single shot shape measurement specular surfaces <eos> system comprised collimated illumination source plenoptic camera <eos> unlike conventional plenoptic camera system captures brdf variation object surface single image addition light field information scene allows recover very fine three dimensional structures surface <eos> shape surface reconstructed based reflectance property material rather than parallax between different views <eos> since only single shot required reconstruct whole surface system able capture dynamic surface deformation video mode <eos> also describe novel calibration technique maps light field viewing directions object space subpixels sensor plane <eos> proposed system evaluated using concave mirror known curvature compared parabolic mirror scanning system well multi illumination photometric stereo approach based simulations experiments <eos> <eop> transcut transparent object segmentation light field image <eos> segmentation transparent object very useful computer vision applications <eos> however because they borrow texture their background similar appearance their surroundings transparent object handled well regular image segmentation method <eos> propose method overcomes problems using consistency distortion properties light field image <eos> graph cut optimization applied pixel labeling problem <eos> light field linearity used estimate likelihood pixel belonging transparent object lambertian background occlusion detector used find occlusion boundary <eos> acquire light field dataset transparent object use dataset evaluate method <eos> result demonstrate proposed method successfully segments transparent object background <eos> <eop> depth recovery light field using focal stack symmetry <eos> describe technique recover depth light field lf using two proposed feature lf focal stack <eos> one feature property non occluding pixels exhibit symmetry along focal depth dimension centered focus slice <eos> other data consistency measure based analysis synthesis <eos> difference between synthesized focal stack given hypothesized depth map lf <eos> terms used iterative optimization framework extract scene depth <eos> experimental result real lytro raytrix data demonstrate technique outperforms state art solutions significantly more robust noise under sampling <eos> <eop> depth map estimation colorization anaglyph image using local color prior reverse intensity distribution <eos> paper present joint iterative anaglyph stereo matching colorization framework obtaining set disparity maps colorized image <eos> conventional stereo matching algorithms fail when addressing anaglyph image similar intensities their two respective view image <eos> resolve problem propose two novel data costs using local color prior reverse intensity distribution factor obtaining accurate depth maps <eos> colorize anaglyph image each pixel one view warped another view using obtained disparity values non occluded region <eos> colorization algorithm using optimization then employed additional constraint colorize remaining occluded region <eos> experimental result confirm proposed unified framework robust produces accurate depth maps colorized stereo image <eos> <eop> learning data driven reflectance priors intrinsic image decomposition <eos> propose data driven approach intrinsic image decomposition process inferring confounding factors reflectance shading image <eos> pose two stage learning problem <eos> first train model predict relative reflectance ordering tween image patches brighter darker same large scale human annotations producing data driven reflectance prior <eos> second show how naturally integrate learned prior into existing energy minimization frame works intrinsic image decomposition <eos> compare method state art approach bell <eos> both decomposition image relighting tasks demonstrating benefits simple relative reflectance prior especially scenes under challenging lighting conditions <eos> <eop> photometric stereo small angular variations <eos> most existing successful photometric stereo setups require large angular variations illumination directions result acquisition rigs large spatial extent <eos> many applications especially involving mobile devices important device spatially compact <eos> naturally implies smaller angular variations illumination directions <eos> paper studies effect small angular variations illumination directions photometric stereo <eos> explore both theoretical justification practical issues design compact portable photometric stereo device camera surrounded ring point light sources <eos> first derive relationship between estimation error surface normal baseline point light sources <eos> armed theoretical insight develop small baseline photometric stereo prototype experimentally examine theory its practicality <eos> <eop> occlusion aware depth estimation using light field cameras <eos> consumer level high end light field cameras now widely available <eos> recent work demonstrated practical method passive depth estimation light field image <eos> however most previous approaches explicitly model occlusions therefore cannot capture sharp transitions around object boundaries <eos> common assumption pixel exhibits photo consistency when focused its correct depth <eos> all viewpoints converge single lambertian point scene <eos> assumption hold presence occlusions making most current approaches unreliable precisely accurate depth information most important depth discontinuities <eos> paper develop depth estimation algorithm treats occlusion explicitly method also enables identification occlusion edges may useful other applications <eos> show although pixels occlusions preserve photo consistency general they still consistent approximately half viewpoints <eos> moreover line separating two view region correct depth vs <eos> occluder same orientation occlusion edge spatial domain <eos> treating two region separately depth estimation improved <eos> occlusion predictions also computed used regularization <eos> experimental result show method outperforms current state art light field depth estimation algorithms especially near occlusion boundaries <eos> <eop> oriented light field windows scene flow <eos> spatial image windows used comparing pixel values computer vision applications such correspondence optical flow three dimensional reconstruction bilateral filtering image segmentation <eos> however pixel window comparisons suffer varying defocus blur perspective different depths also lead loss precision <eos> paper leverage recent use light field cameras propose alternative oriented light field windows enable more robust accurate pixel comparisons <eos> lambertian surfaces focused correct depth distribution angular rays pixel remains consistent <eos> build idea develop oriented light field window accounts shearing depth translation matching windowing <eos> main application scene flow generalization optical flow three dimensional vector field describing motion each point scene <eos> show significant benefits oriented light field windows over standard spatial windows <eos> also demonstrate additional applications oriented light field windows bilateral filtering image segmentation <eos> <eop> extended depth field catadioptric imaging using focal sweep <eos> catadioptric imaging systems use curved mirrors capture wide fields view <eos> however due curvature mirror systems tend very limited depth field dof point spread function psf varying dramatically over field view function scene depth <eos> recent years focal sweep used extensively extend dof conventional imaging systems <eos> shown focal sweep produces integrated point spread function ipsf nearly space invariant depth invariant enabling recovery extended depth field edof image deconvolving captured focal sweep image single ipsf <eos> paper use focal sweep extend dof catadioptric imaging system <eos> show while ipsf spatially varying when curved mirror used remains quasi depth invariant over wide field view imaging system <eos> developed focal sweep system mirrors different shapes used capture wide field view edof image <eos> particular show experimental result using spherical paraboloidal mirrors <eos> <eop> intrinsic depth improving depth transfer intrinsic image <eos> formulate estimation dense depth maps video sequences problem intrinsic image estimation <eos> approach synergistically integrates estimation multiple intrinsic image including depth albedo shading optical flow surface contours <eos> build upon example based framework depth estimation uses label transfer database rgb depth pairs <eos> combine method extracts consistent albedo shading video <eos> contrast raw rgb values albedo shading provide richer more physical foundation depth transfer <eos> additionally train new contour detector predict surface boundaries albedo shading pixel values use improve estimation depth boundaries <eos> also integrate sparse structure motion method improve metric accuracy estimated depth maps <eos> evaluate intrinsic depth method quantitatively estimating depth video nyu rgb sun datasets <eos> find combining estimation multiple intrinsic image improves depth estimation relative baseline method <eos> <eop> separating fluorescent reflective components using single hyperspectral image <eos> paper introduces novel method separate fluorescent reflective components spectral domain <eos> contrast existing method require capture two more image under varying illuminations aim achieve separation task using single hyperspectral image <eos> after identifying critical hurdle single image component separation mathematically design optimal illumination spectrum shown contain substantial high frequency components frequency domain <eos> observation turn leads recognize key difference between reflectance fluorescence response frequency modulation effect illumination fundamentally explains feasibility method <eos> practical side successfully find off shelf lamp light source strong irradiance intensity cheap cost <eos> fast linear separation algorithm developed well <eos> experiments using both synthetic data real image confirmed validity selected illuminant accuracy separation algorithm <eos> <eop> frequency based environment matting compressive sensing <eos> extracting environment mattes using existing approaches often requires either thousands captured image long processing time both <eos> paper propose novel approach capturing extracting matte real scene effectively efficiently <eos> grown out traditional frequency based signal analysis approach accurately locate contributing sources <eos> exploiting recently developed compressive sensing theory simplify data acquisition process frequency based environment matting <eos> incorporating phase information frequency signal into data acquisition further accelerates matte extraction procedure <eos> compared state art method approach achieves superior performance both synthetic real data while consuming only fraction processing time <eos> <eop> complementary set shutter sequences motion deblurring <eos> paper present novel multi image motion deblurring method utilizing coded exposure technique <eos> key idea work capture video frames set complementary fluttering patterns preserve spatial frequency details <eos> introduce algorithm generating complementary set binary sequences based modern communication theory implement coded exposure video system off shelf machine vision camera <eos> effectiveness method demonstrated various challenging examples quantitative qualitative comparisons other computational image capturing method used image deblurring <eos> <eop> hyperspectral compressive sensing using manifold structured sparsity prior <eos> reconstruct hyperspectral image hsi accurately few noisy compressive measurements present novel manifold structured sparsity prior based hyperspectral compressive sensing hcs method study <eos> matrix based hierarchical prior first proposed represent spectral structured sparsity spatial unknown manifold structure hsi simultaneously <eos> then latent variable bayes model introduced learn sparsity prior estimate noise jointly measurements <eos> learned prior fully represent inherent three dimensional structure hsi regulate its shape based estimated noise level <eos> thus learned prior proposed method improves reconstruction accuracy significantly shows strong robustness unknown noise hcs <eos> experiments four real hyperspectral datasets show proposed method outperforms several state art method reconstruction accuracy hsi <eos> <eop> gaussian process latent variable model brdf inference <eos> problem estimating full brdf partial observations already studied using either parametric non parametric approaches <eos> goal each case best match sparse set input measurements <eos> paper address problem inferring higher order reflectance information starting minimal input single brdf slice <eos> begin prototypical case homogeneous sphere lit head light source only holds information about less than <eos> whole brdf domain <eos> propose novel method infer higher dimensional properties material brdf based statistical distribution known material characteristics observed real life sample <eos> evaluated method based large set experiments generated real world brdfs newly measured materials <eos> although inferring higher dimensional brdfs such modest training trivial problem method performs better than state art parametric semi parametric non parametric approaches <eos> finally discuss interesting applications material re lighting flash based photography <eos> <eop> active one shot scan wide depth range using light field projector based coded aperture <eos> central projection model commonly used model cameras well projectors result similar advantages disadvantages both types system <eos> considering case active stereo systems using projector camera setup central projection model creates several problems among them narrow depth range necessity wide baseline crucial <eos> paper solve problems introducing light field projector project depth dependent pattern <eos> light field projector realized attaching coded aperture high frequency mask front lens video projector also projects high frequency pattern <eos> because light field projector cannot approximated thin lens model precise calibration method established yet image based approach proposed apply stereo technique system <eos> although image based techniques usually require large database often imply heavy computational costs propose hierarchical approach feature based search solution <eos> experiments confirmed method accurately recover dense shape curved textured object wide range depths single captured image <eos> <eop> model based tracking hz using raw time flight observations <eos> consumer depth cameras dramatically improved ability track rigid articulated deformable three dimensional object real time <eos> however depth cameras limited temporal resolution frame rate restricts accuracy robustness tracking especially fast unpredictable motion <eos> paper show how perform model based object tracking allows reconstruct object depth order magnitude higher frame rate through simple modifications off shelf depth camera <eos> focus phase based time flight tof sensing reconstructs each low frame rate depth image set short exposure raw infrared captures <eos> raw captures taken quick succession near beginning each depth frame differ modulation their active illumination <eos> make two contributions <eos> first detail how perform model based tracking against raw captures <eos> second show reprogramming camera space raw captures uniformly time obtain higher frame rate thereby improve ability track fast moving object <eos> <eop> hyperspectral super resolution coupled spectral unmixing <eos> hyperspectral cameras capture image many narrow spectral channels densely sample electromagnetic spectrum <eos> detailed spectral resolution useful many image analysis problems but comes cost much lower spatial resolution <eos> hyperspectral super resolution addresses problem fusing low resolution hyperspectral image conventional high resolution image into product both high spatial high spectral resolution <eos> paper propose method performs hyperspectral super resolution jointly unmixing two input image into pure reflectance spectra observed materials associated mixing coefficients <eos> formulation leads coupled matrix factorisation problem number useful constraints imposed elementary physical properties spectral mixing <eos> experiments two benchmark datasets show proposed approach delivers improved hyperspectral super resolution <eos> <eop> depth selective camera direct chip programmable technique depth selectivity photography <eos> time flight tof cameras use temporally modulated light source measure correlation between reflected light sensor modulation pattern order infer scene depth <eos> paper show such correlational sensors also used selectively accept reject light rays certain scene depths <eos> basic idea carefully select illumination sensor modulation patterns such correlation non zero only selected depth range thus light reflected object outside depth range affect correlational measurements <eos> demonstrate prototype depth selective camera highlight two potential applications imaging through scattering media virtual blue screening <eos> depthselectivity used reject back scattering reflection media front subjects interest thereby significantly enhancing ability image through scattering media critical applications such car navigation fog rain <eos> similarly such depth selectivity also utilized virtual blue screen cinematography rejecting light reflecting background while selectively retaining light contributions foreground subject <eos> <eop> groupwise multilinear correspondence optimization three dimensional faces <eos> multilinear face models widely used model space human faces expressions <eos> databases three dimensional human faces different identities performing multiple expressions statistical shape models decouple identity expression variations <eos> compute high quality multilinear face model quality registration database three dimensional face scans used training essential <eos> meanwhile multilinear face model used effective prior register three dimensional face scans typically noisy incomplete <eos> inspired minimum description length approach propose first method jointly optimize multilinear model registration three dimensional scans used training <eos> given initial registration approach fully automatically improves registration optimizing objective function measures compactness multilinear model resulting sparse model <eos> choose continuous representation each face shape allows use quasi newton method parameter space optimization <eos> show approach computationally significantly more efficient leads correspondences higher quality than existing method based linear statistical models <eos> allows evaluate approach large standard three dimensional face databases presence noisy initializations <eos> <eop> selective encoding recognizing unreliably localized faces <eos> most existing face verification systems rely precise face detection registration <eos> however two components fallible under unconstrained scenarios <eos> mobile face authentication due partial occlusions pose variations lighting conditions limited view angle coverage mobile cameras <eos> address unconstrained face verification problem encoding face image directly without any explicit models detection registration <eos> propose selective encoding framework injects relevance information <eos> foreground background probabilities into each cluster descriptor codebook <eos> additional selector component also discards distractive image patches improves spatial robustness <eos> evaluate framework using gaussian mixture models fisher vectors challenging face verification datasets <eos> apply selective encoding fisher vector feature experiments degrade quickly inaccurate face localization framework improves robustness no extra test time computation <eos> also apply approach mobile based active face authentication task demonstrating its utility real scenarios <eos> <eop> confidence preserving machine facial action unit detection <eos> varied sources error contribute challenge facial action unit detection <eos> previous approaches address specific known sources <eos> however many sources unknown <eos> address ubiquity error propose confident preserving machine cpm follows easy hard classification strategy <eos> during training cpm learns two confident classifiers <eos> confident positive classifier separates easily identified positive sample all else confident negative classifier same negative sample <eos> during testing cpm then learns person specific classifier using virtual labels provided confident classifiers <eos> step achieved using quasi semi supervised qss approach <eos> hard sample typically close decision boundary qss approach disambiguates them using spatio temporal constraints <eos> evaluate cpm compared baseline single margin classifier state art semi supervised learning transfer learning boosting method three datasets spontaneous facial behavior <eos> few exceptions cpm outperformed baseline state art method <eos> <eop> learning social relation traits face image <eos> social relation defines association <eos> warm friendliness dominance between two more people <eos> motivated psychological studies investigate if such fine grained high level relation traits characterised quantified face image wild <eos> address challenging problem propose deep model learns rich face representation capture gender expression head pose age related attributes then performs pairwise face reasoning relation prediction <eos> learn heterogeneous attribute sources formulate new network architecture bridging layer leverage inherent correspondences among datasets <eos> also cope missing target attribute labels <eos> extensive experiments show approach effective fine grained social relation learning image video <eos> <eop> robust heart rate measurement video using select random patches <eos> ability remotely measure heart rate video without requiring any special setup beneficial many applications <eos> recent years number papers heart rate hr measurement video proposed <eos> however method typically require human subject stationary illumination controlled <eos> method take into account motion illumination changes strong assumptions still made about environment <eos> background used illumination rectification <eos> paper propose hr measurement method robust motion illumination changes require use environment background <eos> present conditions under cardiac activity extraction local region face treated linear blind source separation problem propose simple but robust algorithm selecting good local region <eos> independent hr estimates multiple local region then combined majority voting scheme robustly recovers hr <eos> validate algorithm large database challenging video <eos> <eop> robust model based three dimensional head pose estimation <eos> introduce method accurate three dimensional head pose estimation using commodity depth camera <eos> perform pose estimation registering morphable face model measured depth data using combination particle swarm optimization pso iterative closest point icp algorithm minimizes cost function includes three dimensional registration overlap term <eos> pose estimated fly without requiring explicit initialization training phase <eos> method handles large pose angles partial occlusions dynamically adapting reliable visible parts face <eos> robust generalizes different depth sensors without modification <eos> biwi kinect dataset achieve best class performance average angular errors <eos> degrees yaw pitch roll respectively average translational error <eos> mm while running fps graphics processing unit <eos> <eop> robust facial landmark detection under significant head poses occlusion <eos> there tremendous improvements facial landmark detection general wild image <eos> however still challenging detect facial landmarks image severe occlusion image large head poses <eos> fact existing algorithms usually only handle one them <eos> work propose unified robust cascade regression framework handle both image severe occlusion image large head poses <eos> specifically method iteratively predicts landmark occlusions landmark locations <eos> occlusion estimation instead directly predicting binary occlusion vectors introduce supervised regression method gradually updates landmark visibility probabilities each iteration achieve robustness <eos> addition explicitly add occlusion pattern constraint improve performance occlusion prediction <eos> landmark detection combine landmark visibility probabilities local appearances local shapes iteratively update their positions <eos> experimental result show proposed method significantly better than state art works image severe occlusion image large head poses <eos> also comparable other method general wild image <eos> <eop> conditional convolutional neural network modality aware face recognition <eos> faces wild usually captured various poses illuminations occlusions thus inherently multimodally distributed many tasks <eos> propose conditional convolutional neural network named cnn handle multimodal face recognition <eos> different traditional cnn adopts fixed convolution kernels sample cnn processed dynamically activated set kernels <eos> particular convolution kernels within each layer only sparsely activated when sample passed through network <eos> given sample activations convolution kernels certain layer conditioned its present intermediate representation activation status lower layer <eos> activated kernels across layer define sample specific adaptive routes reveal distribution underlying modalities <eos> consequently proposed framework rely any prior knowledge modalities contrast most existing method <eos> substantiate generic framework introduce special case cnn via incorporating conditional routing decision tree evaluated two problems multimodality multi view face identification occluded face verification <eos> extensive experiments demonstrate consistent improvements over counterparts unaware modalities <eos> <eop> facial parts responses face detection deep learning approach <eos> paper propose novel deep convolutional network dcn achieves outstanding performance fddb pascal face afw <eos> specifically method achieves high recall rate <eos> challenging fddb benchmark outperforming state art method large margin <eos> importantly consider finding faces new perspective through scoring facial parts responses their spatial structure arrangement <eos> scoring mechanism carefully formulated considering challenging cases faces only partially visible <eos> consideration allows network detect faces under severe occlusion unconstrained pose variation main difficulty bottleneck most existing face detection approaches <eos> show despite use dcn network achieve practical runtime speed <eos> <eop> efficient psd constrained asymmetric metric learning person re identification <eos> person re identification becoming hot research topic due its value both machine learning research video surveillance applications <eos> challenging problem distance metric learning shown effective matching person image <eos> however existing approaches either require heavy computation due positive semidefinite psd constraint ignore psd constraint learn free distance function makes learned metric potentially noisy <eos> argue psd constraint provides useful regularization smooth solution metric hence learned metric more robust than without psd constraint <eos> another problem metric learning algorithms number positive sample pairs very limited learning process largely dominated large amount negative sample pairs <eos> address above issues derive logistic metric learning approach psd constraint asymmetric sample weighting strategy <eos> besides successfully apply accelerated proximal gradient approach find global minimum solution proposed formulation convergence rate number iterations <eos> proposed algorithm termed mlapg shown computationally efficient able perform low rank selection <eos> applied proposed method person re identification achieving state art performance four challenging databases viper qmul grid cuhk campus cuhk compared existing metric learning method well published result <eos> <eop> pose invariant three dimensional face alignment <eos> face alignment aims estimate locations set landmarks given image <eos> problem received much attention evidenced recent advancement both methodology performance <eos> however most existing works neither explicitly handle face image arbitrary poses nor perform large scale experiments non frontal profile face image <eos> order address limitations paper proposes novel face alignment algorithm estimates both three dimensional landmarks their visibilities face image arbitrary pose <eos> integrating three dimensional point distribution model cascaded coupled regressor approach designed estimate both camera projection matrix three dimensional landmarks <eos> furthermore three dimensional model also allows automatically estimate landmark visibilities via surface normal <eos> use substantially larger collection all pose face image evaluate algorithm demonstrate superior performances than state art method <eos> <eop> emotions action units hidden semi hidden task learning <eos> limited annotated training data challenging problem action unit recognition <eos> paper investigate how use large databases labelled according universal facial expressions increase generalization ability action unit classifiers <eos> purpose propose novel learning framework hidden task learning <eos> htl aims learn set hidden tasks action units sample available but contrast training data easier obtain set related visible tasks facial expressions <eos> end htl able exploit prior knowledge about relation between hidden visible tasks <eos> case base prior knowledge empirical psychological studies providing statistical correlations between action units universal facial expressions <eos> additionally extend htl semi hidden task learning shtl assuming action unit training sample also provided <eos> performing exhaustive experiments over four different datasets show htl shtl improve generalization ability au classifiers training them additional facial expression data <eos> additionally show shtl achieves competitive performance compared state art transductive learning approaches face problem limited training data using unlabelled test sample during training <eos> <eop> automated facial trait judgment election outcome prediction social dimensions face <eos> human face primary medium human communication prominent source information used infer various attributes <eos> paper study fully automated system infer perceived traits person his face social dimensions such intelligence honesty competence how traits used predict outcomes real world social events involve long term commitments such political elections job hires marriage engagements <eos> end propose hierarchical model enduring traits inferred faces incorporating high level perceptions intermediate level attributes <eos> show trained model successfully classify outcomes two important political events only using photographs politicians faces <eos> firstly classifies winners series recent <eos> also reveal different political offices require different types preferred traits <eos> secondly model categorize political party affiliations politicians <eos> best knowledge paper first use automated visual trait analysis predict outcomes real world social events <eos> approach more scalable objective than prior behavioral studies opens range new applications <eos> <eop> simultaneous local binary feature learning encoding face recognition <eos> paper propose simultaneous local binary feature learning encoding slbfle method face recognition <eos> different existing hand crafted face descriptors such local binary pattern lbp gabor feature require strong prior knowledge slbfle unsupervised feature learning approach automatically learned raw pixels <eos> unlike existing binary face descriptors such lbp discriminant face descriptor dfd use two stage feature extraction approach slbfle jointly learns binary codes local face patches codebook feature encoding so discriminative information raw pixels simultaneously learned one stage procedure <eos> experimental result four widely used face datasets including lfw youtube face ytf feret pasc clearly demonstrate effectiveness proposed method <eos> <eop> deep learning face attributes wild <eos> predicting face attributes wild challenging due complex face variations <eos> propose novel deep learning framework attribute prediction wild <eos> cascades two cnn lnet anet fine tuned jointly attribute tags but pre trained differently <eos> lnet pre trained massive general object categories face localization while anet pre trained massive face identities attribute prediction <eos> framework only outperforms state art large margin but also reveals valuable facts learning face representation <eos> shows how performances face localization lnet attribute prediction anet improved different pre training strategies <eos> reveals although filters lnet fine tuned only image level attribute tags their response maps over entire image strong indication face locations <eos> fact enables training lnet face localization only image level annotations but without face bounding boxes landmarks required all attribute recognition works <eos> also demonstrates high level hidden neurons anet automatically discover semantic concepts after pre training massive face identities such concepts significantly enriched after fine tuning attribute tags <eos> each attribute well explained sparse linear combination concepts <eos> <eop> multi task learning low rank attribute embedding person re identification <eos> propose novel multi task learning low rank attribute embedding mtl lorae framework person re identification <eos> re identifications multiple cameras regarded related tasks exploit shared information improve re identification accuracy <eos> both low level feature semantic data driven attributes utilized <eos> since attributes generally correlated introduce low rank attribute embedding into mtl formulation embed original binary attributes continuous attribute space incorrect incomplete attributes rectified recovered better describe people <eos> learning objective function consists quadratic loss regarding class labels attribute embedding error solved alternating optimization procedure <eos> experiments three person re identification datasets demonstrated mtl lorae outperforms existing approaches large margin produces state art result <eos> <eop> regressing three dimensional face shape single image <eos> work present method estimate three dimensional face shape single image <eos> method based cascade regression framework directly estimates face landmarks locations three dimensional include knowledge face three dimensional object into learning pipeline show how information decreases localization errors while keeping computational time low <eos> predict actual positions landmarks even if they occluded due face rotation <eos> support ability method reliably reconstruct three dimensional shapes introduce simple method head pose estimation using single image reaches higher accuracy than state art <eos> comparison three dimensional face landmarks localization available state art further supports feasibility single step face shape estimation <eos> code trained models three dimensional annotations will made available research community <eos> <eop> rendering eyes eye shape registration gaze estimation <eos> image eye key several computer vision problems such shape registration gaze estimation <eos> recent large scale supervised method problems require time consuming data collection manual annotation unreliable <eos> propose synthesizing perfectly labelled photo realistic training data fraction time <eos> used computer graphics techniques build collection dynamic eye region models head scan geometry <eos> were randomly posed synthesize close up eye image wide range head poses gaze directions illumination conditions <eos> used model controllability verify importance realistic illumination shape variations eye region training data <eos> finally demonstrate benefits synthesized training data syntheseyes out performing state art method eye shape registration well cross dataset appearance based gaze estimation wild <eos> <eop> multi scale learning low resolution person re identification <eos> real world person re identification re id image people captured very different resolutions different locations need matched <eos> existing re id models typically normalise all person image same size <eos> however low resolution lr image contains much less information about person direct image scaling simple size normalisation done conventional re id method cannot compensate loss information <eos> solve lr person re id problem propose novel joint multi scale learning framework termed joint multi scale discriminant component analysis judea <eos> key component framework heterogeneous class mean discrepancy hcmd criterion cross scale image domain alignment optimised simultaneously discriminant modelling across multiple scales joint learning framework <eos> experiments show proposed judea framework outperforms existing representative re id method well other related lr visual matching models applied lr person re id problem <eos> <eop> learning transfer transferring latent task structures its application person specific facial action unit detection <eos> article explore problem constructing person specific models detection facial action units aus addressing problem point view transfer learning multi task learning <eos> starting point fact some expressions such smiles very easily elicited annotated automatically detected while others much harder elicit annotate <eos> thus consider novel problem all au models target subject learnt using person specific annotated data reference au au case no data little data regarding target au <eos> order design such model propose novel multi task learning associated transfer learning framework consider both relations across subjects aus <eos> say consider tensor structure among tasks <eos> approach hinges learning latent relations among tasks using one single reference au then transferring latent relations other aus <eos> show able effectively make use annotated data au when learning other person specific au models even absence data target task <eos> finally show excellent performance method when small amounts annotated data target tasks made available <eos> <eop> pairwise conditional random forests facial expression recognition <eos> facial expression seen dynamic variation one appearance over time <eos> successful recognition thus involves finding representations high dimensional spatiotemporal patterns generalized unseen facial morphologies variations expression dynamics <eos> paper propose learn random forests heterogeneous derivative feature <eos> facial fiducial point movements texture variations upon pairs image <eos> forests conditioned expression label first frame reduce variability ongoing expression transitions <eos> when testing specific frame video pairs created between frame previous ones <eos> predictions each previous frame used draw trees pairwise conditional random forests pcrf whose pairwise outputs averaged over time produce robust estimates <eos> such pcrf appears natural extension random forests learn spatio temporal patterns leads significant improvements over standard random forests well state art approaches several facial expression benchmarks <eos> <eop> multi conditional latent variable model joint facial action unit detection <eos> propose novel multi conditional latent variable model simultaneous facial feature fusion detection facial action units <eos> approach exploit structure discovery capabilities generative models such gaussian processes discriminative power classifiers such logistic function <eos> leads superior performance compared existing classifiers target task exploit either discriminative generative property but both <eos> model learning performed via efficient newly proposed bayesian learning strategy based monte carlo sampling <eos> consequently learned model robust data overfitting regardless number both input feature jointly estimated facial action units <eos> extensive qualitative quantitative experimental evaluations performed three publicly available datasets ck shoulder pain disfa <eos> show proposed model outperforms state art method target task feature fusion ii multiple facial action unit detection <eos> <eop> leveraging datasets varying annotations face alignment via deep regression network <eos> facial landmark detection vital topic computer vision studied many decades lots datasets collected evaluation <eos> datasets usually different annotations <eos> landmark markup lfpw dataset while landmark markup gtav dataset <eos> intuitively meaningful fuse all datasets predict union all types landmarks multiple datasets <eos> transfer annotations each dataset all other datasets but problem nontrivial due distribution discrepancy between datasets incomplete annotations all types each dataset <eos> work propose deep regression network coupled sparse shape regression drn ssr predict union all types landmarks leveraging datasets varying annotations each dataset one type annotation <eos> specifically deep regression network intends predict union all landmarks sparse shape regression attempts approximate undefined landmarks each dataset so guide learning deep regression network face alignment <eos> extensive experiments two challenging datasets ibug glf demonstrate method effectively leverage multiple datasets different annotations predict union all types landmarks <eos> <eop> spatio temporal appearance representation viceo based pedestrian re identification <eos> pedestrian re identification difficult problem due large variations person appearance caused different poses viewpoints illumination changes occlusions <eos> spatial alignment commonly used address issues treating appearance different body parts independently <eos> however body part also appear differently during different phases action <eos> paper consider temporal alignment problem addition spatial one propose new approach takes video walking person input builds spatio temporal appearance representation pedestrian re identification <eos> particularly given video sequence exploit periodicity exhibited walking person generate spatio temporal body action model consists series body action units corresponding certain action primitives certain body parts <eos> fisher vectors learned extracted individual body action units concatenated into final representation walking person <eos> unlike previous spatio temporal feature only take into account local dynamic appearance information representation aligns spatio temporal appearance pedestrian globally <eos> extensive experiments public datasets show effectiveness approach compared state art <eos> <eop> two birds one stone jointly learning binary code large scale face image retrieval attributes prediction <eos> address challenging large scale content based face image retrieval problem intended searching image based presence specific subject given one face image him her <eos> end one natural demand supervised binary code learning method <eos> while learned codes might discriminating people often further expectation whether some semantic message <eos> visual attributes read human incomprehensible codes <eos> purpose propose novel binary code learning framework jointly encoding identity discriminability number facial attributes into unified binary code <eos> way learned binary codes applied only fine grained face image retrieval but also facial attributes prediction very innovation work just like killing two birds one stone <eos> evaluate effectiveness proposed method extensive experiments conducted new purified large scale web celebrity database named cfw abundant manual identity attributes annotation experimental result exhibit superiority method over state art <eos> <eop> accurate iris segmentation framework under relaxed imaging constraints using total variation model <eos> paper proposes novel more accurate iris segmentation framework automatically segment iris region face image acquired relaxed imaging under visible near infrared illumination provides strong feasibility applications surveillance forensics search missing children etc <eos> proposed framework built novel total variation based formulation uses norm regularization robustly suppress noisy texture pixels accurate iris localization <eos> series novel robust post processing operations introduced more accurately localize limbic boundaries <eos> experimental result three publicly available databases <eos> distance achieve significant performance improvement terms iris segmentation accuracy over state art approaches literature <eos> besides shown using iris masks generated proposed approach helps improve iris recognition performance well <eos> unlike prior work all implementations paper made publicly available further advance research applications biometrics distance <eos> <eop> discriminative pose free descriptors face object matching <eos> pose invariant matching very important challenging problem various applications like recognizing faces uncontrolled scenarios matching object taken different view point etc <eos> paper propose discriminative pose free descriptor dpfd used match faces object across pose variations <eos> training examples very few representative poses used generate virtual intermediate pose subspaces <eos> image image region then represented feature set obtained projecting all subspaces discriminative transform applied feature set make suitable classification tasks <eos> finally discriminative feature set represented single feature vector termed dpfd <eos> dpfd image taken different viewpoints directly compared matching <eos> extensive experiments recognizing faces across pose pose resolution multi pie surveillance cameras face datasets comparisons state art approaches show effectiveness proposed approach <eos> experiments matching general object across viewpoints show generalizability proposed approach beyond faces <eos> <eop> bi shifting auto encoder unsupervised domain adaptation <eos> many real world applications domain model learning referred source domain usually inconsistent even different domain testing referred target domain makes learnt model degenerate target domain <eos> alleviate discrepancy between source target domains propose domain adaptation method named bi shifting auto encoder network bae <eos> proposed bae attempts shift source domain sample target domain also shift target domain sample source domain <eos> non linear transformation bae ensures feasibility shifting between domains distribution consistency between shifted domain desirable domain constrained sparse reconstruction between them <eos> result shifted source domain supervised follows similar distribution target domain <eos> therefore any supervised method applied shifted source domain train classifier classification target domain <eos> proposed method evaluated three domain adaptation scenarios face recognition <eos> domain adaptation across view angle ethnicity imaging sensor promising result demonstrate proposed bae shift sample between domains thus effectively deal domain discrepancy <eos> <eop> regressive tree structured model facial landmark localization <eos> although tree structured model tsm proven effective solving face detection pose estimation landmark localization unified model its sluggish run time makes unfavorable practical applications especially when dealing cases multiple faces <eos> propose regressive tree structure model rtsm improve run time speed localization accuracy <eos> rtsm composed two component tsms coarse tsm tsm refined tsm tsm bilateral support vector regressor bsvr <eos> tsm built low resolution octaves sample so provides coarse but fast face detection <eos> tsm built mid resolution octaves so locate landmarks face candidates given tsm improve precision <eos> tsm based landmarks used forward bsvr references locate dense set landmarks then used backward bsvr relocate landmarks large localization errors <eos> forward backward regression goes iteratively until convergence <eos> performance rtsm validated three benchmark databases multi pie lfpw afw compared latest tsm demonstrate its efficacy <eos> <eop> person recognition personal photo collections <eos> recognising persons everyday photos presents major challenges occluded faces different clothing locations etc <eos> propose convnet based person recognition system provide depth analysis informativeness different body cues impact training data common failure modes system <eos> addition discuss limitations existing benchmarks propose more challenging ones <eos> method simple built open source open data yet improves state art result large dataset social media photos pipa <eos> <eop> robust statistical face frontalization <eos> recently shown excellent result achieved both facial landmark localization pose invariant face recognition <eos> breakthroughs attributed efforts community manually annotate facial image many different poses collect three dimensional facial data <eos> paper propose novel method joint frontal view reconstruction landmark localization using small set frontal image only <eos> observing frontal facial image one having minimum rank all different poses appropriate model able jointly recover frontalized version face well facial landmarks devised <eos> end suitable optimization problem involving minimization nuclear norm matrix norm solved <eos> proposed method assessed frontal face reconstruction face landmark localization pose invariant face recognition face verification unconstrained conditions <eos> relevant experiments conducted databases <eos> experimental result demonstrate effectiveness proposed method comparison state art method target problems <eos> <eop> piefa personalized incremental ensemble face alignment <eos> face alignment especially real time large scale sequential image challenging task broad applications <eos> both generic joint alignment approaches proposed varying degrees success <eos> however many generic method heavily sensitive initializations usually rely offline trained static models limit their performance sequential image extensive variations <eos> other hand joint method restricted offline applications since they require all frames conduct batch alignment <eos> address limitations propose exploit incremental learning personalized ensemble alignment <eos> sample multiple initial shapes achieve image congealing within one frame enables incrementally conduct ensemble alignment group sparse regularized rank minimization <eos> same time personalized modeling obtained subspace adaptation under same incremental framework while correction strategy used alleviate model drifting <eos> experimental result multiple controlled wild databases demonstrate superior performance approach compared state arts terms fitting accuracy efficiency <eos> <eop> understanding everyday hands action rgb image <eos> analyze functional manipulations handheld object formalizing problem one fine grained grasp classification <eos> so make use recently developed fine grained taxonomy human object grasps <eos> introduce large dataset rgb image covering everyday grasps natural interactions <eos> dataset different past work typically addressed robotics perspective terms its scale diversity combination rgb depth data <eos> computer vision perspective dataset allows exploration contact force prediction crucial concepts functional grasp analysis perceptual cues <eos> present extensive experimental result state art baselines illustrating role segmentation object context three dimensional understanding functional grasp analysis <eos> demonstrate near improvement over prior work naive deep baseline while pointing out important directions improvement <eos> <eop> example based modeling facial texture deficient data <eos> present approach modeling ear ear high quality texture one more partial views face possibly poor resolution noise <eos> approach example based reconstruct texture patches database composed previously seen faces <eos> three dimensional morphable model used establish shape correspondence between observed data across views training faces <eos> database built mesh surface segmenting into uniform overlapping patches <eos> texture patches selected belief propagation so consistent neighbors observations appropriate image formation model <eos> also develop variant insensitive light camera parameters incorporate soft symmetry constraints <eos> obtain textures higher quality degraded views small pixels wide than standard model fitted non degraded data <eos> further show applications super resolution substantially improve quality compared state art algorithm texture completion fill missing region remove facial clutter photorealistic manner <eos> <eop> learning predict saliency face image <eos> paper proposes novel method learns detect saliency face image <eos> more specific obtain database eye tracking over extensive face image via conducting eye tracking experiment <eos> analysis eye tracking database verify fixations tend cluster around facial feature when viewing image large faces <eos> modeling attention faces facial feature proposed method learns gaussian mixture model gmm distribution fixations eye tracking data top down feature saliency detection face image <eos> then method top down feature <eos> face facial feature upon learnt gmm linearly combined conventional bottom up feature <eos> color intensity orientation saliency detection <eos> linear combination argue weights corresponding top down feature channels depend face size image relationship between weights face size thus investigated via learning training eye tracking data <eos> finally experimental result show learning based method able advance state art saliency prediction face image <eos> corresponding database code available online www <eos> cn xumfiles saliency detection <eos> <eop> group membership prediction <eos> group membership prediction gmp problem involves predicting whether collection instances share certain semantic property <eos> instance kinship verification given collection image goal predict whether they share familial relationship <eos> context propose novel probability model introduce latent view specific view shared random variables jointly account view specific appearance cross view similarities among data instances <eos> model posits data each view independent conditioned shared variables <eos> postulate leads parametric probability model decomposes group membership likelihood into tensor product data independent parameters data dependent factors <eos> propose learning data independent parameters discriminative way bilinear classifiers test prediction algorithm challenging visual recognition tasks such multi camera person re identification kinship verification <eos> most benchmark datasets method significantly outperform current state art <eos> <eop> extraction virtual baselines distorted document image using curvilinear projection <eos> baselines document page set virtual horizontal parallel lines printed contents document <eos> text lines tables inserted photos aligned <eos> accurate baseline extraction great importance geometric correction curved document image <eos> paper propose efficient method accurate extraction virtual visual cues curved document image <eos> method comes two basic observations baselines documents intersect each other within narrow strip baselines well approximated linear segments <eos> based upon observations propose curvilinear projection based method model estimation curved baselines constrained sequential optimization problem <eos> dynamic programming algorithm then developed efficiently solve problem <eos> proposed method extract complete baselines through each pixel document image high accuracy <eos> also scripts insensitive highly robust image noises non textual object image resolutions image quality degradation like blurring non uniform illumination <eos> extensive experiments number captured document image demonstrate effectiveness proposed method <eos> <eop> robust rgb odometry using point line feature <eos> lighting variation uneven feature distribution main challenges indoor rgb visual odometry color information often combined depth information <eos> meet challenges fuse point line feature form robust odometry algorithm <eos> line feature abundant indoors less sensitive lighting change than point <eos> extract three dimensional point lines rgb data analyze their measurement uncertainties compute camera motion using maximum likelihood estimation <eos> prove fusing point lines produces smaller motion estimate uncertainty than using either feature type alone <eos> experiments compare method state art method including keypoint based approach dense visual odometry algorithm <eos> method outperforms counterparts under both constant varying lighting conditions <eos> specifically method achieves average translational error <eos> smaller than counterparts when tested using public datasets <eos> <eop> learning discriminative model perception realism composite image <eos> makes image appear realistic work answering question data driven perspective learning perception visual realism directly large amounts data <eos> particular train convolutional neural network cnn model distinguishes natural photographs automatically generated composite image <eos> model learns predict visual realism scene terms color lighting texture compatibility without any human annotations pertaining <eos> model outperforms previous works rely hand crafted heuristics task classifying realistic vs <eos> furthermore apply learned model compute optimal parameters compositing method maximize visual realism score predicted cnn model <eos> demonstrate its advantage against existing method via human perception study <eos> <eop> makes tom hanks look like tom hanks <eos> reconstruct controllable model person large photo collection captures his her persona <eos> physical appearance behavior <eos> ability operate unstructured photo collections enables modeling huge number people including celebrities other well photographed people without requiring them scanned <eos> moreover show ability drive puppeteer captured person using any other video different person <eos> scenario acts out role person but retains his her own personality character <eos> system based novel combination three dimensional face reconstruction tracking alignment multi texture modeling applied puppeteering problem <eos> demonstrate convincing result large variety celebrities derived internet imagery video <eos> <eop> wide area image geolocalization aerial reference imagery <eos> propose use deep convolutional neural network address problem cross view image geolocalization geolocation ground level query image estimated matching georeferenced aerial image <eos> use state art feature representations ground level image introduce cross view training approach learning joint semantic feature representation aerial image <eos> also propose network architecture fuses feature extracted aerial image multiple spatial scales <eos> support training network introduce massive database contains pairs aerial ground level image across united states <eos> method significantly out perform state art two benchmark datasets <eos> also show qualitatively proposed feature representations discriminative both local continental spatial scales <eos> <eop> personalized age progression aging dictionary <eos> paper aim automatically render aging faces personalized way <eos> basically set age group specific dictionaries learned dictionary bases corresponding same index yet different dictionaries form particular aging process pattern cross different age groups linear combination patterns expresses particular personalized aging process <eos> moreover two factors taken into consideration dictionary learning process <eos> first beyond aging dictionaries each subject may extra personalized facial characteristics <eos> mole invariant aging process <eos> second challenging even impossible collect faces all age groups particular subject yet much easier more practical get face pairs neighboring age groups <eos> thus personality aware coupled reconstruction loss utilized learn dictionaries based face pairs neighboring age groups <eos> extensive experiments well demonstrate advantages proposed solution over other state arts term personalized aging progression well performance gain cross age face verification synthesizing aging faces <eos> <eop> facedirector continuous control facial performance video <eos> present method continuously blend between multiple facial performances actor contain different facial expressions emotional states <eos> example given sad angry video takes scene method empowers movie director specify arbitrary weighted combinations smooth transitions between two takes post production <eos> contributions include robust nonlinear audio visual synchronization technique exploits complementary properties audio visual cues automatically determine robust dense spatiotemporal correspondences between takes seamless facial blending approach provides director full control interpolate timing facial expression local appearance order generate novel performances after filming <eos> contrast most previous works approach operates entirely image space avoiding need three dimensional facial reconstruction <eos> demonstrate method synthesize visually believable performances applications emotion transition performance correction timing control <eos> <eop> synthesizing illumination mosaics internet photo collections <eos> propose framework automatic creation time lapse mosaics given scene <eos> achieve leveraging illumination variations captured internet photo collections <eos> order depict characterize illumination spectrum scene method relies building discrete representations image appearance space through connectivity graphs defined over pairwise image distance function <eos> smooth appearance transitions found shortest path similarity graph among image robust image alignment achieved leveraging scene semantics multi view geometry image warping techniques <eos> attained result present insightful compact visualization scene illuminations captured crowd sourced imagery <eos> <eop> hot exploring correlations between appearance temperature <eos> paper explore interactions between appearance outdoor scene ambient temperature <eos> studying statistical correlations between image sequences outdoor cameras temperature measurements identify two interesting interactions <eos> first semantically meaningful region such foliage reflective oriented surfaces often highly indicative temperature <eos> second small camera motions correlated temperature some scenes <eos> propose simple scene specific temperature prediction algorithms used turn camera into crude temperature sensor <eos> find task simple feature such local pixel intensities outperform sophisticated global feature such semantically trained convolutional neural network <eos> <eop> spm bp sped up patchmatch belief propagation continuous mrfs <eos> markov random fields widely used model many computer vision problems cast energy minimization framework composed unary pairwise potentials <eos> while computationally tractable discrete optimizers such graph cuts belief propagation bp exist multi label discrete problems they still face prohibitively high computational challenges when labels reside huge very densely sampled space <eos> integrating key ideas patchmatch effective particle propagation resampling patchmatch belief propagation pmbp demonstrated good performance addressing continuous labeling problems runs orders magnitude faster than particle bp pbp <eos> however quality pmbp solution tightly coupled local window size over raw data cost aggregated mitigate ambiguity data constraint <eos> dependency heavily influences overall complexity increasing linearly window size <eos> paper proposes novel algorithm called sped up pmbp spm bp tackle critical computational bottleneck speeds up pmbp times <eos> crux spm bp unifying efficient filter based cost aggregation message passing patchmatch based particle generation highly effective way <eos> though simple its formulation spm bp achieves superior performance sub pixel accurate stereo optical flow benchmark datasets when compared more complex task specific approaches <eos> <eop> flow fields dense correspondence fields highly accurate large displacement optical flow estimation <eos> modern large displacement optical flow algorithms usually use initialization either sparse descriptor matching techniques dense approximate nearest neighbor fields <eos> while latter advantage being dense they major disadvantage being very outlier prone they designed find optical flow but visually most similar correspondence <eos> paper present dense correspondence field approach much less outlier prone thus much better suited optical flow estimation than approximate nearest neighbor fields <eos> approach conceptually novel require explicit regularization smoothing like median filtering new data term but solely novel purely data based search strategy finds most inliers even small object while effectively avoids finding outliers <eos> moreover present novel enhancements outlier filtering <eos> show approach better suited large displacement optical flow estimation than state art descriptor matching techniques <eos> so initializing epicflow so far best method mpi sintel flow fields instead their originally used state art descriptor matching technique <eos> significantly outperform original epicflow mpi sintel kitti middlebury <eos> <eop> dense semantic correspondence every pixel classifier <eos> determining dense semantic correspondences across object scenes difficult problem underpins many higher level computer vision algorithms <eos> unlike canonical dense correspondence problems consider image spatially temporally adjacent semantic correspondence characterized image share similar high level structures whose exact appearance geometry may differ <eos> motivated object recognition literature recent work rapidly estimating linear classifiers treat semantic correspondence constrained detection problem exemplar lda classifier learned each pixel <eos> lda classifiers two distinct benefits they exhibit higher average precision than similarity metrics typically used correspondence problems ii unlike exemplar svm output globally interpretable posterior probabilities without calibration whilst also being significantly faster train <eos> pose correspondence problem graphical model unary potentials computed via convolution set exemplar classifiers joint potentials enforce smoothly varying correspondence assignment <eos> <eop> multi image matching via fast alternating minimization <eos> paper propose global optimization based approach jointly matching set image <eos> estimated correspondences simultaneously maximize pairwise feature affinities cycle consistency across multiple image <eos> unlike previous convex method relying semidefinite programming formulate problem low rank matrix recovery problem show desired semidefiniteness solution spontaneously fulfilled <eos> low rank formulation enables derive fast alternating minimization algorithm order handle practical problems thousands feature <eos> both simulation real experiments demonstrate proposed algorithm achieve competitive performance order magnitude speedup compared state art algorithm <eos> end demonstrate applicability proposed method match image different object instances result potential reconstruct category specific object models image <eos> <eop> differential recurrent neural network action recognition <eos> long short term memory lstm neural network capable processing complex sequential information since utilizes special gating schemes learning representations long input sequences <eos> potential model any time series sequential data current hidden state considered context past hidden states <eos> property makes lstm ideal choice learn complex dynamics various actions <eos> unfortunately conventional lstms consider impact spatio temporal dynamics corresponding given salient motion patterns when they gate information ought memorized through time <eos> address problem propose differential gating scheme lstm neural network emphasizes change information gain caused salient motions between successive frames <eos> change information gain quantified derivative states dos thus proposed lstm model termed differential recurrent neural network drnn <eos> demonstrate effectiveness proposed model automatically recognizing actions real world three dimensional human action datasets <eos> study one first works towards demonstrating potential learning complex time series representations via high order derivatives states <eos> <eop> similarity gaussian process latent variable model multi modal data analysis <eos> data real applications involve multiple modalities representing content same semantics deliver rich information complementary aspects <eos> however relations among heterogeneous modalities simply treated observation fit existing work parameterized cross modal mapping functions lack flexibility directly adapting content divergence semantic complicacy multi modal data <eos> paper build work based gaussian process latent variable model gplvm learn non linear non parametric mapping functions transform heterogeneous data into shared latent space <eos> propose multi modal similarity gaussian process latent variable model simgp learns nonlinear mapping functions between intra modal similarities latent representation <eos> further propose multi modal regularized similarity gplvm rsimgp encouraging similar dissimilar point similar dissimilar output space <eos> overall objective functions solved simple scalable gradient decent techniques <eos> proposed models robust content divergence high dimensionality multi modal representation <eos> they applied various tasks discover non linear correlations obtain comparable low dimensional representation heterogeneous modalities <eos> two widely used real world datasets outperform previous approaches cross modal content retrieval cross modal classification <eos> <eop> learning ensembles potential functions structured prediction latent variables <eos> many visual recognition tasks involve modeling variables structurally related <eos> hidden conditional random fields hcrfs powerful class models encoding structure weakly supervised training examples <eos> paper presents hcrf boost novel general framework learning hcrfs functional space <eos> algorithm proposed learn potential functions hcrf combination abstract nonlinear feature functions expressed regression models <eos> consequently resulting latent structured model restricted traditional log linear potential functions any explicit parameterization <eos> further functional optimization helps avoid direct interactions possibly large parameter space nonlinear models improves efficiency <eos> result complex flexible ensemble method achieved structured prediction successfully used variety applications <eos> validate effectiveness method tasks such group activity recognition human action recognition multi instance learning video events <eos> <eop> simultaneous deep transfer across domains tasks <eos> recent reports suggest generic supervised deep cnn model trained large scale dataset reduces but remove dataset bias <eos> fine tuning deep models new domain require significant amount labeled data many applications simply available <eos> propose new cnn architecture exploit unlabeled sparsely labeled target domain data <eos> approach simultaneously optimizes domain invariance facilitate domain transfer uses soft label distribution matching loss transfer information between tasks <eos> proposed adaptation method offers empirical performance exceeds previously published result two standard benchmark visual domain adaptation tasks evaluated across supervised semi supervised adaptation settings <eos> <eop> low dimensional explicit feature maps <eos> approximating non linear kernels finite dimensional feature maps popular approach speeding up training evaluation support vector machines encode information into efficient match kernels <eos> propose novel method data independent construction low dimensional feature maps <eos> problem cast linear program jointly considers competing objectives quality approximation dimensionality feature map <eos> both shift invariant homogeneous kernels proposed method achieves better approximations same dimensionality comparable approximations lower dimensionality feature map compared state art method <eos> <eop> unsupervised learning spatiotemporally coherent metrics <eos> current state art classification detection algorithms train deep convolutional network using labeled data <eos> work study unsupervised feature learning convolutional network context temporally coherent unlabeled data <eos> focus feature learning unlabeled video data using assumption adjacent video frames contain semantically similar information <eos> assumption exploited train convolutional pooling auto encoder regularized slowness sparsity <eos> establish connection between slow feature learning metric learning <eos> using connection define temporal coherence criterion used select hyper parameters automatically <eos> transfer learning experiment show resulting encoder used define more semantically coherent metric without use labeled data <eos> <eop> multi label cross modal retrieval <eos> work address problem cross modal retrieval presence multi label annotations <eos> particular introduce multi label canonical correlation analysis ml cca extension cca learning shared subspaces taking into account high level semantic information form multi label annotations <eos> unlike cca ml cca rely explicit pairing between modalities instead uses multi label information establish correspondences <eos> result discriminative subspace better suited cross modal retrieval tasks <eos> also present fast ml cca computationally efficient version ml cca able handle large scale datasets <eos> show efficacy approach conducting extensive cross modal retrieval experiments three standard benchmark datasets <eos> result show proposed approach achieves state art retrieval performance three datasets <eos> <eop> improving ferns ensembles sparsifying quantising posterior probabilities <eos> ferns ensembles offer accurate efficient multiclass non linear classification commonly expense consuming large amount memory <eos> introduce two fold contribution produces large reductions their memory consumption <eos> first efficient regularised cost optimisation finds sparse representation posterior probabilities ensemble discarding elements zero contribution valid responses training sample <eos> product produce prediction accuracy gain if required traded further reductions memory size prediction time <eos> secondly posterior probabilities quantised stored memory friendly sparse data structure <eos> reported minimum memory reduction different types classification problems using generative discriminative ferns ensembles without increasing prediction time classification error <eos> image patch recognition proposal produced memory reduction improved several percentage point prediction accuracy <eos> <eop> beyond gauss image set matching riemannian manifold pdfs <eos> state art image set matching techniques typically implicitly model each image set gaussian distribution <eos> here propose go beyond representations model image set probability distribution functions pdfs using kernel density estimators <eos> compare match image set exploit csiszar divergences bear strong connections geodesic distance defined space pdfs <eos> furthermore introduce valid positive definite kernels statistical manifolds let make use more powerful classification schemes match image set <eos> finally introduce supervised dimensionality reduction technique learns latent space divergences reflect class labels data <eos> experiments diverse problems such video based face recognition dynamic texture classification evidence benefits approach over state art image set matching method <eos> <eop> unsupervised domain adaptation imbalanced cross domain data <eos> address challenging unsupervised domain adaptation problem imbalanced cross domain data <eos> standard unsupervised domain adaptation one typically obtains labeled data source domain only observes unlabeled data target domain <eos> however most existing works consider scenarios either label numbers across domains different data source target domains might collected multiple datasets <eos> address aforementioned settings imbalanced cross domain data propose closest common space learning ccsl associating such data capability preserving label structural information within across domains <eos> experiments multiple cross domain visual classification tasks confirm method performs favorably against state art approaches especially when imbalanced cross domain data presented <eos> <eop> secrets matrix factorization approximations numerics manifold optimization random restarts <eos> matrix factorization low rank matrix completion missing data key computation many computer vision machine learning tasks also related broader class nonlinear optimization problems such bundle adjustment <eos> problem received much attention recently renewed interest variable projection approaches yielding dramatic improvements reliability speed <eos> however wide class problems no one approach dominates because various approaches derived multitude different ways difficult unify them <eos> paper provides unified derivation number recent approaches so similarities differences easily observed <eos> also present simple meta algorithm wraps any existing algorithm yielding success rate many standard datasets <eos> given success focus evaluation must turn speed success trivially achieved if care about speed <eos> again unification allows number generic improvements applicable all members family isolated yielding unified algorithm outperforms re implementation existing algorithms some cases already outperform original authors publicly available codes <eos> <eop> geometry aware deep transform <eos> many recent efforts devoted designing sophisticated deep learning structures obtaining revolutionary result benchmark datasets <eos> success deep learning method mostly relies enormous volume labeled training sample learn huge number parameters network therefore understanding generalization ability learned deep network cannot overlooked especially when restricted small training set case many applications <eos> paper propose novel deep learning objective formulation unifies both classification metric learning criteria <eos> then introduce geometry aware deep transform enable non linear discriminative robust feature transform shows competitive performance small training set both synthetic real world data <eos> further support proposed framework formal epsilon robustness analysis <eos> <eop> learning binary codes maximum inner product search <eos> binary coding hashing techniques recognized accomplish efficient near neighbor search thus attracted broad interests recent vision learning studies <eos> however such studies rarely dedicated maximum inner product search mips plays critical role various vision applications <eos> paper investigate learning binary codes exclusively handle mips problem <eos> inspired latest advance asymmetric hashing schemes propose asymmetric binary code learning framework based inner product fitting <eos> specifically two set coding functions learned such inner products between their generated binary codes reveal inner products between original data vectors <eos> also propose alternative simpler objective maximizes correlations between inner products produced binary codes raw data vectors <eos> both objectives binary codes coding functions simultaneously learned without continuous relaxations key achieving high quality binary codes <eos> evaluate proposed method dubbed asymmetric inner product binary coding aibc relying two objectives several large scale image datasets <eos> both them superior state art binary coding hashing method performing mips tasks <eos> <eop> ml mg multi label learning missing labels using mixed graph <eos> work focuses problem multi label learning missing labels mlml aims label each test instance multiple class labels given training instances incomplete partial set labels <eos> some their labels missing <eos> handle missing labels propose unified model label dependencies constructing mixed graph jointly incorporates instance level similarity class co occurrence undirected edges ii semantic label hierarchy directed edges <eos> unlike most mlml method formulate learning problem transductively convex quadratic matrix optimization problem encourages training label consistency encodes both types label dependencies <eos> undirected directed edges using quadratic terms hard linear constraints <eos> alternating direction method multipliers admm used exactly efficiently solve problem <eos> evaluate proposed method consider two popular applications image video annotation label hierarchy derived wordnet <eos> experimental result show method achieves significant improvement over state art method performance robustness missing labels <eos> <eop> zero shot learning via semantic similarity embedding <eos> paper consider version zero shot learning problem seen class source target domain data provided <eos> goal during test time accurately predict class label unseen target domain instance based revealed source domain side information <eos> attributes unseen classes <eos> method based viewing each source target data mixture seen class proportions postulate mixture patterns similar if two instances belong same unseen class <eos> perspective leads learning source target embedding functions map arbitrary source target domain data into same semantic space similarity readily measured <eos> develop max margin framework learn similarity functions jointly optimize parameters means cross validation <eos> test result compelling leading significant improvement terms accuracy most benchmark datasets zero shot recognition <eos> <eop> bayesian model adaptation crowd counts <eos> problem transfer learning considered domain crowd counting <eos> solution based bayesian model adaptation gaussian processes proposed <eos> shown produce intuitive model updates tractable lead adapted model predictive distribution accounts all information both training adaptation data <eos> new adaptation procedure achieves significant gains over previous approaches based multi task learning while requiring much less computation deploy <eos> makes particularly suited problem expanding capacity crowd counting camera network <eos> large video dataset evaluation adaptation approaches crowd counting also introduced <eos> contains number adaptation tasks involving information transfer across video collected single camera under different scene conditions different times day video collected different cameras <eos> evaluation proposed model adaptation procedure dataset shows good performance realistic operating conditions <eos> <eop> nmf perspective binary hashing <eos> pervasiveness massive data repositories led much interest efficient method indexing search retrieval <eos> image data rapidly developing body work applications shows impressive performance method broadly fall under umbrella term binary hashing <eos> given distance matrix binary hashing algorithm solves binary code given set examples whose hamming distance nicely approximates original distances <eos> formulation non convex so existing solutions adopt spectral relaxations perform coordinate descent quantization surrogate objective numerically more tractable <eos> paper first derive augmented lagrangian approach optimize standard binary hashing objective <eos> maintain fidelity given distance matrix <eos> appropriate step sizes find scheme already yields result match substantially outperform state art method most benchmarks used literature <eos> then allow model scale large datasets obtain interesting reformulation binary hashing objective non negative matrix factorization <eos> later leads simple multiplicative updates algorithm whose parallelization properties exploited obtain fast gpu based implementation <eos> give probabilistic analysis initialization scheme present range experiments show method simple implement competes favorably available method both optimization generalization <eos> <eop> multi view domain generalization visual recognition <eos> paper propose new multi view domain generalization mvdg approach visual recognition aim use source domain sample multiple types feature <eos> multi view feature learn robust classifiers generalize well any unseen target domain <eos> considering recent works show domain generalization capability enhanced fusing multiple svm classifiers build upon exemplar svms learn set svm classifiers using one positive sample all negative sample source domain each time <eos> when source domain sample come multiple latent domains expect weight vectors exemplar svm classifiers organized into multiple hidden clusters <eos> exploit such cluster structure organize weight vectors learnt each view weight matrix seek low rank representation reconstructing weight matrix using itself dictionary <eos> enforce consistency inherent cluster structures discovered weight matrices learnt different views introduce new regularizer minimize mismatch between any two representation matrices different views <eos> also develop efficient alternating optimization algorithm further extend mvdg approach domain adaptation exploiting manifold structure unlabeled target domain sample <eos> comprehensive experiments visual recognition clearly demonstrate effectiveness approaches domain generalization domain adaptation <eos> <eop> infinite feature selection <eos> filter based feature selection become crucial many classification settings especially object recognition recently faced feature learning strategies originate thousands cues <eos> paper propose feature selection method exploiting convergence properties power series matrices introducing concept infinite feature selection inf fs <eos> considering selection feature path among feature distributions letting paths tend infinite number permits investigation importance relevance redundancy feature when injected into arbitrary set cues <eos> ranking importance individuates candidate feature turn out effective classification point view proved thoroughly experimental section <eos> inf fs tested thirteen diverse benchmarks comparing against filters embedded method wrappers all cases achieve top performances notably classification tasks pascal voc <eos> <eop> semi supervised zero shot classification label representation learning <eos> given challenge gathering labeled training data zero shot classification transfers information observed classes recognize unseen classes become increasingly popular computer vision community <eos> most existing zero shot learning method require user first provide set semantic visual attributes each class side information before applying two step prediction procedure introduces intermediate attribute prediction problem <eos> paper propose novel zero shot classification approach automatically learns label embeddings input data semi supervised large margin learning framework <eos> proposed framework jointly considers multi class classification over all classes observed unseen tackles target prediction problem directly without introducing intermediate prediction problems <eos> also capacity incorporate semantic label information different sources when available <eos> evaluate proposed approach conduct experiments standard zero shot data set <eos> empirical result show proposed approach outperforms existing state art zero shot learning method <eos> <eop> supervised low rank method learning invariant subspaces <eos> sparse representation low rank matrix decomposition approaches successfully applied several computer vision problems <eos> they build generative representation data often requires complex training well testing robust against data variations induced nuisance factors <eos> introduce invariant components discriminative representation invariant nuisance factors because spans subspaces orthogonal space nuisance factors defined <eos> allows developing framework based geometry ensures uniform inter class separation very efficient robust classification based simple nearest neighbor <eos> addition show how approach equivalent local metric learning local metrics one each class learned jointly rather than independently thus avoiding risk overfitting without need additional regularization <eos> evaluated approach face recognition highly corrupted training testing data obtaining very promising result <eos> <eop> recursive frechet mean computation grassmannian its applications computer vision <eos> past decade grassmann manifolds grassmannian commonly used mathematical formulations many computer vision tasks <eos> averaging point grassmann manifold very common operation many applications including but limited tracking action recognition video face recognition face recognition etc <eos> computing intrinsic frechet mean fm set point grassmann cast finding global optimum if exists sum squared geodesic distances cost function <eos> common approach solve problem involves use gradient descent method <eos> alternative way compute fm develop recursive inductive definition involve optimizing aforementioned cost function <eos> paper propose one such computationally efficient algorithm called grassmann inductive frechet mean estimator gifme <eos> developing recursive solution find fm given set point gifme exploits fact there closed form solution find fm two point grassmann <eos> limit number sample tends infinity prove gifme converges fm called weak consistency result grassmann manifold <eos> further finite sample case limit number sample paths trials goes infinity show gifme converges finite sample fm <eos> moreover present bound geodesic distance between estimate gifme true fm <eos> present several experiments synthetic real data set demonstrate performance gifme comparison gradient descent based batch mode technique <eos> goal applications demonstrate computational advantage achieve comparable accuracy state art <eos> <eop> multi view subspace clustering <eos> many computer vision applications data set distribute certain low dimensional subspaces <eos> subspace clustering find such underlying subspaces cluster data point correctly <eos> paper propose novel multi view subspace clustering method <eos> proposed method performs clustering subspace representation each view simultaneously <eos> meanwhile propose use common cluster structure guarantee consistence among different views <eos> addition efficient algorithm proposed solve problem <eos> experiments four benchmark data set performed validate proposed method <eos> promising result demonstrate effectiveness method <eos> <eop> predicting deep zero shot convolutional neural network using textual descriptions <eos> one main challenges zero shot learning visual categories gathering semantic attributes accompany image <eos> recent work shown learning textual descriptions such wikipedia articles avoids problem having explicitly define attributes <eos> present new model classify unseen categories their textual description <eos> specifically use text feature predict output weights both convolutional fully connected layer deep convolutional neural network cnn <eos> take advantage architecture cnn learn feature different layer rather than just learning embedding space both modalities common existing approaches <eos> proposed model also allows automatically generate list pseudo attributes each visual category consisting words wikipedia articles <eos> train models end end using caltech ucsd bird flower datasets evaluate both roc precision recall curves <eos> empirical result show proposed model significantly outperforms previous method <eos> <eop> structured feature selection <eos> feature dimensionality reduction widely used various computer vision tasks <eos> explore feature selection dimensionality reduction technique propose use structured approach based markov blanket mb select feature <eos> first introduce new mb discovery algorithm simultaneous markov blanket stmb discovery improves efficiency state art algorithms <eos> then theoretically justify three advantages structured feature selection over traditional feature selection method <eos> specifically show markov blanket minimum feature set retains maximal mutual information also gives lowest bayes classification error <eos> then apply structured feature selection two applications introduce new method enables stmb scale up show competitive performance algorithms large scale image classification tasks <eos> propose method structured feature selection handle hierarchical feature show proposed method lead big performance gain facial expression action unit au recognition tasks <eos> <eop> conditional high order boltzmann machine supervised learning model relation learning <eos> relation learning fundamental operation many computer vision tasks <eos> recently high order boltzmann machine its variants exhibited great power modelling various data relation <eos> however most them unsupervised learning models very discriminative thus cannot server standalone solution relation learning tasks <eos> paper explore supervised learning algorithms propose new model named conditional high order boltzmann machine chbm directly used bilinear classifier assign similarity scores pairwise image <eos> then better deal complex data relation propose gated version chbm untangles factors variation exploiting set latent variables gate classification <eos> perform four order tensor factorization parameter reduction present two efficient supervised learning algorithms perspectives being generative discriminative respectively <eos> experimental result image transformation visualization binary way classification face verification demonstrate performing supervised learning models greatly improve performance <eos> <eop> learning image user feature recommendation social network <eos> good representations data help many machine learning tasks such recommendation <eos> often great challenge traditional recommender systems learn representative feature both users image large social network particular social curation network characterized extremely sparse links between users image extremely diverse visual contents image <eos> address challenges propose novel deep model learns unified feature representations both users image <eos> done transforming heterogeneous user image network into homogeneous low dimensional representations facilitate recommender trivially recommend image users feature similarity <eos> also develop fast online algorithm easily scaled up large network asynchronously parallel way <eos> conduct extensive experiments representative subset pinterest containing image users <eos> result image recommendation experiments demonstrate feature learning approach significantly outperforms other state art recommendation method <eos> <eop> dual feature warping based motion model estimation <eos> break down geometry assumptions traditional motion models <eos> homography affine warping based motion model recently becomes very popular adopted many latest applications <eos> image stitching video stabilization <eos> high degrees freedom accuracy model heavily relies data terms keypoint correspondences <eos> some low texture environments <eos> indoor keypoint feature insufficient unreliable warping model often erroneously estimated <eos> paper propose simple effective approach considering both keypoint line segment correspondences data term <eos> line segment prominent feature artificial environments supply sufficient geometrical structural information scenes only helps guild correct warp low texture condition but also prevents undesired distortion induced warping <eos> combination aims complement each other benefit wider range scenes <eos> method general ported many existing applications <eos> experiments demonstrate using dual feature yields more robust accurate result especially low texture image <eos> <eop> adaptive data representation robust point set registration merging <eos> paper presents framework rigid point set registration merging using robust continuous data representation <eos> point set representation constructed training one class support vector machine gaussian radial basis function kernel subsequently approximating output function gaussian mixture model <eos> leverage representation sparse parametrisation robustness noise outliers occlusions efficient registration algorithm minimises distance between support vector parametrised gaussian mixtures <eos> contrast existing techniques such iterative closest point gaussian mixture approaches manifest narrower region convergence less robust occlusions missing data demonstrated evaluation range three dimensional datasets <eos> finally present novel algorithm gmmerge parsimoniously equitably merges aligned mixture models allowing framework used reconstruction mapping <eos> <eop> local subspace collaborative tracking <eos> subspace models widely used appearance based object tracking <eos> most existing subspace based trackers employ linear subspace represent object appearances accurate enough model large variations object <eos> address paper presents local subspace collaborative tracking method robust visual tracking multiple linear nonlinear subspaces learned better model nonlinear relationship object appearances <eos> first retain set key sample compute set local subspaces each key sample <eos> then construct hyper sphere represent local nonlinear subspace each key sample <eos> hyper sphere one key sample passes local key sample also tangent local linear subspace specific key sample <eos> way able represent nonlinear distribution key sample also approximate local linear subspace near specific key sample so local distributions sample represented more accurately <eos> experimental result challenging video sequences demonstrate effectiveness method <eos> <eop> learning spatially regularized correlation filters visual tracking <eos> robust accurate visual tracking one most challenging computer vision problems <eos> due inherent lack training data robust approach constructing target appearance model crucial <eos> recently discriminatively learned correlation filters dcf successfully applied address problem tracking <eos> method utilize periodic assumption training sample efficiently learn classifier all patches target neighborhood <eos> however periodic assumption also introduces unwanted boundary effects severely degrade quality tracking model <eos> propose spatially regularized discriminative correlation filters srdcf tracking <eos> spatial regularization component introduced learning penalize correlation filter coefficients depending their spatial location <eos> srdcf formulation allows correlation filters learned significantly larger set negative training sample without corrupting positive sample <eos> further propose optimization strategy based iterative gauss seidel method efficient online learning srdcf <eos> experiments performed four benchmark datasets otb alov otb vot <eos> approach achieves state art result all four datasets <eos> otb otb obtain absolute gain <eos> respectively mean overlap precision compared best existing trackers <eos> <eop> spedo dof ego motion sensor using speckle defocus imaging <eos> sensors measure their motion respect surrounding environment ego motion sensors broadly classified into two categories <eos> first inertial sensors such accelerometers <eos> order estimate position velocity sensors integrate measured acceleration often result accumulation large errors over time <eos> second camera based approaches such slam measure position directly but their performance depends surrounding scene properties <eos> approaches cannot function reliably if scene low frequency textures small depth variations <eos> present novel ego motion sensor called spedo addresses fundamental limitations <eos> spedo based using coherent light sources cameras large defocus <eos> coherent light interacting scene creates high frequency interferometric pattern captured image called speckle <eos> develop theoretical model speckle flow motion speckle function sensor motion show quasi invariant surrounding scene properties <eos> result spedo measure ego motion derivative motion simply estimating optical flow few image locations <eos> built low cost compact hardware prototype spedo demonstrated high precision dof ego motion estimation complex trajectories scenarios scene properties challenging <eos> repeating no texture well unknown <eos> <eop> unsupervised trajectory clustering via adaptive multi kernel based shrinkage <eos> paper proposes shrinkage based framework unsupervised trajectory clustering <eos> facing challenges trajectory clustering <eos> large variations within cluster ambiguities across clusters first introduce adaptive multi kernel based estimation process estimate shrunk positions speeds trajectories point <eos> kernel based estimation effectively leverages both multiple structural information within trajectory local motion patterns across multiple trajectories such discrimination shrunk point properly increased <eos> further introduce speed regularized optimization process utilizes estimated speeds regularize optimal shrunk point so guarantee smoothness discriminative pattern final shrunk trajectory <eos> using approach variations among similar trajectories reduced while boundaries between different clusters enlarged <eos> experimental result demonstrate approach superior state art approaches both clustering accuracy robustness <eos> besides additional experiments further reveal effectiveness approach when applied trajectory analysis applications such anomaly detection <eos> <eop> tric track tracking regression incrementally learned cascades <eos> paper proposes novel approach part based tracking replacing local matching appearance model direct prediction displacement between local image patches part locations <eos> propose use cascaded regression incremental learning track generic object without any prior knowledge object structure appearance <eos> exploit spatial constraints between parts implicitly learning shape deformation parameters object online fashion <eos> integrate multiple temporal scale motion model initialise cascaded regression search close target allow cope occlusions <eos> experimental result show tracker ranks first cvpr benchmark <eos> <eop> recurrent network models human dynamics <eos> propose encoder recurrent decoder erd model recognition prediction human body pose video motion capture <eos> erd model recurrent neural network incorporates nonlinear encoder decoder network before after recurrent layer <eos> test instantiations erd architectures tasks motion capture mocap generation body pose labeling body pose forecasting video <eos> model handles mocap training data across multiple subjects activity domains synthesizes novel motions while avoiding drifting long periods time <eos> human pose labeling erd outperforms per frame body part detector resolving left right body part confusions <eos> video pose forecasting erd predicts body joint displacements across temporal horizon ms outperforms first order motion model based optical flow <eos> erds extend previous long short term memory lstm models literature jointly learn representations their dynamics <eos> experiments show such representation learning crucial both labeling prediction space time <eos> find distinguishing feature between spatio temporal visual domain comparison text speech handwriting straightforward hard coded representations shown excellent result when directly combined recurrent units <eos> <eop> contour flow middle level motion estimation combining motion segmentation contour alignment <eos> goal estimate contour flow contour pairs consistent point correspondence inconsistent contours extracted independently two video frames <eos> formulate contour flow estimation locally motion segmentation problem motion patterns grouped optical flow field exploited local correspondence measurement <eos> solve local ambiguities contour flow estimation further formulated globally contour alignment problem <eos> propose novel two staged strategy obtain global consistent point correspondence under various contour transitions such splitting merging branching <eos> goal first stage obtain possible accurate contour contour alignments second stage aims make consistent fusion many partial alignments <eos> such strategy properly balance accuracy consistency enables middle level motion representation constructed just concatenating frame frame contour flow estimation <eos> experiments prove effectiveness method <eos> <eop> followme efficient online min cost flow tracking bounded memory computation <eos> one most popular approaches multi target tracking tracking detection <eos> current min cost flow algorithms solve data association problem optimally three main drawbacks they computationally expensive they assume whole video given batch they scale badly memory computation length video sequence <eos> paper address each issues resulting computationally memory bounded solution <eos> first introduce dynamic version successive shortest path algorithm solves data association problem optimally while reusing computation resulting faster inference than standard solvers <eos> second address optimal solution data association problem when dealing incoming stream data <eos> finally present main contribution approximate online solution bounded memory computation capable handling video arbitrary length while performing tracking real time <eos> demonstrate effectiveness algorithms kitti pets benchmarks show state art performance while being significantly faster than existing solvers <eos> <eop> learning divide conquer online multi target tracking <eos> online multiple target tracking mtt often addressed within tracking detection paradigm <eos> detections previously extracted independently each frame then object trajectories built maximizing specifically designed coherence functions <eos> nevertheless ambiguities arise presence occlusions detection errors <eos> paper claim ambiguities tracking could solved selective use feature working more reliable feature if possible exploiting deeper representation target only if necessary <eos> end propose online divide conquer tracker static camera scenes partitions assignment problem local subproblems solves them selectively choosing combining best feature <eos> complete framework cast structural learning task unifies phases learns tracker parameters examples <eos> experiments two different datasets highlights significant improvement tracking performances mota over state art <eos> <eop> minimizing human effort interactive tracking incremental learning model parameters <eos> address problem minimizing human effort interactive tracking learning sequence specific model parameters <eos> determining optimal model parameters each sequence critical problem tracking <eos> demonstrate using optimal model parameters each sequence achieve high precision tracking result significantly less effort <eos> leverage sequential nature interactive tracking formulate efficient method learning model parameters through maximum margin framework <eos> using method able save human effort achieve high precision two datasets virat dataset infant mother interaction dataset <eos> <eop> novel representation parts accurate three dimensional object detection tracking monocular image <eos> present method estimates real time under challenging conditions three dimensional pose known object <eos> method relies only grayscale image since depth cameras fail metallic object handle poorly textured object cluttered changing environments pose predicts degrades gracefully presence large occlusions <eos> result contrast state art method suitable practical augmented reality applications even industrial environments <eos> robust occlusions first learn detect some parts target object <eos> key idea then predict three dimensional pose each part form projections few control point <eos> advantages representation three fold predict three dimensional pose object even when only one part visible when several parts visible combine them easily compute better pose object three dimensional pose obtain usually very accurate even when only few parts visible <eos> <eop> linearization nonlinear learning visual tracking <eos> due unavoidable appearance variations caused occlusion deformation other factors classifiers visual tracking nonlinear necessity <eos> building theory globally linear approximations nonlinear functions introduce elegant method jointly learns nonlinear classifier visual dictionary tracking object semi supervised sparse coding fashion <eos> establishes obvious distinction conventional sparse coding based discriminative tracking algorithms usually maintain two stage learning strategies <eos> learning dictionary unsupervised way then followed training classifier <eos> however treating dictionary learning classifier training separate stages may produce both descriptive discriminative models object <eos> contrast method capable constructing dictionary only fully reflects intrinsic manifold structure data but also possesses discriminative power <eos> paper presents optimization method obtain such optimal dictionary associated sparse coding classifier iterative process <eos> experiments benchmark show tracker attains outstanding performance compared state art algorithms <eos> <eop> self occlusions disocclusions causal video object segmentation <eos> propose method detect disocclusion video sequences three dimensional scenes partition disoccluded region into object defined coherent deformation corresponding surfaces scene <eos> method infers deformation fields piecewise smooth construction without need explicit regularizer associated choice weight <eos> then partitions disoccluded region groups its components object leveraging complementarity motion appearance cues appearance changes within object motion usually reliably inferred used grouping <eos> appearance close constant used grouping directly <eos> integrate both cues energy minimization framework incorporate prior assumptions explicitly into energy propose numerical scheme <eos> <eop> large displacement three dimensional scene flow occlusion reasoning <eos> three dimensional motion estimation fundamental problem many computer vision applications <eos> emergence modern affordable increasingly accurate rgb sensors single view approaches estimating three dimensional motion also known scene flow becoming popular <eos> paper propose novel coarse fine correspondence based scene flow approach account effects large displacements model occlusion based explicit geometric reasoning <eos> methodology enforces piecewise motion rigidity level depth point cloud without explicitly smoothing parameters adjacent neighborhoods <eos> integrating all geometric photometric components single consistent occlusion aware energy model method able deal fast motions large occlusions areas present challenging datasets like mpi sintel flow dataset recently augmented depth information <eos> explicitly modeling large displacements occlusion now more successfully work difficult sequences cannot currently processed state art scene flow method rely small inter frame motion assumptions <eos> also show leveraging depth information obtain superior correspondence fields compared best state art large displacement optical flow method <eos> <eop> co interest person detection multiple wearable camera video <eos> wearable cameras such google glass go pro enable video data collection over larger areas different views <eos> paper tackle new problem locating co interest person cip <eos> one who draws attention most camera wearers temporally synchronized video taken multiple wearable cameras <eos> basic idea exploit motion patterns people use them correlate persons across different video instead performing appearance based matching traditional video co segmentation localization <eos> way identify cip even if group people similar appearance present view <eos> more specifically detect set persons each frame candidates cip then build conditional random field crf model select one consistent motion patterns different video high spacial temporal consistency each video <eos> collect three set wearable camera video testing proposed algorithm <eos> all involved people similar appearances collected video experiments demonstrate effectiveness proposed algorithm <eos> <eop> sparse dynamic three dimensional reconstruction unsynchronized video <eos> target sparse three dimensional reconstruction dynamic object observed multiple unsynchronized video cameras unknown temporal overlap <eos> end develop framework recover unknown structure without sequencing information across video sequences <eos> proposed compressed sensing framework poses estimation three dimensional structure problem dictionary learning <eos> moreover define dictionary temporally varying three dimensional structure while define local sequencing information terms sparse coefficients describing locally linear three dimensional structural interpolation <eos> formulation optimizes biconvex cost function leverages compressed sensing formulation enforces both structural dependency coherence across video streams well motion smoothness across estimates common video sources <eos> experimental result demonstrate effectiveness approach both synthetic data captured imagery <eos> <eop> category blind human action recognition practical recognition system <eos> existing human action recognition systems three dimensional sequences obtained depth camera designed cope only one action category either single person action two person interaction difficult extended scenarios both action categories co exist <eos> paper propose category blind human recognition method charm recognize human action without making assumptions action category <eos> charm approach represent human action either single person action two person interaction class using co occurrence motion primitives <eos> subsequently classify action instance based matching its motion primitive co occurrence patterns each class representation <eos> matching task formulated maximum clique problems <eos> conduct extensive evaluations charm using three datasets single person actions two person interactions their mixtures <eos> experimental result show charm performs favorably when compared several state art single person action two person interaction based method without making explicit assumptions action category <eos> <eop> temporal subspace clustering human motion segmentation <eos> subspace clustering effective technique segmenting data drawn multiple subspaces <eos> however time series data <eos> human motion exploiting temporal information still challenging problem <eos> propose novel temporal subspace clustering tsc approach paper <eos> improve subspace clustering technique two aspects <eos> first temporal laplacian regularization designed encodes sequential relationships time series data <eos> second obtain expressive codings learn non negative dictionary data <eos> efficient optimization algorithm presented jointly learn representation codings dictionary <eos> after constructing affinity graph using codings multiple temporal segments grouped via spectral clustering <eos> experimental result three action gesture datasets demonstrate effectiveness approach <eos> particular tsc significantly improves clustering accuracy compared state art subspace clustering method <eos> <eop> weakly supervised alignment video text <eos> suppose given set video along natural language descriptions form multiple sentences <eos> manual annotations movie scripts sport summaries etc <eos> sentences appear same temporal order their visual counterparts <eos> propose paper method aligning two modalities <eos> automatically providing time frame stamp every sentence <eos> given vectorial feature both video text cast temporal assignment problem implicit linear mapping between two feature modalities <eos> formulate problem integer quadratic program solve its continuous convex relaxation using efficient conditional gradient algorithm <eos> several rounding procedures proposed construct final integer solution <eos> after demonstrating significant improvements over state art related task aligning video symbolic labels evaluate method challenging dataset video associated textual descriptions explore bag words continuous representations text <eos> <eop> learning temporal embeddings complex video analysis <eos> paper propose learn temporal embeddings video frames complex video analysis <eos> large quantities unlabeled video data easily obtained internet <eos> video possess implicit weak label they sequences temporally semantically coherent image <eos> leverage information learn temporal embeddings video frames associating frames temporal context they appear <eos> propose scheme incorporating temporal context based past future frames video compare other contextual representations <eos> addition show how data augmentation using multi resolution sample hard negatives helps significantly improve quality learned embeddings <eos> evaluate various design decisions learning temporal embeddings show embeddings improve performance multiple video tasks such retrieval classification temporal order recovery unconstrained internet video <eos> <eop> unsupervised semantic parsing video collections <eos> human communication typically underlying structure <eos> reflected fact many user generated video starting point ending certain objective steps between two identified <eos> paper propose method parsing video into such semantic steps unsupervised way <eos> proposed method capable providing semantic storyline video composed its objective steps <eos> accomplish utilizing both visual language cues joint generative model <eos> proposed method also provide textual description each identified semantic steps video segments <eos> evaluate method large number complex youtube video show result unprecedented quality new impactful problem <eos> <eop> learning spatiotemporal feature three dimensional convolutional network <eos> propose simple yet effective approach spatiotemporal feature learning using deep dimensional convolutional network three dimensional convnets trained large scale supervised video dataset <eos> findings three fold three dimensional convnets more suitable spatiotemporal feature learning compared convnets homogeneous architecture small convolution kernels all layer among best performing architectures three dimensional convnets learned feature namely convolutional three dimensional simple linear classifier outperform state art method different benchmarks comparable current best method other benchmarks <eos> addition feature compact achieving <eos> accuracy ucf dataset only dimensions also very efficient compute due fast inference convnets <eos> finally they conceptually very simple easy train use <eos> <eop> temporal perception prediction ego centric video <eos> given video activity predict will happen next paper explore two simple tasks related temporal prediction egocentric video everyday activities <eos> provide both human experiments understand how well people perform tasks computational models prediction <eos> experiments indicate humans computers well temporal prediction personalization particular individual environment provides significantly increased performance <eos> developing method temporal prediction could far reaching benefits robots intelligent agents anticipate person will before they <eos> <eop> describing video exploiting temporal structure <eos> recent progress using recurrent neural network rnns image description motivated exploration their application video description <eos> however while image static working video requires modeling their dynamic temporal structure then properly integrating information into natural language description model <eos> context propose approach successfully takes into account both local global temporal structure video produce descriptions <eos> first approach incorporates spatial temporal convolutional neural network cnn representation short temporal dynamics <eos> cnn representation trained video action recognition tasks so produce representation tuned human motion behavior <eos> second propose temporal attention mechanism allows go beyond local temporal modeling learns automatically select most relevant temporal segments given text generating rnn <eos> approach exceeds current state art both bleu meteor metrics youtube text dataset <eos> also present result new larger more challenging dataset paired video natural language descriptions <eos> <eop> person re identification discriminatively trained viewpoint invariant dictionaries <eos> paper introduces new approach address person re identification problem cameras non overlapping fields view <eos> unlike previous approaches learn mahalanobis like distance metrics some transformed feature space propose learn dictionary capable discriminatively sparsely encoding feature representing different people <eos> approach directly addresses two key challenges person re identification viewpoint variations discriminability <eos> first tackle viewpoint associated appearance changes learn single dictionary represent both gallery probe image training phase <eos> then discriminatively train dictionary enforcing explicit constraints associated sparse representations feature vectors <eos> testing phase re identify probe image simply determining gallery image closest sparse representation probe image euclidean sense <eos> extensive performance evaluations three publicly available multi shot re identification datasets demonstrate advantages algorithm over several state art dictionary learning temporal sequence matching spatial appearance metric learning based techniques <eos> <eop> storyline representation egocentric video applications story based search <eos> egocentric video valuable source information daily log lives <eos> however large fraction egocentric video content typically irrelevant boring re watch <eos> agonizing task example manually search moment when your daughter first met mickey mouse hours long egocentric video taken disneyland <eos> although many summarization method successfully proposed create concise representations video practice value subshots users may change according their immediate preference mood thus summaries fixed criteria may fully satisfy users various search intents <eos> address propose storyline representation expresses egocentric video set jointly inferred through mrf inference story elements comprising actors locations supporting object events depicted timeline <eos> construct such storyline very limited annotation data list map locations weak knowledge events may possible each location bootstrapping process data obtained through focused web image video searches <eos> representation promotes story based search queries form graphs span any subset story elements their spatio temporal composition <eos> show effectiveness approach set unconstrained youtube egocentric video visits disneyland <eos> <eop> sequence sequence video text <eos> real world video often complex dynamics method generating open domain video descriptions should senstive temporal structure allow both input sequence frames output sequence words variable length <eos> approach problem propose novel end end sequence sequence model generate captions video <eos> exploit recurrent neural network specifically lstms demonstrated state art performance image caption generation <eos> lstm model trained video sentence pairs learns associate sequence video frames sequence words order generate description event video clip <eos> model naturally able learn temporal structure sequence frames well sequence model generated sentences <eos> evaluate several variants model exploit different visual feature standard set youtube video two movie description datasets vad mpii md <eos> <eop> context aware active learning activity recognition models <eos> activity recognition video recently benefited use context <eos> inter relationships among activities object <eos> however approaches require data labeled entirely available outset <eos> contrast formulate continuous learning framework context aware activity recognition unlabeled video data two distinct advantages over most existing method <eos> first propose novel active learning technique only exploits informativeness individual activity instances but also utilizes their contextual information during query selection process leads significant reduction expensive manual annotation effort <eos> second learned models adapted online more data available <eos> formulate conditional random field crf model encodes context devise information theoretic approach utilizes entropy mutual information nodes compute set most informative query instances need labeled human <eos> labels combined graphical inference techniques incrementally updating model new video come <eos> experiments four challenging datasets demonstrate framework achieves superior performance significantly less amount manual labeling <eos> <eop> action recognition hierarchical mid level action elements <eos> realistic video human actions exhibit rich spatiotemporal structures multiple levels granularity action always decomposed into multiple finer grained elements both space time <eos> capture intuition propose represent video hierarchy mid level action elements maes each mae corresponds action related spatiotemporal segment video <eos> introduce unsupervised method generate representation video <eos> method capable distinguishing action related segments background segments representing actions multiple spatiotemporal resolutions <eos> given set spatiotemporal segments generated training data introduce discriminative clustering algorithm automatically discovers maes multiple levels granularity <eos> develop structured models capture rich set spatial temporal hierarchical relations among segments action label multiple levels mae labels jointly inferred <eos> proposed model achieves state art performance multiple action recognition benchmarks <eos> moreover demonstrate effectiveness model real world applications such action recognition large scale untrimmed video action parsing <eos> <eop> selecting relevant web trained concepts automated event retrieval <eos> complex event retrieval challenging research problem especially when no training video available <eos> alternative collecting training video train large semantic concept bank priori <eos> given text description event event retrieval performed selecting concepts linguistically related event description fusing concept responses unseen video <eos> however defining exhaustive concept lexicon pre training requires vast computational resources <eos> therefore recent approaches automate concept discovery training leveraging large amounts weakly annotated web data <eos> compact visually salient concepts automatically obtained use concept pairs more generally grams <eos> however all visually salient grams necessarily useful event query some combinations concepts may visually compact but irrelevant drastically affects performance <eos> propose event retrieval algorithm constructs pairs automatically discovered concepts then prunes concepts unlikely helpful retrieval <eos> pruning depends both query specific video instance being evaluated <eos> approach also addresses calibration domain adaptation issues arise when applying concept detectors unseen video <eos> demonstrate large improvements over other vision based systems trecvid med dataset <eos> <eop> beyond covariance feature representation nonlinear kernel matrices <eos> covariance matrix recently received increasing attention computer vision leveraging riemannian geometry symmetric positive definite spd matrices <eos> originally proposed region descriptor now used generic representation various recognition tasks <eos> however covariance matrix shortcomings such being prone singular limited capability modeling complicated feature relationship having fixed form representation <eos> paper argues more appropriate spd matrix based representations shall explored achieve better recognition <eos> proposes open framework use kernel matrix over feature dimensions generic representation discusses its properties advantages <eos> proposed framework significantly elevates covariance representation unlimited opportunities provided new representation <eos> experimental study shows representation consistently outperforms its covariance counterpart various visual recognition tasks <eos> particular achieves significant improvement skeleton based human action recognition demonstrating state art performance over both covariance existing non covariance representations <eos> <eop> multiresolution hierarchy co clustering semantic segmentation sequences small variations <eos> paper presents co clustering technique given collection image their hierarchies clusters nodes hierarchies obtain coherent multiresolution representation image collection <eos> formalize co clustering quadratic semi assignment problem solve linear programming relaxation approach makes effective use information hierarchies <eos> initially address problem generating optimal coherent partition per image afterwards extend method multiresolution framework <eos> finally particularize framework iterative multiresolution video segmentation algorithm sequences small variations <eos> evaluate algorithm video occlusion object boundary detection dataset showing produces state art result scenarios <eos> <eop> object action classifying localizing actions without any video example <eos> goal paper recognize actions video without need examples <eos> different traditional zero shot approaches demand design specification attribute classifiers class attribute mappings allow transfer seen classes unseen classes <eos> key contribution object action semantic word embedding spanned skip gram model thousands object categories <eos> action labels assigned object encoding unseen video based convex combination action object affinities <eos> semantic embedding three main characteristics accommodate specifics actions <eos> first propose mechanism exploit multiple word descriptions actions object <eos> second incorporate automated selection most responsive object per action <eos> finally demonstrate how extend zero shot approach spatio temporal localization actions video <eos> experiments four action datasets demonstrate potential approach <eos> <eop> human action recognition using factorized spatio temporal convolutional network <eos> human actions video sequences three dimensional three dimensional spatio temporal signals characterizing both visual appearance motion dynamics involved humans object <eos> inspired success convolutional neural network cnn image classification recent attempts made learn three dimensional cnn recognizing human actions video <eos> however partly due high complexity training three dimensional convolution kernels need large quantities training video only limited success reported <eos> triggered investigate paper new deep architecture handle three dimensional signals more effectively <eos> specifically propose factorized spatio temporal convolutional network fstcn factorize original three dimensional convolution kernel learning sequential process learning spatial kernels lower layer called spatial convolutional layer followed learning temporal kernels upper layer called temporal convolutional layer <eos> introduce novel transformation permutation operator make factorization fstcn possible <eos> moreover address issue sequence alignment propose effective training inference strategy based sampling multiple video clips given action video sequence <eos> tested fstcn two commonly used benchmark datasets ucf hmdb <eos> without using auxiliary training video boost performance fstcn outperforms existing cnn based method achieves comparable performance recent method benefits using auxiliary training video <eos> <eop> bayesian non parametric inference manifold based mocap representation <eos> propose novel approach human action recognition motion capture data mocap based grouping sub body parts <eos> representing configurations actions manifolds joint positions mapped subspace via principal geodesic analysis <eos> reduced space still highly informative allows classification based non parametric bayesian approach generating behaviors each sub body part <eos> having partitioned set joints poses relative sub body part exchangeable given specified prior elicit principle infinite behaviors <eos> generation behaviors specified dirichlet process mixture <eos> show several experiments recognition gives very promising result outperforming method requiring temporal alignment <eos> <eop> semantic video entity linking based visual content metadata <eos> video entity linking connects online video related entities semantic knowledge base enable wide variety video based applications including video retrieval video recommendation <eos> most existing systems video entity linking rely video metadata <eos> paper propose exploit video visual content improve video entity linking <eos> proposed framework video first linked entity candidates using text based method <eos> next entity candidates verified reranked according visual content <eos> order properly handle large variations visual content matching propose use multiple instance metric learning learn set sequence metric specific matching problem <eos> evaluate proposed framework collect annotate video crawled youtube open api <eos> experiment result shown consistent gains proposed framework over several strong baselines <eos> <eop> love thy neighbors image annotation exploiting image metadata <eos> some image difficult recognize their own may become more clear context neighborhood related image similar social network metadata <eos> build intuition improve multilabel image annotation <eos> model uses image metadata nonparametrically generate neighborhoods related image using jaccard similarities then uses deep neural network blend visual information image its neighbors <eos> prior work typically models image metadata parametrically contrast nonparametric treatment allows model perform well even when vocabulary metadata changes between training testing <eos> perform comprehensive experiments nus wide dataset show model outperforms state art method multilabel image annotation even when model forced generalize new types metadata <eos> <eop> unsupervised extraction video highlights via robust recurrent auto encoders <eos> growing popularity short form video sharing platforms such instagram vine there increasing need techniques automatically extract highlights video <eos> whereas prior works approached problem heuristic rules supervised learning present unsupervised learning approach takes advantage abundance user edited video social media websites such youtube <eos> based idea most significant sub events within video class commonly present among edited video while less interesting ones appear less frequently identify significant sub events via robust recurrent auto encoder trained collection user edited video queried each particular class interest <eos> auto encoder trained using proposed shrinking exponential loss function makes robust noise web crawled training data configured bidirectional long short term memory lstm cells better model temporal structure highlight segments <eos> different supervised techniques method infer highlights using only set downloaded edited video without also needing their pre edited counterparts rarely available online <eos> extensive experiments indicate promise proposed solution challenging unsupervised setting <eos> <eop> learning visual clothing style heterogeneous dyadic co occurrences <eos> rapid proliferation smart mobile devices users now take millions photos every day <eos> include large numbers clothing accessory image <eos> would like answer questions like outfit goes well pair shoes answer types questions one go beyond learning visual similarity learn visual notion compatibility across categories <eos> paper propose novel learning framework help answer types questions <eos> main idea framework learn feature transformation image items into latent space expresses compatibility <eos> feature transformation use siamese convolutional neural network cnn architecture training examples pairs items either compatible incompatible <eos> model compatibility based co occurrence large scale user behavior data particular co purchase data amazon <eos> learn cross category fit introduce strategic method sample training data pairs items heterogeneous dyads <eos> two elements pair belong different high level categories <eos> while approach applicable wide variety settings focus representative problem learning compatible clothing style <eos> result indicate proposed framework capable learning semantic information about visual style able generate outfits clothes items different categories go well together <eos> <eop> text flow unified text detection system natural scene image <eos> prevalent scene text detection approach follows four sequential steps comprising character candidate detection false character candidate removal text line extraction text line verification <eos> however errors occur accumulate throughout each sequential steps often lead low detection performance <eos> address issues propose unified scene text detection system namely text flow utilizing minimum cost min cost flow network model <eos> character candidates detected cascade boosting min cost flow network model integrates last three sequential steps into single process solves error accumulation problem both character level text line level effectively <eos> proposed technique tested three public datasets <eos> icdar dataset icdar dataset multilingual dataset outperforms state art method all three datasets much higher recall score <eos> good performance multilingual dataset shows proposed technique used detection texts different languages <eos> <eop> uncovering interactions interactors joint estimation head body orientation formations surveillance video <eos> present novel approach jointly estimating tar gets head body orientations conversational groups called formations distant social scene <eos> cocktail party captured surveillance cameras <eos> differing related works coupled head body pose learning exploiting limited range orientations two jointly take ii determined formations based mutual head but body orientations teractors present unified framework jointly infer both ii <eos> apart exploiting spatial orien tation relationships also integrate cues pertaining temporal consistency occlusions beneficial while handling low resolution data under surveillance set tings <eos> efficacy joint inference framework reflects via increased head body pose formation estimation ac curacy over state art confirmed extensive experiments two social datasets <eos> <eop> generating notifications missing actions don forget turn lights off <eos> all experienced forgetting habitual actions among daily activities <eos> example probably forgotten turn lights off before leaving room turn stove off after cooking <eos> paper propose solution problem issuing notifications actions may missed <eos> involves learning about interdependencies between actions being able predict ongoing action while segmenting input video stream <eos> order show proof concept collected new egocentric dataset people wear camera while making lattes <eos> show promising result extremely challenging task issuing correct timely reminders <eos> also show model reliably segments actions while predicting ongoing one when only few frames beginning action observed <eos> overall prediction accuracy <eos> when only frames action seen sec <eos> moreover overall recognition segmentation accuracy shown <eos> when whole activity sequence observed <eos> finally online prediction segmentation accuracy <eos> when prediction made every time step <eos> <eop> partial person re identification <eos> address new partial person re identification re id problem only partial observation person available matching across different non overlapping camera views <eos> differs significantly conventional person re id setting assumed full body person detected aligned <eos> solve more challenging realistic re id problem without implicit assumption manual body parts alignment propose matching framework consisting local patch level matching model based novel sparse representation classification formulation explicit patch ambiguity modelling global part based matching model providing complementary spatial layout information <eos> framework evaluated new partial person re id dataset well two existing datasets modified include partial person image <eos> result show proposed method outperforms significantly existing re id method well other partial visual matching method <eos> <eop> shape interaction matrix revisited robustified efficient subspace clustering corrupted incomplete data <eos> shape interaction matrix sim one earliest approaches performing subspace clustering <eos> separating point drawn union subspaces <eos> paper revisit sim reveal its connections several recent subspace clustering method <eos> analysis lets derive simple yet effective algorithm robustify sim make applicable realistic scenarios data corrupted noise <eos> justify method intuitive examples matrix perturbation theory <eos> then show how approach extended handle missing data thus yielding efficient general subspace clustering algorithm <eos> demonstrate benefits approach over state art subspace clustering method several challenging motion segmentation face clustering problems data includes corruptions missing measurements <eos> <eop> multiple hypothesis tracking revisited <eos> paper revisits classical multiple hypotheses tracking mht algorithm tracking detection framework <eos> success mht largely depends ability maintain small list potential hypotheses facilitated accurate object detectors currently available <eos> demonstrate classical mht implementation come surprisingly close performance state art method standard benchmark datasets <eos> order further utilize strength mht exploiting higher order information introduce method training online appearance models each track hypothesis <eos> show appearance models learned efficiently via regularized least squares framework requiring only few extra operations each hypothesis branch <eos> obtain state art result popular tracking detection datasets such pets recent mot challenge <eos> <eop> learning track online multi object tracking decision making <eos> online multi object tracking mot wide applications time critical video analysis scenarios such robot navigation autonomous driving <eos> tracking detection major challenge online mot how robustly associate noisy object detections new video frame previously tracked object <eos> work formulate online mot problem decision making markov decision processes mdps lifetime object modeled mdp <eos> learning similarity function data association equivalent learning policy mdp policy learning approached reinforcement learning fashion benefits both advantages offline learning online learning data association <eos> moreover framework naturally handle birth death appearance disappearance targets treating them state transitions mdp while leveraging existing online single object tracking method <eos> conduct experiments mot benchmark verify effectiveness method <eos> <eop> 