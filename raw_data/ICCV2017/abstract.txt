globally optimal inlier set maximisation simultaneous camera pose feature correspondence <eos> estimating dof pose camera single image relative pre computed three dimensional point set important task many computer vision applications <eos> perspective point pnp solvers routinely used camera pose estimation provided good quality set three dimensional feature correspondences known beforehand <eos> however finding optimal correspondences between key point three dimensional point set non trivial especially when only geometric position information known <eos> existing approaches simultaneous pose correspondence problem use local optimisation therefore unlikely find optimal solution without good pose initialisation introduce restrictive assumptions <eos> since large proportion outliers common problem instead propose globally optimal inlier set cardinality maximisation approach jointly estimates optimal camera pose optimal correspondences <eos> approach employs branch bound search space camera poses guaranteeing global optimality without requiring pose prior <eos> geometry se used find novel upper lower bounds number inliers local optimisation integrated accelerate convergence <eos> evaluation empirically supports optimality proof shows method performs much more robustly than existing approaches including large scale outdoor data set <eos> <eop> robust pseudo random fields light field stereo matching <eos> markov random fields widely used model light field stereo matching problems <eos> however most previous approaches used fixed parameters did adapt light field statistics <eos> instead they explored explicit vision cues provide local adaptability thus enhanced depth quality <eos> but such additional assumptions could end up confining their applicability <eos> algorithms designed dense light fields suitable sparse ones <eos> paper develop empirical bayesian framework robust pseudo random field explore intrinsic statistical cues broad applicability <eos> based pseudo likelihood applies soft expectation maximization em good model fitting hard em robust depth estimation <eos> introduce novel pixel difference models enable such adaptability robustness simultaneously <eos> also devise algorithm employ framework dense sparse even denoised light fields <eos> experimental result show estimates scene dependent parameters robustly converges quickly <eos> terms depth accuracy computation speed also outperforms state art algorithms constantly <eos> <eop> lightweight approach fly reflectance estimation <eos> estimating surface reflectance brdf one key component complete three dimensional scene capture wide applications virtual reality augmented reality human computer interaction <eos> prior work either limited controlled environments <eos> gonioreflectometers light stages multi camera domes requires joint optimization shape illumination reflectance often computationally too expensive <eos> hours running time real time applications <eos> moreover most prior work requires hdr image input further complicates capture process <eos> paper propose lightweight practical approach surface reflectance estimation directly bit rgb image real time easily plugged into any three dimensional scanning fusion system commodity rgbd sensor <eos> method learning based inference time less than ms per scene model size less than bytes <eos> propose two novel network architectures hemicnn grouplet deal unstructured input data multiple viewpoints under unknown illumination <eos> further design loss function resolve color constancy scale ambiguity <eos> addition created large synthetic dataset synbrdf comprises total rgbd image rendered physically based ray tracer under variety natural illumination covering materials shapes <eos> synbrdf first large scale benchmark dataset reflectance estimation <eos> experiments both synthetic data real data show proposed method effectively recovers surface reflectance outperforms prior work reflectance estimation uncontrolled environments <eos> <eop> distributed very large scale bundle adjustment global camera consensus <eos> increasing scale structure motion fundamentally limited conventional optimization framework all one global bundle adjustment <eos> paper propose distributed approach coping global bundle adjustment very large scale structure motion computation <eos> first derive distributed formulation classical optimization algorithm admm alternating direction method multipliers based global camera consensus <eos> then analyze conditions under convergence distributed optimization would guaranteed <eos> particular adopt over relaxation self adaption schemes improve convergence rate <eos> after propose split large scale camera point visibility graph order reduce communication overheads distributed computing <eos> experiments both public large scale sfm data set very large scale aerial photo set demonstrate proposed distributed method clearly outperforms state art method efficiency accuracy <eos> <eop> practical projective structure motion sfm <eos> paper presents solution projective structure motion psfm problem able deal efficiently missing data outliers first time large scale three dimensional reconstruction scenarios <eos> embedding projective depths into projective parameters point views decrease number unknowns estimate improve computational speed optimizing standard linear least squares systems instead homogeneous ones <eos> order so show extension linear constraints generalized projective reconstruction theorem transferred projective parameters ensuring also valid projective reconstruction process <eos> use incremental approach starting solvable sub problem incrementally adds views point until completion robust outliers free procedure <eos> experiments simulated data shows approach performing well both term quality reconstruction capacity handle missing data outliers reduced computational time <eos> finally result real datasets shows ability method used medium large scale three dimensional reconstruction scenarios high ratios missing data up <eos> <eop> anticipating daily intention using wrist motion triggered sensing <eos> anticipating human intention observing one actions many applications <eos> instance picking up cellphone then charger actions implies one wants charge cellphone intention <eos> anticipating intention intelligent system guide user closest power outlet <eos> propose wrist motion triggered sensing system anticipating daily intentions wrist sensors help persistently observe one actions <eos> core system novel recurrent neural network rnn policy network pn rnn encodes visual motion observation anticipate intention pn parsimoniously triggers process visual observation reduce computation requirement <eos> jointly trained whole network using policy gradient cross entropy loss <eos> evaluate collect first daily intention dataset consisting video intentions unique action sequences <eos> accuracy three users while processing only visual observation average <eos> <eop> rethinking reprojection closing loop pose aware shape reconstruction single image <eos> emerging problem computer vision reconstruction three dimensional shape pose object single image <eos> hitherto problem addressed through application canonical deep learning method regress image directly three dimensional shape pose labels <eos> approaches however problematic two perspectives <eos> first they minimizing error between three dimensional shapes pose labels little thought about nature label error when reprojecting shape back onto image <eos> second they rely onerous ill posed task hand labeling natural image respect three dimensional shape pose <eos> paper define new task pose aware shape reconstruction single image advocate cheaper annotations object silhouettes natural image utilized <eos> design architectures pose aware shape reconstruction reproject predicted shape back image using predicted pose <eos> evaluation several object categories demonstrates superiority method predicting pose aware three dimensional shapes natural image <eos> <eop> end end learning geometry context deep stereo regression <eos> propose novel deep learning architecture regressing disparity rectified pair stereo image <eos> leverage knowledge problem geometry form cost volume using deep feature representations <eos> learn incorporate contextual information using convolutions over volume <eos> disparity values regressed cost volume using proposed differentiable soft argmin operation allows train method end end sub pixel accuracy without any additional post processing regularization <eos> evaluate method scene flow kitti datasets kitti set new state art benchmark while being significantly faster than competing approaches <eos> <eop> using sparse elimination solving minimal problems computer vision <eos> finding closed form solution system polynomial equations common problem computer vision well many other areas engineering science <eos> groebner basis techniques often employed provide solution but implementing efficient groebner basis solver given problem requires strong expertise algebraic geometry <eos> one also convert equations polynomial eigenvalue problem pep solve using linear algebra more accessible approach who so familiar algebraic geometry <eos> previous works pep successfully applied solving some relative pose problems computer vision but its wider exploitation limited problem finding compact monomial basis <eos> paper propose new algorithm selecting basis general more compact than basis obtained state art algorithm making pep more viable option solving polynomial equations <eos> another contribution present two minimal problems camera self calibration based homography demonstrate experimentally using synthetic real data algorithm provide numerically stable solution camera focal length two homographies unknown planar scene <eos> <eop> high resolution shape completion using deep neural network global structure local geometry inference <eos> propose data driven method recovering missing parts three dimensional shapes <eos> method based new deep learning architecture consisting two sub network global structure inference network local geometry refinement network <eos> global structure inference network incorporates long short term memorized context fusion module lstm cf infers global structure shape based multi view depth information provided part input <eos> also includes three dimensional fully convolutional dfcn module further enriches global structure representation according volumetric information input <eos> under guidance global structure network local geometry refinement network takes input local three dimensional patches around missing region progressively produces high resolution complete surface through volumetric encoder decoder architecture <eos> method jointly trains global structure inference local geometry refinement network end end manner <eos> perform qualitative quantitative evaluations six object categories demonstrating method outperforms existing state art work shape completion <eos> <eop> temporal tessellation unified approach video analysis <eos> present general approach video understanding inspired semantic transfer techniques successfully used image analysis <eos> method considers video sequence clips each one associated its own semantics <eos> nature semantics natural language captions other labels depends task hand <eos> test video processed forming correspondences between its clips clips reference video known semantics following reference semantics transferred test video <eos> describe two matching method both designed ensure reference clips appear similar test clips taken together semantics selected reference clips consistent maintains temporal coherence <eos> use method video captioning lsmdc benchmark video summarization summe tvsum benchmarks temporal action detection thumos benchmark sound prediction greatest hits benchmark <eos> method only surpasses state art four out five benchmarks but importantly only single method know was successfully applied such diverse range tasks <eos> <eop> learning policies adaptive tracking deep feature cascades <eos> visual object tracking fundamental time critical vision task <eos> recent years seen many shallow tracking method based real time pixel based correlation filters well deep method top performance but need high end gpu <eos> paper learn improve speed deep trackers without losing accuracy <eos> fundamental insight take adaptive approach easy frames processed cheap feature such pixel values while challenging frames processed invariant but expensive deep feature <eos> formulate adaptive tracking problem decision making process learn agent decide whether locate object high confidence early layer continue processing subsequent layer network <eos> significantly reduces feed forward cost easy frames distinct slow moving object <eos> train agent offline reinforcement learning fashion further demonstrate learning all deep layer so provide good feature adaptive tracking lead near real time average tracking speed fps single cpu while achieving state art performance <eos> perhaps most tellingly approach provides speedup almost time indicating power adaptive approach <eos> <eop> temporal shape super resolution intra frame motion encoding using high fps structured light <eos> one solutions depth imaging moving scene project static pattern object use just single image reconstruction <eos> however if motion object too fast respect exposure time image sensor patterns captured image blurred reconstruction fails <eos> paper impose multiple projection patterns into each single captured image realize temporal super resolution depth image sequences <eos> method multiple patterns projected onto object higher fps than possible camera <eos> case observed pattern varies depending depth motion object so extract temporal information scene each single image <eos> decoding process realized using learning based approach no geometric calibration needed <eos> experiments confirm effectiveness method sequential shapes reconstructed single image <eos> both quantitative evaluations comparisons recent techniques were also conducted <eos> <eop> real time monocular pose estimation three dimensional object using temporally consistent local color histograms <eos> present novel approach dof pose estimation segmentation rigid three dimensional object using single monocular rgb camera based temporally consistent local color histograms <eos> show approach outperforms previous method cases cluttered backgrounds heterogenous object occlusions <eos> proposed histograms used statistical object descriptors within template matching strategy pose recovery after temporary tracking loss <eos> caused massive occlusion if object leaves camera field view <eos> descriptors trained online within couple seconds moving handheld object front camera <eos> during training stage approach already capable recover accidental tracking loss <eos> demonstrate performance method comparison state art different challenging experiments including popular public data set <eos> <eop> cad priors accurate flexible instance reconstruction <eos> present efficient automatic approach accurate reconstruction instances big three dimensional object multiple unorganized unstructured point clouds presence dynamic clutter occlusions <eos> contrast conventional scanning background assumed rather static aim handling dynamic clutter background drastically changes during object scanning <eos> currently tedious solve available method unless object interest first segmented out rest scene <eos> address problem assuming availability prior cad model roughly resembling object reconstructed <eos> assumption almost always holds applications such industrial inspection reverse engineering <eos> aid prior acting proxy propose fully enhanced pipeline capable automatically detecting segmenting object interest scenes creating pose graph online linear complexity <eos> allows initial scan alignment cad model space then refined without cad constraint fully recover high fidelity three dimensional reconstruction accurate up sensor noise level <eos> also contribute novel object detection method local implicit shape models lism give fast verification scheme <eos> evaluate method multiple datasets demonstrating ability accurately reconstruct object small sizes up <eos> <eop> colored point cloud registration revisited <eos> present algorithm tightly aligning two colored point clouds <eos> key idea optimize joint photometric geometric objective locks alignment along both normal direction tangent plane <eos> extend photometric objective aligning rgb image point clouds locally parameterizing point cloud virtual camera <eos> experiments demonstrate algorithm more accurate more robust than prior point cloud registration algorithms including utilize color information <eos> use presented algorithms enhance state art scene reconstruction system <eos> accuracy resulting system demonstrated real world scenes accurate ground truth models <eos> <eop> learning compact geometric feature <eos> present approach learning feature represent local geometry around point unstructured point cloud <eos> such feature play central role geometric registration supports diverse applications robotics three dimensional vision <eos> current state art local feature unstructured point clouds manually crafted none combines desirable properties precision compactness robustness <eos> show feature properties learned data optimizing deep network map high dimensional histograms into low dimensional euclidean spaces <eos> presented approach yields family feature parameterized dimension both more compact more accurate than existing descriptors <eos> <eop> joint layout estimation global multi view registration indoor reconstruction <eos> paper propose approach jointly solve scene layout estimation global registration problems accurate indoor three dimensional reconstruction <eos> given sequence range data build set scene fragments using kinectfusion register them through pose graph optimization <eos> afterwards alternate layout estimation layout based global registration processes iterative fashion complement each other <eos> extract scene layout through hierarchical agglomerative clustering energy based multi model fitting consideration noisy measurements <eos> having estimated scene layout one hand register all range data through global iterative closest point algorithm positions three dimensional point belong layout such walls ceiling constrained close layout <eos> experimentally verify proposed method publicly available synthetic real world datasets both quantitative qualitative ways <eos> <eop> geometric framework statistical analysis trajectories distinct temporal spans <eos> analyzing data representing multifarious trajectories central many fields science engineering example trajectories representing tennis serve gymnast parallel bar routine progression remission disease so <eos> present novel geometric algorithm performing statistical analysis trajectories distinct number sample representing longitudinal temporal data <eos> key feature proposal unlike existing schemes model deployable regimes each participant provides different number acquisitions trajectories different number sample point <eos> achieve develop novel method involving parallel transport tangent vectors along each given trajectory starting point respective trajectories then use span matrix whose columns consist vectors construct linear subspace <eos> then map linear subspaces single high dimensional hypersphere <eos> enables computing group statistics over trajectories instead performing statistics hypersphere equipped simpler geometry <eos> given point hypersphere representing trajectory also provide reverse mapping algorithm uniquely under certain assumptions reconstruct subspace corresponds point <eos> finally using existing algorithms recursive frechet mean exact principal geodesic analysis hypersphere present several experiments synthetic real vision medical data set showing how group testing such diversely sampled longitudinal data possible analyzing reconstructed data subspace spanned first few pgs <eos> <eop> optimal transportation based univariate neuroimaging index <eos> alterations brain structures functions considered closely correlated change cognitive performance due neurodegenerative diseases such alzheimer disease <eos> paper introduce variational framework compute optimal transformation ot three dimensional space propose univariate neuroimaging index based ot measure such alterations <eos> compute ot each image template measure wasserstein distance between them <eos> comparing distances all image common template obtain concise informative index each image <eos> framework makes use newton method reduces computational cost enables itself applicable large scale datasets <eos> proposed work generic approach thus may applicable various volumetric brain image including structural magnetic resonance smr fluorodeoxyglucose positron emission tomography fdg pet image <eos> classification between alzheimer disease patients healthy controls method achieves accuracy <eos> alzheimer disease neuroimaging initiative adni baseline smri dataset outperforms several other indices <eos> fdg pet dataset boost accuracy <eos> leveraging pairwise wasserstein distances <eos> longitudinal study obtain significance value <eos> test fdg pet <eos> result demonstrate great potential proposed index neuroimage analysis precision medicine research <eos> <eop> fd single shot scale invariant face detector <eos> paper presents real time face detector named single shot scale invariant face detector fd performs superiorly various scales faces single deep neural network especially small faces <eos> specifically try solve common problem anchor based detectors deteriorate dramatically object become smaller <eos> make contributions following three aspects proposing scale equitable face detection framework handle different scales faces well <eos> tile anchors wide range layer ensure all scales faces enough feature detection <eos> besides design anchor scales based effective receptive field proposed equal proportion interval principle improving recall rate small faces scale compensation anchor matching strategy reducing false positive rate small faces via max out background label <eos> consequence method achieves state art detection performance all common face detection benchmarks including afw pascal face fddb wider face datasets run fps nvidia titan pascal vga resolution image <eos> <eop> amulet aggregating multi level convolutional feature salient object detection <eos> fully convolutional neural network fcns shown outstanding performance many dense labeling problems <eos> one key pillar successes mining relevant information feature convolutional layer <eos> however how better aggregate multi level convolutional feature maps salient object detection underexplored <eos> work present amulet generic aggregating multi level convolutional feature framework salient object detection <eos> framework first integrates multi level feature maps into multiple resolutions simultaneously incorporate coarse semantics fine details <eos> then adaptively learns combine feature maps each resolution predict saliency maps combined feature <eos> finally predicted result efficiently fused generate final saliency map <eos> addition achieve accurate boundary inference semantic enhancement edge aware feature maps low level layer predicted result low resolution feature recursively embedded into learning framework <eos> aggregating multi level convolutional feature efficient flexible manner proposed saliency model provides accurate salient object labeling <eos> comprehensive experiments demonstrate method performs favorably against state art approaches terms near all compared evaluation metrics <eos> <eop> learning uncertain convolutional feature accurate saliency detection <eos> deep convolutional neural network cnn delivered superior performance many computer vision tasks <eos> paper propose novel deep fully convolutional network model accurate salient object detection <eos> key contribution work learn deep uncertain convolutional feature ucf encourage robustness accuracy saliency detection <eos> achieve via introducing reformulated dropout dropout after specific convolutional layer construct uncertain ensemble internal feature units <eos> addition propose effective hybrid upsampling method reduce checkerboard artifacts deconvolution operators decoder network <eos> proposed method also applied other deep convolutional network <eos> compared existing saliency detection method proposed ucf model able incorporate uncertainties more accurate object boundary inference <eos> extensive experiments demonstrate proposed saliency model performs favorably against state art approaches <eos> uncertain feature learning mechanism well upsampling method significantly improve performance other pixel wise vision tasks <eos> <eop> zero order reverse filtering <eos> paper study unconventional but practically meaningful reversibility problem commonly used image filters <eos> broadly define filters operations smooth image produce layer via global local algorithms <eos> raise intriguingly problem if they reservable status before filtering <eos> answer present novel strategy understand general filter via contraction mappings metric space <eos> very simple yet effective zero order algorithm proposed <eos> able practically reverse most filters low computational cost <eos> present quite few experiments paper supplementary file thoroughly verify its performance <eos> method also generalized solve other inverse problems enables new applications <eos> <eop> learning blind motion deblurring <eos> handheld video cameras now commonplace available every smartphone image video recorded almost everywhere any time <eos> however taking quick shot frequently ends up blurry result due unwanted camera shake during recording moving object scene <eos> removing artifacts blurry recordings highly ill posed problem neither sharp image nor motion blur known <eos> propagating information between multiple consecutive blurry observations help restore desired sharp image video <eos> solutions blind deconvolution based neural network rely massive amount ground truth data was difficult acquire <eos> work propose efficient approach produce significant amount realistic training data introduce novel recurrent network architecture deblur frames efficiently handle arbitrary spatial temporal input sizes <eos> <eop> joint adaptive sparsity low rankness fly online tensor reconstruction scheme video denoising <eos> recent works adaptive sparse low rank signal modeling demonstrated their usefulness especially image video processing applications <eos> while patch based sparse model imposes local structure low rankness grouped patches exploits non local correlation <eos> applying either approach alone usually limits performance various low level vision tasks <eos> work propose novel video denoising method based online tensor reconstruction scheme joint adaptive sparse low rank model dubbed salt <eos> efficient unsupervised online unitary sparsifying transform learning method introduced impose adaptive sparsity fly <eos> develop efficient three dimensional spatio temporal data reconstruction framework based proposed online learning method exhibits low latency potentially handle streaming video <eos> best knowledge first work combines adaptive sparsity low rankness video denoising first work solving proposed problem online fashion <eos> demonstrate video denoising result over commonly used video public datasets <eos> numerical experiments show proposed video denoising method outperforms competing method <eos> <eop> learning super resolve blurry face text image <eos> present algorithm directly restore clear high resolution image blurry low resolution input <eos> problem highly ill posed basic assumptions existing super resolution method requiring clear input deblurring method requiring high resolution input no longer hold <eos> focus face text image adopt generative adversarial network gan learn category specific prior solve problem <eos> however basic gan formulation generate realistic high resolution image <eos> work introduce novel training losses help recover fine details <eos> also present multi class gan process multi class image restoration tasks <eos> face text image using single generator network <eos> extensive experiments demonstrate method performs favorably against state art method both synthetic real world image lower computational cost <eos> <eop> video frame interpolation via adaptive separable convolution <eos> standard video frame interpolation method first estimate optical flow between input frames then synthesize intermediate frame guided motion <eos> recent approaches merge two steps into single convolution process convolving input frames spatially adaptive kernels account motion re sampling simultaneously <eos> method require large kernels handle large motion limits number pixels whose kernels estimated once due large memory demand <eos> address problem paper formulates frame interpolation local separable convolution over input frames using pairs kernels <eos> compared regular kernels kernels require significantly fewer parameters estimated <eos> method develops deep fully convolutional neural network takes two input frames estimates pairs kernels all pixels simultaneously <eos> since method able estimate kernels synthesizes whole video frame once allows incorporation perceptual loss train neural network produce visually pleasing frames <eos> deep neural network trained end end using widely available video data without any human annotation <eos> both qualitative quantitative experiments show method provides practical solution high quality video frame interpolation <eos> <eop> deep occlusion reasoning multi camera multi target detection <eos> people detection image improved greatly recent years <eos> however comparatively little progress percolated into multi camera multi people tracking algorithms whose performance still degrades severely when scenes become very crowded <eos> work introduce new architecture combines convolutional neural nets conditional random fields explicitly resolve ambiguities <eos> one its key ingredients high order crf terms model potential occlusions give approach its robustness even when many people present <eos> model trained end end show outperforms several state art algorithms challenging scenes <eos> <eop> encouraging lstms anticipate actions very early <eos> contrast widely studied problem recognizing action given complete sequence action anticipation aims identify action only partially available video <eos> such therefore key success computer vision applications requiring react early possible such autonomous navigation <eos> paper propose new action anticipation method achieves high prediction accuracy even presence very small percentage video sequence <eos> end develop multi stage lstm architecture leverages context aware action aware feature introduce novel loss function encourages model predict correct class early possible <eos> experiments standard benchmark datasets evidence benefits approach outperform state art action anticipation method early prediction relative increase accuracy <eos> <eop> pathtrack fast trajectory annotation path supervision <eos> progress multiple object tracking mot limited size available datasets <eos> present efficient framework annotate trajectories use produce mot dataset unprecedented size <eos> novel path supervision paradigm lets annotator loosely track object cursor while watching video <eos> result path annotation each object sequence <eos> path annotations together object detections fed into two step optimization produce full bounding box trajectories <eos> experiments existing datasets prove framework produces more accurate annotations than state art fraction time <eos> further validate approach generating pathtrack dataset more than person trajectories sequences <eos> believe tracking approaches benefit larger dataset like one just was case object recognition <eos> show its potential using re train off shelf person matching network originally trained mot dataset almost halving misclassification rate <eos> additionally training data consistently improves tracking result both dataset mot <eos> latter improve top performing tracker nomt dropping number id switches fragments <eos> <eop> tracking untrackable learning track multiple cues long term dependencies <eos> majority existing solutions multi target tracking mtt problem combine cues over long period time coherent fashion <eos> paper present online method encodes long term temporal dependencies across multiple cues <eos> one key challenge tracking method accurately track occluded targets share similar appearance properties surrounding object <eos> address challenge present structure recurrent neural network rnn jointly reasons multiple cues over temporal window <eos> method allows correct data association errors recover observations occluded states <eos> demonstrate robustness data driven approach tracking multiple targets using their appearance motion even interactions <eos> method outperforms previous works multiple publicly available datasets including challenging mot benchmark <eos> <eop> mirrorflow exploiting symmetries joint optical flow occlusion estimation <eos> optical flow estimation one most studied problems computer vision yet recent benchmark datasets continue reveal problem areas today approaches <eos> occlusions remained one key challenges <eos> paper propose symmetric optical flow method address well known chicken egg relation between optical flow occlusions <eos> contrast many state art method consider occlusions outliers possibly filtered out during post processing highlight importance joint occlusion reasoning optimization show how utilize occlusion important cue estimating optical flow <eos> key feature model fully exploit symmetry properties characterize optical flow occlusions two consecutive image <eos> specifically through utilizing forward backward consistency occlusion disocclusion symmetry energy model jointly estimates optical flow both forward backward direction well consistent occlusion maps both views <eos> demonstrate significant performance benefits standard benchmarks especially occlusion disocclusion symmetry <eos> challenging kitti dataset report most accurate two frame result date <eos> <eop> tracking online decision making learning policy streaming video reinforcement learning <eos> formulate tracking online decision making process tracking agent must follow object despite ambiguous image frames limited computational budget <eos> crucially agent must decide look upcoming frames when reinitialize because believes target lost when update its appearance model tracked object <eos> such decisions typically made heuristically <eos> instead propose learn optimal decision making policy formulating tracking partially observable decision making process pomdp <eos> learn policies deep reinforcement learning algorithms need supervision reward signal only when track gone awry <eos> demonstrate sparse rewards allow quickly train massive datasets several orders magnitude more than past work <eos> interestingly treating data source internet video unlimited streams both learn evaluate trackers single unified computational stream <eos> <eop> non convex rank sparsity regularization local minima <eos> paper considers problem recovering either low rank matrix sparse vector observations linear combinations vector matrix elements <eos> recent method replace non convex regularization nuclear norm relaxations <eos> well known approach recovers near optimal solutions if so called restricted isometry property rip holds <eos> other hand also shrinking bias degrade solution <eos> paper study alternative non convex regularization term suffer bias <eos> main theoretical result show if rip holds then stationary point often well separated sense their differences must high cardinality rank <eos> thus suitable initial solution approach unlikely fall into bad local minimum <eos> numerical tests show approach likely converge better solution than standard nuclear norm relaxation even when starting trivial initializations <eos> many cases result also used verify global optimality method <eos> <eop> revisit sparse coding based anomaly detection stacked rnn framework <eos> motivated capability sparse coding based anomaly detection propose temporally coherent sparse coding tsc enforce similar neighbouring frames encoded similar reconstruction coefficients <eos> then map tsc special type stacked recurrent neural network srnn <eos> taking advantage srnn learning all parameters simultaneously nontrivial hyper parameter selection tsc avoided meanwhile shallow srnn reconstruction coefficients inferred within forward pass reduces computational cost learning sparse coefficients <eos> contributions paper two fold propose tsc mapped srnn facilitates parameter optimization accelerates anomaly prediction <eos> ii build very large dataset even larger than summation all existing dataset anomaly detection terms both volume data diversity scenes <eos> extensive experiments both toy dataset real datasets demonstrate tsc based srnn based method consistently outperform existing method validates effectiveness method <eos> <eop> hydraplus net attentive deep feature pedestrian analysis <eos> pedestrian analysis plays vital role intelligent video surveillance key component security centric computer vision systems <eos> despite convolutional neural network remarkable learning discriminative feature image learning comprehensive feature pedestrians fine grained tasks remains open problem <eos> study propose new attention based deep neural network named hydraplus net hp net multi directionally feeds multi level attention maps different feature layer <eos> attentive deep feature learned proposed hp net bring unique advantages model capable capturing multiple attentions low level semantic level explores multi scale selectiveness attentive feature enrich final feature representations pedestrian image <eos> demonstrate effectiveness generality proposed hp net pedestrian analysis two tasks <eos> pedestrian attribute recognition person re identification <eos> intensive experimental result provided prove hp net outperforms state art method various datasets <eos> <eop> no fuss distance metric learning using proxies <eos> address problem distance metric learning dml defined learning distance consistent notion semantic similarity <eos> traditionally problem supervision expressed form set point follow ordinal relationship anchor point similar set positive point dissimilar set negative point loss defined over distances minimized <eos> while specifics optimization differ work collectively call type supervision triplets all method follow pattern triplet based method <eos> method challenging optimize <eos> main issue need finding informative triplets usually achieved variety tricks such increasing batch size hard semi hard triplet mining etc <eos> even tricks convergence rate such method slow <eos> paper propose optimize triplet loss different space triplets consisting anchor data point similar dissimilar proxy point learned well <eos> proxies approximate original data point so triplet loss over proxies tight upper bound original loss <eos> proxy based loss empirically better behaved <eos> result proxy loss improves state art result three standard zero shot learning datasets up point while converging three times fast other triplet based losses <eos> <eop> benchmarking error diagnosis multi instance pose estimation <eos> propose new method analyze impact errors algorithms multi instance pose estimation principled benchmark used compare them <eos> define characterize three classes errors localization scoring background study how they influenced instance attributes their impact algorithm performance <eos> technique applied compare two leading method human pose estimation coco dataset measure sensitivity pose estimation respect instance size type number visible keypoints clutter due multiple instances relative score instances <eos> performance algorithms types error they make highly dependent all variables but mostly number keypoints clutter <eos> analysis software tools propose offer novel insightful approach understanding behavior pose estimation algorithms effective method measuring their strengths weaknesses <eos> <eop> orientation invariant feature embedding spatial temporal regularization vehicle re identification <eos> paper tackle vehicle re identification reid problem great importance urban surveillance used multiple applications <eos> vehicle reid framework orientation invariant feature embedding module spatial temporal regularization module proposed <eos> orientation invariant feature embedding local region feature different orientations extracted based key point locations well aligned combined <eos> spatial temporal regularization log normal distribution adopted model spatial temporal constraints retrieval result refined <eos> experiments conducted public vehicle reid datasets proposed method achieves state art performance <eos> investigations proposed framework conducted including landmark regressor comparisons attention mechanism <eos> both orientation invariant feature embedding spatio temporal regularization achieve considerable improvements <eos> <eop> fashion forward forecasting visual style fashion <eos> future fashion tackling question data driven vision perspective propose forecast visual style trends before they occur <eos> introduce first approach predict future popularity styles discovered fashion image unsupervised manner <eos> using styles basis train forecasting model represent their trends over time <eos> resulting model hypothesize new mixtures styles will become popular future discover style dynamics trendy vs <eos> classic name key visual attributes will dominate tomorrow fashion <eos> demonstrate idea applied three datasets encapsulating fashion products sold across six years amazon <eos> result indicate fashion forecasting benefits greatly visual analysis much more than textual meta data cues surrounding products <eos> <eop> towards three dimensional human pose estimation wild weakly supervised approach <eos> paper study task three dimensional human pose estimation wild <eos> task challenging due lack training data existing datasets either wild image pose lab image three dimensional pose <eos> propose weakly supervised transfer learning method uses mixed three dimensional labels unified deep neutral network presents two stage cascaded structure <eos> network augments state art pose estimation sub network three dimensional depth regression sub network <eos> unlike previous two stage approaches train two sub network sequentially separately training end end fully exploits correlation between pose depth estimation sub tasks <eos> deep feature better learnt through shared representations <eos> doing so three dimensional pose labels controlled lab environments transferred wild image <eos> addition introduce three dimensional geometric constraint regularize three dimensional pose prediction effective absence ground truth depth labels <eos> method achieves competitive result both three dimensional benchmarks <eos> <eop> flow guided feature aggregation video object detection <eos> extending state art object detectors image video challenging <eos> accuracy detection suffers degenerated object appearances video <eos> motion blur video defocus rare poses etc <eos> existing work attempts exploit temporal information box level but such method trained end end <eos> present flow guided feature aggregation accurate end end learning framework video object detection <eos> leverages temporal coherence feature level instead <eos> improves per frame feature aggregation nearby feature along motion paths thus improves video recognition accuracy <eos> method significantly improves upon strong single frame baselines imagenet vid especially more challenging fast moving object <eos> framework principled par best engineered systems winning imagenet vid challenges without additional bells whistles <eos> <eop> reasoning about fine grained attribute phrases using reference games <eos> present framework learning describe fine grained visual differences between instances using attribute phrases <eos> attribute phrases capture distinguishing aspects object <eos> propeller nose door near wing airplanes compositional manner <eos> instances within category described set phrases collectively they span space semantic attributes category <eos> collect large dataset such phrases asking annotators describe several visual differences between pair instances within category <eos> then learn describe ground phrases image context reference game between speaker listener <eos> goal speaker describe attributes image allows listener correctly identify within pair <eos> data collected pairwise manner improves ability speaker generate ability listener interpret visual descriptions <eos> moreover due compositionality attribute phrases trained listeners interpret descriptions seen during training image retrieval speakers generate attribute based explanations differences between previously unseen categories <eos> also show embedding image into semantic space attribute phrases derived listeners offers improvement accuracy over existing attribute based representations fgvc aircraft dataset <eos> <eop> denet scalable real time object detection directed sparse sampling <eos> define object detection imagery problem estimating very large but extremely sparse bounding box dependent probability distribution <eos> subsequently identify sparse distribution estimation scheme directed sparse sampling employ single end end cnn based detection model <eos> methodology extends formalizes previous state art detection models additional emphasis high evaluation rates reduced manual engineering <eos> introduce two novelties corner based region interest estimator deconvolution based cnn model <eos> resulting model scene adaptive require manually defined reference bounding boxes produces highly competitive result mscoco pascal voc pascal voc real time evaluation rates <eos> further analysis suggests model performs particularly well when finegrained object localization desirable <eos> argue advantage stems significantly larger set available region interest relative other method <eos> source code available github <eos> com lachlants denet <eop> mihash online hashing mutual information <eos> learning based hashing method widely used nearest neighbor retrieval recently online hashing method demonstrated good performance complexity trade offs learning hash functions streaming data <eos> paper first address key challenge online hashing binary codes indexed data must recomputed keep pace updates hash functions <eos> propose efficient quality measure hash functions based information theoretic quantity mutual information use successfully criterion eliminate unnecessary hash table updates <eos> next also show how optimize mutual information objective using stochastic gradient descent <eos> thus develop novel hashing method mihash used both online batch settings <eos> experiments image retrieval benchmarks including <eos> image dataset confirm effectiveness formulation both reducing hash table recomputations learning high quality hash functions <eos> <eop> safetynet detecting rejecting adversarial examples robustly <eos> describe method produce network current method such deepfool great difficulty producing adversarial sample <eos> construction suggests some insights into how deep network work <eos> provide reasonable analyses construction difficult defeat show experimentally method hard defeat both type type ii attacks using several standard network datasets <eos> safetynet architecture used important novel application sceneproof reliably detect whether image picture real scene <eos> sceneproof applies image captured depth maps rgbd image checks if pair image depth map consistent <eos> relies relative difficulty producing naturalistic depth maps image post processing <eos> demonstrate safetynet robust adversarial examples built currently known attacking approaches <eos> <eop> recurrent models situation recognition <eos> work proposes recurrent neural network rnn models predict structured image situations actions noun entities fulfilling semantic roles related action <eos> contrast prior work relying conditional random fields crfs use specialized action prediction network followed rnn noun prediction <eos> system obtains state art accuracy challenging recent imsitu dataset beating crf based models including ones trained additional data <eos> further show specialized feature learned situation prediction transferred task image captioning more accurately describe human object interactions <eos> <eop> multi label image recognition recurrently discovering attentional region <eos> paper proposes novel deep architecture address multi label image recognition fundamental practical task towards general visual understanding <eos> current solutions task usually rely extra step extracting hypothesis region <eos> region proposals resulting redundant computation sub optimal performance <eos> work achieve interpretable contextualized multi label image classification developing recurrent memorized attention module <eos> module consists two alternately performed components spatial transformer layer locate attentional region convolutional feature maps region proposal free way ii lstm long short term memory sub network sequentially predict semantic labeling scores located region while capturing global dependencies region <eos> lstm also output parameters computing spatial transformer <eos> large scale benchmarks multi label image classification <eos> ms coco pascal voc approach demonstrates superior performances over other existing state arts both accuracy efficiency <eos> <eop> deep determinantal point process large scale multi label classification <eos> study large scale multi label classification mlc two recently released datasets youtube open image contain millions data instances thousands classes <eos> unprecedented problem scale poses great challenges mlc <eos> first finding out correct label subset out exponentially many choices incurs substantial ambiguity uncertainty <eos> second large data size class size entail considerable computational cost <eos> address first challenge investigate two strategies capturing label correlations training data incorporating label co occurrence relations obtained external knowledge effectively eliminate semantically inconsistent labels provide contextual clues differentiate visually ambiguous labels <eos> specifically propose deep determinantal point process ddpp model seamlessly integrates dpp deep neural network dnns supports end end multi label learning deep representation learning <eos> dpp able capture label correlations any order polynomial computational cost while dnns learn hierarchical feature image video capture dependency between input data labels <eos> incorporate external knowledge about label co occurrence relations impose relational regularization over kernel matrix ddpp <eos> address second challenge study efficient low rank kernel learning algorithm based inducing point method <eos> experiments two datasets demonstrate efficacy efficiency proposed method <eos> <eop> visual semantic planning using deep successor representations <eos> crucial capability real world intelligent agents their ability plan sequence actions achieve their goals visual world <eos> work address problem visual semantic planning task predicting sequence actions visual observations transform dynamic environment initial state goal state <eos> doing so entails knowledge about object their affordances well actions their preconditions effects <eos> propose learning through interacting visual dynamic environment <eos> proposed solution involves bootstrapping reinforcement learning imitation learning <eos> ensure cross task generalization develop deep predictive model based successor representations <eos> experimental result show near optimal result across wide range tasks challenging thor environment <eos> supplementary video accessed following link goo <eos> <eop> neural person search machines <eos> investigate problem person search wild work <eos> instead comparing query against all candidate region generated query blind manner propose recursively shrink search area whole image till achieving precise localization target person fully exploiting information query contextual cues every recursive search step <eos> develop neural person search machines npsm implement such recursive localization person search <eos> benefiting its neural search mechanism npsm able selectively shrink its focus loose region tighter one containing target automatically <eos> process npsm employs internal primitive memory component memorize query representation modulates attention augments its robustness other distracting region <eos> evaluations two benchmark datasets cuhk sysu person search dataset prw dataset demonstrated method outperform current state arts both map top evaluation protocols <eos> <eop> dualnet learn complementary feature image recognition <eos> work propose novel framework named dualnet aiming learning more accurate representation image recognition <eos> here two parallel neural network coordinated learn complementary feature thus wider network constructed <eos> specifically logically divide end end deep convolutional neural network into two functional parts <eos> feature extractor image classifier <eos> extractors two subnetworks placed side side exactly form feature extractor dualnet <eos> then two stream feature aggregated final classifier overall classification while two auxiliary classifiers appended behind feature extractor each subnetwork make separately learned feature discriminative alone <eos> complementary constraint imposed weighting three classifiers indeed key dualnet <eos> corresponding training strategy also proposed consisting iterative training joint finetuning make two subnetworks cooperate well each other <eos> finally dualnet based well known caffenet vggnet nin resnet thoroughly investigated experimentally evaluated multiple datasets including cifar stanford dogs uec food <eos> result demonstrate dualnet really help learn more accurate image representation thus result higher accuracy recognition <eos> particular performance cifar state art compared recent works <eos> <eop> higher order integration hierarchical convolutional activations fine grained visual categorization <eos> success fine grained visual categorization fgvc extremely relies modeling appearance interactions various semantic parts <eos> makes fgvc very challenging because part annotation detection require expert guidance very expensive ii parts different sizes iii part interactions complex higher order <eos> address issues propose end end framework based higher order integration hierarchical convolutional activations fgvc <eos> treating convolutional activations local descriptors hierarchical convolutional activations serve representation local parts different scales <eos> polynomial kernel based predictor proposed capture higher order statistics convolutional activations modeling part interaction <eos> model inter layer part interactions extend polynomial predictor integrate hierarchical activations via kernel fusion <eos> work also provides new perspective combining convolutional activations multiple layer <eos> while hypercolumns simply concatenate maps different layer holistically nested network uses weighted fusion combine side outputs approach exploits higher order intra layer inter layer relations better integration hierarchical convolutional feature <eos> proposed framework yields more discriminative representation achieves competitive result widely used fgvc datasets <eos> <eop> show adapt tell adversarial training cross domain image captioner <eos> impressive image captioning result achieved domains plenty training image sentence pairs <eos> however transferring target domain significant domain shifts but no paired training data referred cross domain image captioning remains largely unexplored <eos> propose novel adversarial training procedure leverage unpaired data target domain <eos> two critic network introduced guide captioner namely domain critic multi modal critic <eos> domain critic assesses whether generated sentences indistinguishable sentences target domain <eos> multi modal critic assesses whether image its generated sentence valid pair <eos> during training critics captioner act adversaries captioner aims generate indistinguishable sentences whereas critics aim distinguishing them <eos> assessment improves captioner through policy gradient updates <eos> during inference further propose novel critic based planning method select high quality sentences without additional supervision <eos> evaluate use mscoco source domain four other datasets cub oxford tgif flickr target domains <eos> method consistently performs well all datasets <eos> particular cub achieve <eos> cider improvement after adaptation <eos> utilizing critics during inference further gives another <eos> <eop> attribute recognition joint recurrent learning context correlation <eos> recognising semantic pedestrian attributes surveillance image challenging task computer vision particularly when imaging quality poor complex background clutter uncontrolled viewing conditions number labelled training data small <eos> work formulate joint recurrent learning jrl model exploring attribute context correlation order improve attribute recognition given small sized training data poor quality image <eos> jrl model learns jointly pedestrian attribute correlations pedestrian image particular their sequential ordering dependencies latent high order correlation end end encoder decoder recurrent network <eos> demonstrate performance advantage robustness jrl model over wide range state art deep models pedestrian attribute recognition multi label image classification multi person image annotation two largest pedestrian attribute benchmarks peta rap <eos> <eop> vegfru domain specific dataset fine grained visual categorization <eos> vegfru domain specific dataset fine grained visual categorization paper propose novel domain specific dataset named vegfru fine grained visual categorization fgvc <eos> while existing datasets fgvc mainly focused animal breeds man made object limited labelled data vegfru larger dataset consisting vegetables fruits closely associated daily life everyone <eos> aiming domestic cooking food management vegfru categorizes vegetables fruits according their eating characteristics each image contains least one edible part vegetables fruits same cooking usage <eos> particularly all image labelled hierarchically <eos> current version covers vegetables fruits upper level categories subordinate classes <eos> contains more than image total least image each subordinate class <eos> accompanying dataset also propose effective framework called hybridnet exploit label hierarchy fgvc <eos> specifically multiple granularity feature first extracted dealing hierarchical labels separately <eos> then they fused through explicit operation <eos> compact bilinear pooling form unified representation ultimate recognition <eos> experimental result novel vegfru public fgvc aircraft cub indicate hybridnet achieves one top performance datasets <eos> dataset code available github <eos> com hshustc vegfru <eos> <eop> increasing cnn robustness occlusions reducing filter support <eos> convolutional neural network cnn provide current state art visual object classification but they far less accurate when classifying partially occluded object <eos> straightforward way improve classification under occlusion conditions train classifier using partially occluded object examples <eos> however training network many combinations object instances occlusions may computationally expensive <eos> work proposes alternative approach increasing robustness cnn occlusion <eos> start studying effect partial occlusions trained cnn show empirically training partially occluded examples reduces spatial support filters <eos> building upon finding argue smaller filter support beneficial occlusion robustness <eos> propose training process uses special regularization term acts shrink spatial support filters <eos> consider three possible regularization terms based second central moments group sparsity mutually reweighted respectively <eos> when trained normal unoccluded examples resulting classifier highly robust occlusions <eos> large training set limited training time proposed classifier even more accurate than standard classifiers trained occluded object examples <eos> <eop> exploiting multi grain ranking constraints precisely searching visually similar vehicles <eos> precise search visually similar vehicles poses great challenge computer vision needs find exactly same vehicle among massive vehicles visually similar appearances given query image <eos> paper model relationship vehicle image multiple grains <eos> following propose two approaches alleviate precise vehicle search problem exploiting multi grain ranking constraints <eos> one generalized pairwise ranking generalizes conventional pairwise considering only binary similar dissimilar relations multiple relations <eos> other multi grain based list ranking introduces permutation probability score permutation multi grain list further optimizes ranking likelihood loss function <eos> implement two approaches multi attribute classification multi task deep learning framework <eos> further facilitate research precise vehicle search also contribute two high quality well annotated vehicle datasets named vd vd collected two different cities diverse annotated attributes <eos> two largest publicly available precise vehicle search datasets they contain vehicle image respectively <eos> experimental result show approaches achieve state art performance both datasets <eos> <eop> recurrent scale approximation object detection cnn <eos> since convolutional neural network cnn lacks inherent mechanism handle large scale variations always need compute feature maps multiple times multi scale object detection bottleneck computational cost practice <eos> address devise recurrent scale approximation rsa compute feature map once only only through map approximate rest maps other levels <eos> core rsa recursive rolling out mechanism given initial map particular scale generates prediction smaller scale half size input <eos> further increase efficiency accuracy design scale forecast network globally predict potential scales image since there no need compute maps all levels pyramid <eos> propose landmark retracing network lrn retrace back locations regressed landmarks generate confidence score each landmark lrn effectively alleviate false positives due accumulated error rsa <eos> whole system could trained end end unified cnn framework <eos> experiments demonstrate proposed algorithm superior against state arts face detection benchmarks achieves comparable result generic proposal generation <eos> source code rsa available github <eos> com sciencefans rsa object detection <eos> <eop> embedding three dimensional geometric feature rigid object part segmentation <eos> object part segmentation challenging fundamental problem computer vision <eos> its difficulties may caused varying viewpoints poses topological structures attributed essential reason <eos> specific object three dimensional model rather than figure <eos> therefore conjecture only appearance feature but also three dimensional geometric feature could helpful <eos> mind propose stream fcn <eos> one stream named appnet extract appearance feature input image <eos> other stream named geonet extract three dimensional geometric feature <eos> however problem input just image <eos> end design convolution based cnn structure extract three dimensional geometric feature three dimensional volume named volnet <eos> then teacher student strategy adopted volnet teaches geonet how extract three dimensional geometric feature image <eos> perform teaching process synthesize training data using three dimensional models <eos> each training sample consists image its corresponding volume <eos> perspective voxelization algorithm further proposed align them <eos> experimental result verify conjecture effectiveness both proposed stream cnn volnet <eos> <eop> towards context aware interaction recognition visual relationship detection <eos> recognizing how object interact each other crucial task visual recognition <eos> if define context interaction object involved then most current method categorized either training single classifier combination interaction its context ii aiming recognize interaction independently its explicit context <eos> both method suffer limitations former scales poorly number combinations fails generalize unseen combinations while latter often leads poor interaction recognition performance due difficulty designing context independent interaction classifier <eos> mitigate drawbacks paper proposes alternative context aware interaction recognition framework <eos> key method explicitly construct interaction classifier combines context interaction <eos> context encoded via word vec into semantic space used derive classification result interaction <eos> proposed method still builds one classifier one interaction per type ii above but classifier built adaptive context via weights context dependent <eos> benefit using semantic space naturally leads zero shot generalizations semantically similar contexts subject object pairs recognized suitable contexts interaction even if they were observed training set <eos> method also scales number interaction context pairs since model parameters increase number interactions <eos> thus method avoids limitation both approaches <eos> demonstrate experimentally proposed framework leads improved performance all investigated interaction representations datasets <eos> <eop> when unsupervised domain adaptation meets tensor representations <eos> domain adaption da allows machine learning method trained data sampled one distribution applied data sampled another <eos> thus great practical importance application such method <eos> despite fact tensor representations widely used computer vision capture multi linear relationships affect data most existing da method applicable vectors only <eos> renders them incapable reflecting preserving important structure many problems <eos> thus propose here learning based method adapt source target tensor representations directly without vectorization <eos> particular set alignment matrices introduced align tensor representations both domains into invariant tensor subspace <eos> alignment matrices tensor subspace modeled joint optimization problem learned adaptively data using proposed alternative minimization scheme <eos> extensive experiments show approach capable preserving discriminative power source domain resisting effects label noise works effectively small sample sizes even one shot da <eos> show method outperforms state art task cross domain visual recognition both efficacy efficiency particularly outperforms all comparators when applied da convolutional activations deep convolutional network <eos> <eop> look listen learn <eos> consider question learnt looking listening large number unlabelled video there valuable but so far untapped source information contained video itself correspondence between visual audio streams introduce novel audio visual correspondence learning task makes use <eos> training visual audio network scratch without any additional supervision other than raw unconstrained video themselves shown successfully solve task more interestingly result good visual audio representations <eos> feature set new state art two sound classification benchmarks perform par state art self supervised approaches imagenet classification <eos> also demonstrate network able localize object both modalities well perform fine grained recognition tasks <eos> <eop> grad cam visual explanations deep network via gradient based localization <eos> propose technique producing visual explanations decisions large class convolutional neural network cnn based models making them more transparent <eos> approach gradient weighted class activation mapping grad cam uses gradients any target concept say logits dog even caption flowing into final convolutional layer produce coarse localization map highlighting important region image predicting concept <eos> unlike previous approaches grad cam applicable wide variety cnn model families cnn fully connected layer <eos> vgg cnn used structured outputs <eos> captioning cnn used tasks multi modal inputs <eos> vqa reinforcement learning needs no architectural changes re training <eos> combine grad cam existing fine grained visualizations create high resolution class discriminative visualization apply image classification image captioning visual question answering vqa models including resnet based architectures <eos> context image classification models visualizations lend insights into failure modes models showing seemingly unreasonable predictions reasonable explanations outperform previous method ilsvrc weakly supervised localization task more faithful underlying model help achieve model generalization identifying dataset bias <eos> image captioning vqa visualizations show even non attention based models localize inputs <eos> finally design conduct human studies measure if grad cam explanations help users establish appropriate trust predictions deep network show grad cam helps untrained users successfully discern stronger deep network weaker one even when both make identical predictions <eos> code available github <eos> com ramprs grad cam along demo cloudcv video youtu <eos> <eop> image based localization using lstms structured feature correlation <eos> work propose new cnn lstm architecture camera pose regression indoor outdoor scenes <eos> cnn allow learn suitable feature representations localization robust against motion blur illumination changes <eos> make use lstm units cnn output play role structured dimensionality reduction feature vector leading drastic improvements localization performance <eos> provide extensive quantitative comparison cnn based sift based localization method showing weaknesses strengths each <eos> furthermore present new large scale indoor dataset accurate ground truth laser scanner <eos> experimental result both indoor outdoor public datasets show method outperforms existing deep architectures localize image hard conditions <eos> presence mostly textureless surfaces classic sift based method fail <eos> <eop> personalized image aesthetics <eos> automatic image aesthetics rating received growing interest recent breakthrough deep learning <eos> although many studies exist learning generic universal aesthetics model investigation aesthetics models incorporating individual user preference quite limited <eos> address personalized aesthetics problem showing individual aesthetic preferences exhibit strong correlations content aesthetic attributes hence deviation individual perception generic image aesthetics predictable <eos> accommodate study first collect two distinct datasets large image dataset flickr annotated amazon mechanical turk small dataset real personal albums rated owners <eos> then propose new approach personalized aesthetics learning trained even small set annotated image user <eos> approach based residual based model adaptation scheme learns offset compensate generic aesthetics score <eos> finally introduce active learning algorithm optimize personalized aesthetics prediction real world application scenarios <eos> experiments demonstrate approach effectively learn personalized aesthetics preferences outperforms existing method quantitative comparisons <eos> <eop> predicting deeper into future semantic segmentation <eos> ability predict therefore anticipate future important attribute intelligence <eos> also utmost importance real time systems <eos> robotics autonomous driving depend visual scene understanding decision making <eos> while prediction raw rgb pixel values future video frames studied previous work here introduce novel task predicting semantic segmentations future frames <eos> given sequence video frames goal predict segmentation maps yet observed video frames lie up second further future <eos> develop autoregressive convolutional neural network learns iteratively generate multiple frames <eos> result cityscapes dataset show directly predicting future segmentations substantially better than predicting then segmenting future rgb frames <eos> prediction result up half second future visually convincing much more accurate than baseline based warping semantic segmentations using optical flow <eos> <eop> coordinating filters faster deep neural network <eos> very large scale deep neural network dnns achieved remarkable successes large variety computer vision tasks <eos> however high computation intensity dnns makes challenging deploy models resource limited systems <eos> some studies used low rank approaches approximate filters low rank basis accelerate testing <eos> works directly decomposed pre trained dnns low rank approximations lra <eos> how train dnns toward lower rank space more efficient dnns however remains open area <eos> solve issue work propose force regularization uses attractive forces enforce filters so coordinate more weight information into lower rank space <eos> mathematically empirically verify after applying technique standard lra method reconstruct filters using much lower basis thus result faster dnns <eos> effectiveness approach comprehensively evaluated resnets alexnet googlenet <eos> alexnet example force regularization gains speedup modern gpu without accuracy loss <eos> speedup cpu paying small accuracy degradation <eos> moreover force regularization better initializes low rank dnns such fine tuning converge faster toward higher accuracy <eos> obtained lower rank dnns further sparsified proving force regularization integrated state art sparsity based acceleration method <eos> <eop> unsupervised representation learning sorting sequences <eos> present unsupervised representation learning approach using video without semantic labels <eos> leverage temporal coherence supervisory signal formulating representation learning sequence sorting task <eos> take temporally shuffled frames <eos> non chronological order inputs train convolutional neural network sort shuffled sequences <eos> similar comparison based sorting algorithms propose extract feature all frame pairs aggregate them predict correct order <eos> sorting shuffled image sequence requires understanding statistical temporal structure image training such proxy task allows learn rich generalizable visual representation <eos> validate effectiveness learned representation using method pre training high level recognition problems <eos> experimental result show method compares favorably against state art method action recognition image classification object detection tasks <eos> <eop> read write memory network movie story understanding <eos> propose novel memory network model named read write memory network rwmn perform question answering tasks large scale multimodal movie story understanding <eos> key focus rwmn model design read network write network consist multiple convolutional layer enable memory read write operations high capacity flexibility <eos> while existing memory augmented network models treat each memory slot independent block use multi layered cnn allows model read write sequential memory cells chunks more reasonable represent sequential story because adjacent memory blocks often strong correlations <eos> evaluation apply model all six tasks movieqa benchmark achieve best accuracies several tasks especially visual qa task <eos> model shows potential better understand only content story but also more abstract information such relationships between characters reasons their actions <eos> <eop> segflow joint learning video object segmentation optical flow <eos> paper proposes end end trainable network segflow simultaneously predicting pixel wise object segmentation optical flow video <eos> proposed segflow two branches useful information object segmentation optical flow propagated bidirectionally unified framework <eos> segmentation branch based fully convolutional network proved effective image segmentation task optical flow branch takes advantage flownet model <eos> unified framework trained iteratively offline learn generic notion fine tuned online specific object <eos> extensive experiments both video object segmentation optical flow datasets demonstrate introducing optical flow improves performance segmentation vice versa against state art algorithms <eos> <eop> unsupervised action discovery localization video <eos> paper first address problem unsupervised action localization video <eos> given unlabeled data without bounding box annotations propose novel approach discovers action class labels spatio temporally localizes actions video <eos> begins computing local video feature apply spectral clustering set unlabeled training video <eos> each cluster video undirected graph constructed extract dominant set known high internal homogeneity inhomogeneity between vertices outside <eos> next discriminative clustering approach applied training classifier each cluster iteratively select video non dominant set obtain complete video action classes <eos> once classes discovered training video within each cluster selected perform automatic spatio temporal annotations first oversegmenting video each discovered class into supervoxels constructing directed graph apply variant knapsack problem temporal constraints <eos> knapsack optimization jointly collects subset supervoxels enforcing annotated action spatio temporally connected its volume size actor <eos> annotations used train svm action classifiers <eos> during testing actions localized using similar knapsack approach supervoxels grouped together svm learned using video discovered action classes used recognize actions <eos> evaluate approach ucf sports sub jhmdb jhmdb thumos ucf datasets <eos> experiments suggest despite using no action class labels no bounding box annotations able get competitive result state art supervised method <eos> <eop> dense captioning events video <eos> most natural video contain numerous events <eos> example video man playing piano video might also contain another man dancing crowd clapping <eos> introduce task dense captioning events involves both detecting describing events video <eos> propose new model able identify all such events single pass video while simultaneously describing detected events natural language <eos> model introduces variant existing proposal module designed capture both short well long events span minutes <eos> capture dependencies between events video model introduces new captioning module uses contextual information past future events jointly describe all events <eos> also introduce activitynet captions large scale benchmark dense captioning events <eos> activitynet captions contains video amounting video hours total descriptions each unique start end time <eos> finally report performances model dense captioning events video retrieval localization <eos> <eop> learning long term dependencies action recognition biologically inspired deep network <eos> despite lot research efforts devoted recent years how efficiently learn long term dependencies sequences still remains pretty challenging task <eos> one key models sequence learning recurrent neural network rnn its variants such long short term memory lstm gated recurrent unit gru still powerful enough practice <eos> one possible reason they only feedforward connections different biological neural system typically composed both feedforward feedback connections <eos> address problem paper proposes biologically inspired deep network called shuttlenet <eos> technologically shuttlenet consists several processors each gru while associated multiple groups hidden states <eos> unlike traditional rnns all processors inside shuttlenet loop connected mimic brain feedforward feedback connections they shared across multiple pathways loop connection <eos> attention mechanism then employed select best information flow pathway <eos> extensive experiments conducted two benchmark datasets <eos> ucf hmdb show beat state art method simply embedding shuttlenet into cnn rnn framework <eos> <eop> compressive quantization fast object instance search video <eos> most current visual search systems focus image image point point search such image object retrieval <eos> nevertheless fast image video point set search much less exploited <eos> paper tackles object instance search video efficient point set matching essential <eos> through jointly optimizing vector quantization hashing propose compressive quantization method compress object proposals extracted each video into only binary codes <eos> then similarity between query object whole video determined hamming distance between query binary code video best matched binary code <eos> compressive quantization only enables fast search but also significantly reduces memory cost storing video feature <eos> despite high compression ratio proposed compressive quantization still effectively retrieve small object large video datasets <eos> systematic experiments three benchmark datasets verify effectiveness efficiency compressive quantization <eos> <eop> complex event detection identifying reliable shots untrimmed video <eos> goal complex event detection automatically detect whether event interest happens temporally untrimmed long video usually consist multiple video shots <eos> observing some video shots positive resp <eos> negative video irrelevant resp <eos> relevant given event class formulate task multi instance learning mil problem taking each video bag video shots each video instances <eos> end propose new mil method simultaneously learns linear svm classifier infers binary indicator each instance order select reliable training instances each positive negative bag <eos> new objective function balance weighted training errors mixed norm regularization term adaptively selects reliable shots training instances different video them diverse possible <eos> also develop alternating optimization approach efficiently solve proposed objective function <eos> extensive experiments challenging real world multimedia event detection med datasets medtest medtest ccv clearly demonstrate effectiveness proposed mil approach complex event detection <eos> <eop> deep direct regression multi oriented scene text detection <eos> paper first provide new perspective divide existing high performance object detection method into direct indirect regressions <eos> direct regression performs boundary regression predicting offsets given point while indirect regression predicts offsets some bounding box proposals <eos> context multi oriented scene text detection analyze drawbacks indirect regression covers state art detection structures faster rcnn ssd instances point out potential superiority direct regression <eos> verify point view propose deep direct regression based method multi oriented scene text detection <eos> detection framework simple effective fully convolutional network one step post processing <eos> fully convolutional network optimized end end way bi task outputs one pixel wise classification between text non text other direct regression determine vertex coordinates quadrilateral text boundaries <eos> proposed method particularly beneficial localize incidental scene texts <eos> icdar incidental scene text benchmark method achieves measure new state art significantly outperforms previous approaches <eos> other standard datasets focused scene texts method also reaches state art performance <eos> <eop> open set domain adaptation <eos> when training test data belong different domains accuracy object classifier significantly reduced <eos> therefore several algorithms proposed last years diminish so called domain shift between datasets <eos> however all available evaluation protocols domain adaptation describe closed set recognition task both domains namely source target contain exactly same object classes <eos> work also explore field domain adaptation open set more realistic scenario only few categories interest shared between source target data <eos> therefore propose method fits both closed open set scenarios <eos> approach learns mapping source target domain jointly solving assignment problem labels target instances potentially belong categories interest present source dataset <eos> thorough evaluation shows approach outperforms state art <eos> <eop> deformable convolutional network <eos> convolutional neural network cnn inherently limited model geometric transformations due fixed geometric structures its building modules <eos> work introduce two new modules enhance transformation modeling capacity cnn namely deformable convolution deformable roi pooling <eos> both based idea augmenting spatial sampling locations modules additional offsets learning offsets target tasks without additional supervision <eos> new modules readily replace their plain counterparts existing cnn easily trained end end standard back propagation giving rise deformable convolutional network <eos> extensive experiments validate effectiveness approach sophisticated vision tasks object detection semantic segmentation <eos> code would released <eos> <eop> ensemble diffusion retrieval <eos> postprocessing procedure diffusion process demonstrated its ability substantially improving performance various visual retrieval systems <eos> whereas great efforts also devoted similarity metric fusion seeing only one individual type similarity cannot fully reveal intrinsic relationship between object <eos> stimulates great research interest considering similarity fusion framework diffusion process <eos> fusion diffusion robust retrieval <eos> paper firstly revisit representative method about fusion diffusion provide new insights ignored previous researchers <eos> then observing existing algorithms susceptible noisy similarities proposed regularized ensemble diffusion red bundled automatic weight learning paradigm so negative impacts noisy similarities suppressed <eos> last integrate several recently proposed similarities proposed framework <eos> experimental result suggest achieve new state art performances various retrieval tasks including three dimensional shape retrieval modelnet dataset image retrieval holidays ukbench dataset <eos> <eop> foveanet perspective aware urban scene parsing <eos> parsing urban scene image critical self driving <eos> most current solutions employ generic image parsing models treat all scales locations image equally consider geometry property car captured urban scene image <eos> thus they suffer heterogeneous object scales caused perspective projection cameras actual scenes inevitably encounter parsing failures distant object well other boundary recognition errors <eos> work propose new foveanet model fully exploit perspective geometry scene image address common failures generic parsing models <eos> foveanet estimates perspective geometry scene image through convolutional network integrates supportive evidence contextual object within image <eos> based perspective geometry information foveanet undoes camera perspective projection analyzing region space actual scene thus provides much more reliable parsing result <eos> furthermore effectively address recognition errors foveanet introduces new dense crf model takes perspective geometry prior potential <eos> evaluate foveanet two urban scene parsing datasets cityspaces camvid demonstrates foveanet outperform all well established baselines provide new state art performance <eos> <eop> beyond planar symmetry modeling human perception reflection rotation symmetries wild <eos> humans take advantage real world symmetries various tasks yet capturing their superb symmetry perception mechanism computational model remains elusive <eos> motivated new study demonstrating extremely high inter person accuracy human perceived symmetries wild constructed first deep learning neural network reflection rotation symmetry detection sym net trained photos ms coco microsoft common object context dataset nearly consistent symmetry labels more than human observers <eos> employ novel method convert discrete human labels into symmetry heatmaps capture symmetry densely image quantitatively evaluate sym net against multiple existing computer vision algorithms <eos> cvpr symmetry competition testsets unseen ms coco photos sym net significantly outperforms all other competitors <eos> beyond mathematically well defined symmetries plane sym net demonstrates abilities identify viewpoint varied three dimensional symmetries partially occluded symmetrical object symmetries semantic level <eos> <eop> learning reason end end module network visual question answering <eos> natural language questions inherently compositional many most easily answered reasoning about their decomposition into modular sub problems <eos> example answer there equal number balls boxes look balls look boxes count them compare result <eos> recently proposed neural module network nmn architecture implements approach question answering parsing questions into linguistic substructures assembling question specific deep network smaller modules each solve one subtask <eos> however existing nmn implementations rely brittle off shelf parsers restricted module configurations proposed parsers rather than learning them data <eos> paper propose end end module network nmns learn reason directly predicting instance specific network layouts without aid parser <eos> model learns generate network structures imitating expert demonstrations while simultaneously learning network parameters using downstream task loss <eos> experimental result new clevr dataset targeted compositional question answering show nmns achieve error reduction nearly relative state art attentional approaches while discovering interpretable network architectures specialized each question <eos> <eop> hard aware deeply cascaded embedding <eos> riding waves deep neural network deep metric learning achieved promising result various tasks using triplet network siamese network <eos> though basic goal making image same category closer than ones different categories intuitive hard optimize objective directly due quadratic cubic sample size <eos> hard example mining widely used solve problem spends expensive computation subset sample considered hard <eos> however hard defined relative specific model <eos> then complex models will treat most sample easy ones vice versa simple models both good training <eos> difficult define model just right complexity choose hard examples adequately different sample diverse hard levels <eos> motivates propose novel framework named hard aware deeply cascaded embedding hdc ensemble set models different complexities cascaded manner mine hard examples multiple levels <eos> sample judged series models increasing complexities only updates models consider sample hard case <eos> hdc evaluated cars cub stanford online products vehicleid deepfashion datasets outperforms state art method large margin <eos> <eop> query guided regression network context policy phrase grounding <eos> given textual description image phrase grounding localizes object image referred query phrases description <eos> state art method address problem ranking set proposals based relevance each query limited performance independent proposal generation systems ignore useful cues context description <eos> paper adopt spatial regression method break performance limit introduce reinforcement learning techniques further leverage semantic context information <eos> propose novel query guided regression network context policy qrc net jointly learns proposal generation network pgn query guided regression network qrn context policy network cpn <eos> experiments show qrc net provides significant improvement accuracy two popular datasets flickr entities referit game <eos> increase over state arts respectively <eos> <eop> subic supervised structured binary code image search <eos> large scale visual search highly compressed yet meaningful representations image essential <eos> structured vector quantizers based product quantization its variants usually employed achieve such compression while minimizing loss accuracy <eos> yet unlike binary hashing schemes unsupervised method yet benefited supervision end end learning novel architectures ushered deep learning revolution <eos> hence propose herein novel method make deep convolutional neural network produce supervised compact structured binary codes visual search <eos> method makes use novel block softmax non linearity batch based entropy losses together induce structure learned encodings <eos> show method outperforms state art compact representations based deep hashing structured quantization single cross domain category retrieval instance retrieval classification <eos> make code models publicly available online <eos> <eop> revisiting unreasonable effectiveness data deep learning era <eos> success deep learning vision attributed models high capacity increased computational power availability large scale labeled data <eos> since there significant advances representation capabilities models computational capabilities gpus <eos> but size biggest dataset surprisingly remained constant <eos> will happen if increase dataset size paper takes step towards clearing clouds mystery surrounding relationship between enormous data visual deep learning <eos> exploiting jft dataset more than noisy labels image investigate how performance current vision tasks would change if data was used representation learning <eos> paper delivers some surprising some expected findings <eos> first find performance vision tasks increases logarithmically based volume training data size <eos> second show representation learning pre training still holds lot promise <eos> one improve performance many vision tasks just training better base model <eos> finally expected present new state art result different vision tasks including image classification object detection semantic segmentation human pose estimation <eos> sincere hope inspires vision community undervalue data develop collective efforts building larger datasets <eos> <eop> generative model people clothing <eos> present first image based generative model people clothing full body <eos> sidestep commonly used complex graphics rendering pipeline need high quality three dimensional scans dressed people <eos> instead learn generative models large image database <eos> main challenge cope high variance human pose shape appearance <eos> reason pure image based approaches considered so far <eos> show challenge overcome splitting generating process two parts <eos> first learn generate semantic segmentation body clothing <eos> second learn conditional model resulting segments creates realistic image <eos> full model differentiable conditioned pose shape color <eos> result sample people different clothing items styles <eos> proposed model generate entirely new people realistic clothing <eos> several experiments present encouraging result suggest entirely data driven approach people generation possible <eos> <eop> escape cells deep kd network recognition three dimensional point cloud models <eos> present new deep learning architecture called kd network designed three dimensional model recognition tasks works unstructured point clouds <eos> new architecture performs multiplicative transformations shares parameters transformations according subdivisions point clouds imposed onto them kd trees <eos> unlike currently dominant convolutional architectures usually require rasterization uniform two dimensional three dimensional grids kd network rely such grids any way therefore avoid poor scaling behavior <eos> series experiments popular shape recognition benchmarks kd network demonstrate competitive performance number shape recognition tasks such shape classification shape retrieval shape part segmentation <eos> <eop> improved image captioning via policy gradient optimization spider <eos> current image captioning method usually trained via maximum likelihood estimation <eos> however log likelihood score caption correlate well human assessments quality <eos> standard syntactic evaluation metrics such bleu meteor rouge also well correlated <eos> newer spice cider metrics better correlated but traditionally hard optimize <eos> paper show how use policy gradient pg method directly optimize linear combination spice cider combination call spider spice score ensures captions semantically faithful image while cider score ensures captions syntactically fluent <eos> pg method propose improves prior mixer approach using monte carlo rollouts instead mixing mle training pg <eos> show empirically algorithm leads easier optimization improved result compared mixer <eos> finally show using pg method optimize any metrics including proposed spider metric result image captions strongly preferred human raters compared captions generated same model but trained optimize mle coco metrics <eos> <eop> rolling shutter correction manhattan world <eos> vast majority consumer cameras operate rolling shutter mechanism often produces distorted image due inter row delay while capturing image <eos> recent method monocular rolling shutter compensation utilize blur kernel straightness line segments well angle length preservation <eos> however they incorporate scene geometry explicitly rolling shutter correction therefore information about three dimensional scene geometry often distorted correction process <eos> paper propose novel method leverages geometric properties scene particular vanishing directions estimate camera motion during rolling shutter exposure single distorted image <eos> proposed method jointly estimates orthogonal vanishing directions rolling shutter camera motion <eos> performed extensive experiments synthetic real datasets demonstrate benefits approach both terms qualitative quantitative result terms geometric structure fitting well respect computation time <eos> <eop> local global point cloud registration using dictionary viewpoint descriptors <eos> local global point cloud registration challenging task due substantial differences between two types data different techniques used acquire them <eos> global clouds cover large scale environments usually acquired aerially <eos> three dimensional modeling city using airborne laser scanning als <eos> contrast local clouds often acquired ground level much smaller range example using terrestrial laser scanning tls <eos> differences often manifested point density distribution occlusions nature measurement noise <eos> result differences existing point cloud registration approaches such keypoint based registration tend fail <eos> improve upon different approach recently proposed based converting global cloud into viewpoint based cloud dictionary <eos> propose local global registration method replace dictionary clouds viewpoint descriptors consisting panoramic range image <eos> then use efficient dictionary search discrete fourier transform dft domain using phase correlation rapidly find plausible transformations local global reference frame <eos> demonstrate method significant advantages over previous cloud dictionary approach terms computational efficiency memory requirements <eos> addition show its superior registration performance comparison state art keypoint based method fpfh <eos> evaluation use challenging dataset tls local clouds als large scale global cloud urban environment <eos> <eop> prnn generating shape primitives recurrent neural network <eos> success various applications including robotics digital content creation visualization demand structured abstract representation three dimensional world limited sensor data <eos> inspired nature human perception three dimensional shapes collection simple parts explore such abstract shape representation based primitives <eos> given single depth image object present three dimensional prnn generative recurrent neural network synthesizes multiple plausible shapes composed set primitives <eos> generative model encodes symmetry characteristics common man made object preserves long range structural coherence describes object varying complexity compact representation <eos> also propose method based gaussian fields generate large scale dataset primitive based shape representations train network <eos> evaluate approach wide range examples show outperforms nearest neighbor based shape retrieval method par voxel based generative models while using significantly reduced parameter space <eos> <eop> bodyfusion real time capture human motion surface geometry using single depth camera <eos> propose bodyfusion novel real time geometry fusion method track reconstruct non rigid surface motion human performance using single consumer grade depth camera <eos> reduce ambiguities non rigid deformation parameterization surface graph nodes take advantage internal articulated motion prior human performance contribute skeleton embedded surface fusion ssf method <eos> key feature method jointly solves both skeleton graph node deformations based information attachments between skeleton graph nodes <eos> attachments also updated frame frame based fused surface geometry computed deformations <eos> overall method enables increasingly denoised detailed complete surface reconstruction well updating skeleton attachments temporal depth frames fused <eos> experimental result show method exhibits substantially improved nonrigid motion fusion performance tracking robustness compared previous state art fusion method <eos> also contribute dataset quantitative evaluation fusion based dynamic scene reconstruction algorithms using single depth camera <eos> <eop> quasiconvex plane sweep triangulation outliers <eos> triangulation fundamental task three dimensional computer vision <eos> unsurprisingly well investigated problem many mature algorithms <eos> however algorithms robust triangulation necessary produce correct result presence egregiously incorrect measurements <eos> outliers received much less attention <eos> default approach deal outliers triangulation random sampling <eos> randomized heuristic only suboptimal could fact computationally inefficient large scale datasets <eos> paper propose novel locally optimal algorithm robust triangulation <eos> key feature method efficiently derive local update step plane sweeping set quasiconvex functions <eos> underpinning method new theory behind quasiconvex plane sweep examined previously computational geometry <eos> relative random sampling heuristic algorithm only guarantees deterministic convergence local minimum typically achieves higher quality solutions similar runtimes <eos> <eop> maximizing rigidity revisited convex programming approach generic three dimensional shape reconstruction multiple perspective views <eos> rigid structure motion rsfm non rigid structure motion nrsfm long treated literature separate different problems <eos> inspired previous work solved directly three dimensional scene structure factoring relative camera poses out revisit principle maximizing rigidity structure motion literature develop unified theory applicable both rigid non rigid structure reconstruction rigidity agnostic way <eos> formulate problems convex semi definite program imposing constraints seek apply principle minimizing non rigidity <eos> result demonstrate efficacy approach state art accuracy various three dimensional reconstruction problems <eos> <eop> surface registration via foliation <eos> work introduces novel surface registration method based foliation <eos> foliation decomposes surface into family closed loops such decomposition local tensor product structure <eos> projecting each loop point surface collapsed into graph <eos> two homeomorphic surfaces consistent foliations registered first matching their foliation graphs then matching corresponding leaves <eos> foliation based method capable handling surfaces complicated topologies large non isometric deformations rigorous solid theoretic foundation easy implement robust compute <eos> result mapping diffeomorphic <eos> experimental result show efficiency efficacy proposed method <eos> <eop> rolling shutter aware differential sfm image rectification <eos> paper develop modified differential structure motion sfm algorithm estimate relative pose two frames despite rolling shutter rs artifacts <eos> particular show under constant velocity assumption errors induced rolling shutter effect easily rectified linear scaling operation each optical flow <eos> further propose point algorithm recover relative pose rolling shutter camera undergoes constant acceleration motion <eos> demonstrate dense depth maps recovered relative pose rs camera used rs aware warping image rectification recover high quality global shutter gs image <eos> experiments both synthetic real rs image show rs aware differential sfm algorithm produces more accurate result relative pose estimation three dimensional reconstruction image distorted rs effect compared standard sfm algorithms assume gs camera model <eos> also demonstrate rs aware warping image rectification method outperforms state art commercial software products <eos> adobe after effects apple imovie removing rs artifacts <eos> <eop> corner based geometric calibration multi focus plenoptic cameras <eos> propose method geometric calibration multi focus plenoptic cameras using raw image <eos> multi focus plenoptic cameras feature several types micro lenses spatially aligned front camera sensor generate micro image different magnifications <eos> multi lens arrangement provides computational photography benefits but complicates calibration <eos> methodology achieves detection type micro lenses retrieval their spatial arrangement estimation intrinsic extrinsic camera parameters therefore fully characterising specialised camera class <eos> motivated classic pinhole camera calibration presented algorithm operates based checker board corners retrieved custom micro image corner detector <eos> approach enables introduction re projection error used minimisation framework <eos> algorithm compares favourably state art demonstrated controlled free hand experiments making first step towards accurate three dimensional reconstruction structure motion <eos> <eop> focal track depth accommodation oscillating lens deformation <eos> focal track sensor monocular computationally efficient depth sensor based defocus controlled liquid membrane lens <eos> synchronizes small lens oscillations photosensor produce real time depth maps means differential defocus couples oscillations bigger lens deformations adapt defocus working range track object over large axial distances <eos> create focal track sensor derive texture invariant family equations relate image derivatives scene depth when lens changes its focal length differentially <eos> based equations design feed forward sequence computations robustly incorporates image derivatives multiple scales produces confidence maps along depth trained end end mitigate against noise aberrations other non idealities <eos> prototype inch optics produces depth confidence maps frames per second over axial range more than cm <eos> <eop> reconfiguring imaging pipeline computer vision <eos> advancements deep learning ignited explosion research efficient hardware embedded computer vision <eos> hardware vision acceleration however address cost capturing processing image data feeds algorithms <eos> examine role image signal processing isp pipeline computer vision identify opportunities reduce computation save energy <eos> key insight imaging pipelines should designed configurable switch between traditional photography mode low power vision mode produces lower quality image data suitable only computer vision <eos> use eight computer vision algorithms reversible pipeline simulation tool study imaging system impact vision performance <eos> both cnn based classical vision algorithms observe only two isp stages demosaicing gamma compression critical task performance <eos> propose new image sensor design compensate skipping stages <eos> sensor design feature adjustable resolution tunable analog digital converters adcs <eos> proposed imaging system vision mode disables isp entirely configures sensor produce subsampled lower precision image data <eos> vision mode save average energy baseline photography mode while having only small impact vision task accuracy <eos> <eop> catadioptric hyperspectral light field imaging <eos> complete plenoptic function records radiance rays every location every angle every wavelength every time <eos> signal multi dimensional long relied multi modal sensing such hybrid light field camera arrays <eos> paper present single camera hyperspectral light field imaging solution call snapshot plenoptic imager spi <eos> spi uses spectral coded catadioptric mirror arrays simultaneously acquiring spatial angular spectral dimensions <eos> further apply learning based approach improve spectral resolution very few measurements <eos> specifically demonstrate then employ new spectral sparsity prior allows hyperspectral profiles sparsely represented under pre trained dictionary <eos> comprehensive experiments synthetic real data show technique effective reliable accurate <eos> particular able produce first wide fov multi spectral light field database <eos> <eop> cross view asymmetric metric learning unsupervised person re identification <eos> while metric learning important person re identification re id significant problem visual surveillance cross view pedestrian matching existing metric models re id mostly based supervised learning requires quantities labeled sample all pairs camera views training <eos> however limits their scalabilities realistic applications large amount data over multiple disjoint camera views available but labelled <eos> overcome problem propose unsupervised asymmetric metric learning model unsupervised re id <eos> model aims learn asymmetric metric <eos> specific projection each view effectively based clustering cross view person image <eos> model finds shared space view specific bias alleviated thus better matching performance achieved <eos> extensive experiments conducted baseline five large scale re id datasets demonstrate effectiveness proposed model <eos> through comparison show unsupervised asymmetric metric model works much more suitable unsupervised re id compared classical unsupervised metric learning models <eos> also compare existing unsupervised re id method model outperforms them notable margins especially report performance large scale unlabelled re id dataset unfortunately less concerned literatures <eos> <eop> real time eye gaze tracking three dimensional deformable eye face model <eos> three dimensional model based gaze estimation method widely explored because their good accuracy ability handle free head movement <eos> traditional method complex hardware systems eg <eos> infrared lights three dimensional sensors etc <eos> restricted controlled environments significantly limit their practical utilities <eos> paper propose three dimensional model based gaze estimation method single web camera enables instant portable eye gaze tracking <eos> key idea leverage proposed three dimensional eye face model estimate three dimensional eye gaze observed facial landmarks <eos> proposed system includes three dimensional deformable eye face model learned offline multiple training subjects <eos> given deformable model individual three dimensional eye face models personal eye parameters recovered through unified calibration algorithm <eos> experimental result show proposed method outperforms state art method while allowing convenient system setup free head movement <eos> real time eye tracking system running fps also validates effectiveness efficiency proposed method <eos> <eop> ensemble deep learning skeleton based action recognition using temporal sliding lstm network <eos> paper addresses problems feature representation skeleton joints modeling temporal dynamics recognize human actions <eos> traditional method generally use relative coordinate systems dependent some joints model only long term dependency while excluding short term medium term dependencies <eos> instead taking raw skeletons input transform skeletons into another coordinate system obtain robustness scale rotation translation then extract salient motion feature them <eos> considering long short term memory lstm network various time step sizes model various attributes well propose novel ensemble temporal sliding lstm ts lstm network skeleton based action recognition <eos> proposed network composed multiple parts containing short term medium term long term ts lstm network respectively <eos> network utilize average ensemble among multiple parts final feature capture various temporal dependencies <eos> evaluate proposed network additional other architectures verify effectiveness proposed network also compare them several other method five challenging datasets <eos> experimental result demonstrate network models achieve state art performance through various temporal feature <eos> additionally analyze relation between recognized actions multi term ts lstm feature visualizing softmax feature multiple parts <eos> <eop> how far solving three dimensional face alignment problem dataset three dimensional facial landmarks <eos> paper investigates how far very deep neural network attaining close saturating performance existing three dimensional face alignment datasets <eos> end make following contributions construct first time very strong baseline combining state art architecture landmark localization state art residual block train very large yet synthetically expanded facial landmark dataset finally evaluate all other facial landmark datasets <eos> create guided landmarks network converts landmark annotations three dimensional unifies all existing datasets leading creation ls largest most challenging three dimensional facial landmark dataset date image <eos> following train neural network three dimensional face alignment evaluate newly introduced ls <eos> further look into effect all traditional factors affecting face alignment performance like large pose initialization resolution introduce new one namely size network <eos> show both three dimensional face alignment network achieve performance remarkable accuracy probably close saturating datasets used <eos> training testing code well dataset downloaded www <eos> com face alignment <eop> large pose three dimensional face reconstruction single image via direct volumetric cnn regression <eos> three dimensional face reconstruction fundamental computer vision problem extraordinary difficulty <eos> current systems often assume availability multiple facial image sometimes same subject input must address number methodological challenges such establishing dense correspondences across large facial poses expressions non uniform illumination <eos> general method require complex inefficient pipelines model building fitting <eos> work propose address many limitations training convolutional neural network cnn appropriate dataset consisting image three dimensional facial models scans <eos> cnn works just single facial image require accurate alignment nor establishes dense correspondence between image works arbitrary facial poses expressions used reconstruct whole three dimensional facial geometry including non visible parts face bypassing construction during training fitting during testing three dimensional morphable model <eos> achieve via simple cnn architecture performs direct regression volumetric representation three dimensional facial geometry single image <eos> also demonstrate how related task facial landmark localization incorporated into proposed framework help improve reconstruction quality especially cases large poses facial expressions <eos> <eop> rankiqa learning rankings no reference image quality assessment <eos> propose no reference image quality assessment nr iqa approach learns rankings rankiqa <eos> address problem limited iqa dataset size train siamese network rank image terms image quality using synthetically generated distortions relative image quality known <eos> ranked image set automatically generated without laborious human labeling <eos> then use fine tuning transfer knowledge represented trained siamese network traditional cnn estimates absolute image quality single image <eos> demonstrate how approach made significantly more efficient than traditional siamese network forward propagating batch image through single network backpropagating gradients derived all pairs image batch <eos> experiments tid benchmark show improve state art over <eos> furthermore live benchmark show approach superior existing nr iqa techniques even outperform state art full reference iqa fr iqa method without having resort high quality reference image infer iqa <eos> <eop> look perceive segment finding salient object image via two stream fixation semantic cnn <eos> recently cnn based models achieved remarkable success image based salient object detection sod <eos> models key issue find proper network architecture best fits task sod <eos> toward end paper proposes two stream fixation semantic cnn whose architecture inspired fact salient object complex image unambiguously annotated selecting pre segmented semantic object receive highest fixation density eye tracking experiments <eos> two stream cnn fixation stream pre trained eye tracking data whose architecture well fits task fixation prediction semantic stream pre trained image semantic tags proper architecture semantic perception <eos> fusing two streams into inception segmentation module jointly fine tuning them image manually annotated salient object proposed network show impressive performance segmenting salient object <eos> experimental result show approach outperforms state art models deep non deep datasets <eos> <eop> delving into salient object subitizing detection <eos> subitizing <eos> instant judgement number detection salient object human inborn abilities <eos> two tasks influence each other human visual system <eos> paper delve into complementarity two tasks <eos> propose multi task deep neural network weight prediction salient object detection parameters adaptive weight layer dynamically determined auxiliary subitizing network <eos> numerical representation salient object therefore embedded into spatial representation <eos> proposed joint network trained end end using back propagation <eos> experiments show proposed multi task network outperforms existing multi task architectures auxiliary subitizing network provides strong guidance salient object detection reducing false positives producing coherent saliency maps <eos> moreover proposed method unconstrained method able handle image without salient object <eos> finally show state theart performance different salient object datasets <eos> <eop> learning discriminative data fitting functions blind image deblurring <eos> solving blind image deblurring usually requires defining data fitting function image priors <eos> while existing algorithms mainly focus developing image priors blur kernel estimation non blind deconvolution only few method consider effect data fitting functions <eos> contrast state art method use single fixed data fitting term propose data driven approach learn effective data fitting functions large set motion blurred image associated ground truth blur kernels <eos> learned data fitting function facilitates estimating accurate blur kernels generic image domain specific problems corresponding image priors <eos> addition extend learning approach data fitting function latent image restoration non uniform deblurring <eos> extensive experiments challenging motion blurred image demonstrate proposed algorithm performs favorably against state art method <eos> <eop> video deblurring via semantic segmentation pixel wise non linear kernel <eos> video deblurring challenging problem blur complex usually caused combination camera shakes object motions depth variations <eos> optical flow used kernel estimation since predicts motion trajectories <eos> however estimates often inaccurate complex scenes object boundaries crucial kernel estimation <eos> paper exploit semantic segmentation each blurry frame understand scene contents use different motion models image region guide optical flow estimation <eos> while existing pixel wise blur models assume blur kernel same optical flow during exposure time assumption hold when motion blur trajectory pixel different estimated linear optical flow <eos> analyze relationship between motion blur trajectory optical flow present novel pixel wise non linear kernel model account motion blur <eos> proposed blur model based non linear optical flow describes complex motion blur more effectively <eos> extensive experiments challenging blurry video demonstrate proposed algorithm performs favorably against state art method <eos> <eop> demand learning deep image restoration <eos> while machine learning approaches image restoration offer great promise current method risk training models fixated performing well only image corruption particular level difficulty such certain level noise blur <eos> first examine weakness conventional fixated models demonstrate training general models handle arbitrary levels corruption indeed non trivial <eos> then propose demand learning algorithm training image restoration models deep convolutional neural network <eos> main idea exploit feedback mechanism self generate training instances they needed most thereby learning models generalize across difficulty levels <eos> four restoration tasks image inpainting pixel interpolation image deblurring image denoising three diverse datasets approach consistently outperforms both status quo training procedure curriculum learning alternatives <eos> <eop> multi channel weighted nuclear norm minimization real color image denoising <eos> most existing denoising algorithms developed grayscale image <eos> trivial extend them color image denoising since noise statistics channels very different real noisy image <eos> paper propose multi channel mc optimization model real color image denoising under weighted nuclear norm minimization wnnm framework <eos> concatenate rgb patches make use channel redundancy introduce weight matrix balance data fidelity three channels consideration their different noise statistics <eos> proposed mc wnnm model analytical solution <eos> reformulate into linear equality constrained problem solve via alternating direction method multipliers <eos> each alternative updating step closed form solution convergence guaranteed <eos> experiments both synthetic real noisy image datasets demonstrate superiority proposed mc wnnm over state art denoising method <eos> <eop> coherent online video style transfer <eos> training feed forward network fast neural style transfer image proven successful but naive extension processing video frame frame prone producing flickering result <eos> propose first end end network online video style transfer generates temporally coherent stylized video sequences near real time <eos> two key ideas include efficient network incorporating short term coherence propagating short term coherence long term ensures consistency over longer period time <eos> network incorporate different image stylization network clearly outperforms per frame baseline both qualitatively quantitatively <eos> moreover achieve visually comparable coherence optimization based video style transfer but three orders magnitude faster <eos> <eop> shape novel graph theoretic algorithm making consensus based decisions person re identification systems <eos> person re identification challenge video based surveillance goal identify same person different camera views <eos> recent years many algorithms proposed approach problem designing suitable feature representations image persons training appropriate distance metrics learn distinguish between image different persons <eos> aggregating result multiple algorithms person re identification relatively less explored area research <eos> paper formulate algorithm maps ranking process person re identification algorithm problem graph theory <eos> then extend formulation allow use result multiple algorithms make consensus based decision person re identification problem <eos> algorithm unsupervised takes into account only matching scores generated multiple algorithms creating consensus result <eos> further show how graph theoretic problem solved two step process <eos> first obtain rough estimate solution using greedy algorithm <eos> then extend construction proposed graph so problem efficiently solved means ant colony optimization heuristic path searching algorithm complex graphs <eos> while present algorithm context person re identification potentially applied general problem ranking items based consensus multiple set scores metric values <eos> <eop> need speed benchmark higher frame rate object tracking <eos> paper propose first higher frame rate video dataset called need speed nfs benchmark visual object tracking <eos> dataset consists video frames captured now commonly available higher frame rate fps cameras real world scenarios <eos> all frames annotated axis aligned bounding boxes all sequences manually labelled nine visual attributes such occlusion fast motion background clutter etc <eos> benchmark provides extensive evaluation many recent state art trackers higher frame rate sequences <eos> ranked each trackers according their tracking accuracy real time performance <eos> one surprising conclusions higher frame rates simple trackers such correlation filters outperform complex method based deep network <eos> suggests practical applications such robotics embedded vision one needs carefully tradeoff bandwidth constraints associated higher frame rate acquisition computational costs real time analysis required application accuracy <eos> dataset benchmark allows first time knowledge systematic exploration such issues will made available allow further research space <eos> <eop> learning background aware correlation filters visual tracking <eos> correlation filters cfs recently demonstrated excellent performance terms rapidly tracking object under challenging photometric geometric variations <eos> strength approach comes its ability efficiently learn fly how object changing over time <eos> fundamental drawback cfs however background target modeled over time result suboptimal performance <eos> recent tracking algorithms suggested resolve drawback either learning cfs more discriminative deep feature <eos> deepsrdcf ccot learning complex deep trackers <eos> while such method shown work well they suffer high complexity extracting deep feature applying deep tracking frameworks very computationally expensive <eos> limits real time performance such method even high end gpus <eos> work proposes background aware cf based hand crafted feature hog efficiently model how both foreground background object varies over time <eos> approach like conventional cfs extremely computationally efficient extensive experiments over multiple tracking benchmarks demonstrate superior accuracy real time performance method compared state art trackers <eos> <eop> robust object tracking based temporal spatial deep network <eos> recently deep neural network widely employed deal visual tracking problem <eos> work present new deep architecture incorporates temporal spatial information boost tracking performance <eos> deep architecture contains three network feature net temporal net spatial net <eos> feature net extracts general feature representations target <eos> feature representations temporal net encodes trajectory target directly learns temporal correspondences estimate object state global perspective <eos> based learning result temporal net spatial net further refines object tracking state using local spatial object information <eos> extensive experiments four largest tracking benchmarks including vot vot otb otb demonstrate competing performance proposed tracker over number state art algorithms <eos> <eop> real time hand tracking under occlusion egocentric rgb sensor <eos> present approach real time robust accurate hand pose estimation moving egocentric rgb cameras cluttered real environments <eos> existing method typically fail hand object interactions cluttered scenes imaged egocentric viewpoints common virtual augmented reality applications <eos> approach uses two subsequently applied convolutional neural network cnn localize hand regress three dimensional joint locations <eos> hand localization achieved using cnn estimate position hand center input even presence clutter occlusions <eos> localized hand position together corresponding input depth value used generate normalized cropped image fed into second cnn regress relative three dimensional hand joint locations real time <eos> added accuracy robustness temporal stability refine pose estimates using kinematic pose tracking energy <eos> train cnn introduce new photorealistic dataset uses merged reality approach capture synthesize large amounts annotated data natural hand interaction cluttered scenes <eos> through quantitative qualitative evaluation show method robust self occlusion occlusions object specifically moving egocentric perspectives <eos> <eop> predicting human activities using stochastic grammar <eos> paper presents novel method predict future human activities partially observed rgb video <eos> human activity prediction generally difficult due its non markovian property rich context between human environments <eos> use stochastic grammar model capture compositional structure events integrating human actions object their affordances <eos> represent event spatial temporal graph st aog <eos> st aog composed temporal stochastic grammar defined sub activities spatial graphs representing sub activities consist human actions object their affordances <eos> future sub activities predicted using temporal grammar earley parsing algorithm <eos> corresponding action object affordance labels then inferred accordingly <eos> extensive experiments conducted show effectiveness model both semantic event parsing future activity prediction <eos> <eop> probflow joint optical flow uncertainty estimation <eos> optical flow estimation remains challenging due untextured areas motion boundaries occlusions more <eos> thus estimated flow equally reliable across image <eos> end post hoc confidence measures introduced assess per pixel reliability flow <eos> overcome artificial separation optical flow confidence estimation introducing method jointly predicts optical flow its underlying uncertainty <eos> starting common energy based formulations rely corresponding posterior distribution flow given image <eos> derive variational inference scheme based mean field incorporates best practices energy minimization <eos> uncertainty measure obtained along flow every pixel marginal entropy variational distribution <eos> demonstrate flexibility probabilistic approach applying two different energies two benchmarks <eos> only obtain flow result competitive underlying energy minimization approach but also reliable uncertainty measure significantly outperforms existing post hoc approaches <eos> <eop> sublabel accurate discretization nonconvex free discontinuity problems <eos> work show how sublabel accurate multilabeling approaches derived approximating classical label continuous convex relaxation nonconvex free discontinuity problems <eos> insight allows extend sublabel accurate approaches total variation general convex nonconvex regularizations <eos> furthermore leads systematic approach discretization continuous convex relaxations <eos> study relationship existing discretizations discrete continuous mrfs <eos> finally apply proposed approach obtain sublabel accurate convex solution vectorial mumford shah functional show several experiments leads more precise solutions using fewer labels <eos> <eop> deepcontext context encoding neural pathways three dimensional holistic scene understanding <eos> three dimensional context shown extremely important cue scene understanding yet very little research done integrating context information deep models <eos> paper presents approach embed three dimensional context into topology neural network trained perform holistic scene understanding <eos> given depth image depicting three dimensional scene network aligns observed scene predefined three dimensional scene template then reasons about existence location each object within scene template <eos> doing so model recognizes multiple object single forward pass three dimensional convolutional neural network capturing both global scene local object information simultaneously <eos> create training data three dimensional network generate partly hallucinated depth image rendered replacing real object repository cad models same object category <eos> extensive experiments demonstrate effectiveness algorithm compared state art <eos> <eop> bam behance artistic media dataset recognition beyond photography <eos> computer vision systems designed work well within context everyday photography <eos> however artists often render world around them ways resemble photographs <eos> artwork produced people constrained mimic physical world making more challenging machines recognize <eos> work step toward teaching machines how categorize image ways valuable humans <eos> first collect large scale dataset contemporary artwork behance website containing millions portfolios professional commercial artists <eos> annotate behance imagery rich attribute labels content emotions artistic media <eos> furthermore carry out baseline experiments show value dataset artistic style prediction improving generality existing object classifiers study visual domain adaptation <eos> believe behance artistic media dataset will good starting point researchers wishing study artistic imagery relevant problems <eos> dataset found bam dataset <eos> org <eop> adversarial posenet structure aware convolutional network human pose estimation <eos> human pose estimation monocular image joint occlusions overlapping upon human bodies often result deviated pose predictions <eos> under circumstances bi ologically implausible pose predictions may produced <eos> contrast human vision able predict poses exploiting geometric constraints joint inter connectivity <eos> address problem incorporating priors about structure human bodies propose novel structure aware convo lutional network implicitly take such priors into account during training deep network <eos> explicit learning such constraints typically challenging <eos> instead design discriminators distinguish real poses fake ones such biologically implausible ones <eos> if pose generator generates result discriminator fails distinguish real ones network successfully learns priors <eos> better capture structure dependency human body joints generator designed stacked multi task manner predict poses well occlusion heatmaps <eos> then pose occlusion heatmaps sent discrimina tors predict likelihood pose being real <eos> training network follows strategy conditional generative adversarial network gans <eos> effectiveness pro posed network evaluated two widely used human pose estimation benchmark datasets <eos> approach significantly outperforms state art method almost always generates plausible human pose predictions <eos> <eop> empirical study language cnn image captioning <eos> language models based recurrent neural network dominated recent image caption generation tasks <eos> paper introduce language cnn model suitable statistical language modeling tasks shows competitive performance image captioning <eos> contrast previous models predict next word based one previous word hidden state language cnn fed all previous words model long range dependencies history words critical image captioning <eos> effectiveness approach validated two datasets flickr ms coco <eos> extensive experimental result show method outperforms vanilla recurrent neural network based language models competitive state art method <eos> <eop> attributes classname discriminative model attribute based unsupervised zero shot learning <eos> propose novel approach unsupervised zero shot learning zsl classes based their names <eos> most existing unsupervised zsl method aim learn model directly comparing image feature class names <eos> however proves difficult task due dominance non visual semantics underlying vector space embeddings class names <eos> address issue discriminatively learn word representation such similarities between class combination attribute names fall line visual similarity <eos> contrary traditional zero shot learning approaches built upon attribute presence approach bypasses laborious attribute class relation annotations unseen classes <eos> addition proposed approach renders text only training possible hence training augmented without need collect additional image data <eos> experimental result show method yields state art result unsupervised zsl three benchmark datasets <eos> <eop> areas attention image captioning <eos> propose areas attention novel attention based model automatic image captioning <eos> approach models dependencies between image region caption words state rnn language model using three pairwise interactions <eos> contrast previous attention based approaches associate image region rnn state method allows direct association between caption words image region <eos> during training associations inferred image level captions akin weakly supervised object detector training <eos> associations help improve captioning localizing corresponding region during testing <eos> also propose compare different ways generating attention areas cnn activation grids object proposals spatial transformers nets applied convolutional fashion <eos> spatial transformers give best result since they allow image specific attention areas trained jointly rest network <eos> attention mechanism spatial transformer attention areas together yield state art result mscoco dataset <eos> <eop> generative modeling audible shapes object perception <eos> humans infer rich knowledge object both auditory visual cues <eos> building machine such competency however very challenging due great difficulty capturing large scale clean data object both their appearance sound they make <eos> paper present novel open source pipeline generates audio visual data purely three dimensional object shapes their physical properties <eos> through comparison audio recordings human behavioral studies validate accuracy sounds generates <eos> using generative model able construct synthetic audio visual dataset namely sound object perception tasks <eos> demonstrate auditory visual information play complementary roles object perception further representation learned synthetic audio visual data transfer real world scenarios <eos> <eop> scene graph generation object phrases region captions <eos> object detection scene graph generation region captioning three scene understanding tasks different semantic levels tied together scene graphs generated top object detected image their pairwise relationship predicted while region captioning gives language description object their attributes relations other context information <eos> work leverage mutual connections across semantic levels propose novel neural network model termed multi level scene description network denoted msdn solve three vision tasks jointly end end manner <eos> object phrase caption region first aligned dynamic graph based their spatial semantic connections <eos> then feature refining structure used pass messages across three levels semantic tasks through graph <eos> benchmark learned model three tasks show joint learning across three tasks proposed method bring mutual improvements over previous models <eos> particularly scene graph generation task proposed method outperforms state art method more than margin <eos> <eop> recurrent multimodal interaction referring image segmentation <eos> paper interested problem image segmentation given natural language descriptions <eos> existing works tackle problem first modeling image sentences independently then segment image combining two types representations <eos> argue learning word image interaction more native sense jointly modeling two modalities image segmentation task propose convolutional multimodal lstm encode sequential interactions between individual words visual information spatial information <eos> show proposed model outperforms baseline model benchmark datasets <eos> addition analyze intermediate output proposed multimodal lstm approach empirically explain how approach enforces more effective word image interaction <eos> <eop> learning feature pyramids human pose estimation <eos> articulated human pose estimation fundamental yet challenging task computer vision <eos> difficulty particularly pronounced scale variations human body parts when camera view changes severe foreshortening happens <eos> although pyramid method widely used handle scale changes inference time learning feature pyramids deep convolutional neural network dcnns still well explored <eos> work design pyramid residual module prms enhance invariance scales dcnns <eos> given input feature prms learn convolutional filters various scales input feature obtained different subsampling ratios multi branch network <eos> moreover observe inappropriate adopt existing method initialize weights multi branch network achieve superior performance than plain network many tasks recently <eos> therefore provide theoretic derivation extend current weight initialization scheme multi branch network structures <eos> investigate method two standard benchmarks human pose estimation <eos> approach obtains state art result both benchmarks <eos> code available github <eos> com bearpaw pyranet <eos> <eop> structured attentions visual question answering <eos> visual attention assigns weights image region according their relevance question considered indispensable part most visual question answering models <eos> although questions may involve complex relations among multiple region few attention models effectively encode such cross region relations <eos> paper emonstrate importance encoding such relations showing limited effective receptive field resnet two datasets propose model visual attention multivariate distribution over grid structured conditional random field image region <eos> demonstrate how convert iterative inference algorithms mean field loopy belief propagation recurrent layer end end neural network <eos> empirically evaluated model datasets surpasses best baseline model newly released clevr dataset <eos> best published model vqa dataset <eos> source code available github <eos> com zhuchen vqa sva <eos> <eop> cut paste learn surprisingly easy synthesis instance detection <eos> major impediment rapidly deploying object detection models instance detection lack large annotated datasets <eos> example finding large labeled dataset containing instances particular kitchen unlikely <eos> each new environment new instances requires expensive data collection annotation <eos> paper propose simple approach generate large annotated instance datasets minimal effort <eos> key insight ensuring only patch level realism provides enough training signal current object detector models <eos> automatically cut object instances paste them random backgrounds <eos> naive way result pixel artifacts result poor performance trained models <eos> show how make detectors ignore artifacts during training generate data gives competitive performance real data <eos> method outperforms existing synthesis approaches when combined real image improves relative performance more than benchmark datasets <eos> cross domain setting synthetic data combined just real data outperforms models trained all real data <eos> <eop> cascaded feature network semantic segmentation rgb image <eos> fully convolutional network fcn successfully applied semantic segmentation scenes represented rgb image <eos> image augmented depth channel provide more understanding geometric information scene image <eos> question how best exploit additional information improve segmentation performance <eos> paper present neural network multiple branches segmenting rgb image <eos> approach use available depth split image into layer common visual characteristic object scenes common scene resolution <eos> introduce context aware receptive field carf provides better control relevant contextual information learned feature <eos> equipped carf each branch network semantically segments relevant similar scene resolution leading more focused domain easier learn <eos> furthermore network cascaded feature one branch augmenting feature adjacent branch <eos> show such cascading feature enriches contextual information each branch enhances overall performance <eos> accuracy network achieves outperforms state art method two public datasets <eos> <eop> encoder based lifelong learning <eos> paper introduces new lifelong learning solution single model trained sequence tasks <eos> main challenge vision systems face context catastrophic forgetting they tend adapt most recently seen task they lose performance tasks were learned previously <eos> method aims preserving knowledge previous tasks while learning new one using autoencoders <eos> each task under complete autoencoder learned capturing feature crucial its achievement <eos> when new task presented system prevent reconstructions feature autoencoders changing effect preserving information previous tasks mainly relying <eos> same time feature given space adjust most recent environment only their projection into low dimension submanifold controlled <eos> proposed system evaluated image classification tasks shows reduction forgetting over state art <eos> <eop> transitive invariance self supervised visual representation learning <eos> learning visual representations self supervised learning become popular computer vision <eos> idea design auxiliary tasks labels free obtain <eos> most tasks end up providing data learn specific kinds invariance useful recognition <eos> paper propose exploit different self supervised approaches learn representations invariant inter instance variations two object same class should similar feature ii intra instance variations viewpoint pose deformations illumination etc <eos> instead combining two approaches multi task learning argue organize reason data multiple variations <eos> specifically propose generate graph millions object mined hundreds thousands video <eos> object connected two types edges correspond two types invariance different instances but similar viewpoint category different viewpoints same instance <eos> applying simple transitivity graph edges obtain pairs image exhibiting richer visual invariance <eos> use data train triplet siamese network vgg base architecture apply learned representations different recognition tasks <eos> object detection achieve <eos> map pascal voc using fast cnn compare <eos> imagenet pre training <eos> challenging coco dataset method surprisingly close <eos> imagenet supervised counterpart <eos> using faster cnn framework <eos> also show network perform significantly better than imagenet network surface normal estimation task <eos> <eop> weakly supervised learning deep metrics stereo reconstruction <eos> deep learning metrics recently demonstrated extremely good performance match image patches stereo reconstruction <eos> however training such metrics requires large amount labeled stereo image difficult costly collect certain applications consider example satellite stereo imaging <eos> moreover labels depth sensors often noisy <eos> main contribution work new weakly supervised method learning deep metrics unlabeled stereo image given coarse information about scenes optical system <eos> method alternatively optimizes metric standard stochastic gradient descent applies stereo constraints regularize its prediction <eos> experiments reference data set show given network architecture training new method without ground truth produces metric performance good state art baselines trained said ground truth <eos> work three practical implications <eos> firstly helps overcome limitations training set particular noisy ground truth <eos> secondly allows use much more training data during learning <eos> thirdly allows tune deep metric particular stereo system even if ground truth available <eos> <eop> fine grained recognition wild multi task domain adaptation approach <eos> while fine grained object recognition important problem computer vision current models unlikely accurately classify object wild <eos> fully supervised models need additional annotated image classify object every new scenario task infeasible <eos> however sources such commerce websites field guides provide annotated image many classes <eos> work study fine grained domain adaptation step towards overcoming dataset shift between easily acquired annotated image real world <eos> adaptation studied fine grained setting annotations such attributes could used increase performance <eos> work uses attribute based multitask adaptaion loss increase accuracy baseline <eos> semi supervised adaptation case <eos> prior domain adaptation works benchmarked small datasets such total image some domains simplistic datasets such consisting digits <eos> perform experiments new challenging fine grained dataset cars consisting image categories cars drawn commerce websites google street view <eos> <eop> sort second order response transform visual recognition <eos> paper reveal importance benefits introducing second order operations into deep neural network <eos> propose novel approach named second order response transform sort appends element wise product transform linear sum two branch network module <eos> direct advantage sort facilitate cross branch response propagation so each branch update its weights based current status other branch <eos> moreover sort augments family transform operations increases nonlinearity network making possible learn flexible functions fit complicated distribution feature space <eos> sort applied wide range network architectures including branched variant chain styled network residual network very light weighted modifications <eos> observe consistent accuracy gain both small cifar cifar svhn big ilsvrc datasets <eos> addition sort very efficient extra computation overhead less than <eos> <eop> adversarial examples semantic segmentation object detection <eos> well demonstrated adversarial examples <eos> natural image visually imperceptible perturbations added cause deep network fail image classification <eos> paper extend adversarial examples semantic segmentation object detection much more difficult <eos> observation both segmentation detection based classifying multiple targets image <eos> target pixel receptive field segmentation object proposal detection <eos> inspires optimize loss function over set targets generating adversarial perturbations <eos> based propose novel algorithm named dense adversary generation dag applies state art network segmentation detection <eos> find adversarial perturbations transferred across network different training data based different architectures even different recognition tasks <eos> particular transfer ability across network same architecture more significant than other cases <eos> besides show summing up heterogeneous perturbations often leads better transfer performance provides effective method black box adversarial attack <eos> <eop> genetic cnn <eos> deep convolutional neural network cnn state art solution large scale visual recognition <eos> following some basic principles such increasing network depth constructing highway connections researchers manually designed lot fixed network architectures verified their effectiveness <eos> paper discuss possibility learning deep network structures automatically <eos> note number possible network structures increases exponentially number layer network motivates adopt genetic algorithm efficiently explore large search space <eos> core idea propose encoding method represent each network structure fixed length binary string <eos> genetic algorithm initialized generating set randomized individuals <eos> each generation define standard genetic operations <eos> selection mutation crossover generate competitive individuals eliminate weak ones <eos> competitiveness each individual defined its recognition accuracy obtained via standalone training process reference dataset <eos> run genetic process cifar small scale dataset demonstrating its ability find high quality structures little studied before <eos> learned powerful structures also transferrable ilsvrc dataset large scale visual recognition <eos> <eop> channel pruning accelerating very deep neural network <eos> paper introduce new channel pruning method accelerate very deep convolutional neural network <eos> given trained cnn model propose iterative two step algorithm effectively prune each layer lasso regression based channel selection least square reconstruction <eos> further generalize algorithm multi layer multi branch cases <eos> method reduces accumulated error enhance compatibility various architectures <eos> pruned vgg achieves state art result speed up along only <eos> more importantly method able accelerate modern network like resnet xception suffers only <eos> accuracy loss under speed up respectively significant <eos> <eop> infinite latent feature selection probabilistic latent graph based ranking approach <eos> feature selection playing increasingly significant role respect many computer vision applications spanning object recognition visual object tracking <eos> however most recent solutions feature selection robust across different heterogeneous set data <eos> paper address issue proposing robust probabilistic latent graph based feature selection algorithm performs ranking step while considering all possible subsets feature paths graph bypassing combinatorial problem analytically <eos> appealing characteristic approach aims discover abstraction behind low level sensory data relevancy <eos> relevancy modelled latent variable plsa inspired generative process allows investigation importance feature when injected into arbitrary set cues <eos> proposed method tested ten diverse benchmarks compared against eleven state art feature selection method <eos> result show proposed approach attains highest performance levels across many different scenarios difficulties thereby confirming its strong robustness while setting new state art feature selection domain <eos> <eop> video fill blank using lr rl lstms spatial temporal attentions <eos> given video description sentence one missing word source sentence video fill blank vfib problem find missing word automatically <eos> contextual information sentence well visual cues video important infer missing word accurately <eos> since source sentence broken into two fragments sentence left fragment before blank sentence right fragment after blank traditional recurrent neural network cannot encode structure accurately because many possible variations missing word terms location type word source sentence <eos> example missing word first word middle sentence verb adjective <eos> paper propose framework tackle textual encoding two separate lstms lr rl lstms employed encode left right sentence fragments novel structure introduced combine each fragment external memory corresponding opposite fragments <eos> visual encoding end end spatial temporal attention models employed select discriminative visual representations find missing word <eos> experiments demonstrate superior performance proposed method challenging vfib problem <eos> furthermore introduce extended more generalized version vfib limited single blank <eos> experiments indicate generalization capability method dealing such more realistic scenarios <eos> <eop> primary video object segmentation via complementary cnn neighborhood reversible flow <eos> paper proposes novel approach segmenting primary video object using complementary convolutional neural network ccnn neighborhood reversible flow <eos> proposed approach first pre trains ccnn massive image manually annotated salient object end end manner trained ccnn two separate branches simultaneously handle two complementary tasks <eos> foregroundness backgroundness estimation <eos> applying ccnn each video frame spatial foregroundness backgroundness maps initialized then propagated between various frames so segment primary video object suppress distractors <eos> enforce efficient temporal propagation divide each frame into superpixels construct neighborhood reversible flow reflects most reliable temporal correspondences between superpixels far away frames <eos> within such flow initialized foregroundness backgroundness efficiently accurately propagated along temporal axis so primary video object gradually pop out distractors well suppressed <eos> extensive experimental result three video datasets show proposed approach achieves impressive performance comparisons state art models <eos> <eop> attentive semantic video generation using captions <eos> paper proposes network architecture perform variable length semantic video generation using captions <eos> adopt new perspective towards video generation allow captions combined long term short term dependencies between video frames thus generate video incremental manner <eos> experiments demonstrate network architecture ability distinguish between object actions interactions video combine them generate video unseen captions <eos> network also exhibits capability perform spatio temporal style transfer when asked generate video sequence captions <eos> also show network ability learn latent representation allows generate video unsupervised manner perform other tasks such action recognition <eos> <eop> following gaze video <eos> following gaze people inside video important signal understanding people their actions <eos> paper present approach following gaze video predicting person video looking even when object different frame <eos> collect videogaze new dataset use benchmark both train evaluate models <eos> given one frame person model estimates density gaze location every frame probability person looking particular frame <eos> key aspect approach end end model jointly estimates saliency gaze pose geometric relationships between views while only using gaze supervision <eos> visualizations suggest model learns internally solve intermediate tasks automatically without additional supervision <eos> experiments show approach follows gaze video better than existing approaches enabling richer understanding human activities video <eos> <eop> adaptive rnn tree large scale human action recognition <eos> work present rnn tree rnn adaptive learning framework skeleton based human action recognition <eos> method categorizes action classes uses multiple recurrent neural network rnns tree like hierarchy <eos> rnns rnn co trained action category hierarchy determines structure rnn <eos> actions skeletal representations recognized via hierarchical inference process during individual rnns differentiate finer grained action classes increasing confidence <eos> inference rnn ends when any rnn tree recognizes action high confidence leaf node reached <eos> rnn effectively addresses two main challenges large scale action recognition able distinguish fine grained action classes intractable using single network ii adaptive new action classes augmenting existing model <eos> demonstrate effectiveness rnn ach method compare state art method large scale dataset several existing benchmarks <eos> <eop> spatio temporal person retrieval via natural language queries <eos> paper address problem spatio temporal person retrieval video using natural language query output tube <eos> sequence bounding boxes encloses person described query <eos> problem introduce novel dataset consisting video containing people annotated bounding boxes each second five natural language descriptions <eos> retrieve tube person described given natural language query design model combines method spatio temporal human detection multimodal retrieval <eos> conduct comprehensive experiments compare variety tube text representations multimodal retrieval method present strong baseline task well demonstrate efficacy tube representation multimodal feature embedding technique <eos> finally demonstrate versatility model applying two other important tasks <eos> <eop> automatic spatially aware fashion concept discovery <eos> paper proposes automatic spatially aware concept discovery approach using weakly labeled image text data shopping websites <eos> first fine tune googlenet jointly modeling clothing image their corresponding descriptions visual semantic embedding space <eos> then each attribute word generate its spatially aware representation combining its semantic word vector representation its spatial representation derived convolutional maps fine tuned network <eos> resulting spatially aware representations further used cluster attributes into multiple groups form spatially aware concepts <eos> neckline concept might consist attributes like neck round neck etc <eos> finally decompose visual semantic embedding space into multiple concept specific subspaces facilitates structured browsing attribute feedback product retrieval exploiting multimodal linguistic regularities <eos> conducted extensive experiments newly collected fashion dataset result clustering quality evaluation attribute feedback product retrieval task demonstrate effectiveness automatically discovered spatially aware concepts <eos> <eop> chromatag colored marker fast detection algorithm <eos> current fiducial marker detection algorithms rely marker ids false positive rejection <eos> time wasted potential detections will eventually rejected false positives <eos> introduce chromatag fiducial marker detection algorithm designed use opponent colors limit quickly reject initial false detections grayscale precise localization <eos> through experiments show chromatag significantly faster than current fiducial markers while achieving similar better detection accuracy <eos> also show how tag size viewing direction effect detection accuracy <eos> contribution significant because fiducial markers often used real time applications <eos> marker assisted robot navigation heavy computation required other parts system <eos> <eop> adversarial image perturbation privacy protection game theory perspective <eos> users like sharing personal photos others through social media <eos> same time they might want make automatic identification such photos difficult even impossible <eos> classic obfuscation method such blurring only unpleasant but also effective one would expect <eos> recent studies adversarial image perturbations aip suggest possible confuse recognition systems effectively without unpleasant artifacts <eos> however presence counter measures against aips unclear how effective aip would particular when choice counter measure unknown <eos> game theory provides tools studying interaction between agents uncertainties strategies <eos> introduce general game theoretical framework user recogniser dynamics present case study involves current state art aip person recognition techniques <eos> derive optimal strategy user assures upper bound recognition rate independent recogniser counter measure <eos> code available goo <eos> <eop> wetext scene text detection under weak supervision <eos> requiring large amounts annotated training data become common constraint various deep learning systems <eos> paper propose weakly supervised scene text detection method wetext trains robust accurate scene text detection models learning unannotated weakly annotated data <eos> light supervised model trained small fully annotated dataset explore semi supervised weakly supervised learning large unannotated dataset large weakly annotated dataset respectively <eos> unsupervised learning light supervised model applied unannotated dataset search more character training sample further combined small annotated dataset retrain superior character detection model <eos> weakly supervised learning character searching guided high level annotations words text lines widely available also much easier prepare <eos> addition design unified scene character detector adapting regression based deep network greatly relieves error accumulation issue widely exists most traditional approaches <eos> extensive experiments across different unannotated weakly annotated datasets show scene text detection performance clearly boosted under both scenarios weakly supervised learning achieve state art performance using only fully annotated scene text image <eos> <eop> arbitrary style transfer real time adaptive instance normalization <eos> gatys <eos> recently introduced neural algorithm renders content image style another image achieving so called style transfer <eos> however their framework requires slow iterative optimization process limits its practical application <eos> fast approximations feed forward neural network proposed speed up neural style transfer <eos> unfortunately speed improvement comes cost network usually tied fixed set styles cannot adapt arbitrary new styles <eos> paper present simple yet effective approach first time enables arbitrary style transfer real time <eos> heart method novel adaptive instance normalization adain layer aligns mean variance content feature style feature <eos> method achieves speed comparable fastest existing approach without restriction pre defined set styles <eos> addition approach allows flexible user controls such content style trade off style interpolation color spatial controls all using single feed forward neural network <eos> <eop> photographic image synthesis cascaded refinement network <eos> present approach synthesizing photographic image conditioned semantic layouts <eos> given semantic label map approach produces image photographic appearance conforms input layout <eos> approach thus functions rendering engine takes two dimensional semantic specification scene produces corresponding photographic image <eos> unlike recent contemporaneous work approach rely adversarial training <eos> show photographic image synthesized semantic layouts single feedforward network appropriate structure trained end end direct regression objective <eos> presented approach scales seamlessly high resolutions demonstrate synthesizing photographic image megapixel resolution full resolution training data <eos> extensive perceptual experiments datasets outdoor indoor scenes demonstrate image synthesized presented approach considerably more realistic than alternative approaches <eos> <eop> ssd making rgb based three dimensional detection pose estimation great again <eos> present novel method detecting three dimensional model instances estimating their poses rgb data single shot <eos> end extend popular ssd paradigm cover full pose space train synthetic model data only <eos> approach competes surpasses current state art method leverage rgb data multiple challenging datasets <eos> furthermore method produces result around hz many times faster than related method <eos> sake reproducibility make trained network detection code publicly available <eos> <eop> unsupervised creation parameterized avatars <eos> study problem mapping input image tied pair consisting vector parameters image created using graphical engine vector parameters <eos> mapping objective output image similar possible input image <eos> during training no supervision given form matching inputs outputs <eos> learning problem extends two literature problems unsupervised domain adaptation cross domain transfer <eos> define generalization bound based discrepancy employ gan implement network solution corresponds bound <eos> experimentally method shown solve problem automatically creating avatars <eos> <eop> learning active three dimensional mapping <eos> propose active three dimensional mapping method depth sensors allow individual control depth measuring rays such newly emerging solid state lidars <eos> method simultaneously learns reconstruct dense three dimensional voxel map sparse depth measurements ii optimizes reactive control depth measuring rays <eos> make first step towards online control optimization propose fast greedy algorithm needs update its cost function only small fraction possible rays <eos> approximation ratio greedy algorithm derived <eos> experimental evaluation subset kitti dataset demonstrates significant improvement three dimensional map accuracy when learning reconstruct sparse measurements coupled optimization measure <eos> <eop> toward perceptually consistent stereo scanline study <eos> two types information exist stereo pair correlation matching decorrelation half occlusion <eos> vision science shown both types information used visual cortex people perceive depth even when correlation cues absent very weak capability remains absent most computational stereo systems <eos> step toward stereo algorithms more consistent perceptual phenomena re examine topic scanline stereo energy minimization <eos> represent disparity profile piecewise smooth function explicit breakpoints between its smooth pieces show allows correlation decorrelation integrated into objective requires only two types local information correlation its spatial gradient <eos> experimentally show global optimum objective matches human perception broad collection wellknown perceptual stimuli also provides reasonable piecewise smooth interpretations depth natural image even without exploiting monocular boundary cues <eos> <eop> surface normals wild <eos> study problem single image depth estimation image wild <eos> collect human annotated surface normals use them help train neural network directly predicts pixel wise depth <eos> propose two novel loss functions training surface normal annotations <eos> experiments nyu depth kitti own dataset demonstrate approach significantly improve quality depth estimation wild <eos> <eop> unsupervised learning stereo matching <eos> recent years convolutional neural network shown its strong power stereo matching cost learning <eos> current approaches learn parameters their models public datasets ground truth disparity <eos> however due limitations datasets difficulty collecting new stereo data current method fail real life cases <eos> work present framework learning stereo matching cost without human supervision <eos> method updates network parameter iterative manner <eos> starts randomly initialized network <eos> correct matchings carefully picked used training data each round <eos> end network converges stable state performs comparably supervised method various benchmarks <eos> <eop> unrestricted facial geometry reconstruction using image image translation <eos> recently shown neural network recover geometric structure face single given image <eos> common denominator most existing face geometry reconstruction method restriction solution space some low dimensional subspace <eos> while such model significantly simplifies reconstruction problem inherently limited its expressiveness <eos> alternative propose image image translation network jointly maps input image depth image facial correspondence map <eos> explicit pixel based mapping then utilized provide high quality reconstructions diverse faces under extreme expressions using purely geometric refinement process <eos> spirit recent approaches network trained only synthetic data then evaluated wild facial image <eos> both qualitative quantitative analyses demonstrate accuracy robustness approach <eos> <eop> learned multi patch similarity <eos> estimating depth map multiple views scene fundamental task computer vision <eos> soon more than two viewpoints available one faces very basic question how measure similarity across image patches <eos> surprisingly no direct solution exists instead common fall back more less robust averaging two view similarities <eos> encouraged success machine learning particular convolutional neural network propose learn matching function directly maps multiple image patches scalar similarity score <eos> experiments several multi view datasets demonstrate approach advantages over method based pairwise patch similarity <eos> <eop> click here human localized keypoints guidance viewpoint estimation <eos> motivate address human loop variant monocular viewpoint estimation task location class one semantic object keypoint available test time <eos> order leverage keypoint information devise convolutional neural network called click here cnn ch cnn integrates keypoint information activations layer process image <eos> transforms keypoint information into map used weigh feature certain parts image more heavily <eos> weighted sum spatial feature combined global image feature provide relevant information prediction layer <eos> train network collect novel dataset three dimensional keypoint annotations thousands cad models synthetically render millions image keypoint information <eos> test instances pascal three dimensional model achieves mean class accuracy <eos> whereas state art baseline only obtains <eos> mean class accuracy justifying argument human loop inference <eos> <eop> unsupervised adaptation deep stereo <eos> recent ground breaking works shown deep neural network trained end end regress dense disparity maps directly image pairs <eos> computer generated imagery deployed gather large data corpus required train such network additional fine tuning allowing adapt model work well also real possibly diverse environments <eos> yet besides few public datasets such kitti ground truth needed adapt network new scenario hardly available practice <eos> paper propose novel unsupervised adaptation approach enables fine tune deep learning stereo model without any ground truth information <eos> rely off shelf stereo algorithms together state art confidence measures latter able ascertain upon correctness measurements yielded former <eos> thus train network based novel loss function penalizes predictions disagreeing highly confident disparities provided algorithm enforces smoothness constraint <eos> experiments popular datasets kitti kitti middlebury other challenging test image demonstrate effectiveness proposal <eos> <eop> composite focus measure high quality depth maps <eos> depth focus highly accessible method estimate three dimensional structure everyday scenes <eos> today dslr mobile cameras facilitate easy capture multiple focused image scene <eos> focus measures fms estimate amount focus each pixel form basis depth focus method <eos> several fms proposed past new ones will emerge future each their own strengths <eos> estimate weighted combination standard fms outperforms others wide range scene types <eos> resulting composite focus measure consists fms consensus one another but chorus <eos> two stage pipeline first estimates fine depth each pixel using composite focus measure <eos> cost volume propagation step then assigns depths confident pixels others <eos> generate high quality depth maps using just top five fms composite focus measure <eos> positive step towards depth estimation everyday scenes no special equipment <eos> <eop> reconstruction based disentanglement pose invariant face recognition <eos> deep neural network dnns trained large scale datasets recently achieved impressive improvements face recognition <eos> but persistent challenge remains develop method capable handling large pose variations relatively under represented training data <eos> paper presents method learning feature representation invariant pose without requiring extensive pose coverage training data <eos> first propose generate non frontal views single frontal face order increase diversity training data while preserving accurate facial details critical identity discrimination <eos> next contribution seek rich embedding encodes identity feature well non identity ones such pose landmark locations <eos> finally propose new feature reconstruction metric learning explicitly disentangle identity pose demanding alignment between feature reconstructions through various combinations identity pose feature obtained two image same subject <eos> experiments both controlled wild face datasets such multipie wlp profile view database cfp show method consistently outperforms state art especially image large head pose variations <eos> <eop> recurrent three dimensional dual learning large pose facial landmark detection <eos> despite remarkable progress face analysis techniques detecting landmarks large pose faces still difficult due self occlusion subtle landmark difference incomplete information <eos> address challenging issues introduce novel recurrent three dimensional dual learning model alternatively performs based three dimensional face model refinement three dimensional projection based landmark refinement reliably reason about self occluded landmarks precisely capture subtle landmark displacement accurately detect landmarks even presence extremely large poses <eos> proposed model presents first loop closed learning framework effectively exploits informative feedback three dimensional learning its dual three dimensional refinement tasks recurrent manner <eos> benefiting two mutual boosting steps proposed model demonstrates appealing robustness large poses up profile pose outstanding ability capture fine scale landmark displacement compared existing three dimensional models <eos> achieves new state art challenging aflw benchmark <eos> moreover proposed model introduces new architectural design economically utilizes intermediate feature achieves faster speed than its deep learning based counterparts <eos> <eop> anchored regression network applied age estimation super resolution <eos> propose anchored regression network arn nonlinear regression network seamlessly integrated into various network used stand alone when feature already fixed <eos> arn smoothed relaxation piecewise linear regressor through combination multiple linear regressors over soft assignments anchor point <eos> when anchor point fixed optimal arn regressors obtained closed form global solution otherwise arn admits end end learning standard gradient based method <eos> demonstrate power arn applying two very diverse challenging tasks age prediction face image image super resolution <eos> both cases arns yield strong result <eos> <eop> infant footprint recognition <eos> infant recognition received increasing attention recent years many applications such tracking child vaccination identifying missing children <eos> due lack efficient identification method infants newborns current method infant recognition rely identification parents certificates identity <eos> while biometric recognition technologies <eos> face fingerprint recognition widely deployed many applications recognizing adults teenagers no such recognition systems yet exist infants newborns <eos> one major problems biometric traits infants newborns either permanent <eos> face difficult capture <eos> fingerprint due lack appropriate sensors <eos> paper investigate feasibility infant recognition their footprint using ppi commodity friction ridge sensor <eos> collected infant footprint dataset three sessions consisting subjects age range months <eos> proposed new minutia descriptor based deep convolutional neural network measuring minutiae similarity <eos> descriptor compact highly discriminative <eos> conducted verification experiments both single enrolled template fusion multiple enrolled templates show impact age time gap matching performance <eos> comparison experiments state art algorithm show advantage proposed minutia descriptor <eos> <eop> self paced kernel estimation robust blind image deblurring <eos> challenge blind image deblurring remove effects blur limited prior information about nature blur process <eos> existing method often assume blur image produced linear convolution additive gaussian noise <eos> however including even small number outliers model kernel estimation process significantly reduce resulting image quality <eos> previous method mainly rely some simple but unreliable heuristics identify outliers kernel estimation <eos> rather than attempt identify outliers model priori instead propose sequentially identify inliers gradually incorporate them into estimation process <eos> self paced kernel estimation scheme propose represents generalization existing self paced learning approaches gradually detect include reliable inlier pixel set blurred image kernel estimation <eos> moreover automatically activate subset significant gradients <eos> reliable inlier pixels then update intermediate sharp image kernel accordingly <eos> experiments both synthetic data real world image various kinds outliers demonstrate effectiveness robustness proposed method compared state art method <eos> <eop> super trajectory video segmentation <eos> introduce novel semi supervised video segmentation approach based efficient video representation called super trajectory <eos> each super trajectory corresponds group compact trajectories exhibit consistent motion patterns similar appearance close spatiotemporal relationships <eos> generate trajectories using probabilistic model handles occlusions drifts robust natural way <eos> reliably group trajectories adopt modified version density peaks based clustering algorithm allows capturing rich spatiotemporal relations among trajectories clustering process <eos> presented video representation discriminative enough accurately propagate initial annotations first frame onto remaining video frames <eos> extensive experimental analysis challenging benchmarks demonstrate method capable distinguishing target object complex backgrounds even reidentifying them after occlusions <eos> <eop> your own prada fashion synthesis structural coherence <eos> present novel effective approach generating new clothing wearer through generative adversarial learning <eos> given input image person sentence describing different outfit model redresses person desired while same time keeping wearer her his pose unchanged <eos> generating new outfits precise region conforming language description while retaining wearer body structure new challenging task <eos> existing generative adversarial network ideal ensuring global coherence structure given both input photograph language description conditions <eos> address challenge decomposing complex generative process into two conditional stages <eos> first stage generate plausible semantic segmentation map obeys wearer pose latent spatial arrangement <eos> effective spatial constraint formulated guide generation semantic segmentation map <eos> second stage generative model newly proposed compositional mapping layer used render final image precise region textures conditioned map <eos> extended deepfashion dataset collecting sentence descriptions image <eos> demonstrate effectiveness approach through both quantitative qualitative evaluations <eos> user study also conducted <eos> <eop> wavelet srnet wavelet based cnn multi scale face super resolution <eos> most modern face super resolution method resort convolutional neural network cnn infer high resolution hr face image <eos> when dealing very low resolution lr image performance cnn based method greatly degrades <eos> meanwhile method tend produce over smoothed outputs miss some textural details <eos> address challenges paper presents wavelet based cnn approach ultra resolve very low resolution face image smaller pixel size its larger version multiple scaling factors even unified framework <eos> different conventional cnn method directly inferring hr image approach firstly learns predict lr corresponding series hr wavelet coefficients before reconstructing hr image them <eos> capture both global topology information local texture details human faces present flexible extensible convolutional neural network three types loss wavelet prediction loss texture loss full image loss <eos> extensive experiments demonstrate proposed approach achieves more appealing result both quantitatively qualitatively than state art super resolution method <eos> <eop> learning gaze transitions depth improve video saliency estimation <eos> paper introduce novel depth aware video saliency approach predict human focus attention when viewing video contain depth map rgbd screen <eos> saliency estimation scenario highly important since near future three dimensional video content will easily acquired yet hard display <eos> despite considerable progress three dimensional display technologies most still expensive require special glasses viewing so rgbd content primarily viewed screens removing depth channel final viewing experience <eos> train generative convolutional neural network predicts viewing saliency map given frame using rgbd pixel values previous fixation estimates video <eos> evaluate performance approach present new comprehensive database viewing eye fixation ground truth rgbd video <eos> experiments indicate beneficial integrate depth into video saliency estimates content viewed display <eos> demonstrate approach outperforms state art method video saliency achieving relative improvement <eos> <eop> joint convolutional analysis synthesis sparse representation single image layer separation <eos> analysis sparse representation asr synthesis sparse representation ssr two representative approaches sparsity based image modeling <eos> image described mainly non zero coefficients ssr while characterized indices zeros asr <eos> exploit complementary representation mechanisms asr ssr integrate two models propose joint convolutional analysis synthesis jcas sparse representation model <eos> convolutional implementation adopted more effectively exploit image global information <eos> jcas single image decomposed into two layer one approximated asr represent image large scale structures other ssr represent image fine scale textures <eos> synthesis dictionary adaptively learned jcas describe texture patterns different single image layer separation tasks <eos> evaluate proposed jcas model variety applications including rain streak removal high dynamic range image tone mapping etc <eos> result show jcas method outperforms state ofthe arts applications terms both quantitative measure visual perception quality <eos> <eop> modelling scene dependent imaging cameras deep neural network <eos> present novel deep learning framework models scene dependent image processing inside cameras <eos> often called radiometric calibration process recovering raw image processed image jpeg format srgb color space essential many computer vision tasks rely physically accurate radiance values <eos> all previous works rely deterministic imaging model color transformation stays same regardless scene thus they only applied image taken under manual mode <eos> paper propose data driven approach learn scene dependent locally varying image processing inside cameras under automode <eos> method incorporates both global local scene context into pixel wise feature via multi scale pyramid learnable histogram layer <eos> result show model imaging pipeline different cameras operate under automode accurately both directions raw srgb srgb raw show how apply method improve performance image deblurring <eos> <eop> transformed low rank model line pattern noise removal <eos> paper addresses problem line pattern noise removal single image such rain streak hyperspectral stripe so <eos> most previous method model line pattern noise original image domain fail explicitly exploit directional characteristic thus resulting redundant subspace poor representation ability line pattern noise <eos> achieve compact subspace line pattern structure work incorporate transformation into image decomposition model so maps input image domain line pattern streak stripe appearance extremely distinct low rank structure naturally allows enforce low rank prior extract line pattern streak stripe noisy image <eos> moreover random noise usually mixed up line pattern noise makes challenging problem much more difficult <eos> while previous method resort spectral temporal correlation multi image give detailed analysis between noisy clean image both local gradient nonlocal domain propose compositional directional total variational low rank prior image layer thus simultaneously accommodate both types noise <eos> proposed method evaluated two different tasks including remote sensing image mixed random stripe noise removal rain streak removal all obtain very impressive performances <eos> <eop> weakly supervised manifold learning dense semantic object correspondence <eos> goal semantic object correspondence problem compute dense association maps pair image such same object parts get matched very different appearing object instances <eos> method builds recent findings deep convolutional neural network dcnns implicitly learn latent model object parts even when trained classification <eos> also leverage key correspondence problem insight geometric structure between object parts consistent across multiple object instances <eos> two concepts then combined form novel optimization scheme <eos> optimization learns feature embedding rewarding projecting feature closer manifold if they low feature space distance <eos> simultaneously optimization penalizes feature clusters whose geometric structure inconsistent observed geometric structure object parts <eos> manner accounting feature space similarities feature neighborhood context together manifold learned feature belonging semantically similar object parts cluster together <eos> also describe transferring embedded feature sister tasks semantic keypoint classification localization task via siamese dcnn <eos> provide qualitative result pascal voc image quantitative result pascal berkeley dataset improve state art over classification over localization tasks <eos> <eop> dual motion gan future flow embedded video prediction <eos> future frame prediction video promising avenue unsupervised video representation learning <eos> video frames naturally generated inherent pixel flows preceding frames based appearance motion dynamics video <eos> however existing method focus directly hallucinating pixel values resulting blurry predictions <eos> paper develop dual motion generative adversarial net gan architecture learns explicitly enforce future frame predictions consistent pixel wise flows video through dual learning mechanism <eos> primal future frame prediction dual future flow prediction form closed loop generating informative feedback signals each other better video prediction <eos> make both synthesized future frames flows indistinguishable reality dual adversarial training method proposed ensure future flow prediction able help infer realistic future frames while future frame prediction turn leads realistic optical flows <eos> dual motion gan also handles natural motion uncertainty different pixel locations new probabilistic motion encoder based variational autoencoders <eos> extensive experiments demonstrate proposed dual motion gan significantly outperforms state art approaches synthesizing new video frames predicting future flows <eos> model generalizes well across diverse visual scenes shows superiority unsupervised video representation learning <eos> <eop> online robust image alignment via subspace learning gradient orientations <eos> robust efficient image alignment remains challenging task due massiveness image great illumination variations between image partial occlusion corruption <eos> address challenges propose online image alignment method via subspace learning image gradient orientations igo <eos> proposed method integrates subspace learning transformed igo reconstruction image alignment into unified online framework robust aligning image severe intensity distortions <eos> method motivated principal component analysis pca gradient orientations provides more reliable low dimensional subspace than pixel intensities <eos> instead processing intensity domain like conventional method seek alignment igo domain such aligned igo newly arrived image decomposed sum sparse error linear composition igo pca basis learned previously well aligned ones <eos> optimization problem accomplished iterative linearization minimizes norm sparse error <eos> furthermore igo pca basis adaptively updated based incremental thin singular value decomposition svd takes shift igo mean into consideration <eos> efficacy proposed method validated extensive challenging datasets through image alignment face recognition <eos> experimental result demonstrate algorithm provides more illumination occlusion robust image alignment than state art method <eos> <eop> learning dynamic siamese network visual object tracking <eos> how effectively learn temporal variation target appearance exclude interference cluttered background while maintaining real time response essential problem visual object tracking <eos> recently siamese network shown great potentials matching based trackers achieving balanced accuracy beyond real time speed <eos> however they still big gap classification updating based trackers tolerating temporal changes object imaging conditions <eos> paper propose dynamic siamese network via fast transformation learning model enables effective online learning target appearance variation background suppression previous frames <eos> then present elementwise multi layer fusion adaptively integrate network outputs using multi level deep feature <eos> unlike state art trackers approach allows usage any feasible generally particularly trained feature such siamfc vgg <eos> more importantly proposed dynamic siamese network jointly trained whole directly labeled video sequences thus take full advantage rich spatial temporal information moving object <eos> result approach achieves state art performance otb vot benchmarks while exhibits superiorly balanced accuracy real time response over state art competitors <eos> <eop> high order tensor formulation convolutional sparse coding <eos> convolutional sparse coding csc gained attention its successful role reconstruction classification tool computer vision machine learning community <eos> current csc method only reconstruct single feature image independently <eos> however learning multi dimensional dictionaries sparse codes reconstruction multi dimensional data very important examines correlations among all data jointly <eos> provides more capacity learned dictionaries better reconstruct data <eos> paper propose generic novel formulation csc problem handle arbitrary order tensor data <eos> backed experimental result proposed formulation only tackle applications possible standard csc solvers including colored video reconstruction tensors but also performs favorably reconstruction much fewer parameters compared naive extensions standard csc multiple feature channels <eos> <eop> learning proximal operators using denoising network regularizing inverse imaging problems <eos> while variational method among most powerful tools solving linear inverse problems imaging deep convolutional neural network recently taken lead many challenging benchmarks <eos> remaining drawback deep learning approaches their requirement expensive retraining whenever specific problem noise level noise type desired measure fidelity changes <eos> contrary variational method plug play nature they usually consist separate data fidelity regularization terms <eos> paper study possibility replacing proximal operator regularization used many convex energy minimization algorithms denoising neural network <eos> latter therefore serves implicit natural image prior while data term still chosen independently <eos> using fixed denoising neural network exemplary problems image deconvolution different blur kernels image demosaicking obtain state art reconstruction result <eos> indicate high generalizability approach reduction need problem specific training <eos> additionally discuss novel result analysis possible optimization algorithms incorporate network into well choices algorithm parameters their relation noise level neural network trained <eos> <eop> scalenet guiding object proposal generation supermarkets beyond <eos> motivated product detection supermarkets paper studies problem object proposal generation supermarket image other natural image <eos> argue estimation object scales image helpful generating object proposals especially supermarket image object scales usually within small range <eos> therefore propose estimate object scales image before generating object proposals <eos> proposed method predicting object scales called scalenet <eos> validate effectiveness scalenet build three supermarket datasets two real world datasets used testing other one synthetic dataset used training <eos> short extend previous state art object proposal method adding scale prediction phase <eos> resulted method outperforms previous state art supermarket datasets large margin <eos> also show approach works object proposal other natural image outperforms previous state art object proposal method ms coco dataset <eos> supermarket datasets virtual supermarkets tools creating more synthetic datasets will made public <eos> <eop> temporal dynamic graph lstm action driven video object detection <eos> paper investigate weakly supervised object detection framework <eos> most existing frameworks focus using static image learn object detectors <eos> however detectors often fail generalize video because existing domain shift <eos> therefore investigate learning detectors directly boring video daily activities <eos> instead using bounding boxes explore use action descriptions supervision since they relatively easy gather <eos> common issue however object interest involved human actions often absent global action descriptions known missing label <eos> tackle problem propose novel temporal dynamic graph long short term memory network td graph lstm <eos> td graph lstm enables global temporal reasoning constructing dynamic graph based temporal correlations object proposals spans entire video <eos> missing label issue each individual frame thus significantly alleviated transferring knowledge across correlated object proposals whole video <eos> extensive evaluations large scale daily life action dataset <eos> charades demonstrates superiority proposed method <eos> also release object bounding box annotations more than frames charades <eos> believe annotated data also benefit other research video based object recognition future <eos> <eop> vqs linking segmentations questions answers supervised attention vqa question focused semantic segmentation <eos> rich dense human labeled datasets main enabling factor among others recent exciting work vision language understanding <eos> many seemingly distinct annotations <eos> semantic segmentation visual questions answering vqa inherently connected they reveal different levels perspectives human understandings about same visual scenes even same set ms coco image <eos> popularity ms coco could strongly correlate annotations tasks <eos> explicitly linking them up envision significantly benefit only individual tasks but also overarching goal unified vision language understand <eos> present preliminary work linking instance segmentations provided ms coco questions answers qa vqa dataset <eos> call collected links visual questions segmentation answers vqs <eos> they transfer human supervision between previously separate tasks offer more effective leverage existing problems also open door new tasks richer models <eos> study two applications vqs data paper supervised attention vqa novel question focused semantic segmentation task <eos> former obtain state art result vqa real multiple choice task simply augmenting multilayer perceptrons some attention feature learned using segmentation qa links explicit supervision <eos> put latter perspective study two plausible method oracle upper bound <eos> <eop> multi modal factorized bilinear pooling co attention learning visual question answering <eos> visual question answering vqa challenging because requires simultaneous understanding both visual content image textual content questions <eos> approaches used represent image questions fine grained manner questions fuse multi modal feature play key roles performance <eos> bilinear pooling based models shown outperform traditional linear models vqa but their high dimensional representations high computational complexity may seriously limit their applicability practice <eos> multi modal feature fusion here develop multi modal factorized bilinear mfb pooling approach efficiently effectively combine multi modal feature result superior performance vqa compared other bilinear pooling approaches <eos> fine grained image question representation develop co attention mechanism using end end deep network architecture jointly learn both image question attentions <eos> combining proposed mfb approach co attention learning new network architecture provides unified model vqa <eos> experimental result demonstrate single mfb co attention model achieves new state art performance real world vqa dataset <eos> code available github <eos> com yuzcccc mfb <eop> scnet learning semantic correspondence <eos> paper addresses problem establishing semantic correspondences between image depicting different instances same object scene category <eos> previous approaches focus either combining spatial regularizer hand crafted feature learning correspondence model appearance only <eos> propose instead convolutional neural network architecture called scnet learning geometrically plausible model semantic correspondence <eos> scnet uses region proposals matching primitives explicitly incorporates geometric consistency its loss function <eos> trained image pairs obtained pascal voc keypoint dataset comparative evaluation several standard benchmarks demonstrates proposed approach substantially outperforms both recent deep learning architectures previous method based hand crafted feature <eos> <eop> soft proposal network weakly supervised object localization <eos> weakly supervised object localization remains challenging only image labels instead bounding boxes available during training <eos> object proposal effective component localization but often computationally expensive incapable joint optimization some remaining modules <eos> paper best knowledge first time integrate weakly supervised object proposal into convolutional neural network cnn end end learning manner <eos> design network component soft proposal sp plugged into any standard convolutional architecture introduce nearly cost free object proposal orders magnitude faster than state art method <eos> sp augmented cnn referred soft proposal network spns iteratively evolved object proposals generated based deep feature maps then projected back further jointly optimized network parameters image level supervision only <eos> through unified learning process spns learn better object centric filters discover more discriminative visual evidence suppress background interference significantly boosting both weakly supervised object localization classification performance <eos> report best result popular benchmarks including pascal voc ms coco imagenet <eos> <eop> class rectification hard mining imbalanced deep learning <eos> recognising detailed facial clothing attributes image people challenging task computer vision especially when training data both very large scale extremely imbalanced among different attribute classes <eos> address problem formulate novel scheme batch incremental hard sample mining minority attribute classes imbalanced large scale training data <eos> develop end end deep learning framework capable avoiding dominant effect majority classes discovering sparsely sampled boundaries minority classes <eos> made possible introducing class rectification loss crl regularising algorithm <eos> demonstrate advantages scalability crl over existing state art attribute recognition imbalanced data learning models two large scale imbalanced benchmark datasets celeba facial attribute dataset domain clothing attribute dataset <eos> <eop> generating high quality crowd density maps using contextual pyramid cnn <eos> present novel method called contextual pyramid cnn cp cnn generating high quality crowd density count estimation explicitly incorporating global local contextual information crowd image <eos> proposed cp cnn consists four modules global context estimator gce local context estimator lce density map estimator dme fusion cnn cnn <eos> gce vgg based cnn encodes global context trained classify input image into different density classes whereas lce another cnn encodes local context information trained perform patch wise classification input image into different density classes <eos> dme multi column architecture based cnn aims generate high dimensional feature maps input image fused contextual information estimated gce lce using cnn <eos> generate high resolution high quality density maps cnn uses set convolutional fractionally strided convolutional layer trained along dme end end fashion using combination adversarial loss pixel level euclidean loss <eos> extensive experiments highly challenging datasets show proposed method achieves significant improvements over state art method <eos> <eop> see glass half full reasoning about liquid containers their volume content <eos> humans rich understanding liquid containers their contents example effortlessly pour water pitcher cup <eos> doing so requires estimating volume cup approximating amount water pitcher predicting behavior water when tilt pitcher <eos> very little attention computer vision made liquids their containers <eos> paper study liquid containers their contents propose method estimate volume containers approximate amount liquid them perform comparative volume estimations all single rgb image <eos> furthermore show result proposed model predicting behavior liquids inside containers when one tilts containers <eos> also introduce new dataset containers liquid content coqe contains more than image liquid containers context labelled volume amount content bounding box annotation corresponding similar three dimensional cad models <eos> <eop> hierarchical multimodal lstm dense visual semantic embedding <eos> address problem dense visual semantic embedding maps only full sentences whole image but also phrases within sentences salient region within image into multimodal embedding space <eos> result produce several region oriented expressive phrases rather than just overview sentence describe image <eos> particular present hierarchical structured recurrent neural network rnn namely hierarchical multimodal lstm hm lstm model <eos> different chain structured rnn model presents hierarchical structure so naturally build representations phrases image region further exploit their hierarchical relations <eos> moreover fine grained correspondences between phrases image region automatically learned utilized boost learning dense embedding space <eos> extensive experiments several datasets validate efficacy proposed method compares favorably state art method <eos> <eop> identity aware textual visual matching latent co attention <eos> textual visual matching aims measuring similarities between sentence descriptions image <eos> most existing method tackle problem without effectively utilizing identity level annotations <eos> paper propose identity aware two stage framework textual visual matching problem <eos> stage cnn lstm network learns embed cross modal feature novel cross modal cross entropy cmce loss <eos> stage network able efficiently screen easy incorrect matchings also provide initial training point stage training <eos> stage cnn lstm network refines matching result latent co attention mechanism <eos> spatial attention relates each word corresponding image region while latent semantic attention aligns different sentence structures make matching result more robust sentence structure variations <eos> extensive experiments three datasets identity level annotations show framework outperforms state art approaches large margins <eos> <eop> learning deep neural network vehicle re id visual spatio temporal path proposals <eos> vehicle re identification important problem many applications video surveillance intelligent transportation <eos> gains increasing attention because recent advances person re identification techniques <eos> however unlike person re identification visual differences between pairs vehicle image usually subtle even challenging humans distinguish <eos> incorporating additional spatio temporal information vital solving challenging re identification task <eos> existing vehicle re identification method ignored used over simplified models spatio temporal relations between vehicle image <eos> paper propose two stage framework incorporates complex spatio temporal information effectively regularizing re identification result <eos> given pair vehicle image their spatio temporal information candidate visual spatio temporal path first generated chain mrf model deeply learned potential function each visual spatio temporal state corresponds actual vehicle image its spatio temporal information <eos> siamese cnn path lstm model takes candidate path well pairwise queries generate their similarity score <eos> extensive experiments analysis show effectiveness proposed method individual components <eos> <eop> learning noisy labels distillation <eos> ability learning noisy labels very useful many visual recognition tasks vast amount data noisy labels relatively easy obtain <eos> traditionally label noise treated statistical outliers techniques such importance re weighting bootstrapping proposed alleviate problem <eos> according observation real world noisy labels exhibit multi mode characteristics true labels rather than behaving like independent random outliers <eos> work propose unified distillation framework use side information including small clean dataset label relations knowledge graph hedge risk learning noisy labels <eos> unlike traditional approaches evaluated based simulated label noises propose suite new benchmark datasets sports species artifacts domains evaluate task learning noisy labels practical setting <eos> empirical study demonstrates effectiveness proposed method all domains <eos> <eop> dsod learning deeply supervised object detectors scratch <eos> present deeply supervised object detector dsod framework learn object detectors scratch <eos> state art object objectors rely heavily off shelf network pre trained large scale classification datasets like imagenet incurs learning bias due difference both loss functions category distributions between classification detection tasks <eos> model fine tuning detection task could alleviate bias some extent but fundamentally <eos> besides transferring pre trained models classification detection between discrepant domains even more difficult <eos> rgb depth image <eos> better solution tackle two critical problems train object detectors scratch motivates proposed dsod <eos> previous efforts direction mostly failed due much more complicated loss functions limited training data object detection <eos> dsod contribute set design principles training object detectors scratch <eos> one key findings deep supervision enabled dense layer wise connections plays critical role learning good detector <eos> combining several other principles develop dsod following single shot detection ssd framework <eos> experiments pascal voc ms coco datasets demonstrate dsod achieve better result than state art solutions much more compact models <eos> instance dsod outperforms ssd all three benchmarks real time detection speed while requires only parameters ssd parameters faster rcnn <eos> <eop> phrase localization visual relationship detection comprehensive image language cues <eos> paper presents framework localization grounding phrases image using large collection linguistic visual cues <eos> model appearance size position entity bounding boxes adjectives contain attribute information spatial relationships between pairs entities connected verbs prepositions <eos> special attention given relationships between people clothing body part mentions they useful distinguishing individuals <eos> automatically learn weights combining cues test time perform joint inference over all phrases caption <eos> resulting system produces state art performance phrase localization flickr entities dataset visual relationship detection stanford vrd dataset <eos> <eop> chained cascade network object detection <eos> cascade widely used approach rejects obvious negative sample early stages learning better classifier faster inference <eos> paper presents chained cascade network cc net <eos> cc net there many cascade stages <eos> preceding cascade stages placed shallow layer <eos> easy hard examples rejected shallow layer so computation deeper wider layer required <eos> way feature classifiers latter stages handle more difficult sample help feature classifiers previous stages <eos> yields consistent boost detection performance pascal voc imagenet both fast rcnn faster rcnn <eos> cc net saves computation both training testing <eos> <eop> vpgnet vanishing point guided network lane road marking detection recognition <eos> paper propose unified end end trainable multi task network jointly handles lane road marking detection recognition guided vanishing point under adverse weather conditions <eos> tackle rainy low illumination conditions extensively studied until now due clear challenges <eos> example image taken under rainy days subject low illumination while wet roads cause light reflection distort appearance lane road markings <eos> night color distortion occurs under limited illumination <eos> result no benchmark dataset exists only few developed algorithms work under poor weather conditions <eos> address shortcoming build up lane road marking benchmark consists about image lane road marking classes under four different scenarios no rain rain heavy rain night <eos> train evaluate several versions proposed multi task network validate importance each task <eos> resulting approach vpgnet detect classify lanes road markings predict vanishing point single forward pass <eos> experimental result show approach achieves high accuracy robustness under various conditions real time fps <eos> benchmark vpgnet model will publicly available <eos> <eop> unsupervised learning important object first person video <eos> first person camera placed person head captures object important camera wearer <eos> most prior method task learn detect such important object manually labeled first person data supervised fashion <eos> however important object strongly related camera wearer internal state such his intentions attention thus only person wearing camera provide importance labels <eos> such constraint makes annotation process costly limited scalability <eos> work show detect important object first person image without supervision camera wearer even third person labelers <eos> formulate important detection problem interplay between segmentation recognition agents <eos> segmentation agent first proposes possible important object segmentation mask each image then feeds recognition agent learns predict important object mask using visual semantics spatial feature <eos> implement such interplay between both agents via alternating cross pathway supervision scheme inside proposed visual spatial network vsn <eos> vsn consists spatial visual pathways one learns common visual semantics while other focuses spatial location cues <eos> unsupervised learning accomplished via cross pathway supervision one pathway feeds its predictions segmentation agent proposes candidate important object segmentation mask then used other pathway supervisory signal <eos> show method success two different important object datasets method achieves similar better result supervised method <eos> <eop> analysis visual question answering algorithms <eos> visual question answering vqa algorithm must answer text based questions about image <eos> while multiple datasets vqa created since late they all flaws both their content way algorithms evaluated them <eos> result evaluation scores inflated predominantly determined answering easier questions making difficult compare different method <eos> paper analyze existing vqa algorithms using new dataset called task driven image understanding challenge tdiuc over <eos> million questions organized into different categories <eos> also introduce questions meaningless given image force vqa system reason about image content <eos> propose new evaluation schemes compensate over represented question types make easier study strengths weaknesses algorithms <eos> analyze performance both baseline state art vqa models including multi modal compact bilinear pooling mcb neural module network recurrent answering units <eos> experiments establish how attention helps certain categories more than others determine models work better than others explain how simple models <eos> mlp surpass more complex models mcb simply learning answer large easy question categories <eos> <eop> visual relationship detection internal external linguistic knowledge distillation <eos> understanding visual relationship between two object involves identifying subject object predicate relating them <eos> leverage strong correlations between predicate subj obj pair both semantically spatially predict predicates conditioned subjects object <eos> modeling three entities jointly more accurately reflects their relationships compared modeling them independently but complicates learning since semantic space visual relationships huge training data limited especially long tail relationships few instances <eos> overcome use knowledge linguistic statistics regularize visual model learning <eos> obtain linguistic knowledge mining both training annotations internal knowledge publicly available text <eos> wikipedia external knowledge computing conditional probability distribution predicate given subj obj pair <eos> train visual model distill knowledge into deep model achieve better generalization <eos> experimental result visual relationship detection vrd visual genome datasets suggest linguistic knowledge distillation model outperforms state art method significantly especially when predicting unseen relationships <eos> vrd zero shot testing set <eos> <eop> two stream siamese convolutional neural network person re identification <eos> person re identification important task video surveillance systems <eos> formally defined establishing correspondence between image person taken different cameras different times <eos> pa per present two stream convolutional neural network each stream siamese network <eos> architecture learn spatial temporal information separately <eos> also propose weighted two stream training objective function combines siamese cost spatial temporal streams objective predicting person identity <eos> evaluate proposed method publicly available prid ilids vid datasets demonstrate efficacy proposed method <eos> average top rank matching accuracy higher than accuracy achieved cross view quadratic discriminant analysis used combination hierarchical gaussian descriptor gog xqda higher than recurrent neural network method <eos> <eop> no more discrimination cross city adaptation road scene segmenters <eos> despite recent success deep learning based semantic segmentation deploying pre trained road scene segmenter city whose image presented training set would achieve satisfactory performance due dataset biases <eos> instead collecting large number annotated image each city interest train refine segmenter propose unsupervised learning approach adapt road scene segmenters across different cities <eos> utilizing google street view its time machine feature collect unannotated image each road scene different times so associated static object priors extracted accordingly <eos> advancing joint global class specific domain adversarial learning framework adaptation pre trained segmenters city achieved without need any user annotation interaction <eos> show method improves performance semantic segmentation multiple cities across continents while performs favorably against state art approaches requiring annotated training data <eos> <eop> open vocabulary scene parsing <eos> recognizing arbitrary object wild challenging problem due limitations existing classification models datasets <eos> paper propose new task aims parsing scenes large open vocabulary several evaluation metrics explored problem <eos> approach joint image pixel word concept embeddings framework word concepts connected semantic relations <eos> validate open vocabulary prediction ability framework ade dataset covers wide variety scenes object <eos> further explore trained joint embedding space show its interpretability <eos> <eop> learned watershed end end learning seeded segmentation <eos> learned boundary maps known outperform hand crafted ones basis watershed algorithm <eos> show first time how train watershed computation jointly boundary map prediction <eos> estimator merging priorities cast neural network convolutional over space recurrent over iterations <eos> latter allows learning complex shape priors <eos> method gives best known seeded segmentation result cremi segmentation challenge <eos> <eop> curriculum domain adaptation semantic segmentation urban scenes <eos> during last half decade convolutional neural network cnn triumphed over semantic segmentation core task various emerging industrial applications such autonomous driving medical imaging <eos> however train cnn requires huge amount data difficult collect laborious annotate <eos> recent advances computer graphics make possible train cnn models photo realistic synthetic data computer generated annotations <eos> despite domain mismatch between real image synthetic data significantly decreases models performance <eos> hence propose curriculum style learning approach minimize domain gap semantic segmentation <eos> curriculum domain adaptation solves easy tasks first order infer some necessary properties about target domain particular first task learn global label distributions over image local distributions over landmark superpixels <eos> easy estimate because image urban traffic scenes strong idiosyncrasies <eos> size spatial relations buildings streets cars etc <eos> then train segmentation network such way network predictions target domain follow inferred properties <eos> experiments method significantly outperforms baselines well only known existing approach same problem <eos> <eop> scale adaptive convolutions scene parsing <eos> many existing scene parsing method adopt convolutional neural network fixed size receptive fields frequently result inconsistent predictions large object invisibility small object <eos> tackle issue propose scale adaptive convolution acquire flexible size receptive fields during scene parsing <eos> through adding new scale regression layer dynamically infer position adaptive scale coefficients adopted resize convolutional patches <eos> consequently receptive fields adjusted automatically according various sizes object scene image <eos> thus problems invisible small object inconsistent large object predictions alleviated <eos> furthermore proposed scale adaptive convolutions only differentiable learn convolutional parameters scale coefficients end end way but also high parallelizability convenience gpu implementation <eos> additionally since new scale regression layer learned implicitly any extra training supervision object sizes unnecessary <eos> extensive experiments cityscapes ade datasets well demonstrate effectiveness proposed scale adaptive convolutions <eos> <eop> privacy preserving visual learning using doubly permuted homomorphic encryption <eos> propose privacy preserving framework learning visual classifiers leveraging distributed private image data <eos> framework designed aggregate multiple classifiers updated locally using private data ensure no private information about data exposed during after its learning procedure <eos> utilize homomorphic cryptosystem aggregate local classifiers while they encrypted thus kept secret <eos> overcome high computational cost homomorphic encryption high dimensional classifiers impose sparsity constraints local classifier updates propose novel efficient encryption scheme named doubly permuted homomorphic encryption dphe tailored sparse high dimensional data <eos> dphe decomposes sparse data into its constituent non zero values their corresponding support indices ii applies homomorphic encryption only non zero values iii employs double permutations support indices make them secret <eos> experimental evaluation several public datasets shows proposed approach achieves comparable performance against state art visual recognition method while preserving privacy significantly outperforms other privacy preserving method <eos> <eop> multi task self supervised visual learning <eos> investigate method combining multiple self supervised tasks <eos> supervised tasks data collected without manual labeling order train single visual representation <eos> first provide apples apples comparison four different self supervised tasks using very deep resnet architecture <eos> then combine tasks jointly train network <eos> also explore lasso regularization encourage network factorize information its representation method harmonizing network inputs order learn more unified representation <eos> evaluate all method imagenet classification pascal voc detection nyu depth prediction <eos> result show deeper network work better combining tasks even via naive multi head architecture always improves performance <eos> best joint network nearly matches pascal performance model pre trained imagenet classification matches imagenet network nyu depth prediction <eos> <eop> self balanced min cut algorithm image clustering <eos> many spectral clustering algorithms proposed successfully applied image data analysis such content based image retrieval image annotation image indexing <eos> conventional spectral clustering algorithms usually involve two stage process eigendecomposition similarity matrix clustering assignments eigenvectors means spectral rotation <eos> however final clustering assignments obtained two stage process may deviate assignments directly optimize original objective function <eos> moreover most method usually very high computational complexities <eos> paper propose new min cut algorithm image clustering scales linearly data size <eos> new method self balanced min cut model proposed exclusive lasso implicitly introduced balance regularizer order produce balanced partition <eos> propose iterative algorithm solve new model time complexity number sample <eos> theoretical analysis reveals new method simultaneously minimize graph cut balance partition across all clusters <eos> series experiments were conducted both synthetic benchmark data set experimental result show superior performance new method <eos> <eop> second order information helpful large scale visual recognition <eos> stacking layer convolution nonlinearity convolutional network convnets effectively learn low level high level feature discriminative representations <eos> since end goal large scale recognition delineate complex boundaries thousands classes adequate exploration feature distributions important realizing full potentials convnets <eos> however state art works concentrate only deeper wider architecture design while rarely exploring feature statistics higher than first order <eos> take step towards addressing problem <eos> method consists covariance pooling instead most commonly used first order pooling high level convolutional feature <eos> main challenges involved robust covariance estimation given small sample large dimensional feature usage manifold structure covariance matrices <eos> address challenges present matrix power normalized covariance mpn cov method <eos> develop forward backward propagation formulas regarding nonlinear matrix functions such mpn cov trained end end <eos> addition analyze both qualitatively quantitatively its advantage over well known log euclidean metric <eos> imagenet validation set combining mpn cov achieve over <eos> gains alexnet vgg vgg respectively integration mpn cov into layer resnet outperforms resnet comparable resnet <eos> source code will available project page www <eos> org mpn cov <eos> <eop> factorized bilinear models image recognition <eos> although deep convolutional neural network cnn liberated their power various computer vision tasks most important components cnn convolutional layer fully connected layer still limited linear transformations <eos> paper propose novel factorized bilinear fb layer model pairwise feature interactions considering quadratic terms transformations <eos> compared existing method tried incorporate complex non linearity structures into cnn factorized parameterization makes fb layer only require linear increase parameters affordable computational cost <eos> further reduce risk overfitting fb layer specific remedy called dropfactor devised during training process <eos> also analyze connection between fb layer some existing models show fb layer generalization them <eos> finally validate effectiveness fb layer several widely adopted datasets including cifar cifar imagenet demonstrate superior result compared various state art deep models <eos> <eop> octree generating network efficient convolutional architectures high resolution three dimensional outputs <eos> present deep convolutional decoder architecture generate volumetric three dimensional outputs compute memory efficient manner using octree representation <eos> network learns predict both structure octree occupancy values individual cells <eos> makes particularly valuable technique generating three dimensional shapes <eos> contrast standard decoders acting regular voxel grids architecture cubic complexity <eos> allows representing much higher resolution outputs limited memory budget <eos> demonstrate several application domains including three dimensional convolutional autoencoders generation object whole scenes high level representations shape single image <eos> <eop> truncating wide network using binary tree architectures <eos> paper propose binary tree architecture truncate architecture wide network reducing width network <eos> more precisely proposed architecture width incrementally reduced lower layer higher layer order increase expressive capacity network less increase parameter size <eos> also order ease gradient vanishing problem feature obtained different layer concatenated form output architecture <eos> employing proposed architecture baseline wide network construct train new network same depth but considerably less number parameters <eos> experimental analyses observe proposed architecture enables obtain better parameter size accuracy trade off compared baseline network using various benchmark image classification datasets <eos> result show model decrease classification error baseline <eos> cifar using only parameters baseline <eos> <eop> bringing background into foreground making all classes equal weakly supervised video semantic segmentation <eos> pixel level annotations expensive time consuming obtain <eos> hence weak supervision using only image tags could significant impact semantic segmentation <eos> recent years seen great progress weakly supervised semantic segmentation whether single image video <eos> however most existing method designed handle single background class <eos> practical applications such autonomous navigation often crucial reason about multiple background classes <eos> paper introduce approach doing so making use classifier heatmaps <eos> then develop two stream deep architecture jointly leverages appearance motion design loss based heatmaps train <eos> experiments demonstrate benefits classifier heatmaps two stream architecture challenging urban scene datasets youtube object benchmark obtain state art result <eos> <eop> view adaptive recurrent neural network high performance human action recognition skeleton data <eos> skeleton based human action recognition recently attracted increasing attention due popularity three dimensional skeleton data <eos> one main challenge lies large view variations captured human actions <eos> propose novel view adaptation scheme automatically regulate observation viewpoints during occurrence action <eos> rather than re positioning skeletons based human defined prior criterion design view adaptive recurrent neural network rnn lstm architecture enables network itself adapt most suitable observation viewpoints end end <eos> extensive experiment analyses show proposed view adaptive rnn model strives transform skeletons various views much more consistent viewpoints maintain continuity action rather than transforming every frame same position same body orientation <eos> model achieves significant improvement over state art approaches three benchmark datasets <eos> <eop> joint discovery object states manipulation actions <eos> many human activities involve object manipulations aiming modify object state <eos> examples common state changes include full empty bottle open closed door attached detached car wheel <eos> work seek automatically discover states object associated manipulation actions <eos> given set video particular task propose joint model learns identify object states localize state modifying actions <eos> model formulated discriminative clustering cost constraints <eos> assume consistent temporal order changes object states manipulation actions introduce new optimization techniques learn model parameters without additional supervision <eos> demonstrate successful discovery seven manipulation actions corresponding object states new dataset video depicting real life object manipulations <eos> show joint formulation result improvement object state discovery action recognition vice versa <eos> <eop> actions needed understanding human actions video <eos> right way reason about human activities directions forward most promising work analyze current state human activity understanding video <eos> goal paper examine datasets evaluation metrics algorithms potential future directions <eos> look qualitative attributes define activities such pose variability brevity density <eos> experiments consider multiple state art algorithms multiple datasets <eos> result demonstrate while there inherent ambiguity temporal extent activities current datasets still permit effective benchmarking <eos> discover fine grained understanding object pose when combined temporal reasoning likely yield substantial improvements algorithmic accuracy <eos> present many kinds information will needed achieve substantial gains activity understanding object verbs intent sequential reasoning <eos> software additional information will made available provide other researchers detailed diagnostics understand their own algorithms <eos> <eop> lattice long short term memory human action recognition <eos> human actions captured video sequences three dimensional signals characterizing visual appearance motion dynamics <eos> learn action patterns existing method adopt convolutional recurrent neural network cnn rnns <eos> cnn based method effective learning spatial appearances but limited modeling long term motion dynamics <eos> rnns especially long short term memory lstm able learn temporal motion dynamics <eos> however naively applying rnns video sequences convolutional manner implicitly assumes motions video stationary across different spatial locations <eos> assumption valid short term motions but invalid when duration motion long <eos> work propose lattice lstm extends lstm learning independent hidden state transitions memory cells individual spatial locations <eos> method effectively enhances ability model dynamics across time addresses non stationary issue long term motion dynamics without significantly increasing model complexity <eos> additionally introduce novel multi modal training procedure training network <eos> unlike traditional two stream architectures use rgb optical flow information input two stream model leverages both modalities jointly train both input gates both forget gates network rather than treating two streams separate entities no information about other <eos> apply end end system benchmark datasets ucf hmdb human action recognition <eos> experiments show both datasets proposed method outperforms all existing ones based lstm cnn similar model complexities <eos> <eop> common action discovery localization unconstrained video <eos> similar common object discovery image video great interests discover locate common actions video benefit many video analytics applications such video summarization search understanding <eos> work tackle problem common action discovery localization unconstrained video assume know types numbers locations common actions video <eos> furthermore each video contain zero one several common action instances <eos> perform automatic discovery localization such challenging scenarios first generate action proposals using human prior <eos> building affinity graph among all action proposals formulate common action discovery subgraph density maximization problem select proposals containing common actions <eos> avoid enumerating exponentially large solution space propose efficient polynomial time optimization algorithm <eos> solves problem up user specified error bound respect global optimal solution <eos> experimental result several datasets show even without any prior knowledge common actions method robustly locate common actions collection video <eos> <eop> pixel level matching video object segmentation using convolutional neural network <eos> propose novel video object segmentation algorithm based pixel level matching using convolutional neural network cnn <eos> network aims distinguish target area background basis pixel level similarity between two object units <eos> proposed network represents target object using feature different depth layer order take advantage both spatial details category level semantic information <eos> furthermore propose feature compression technique drastically reduces memory requirements while maintaining capability feature representation <eos> two stage training pre training fine tuning allows network handle any target object regardless its category even if object type belong pre training data variations its appearance through video sequence <eos> experiments large datasets demonstrate effectiveness model against related method terms accuracy speed stability <eos> finally introduce transferability network different domains such infrared data domain <eos> <eop> am baller basketball performance assessment first person video <eos> paper presents method assess basketball player performance his her first person video <eos> key challenge lies fact evaluation metric highly subjective specific particular evaluator <eos> leverage first person camera address challenge <eos> spatiotemporal visual semantics provided first person view allows reason about camera wearer actions while he she participating unscripted basketball game <eos> method takes player first person video provides player performance measure specific evaluator preference <eos> achieve goal first use convolutional lstm network detect atomic basketball events first person video <eos> network ability zoom salient region addresses issue severe camera wearer head movement first person video <eos> detected atomic events then passed through gaussian mixtures construct highly non linear visual spatiotemporal basketball assessment feature <eos> finally use feature learn basketball assessment model pairs labeled first person basketball video basketball expert indicates two players better <eos> demonstrate despite knowing basketball evaluator criterion model learns accurately assess players real world games <eos> furthermore model also discover basketball events contribute positively negatively player performance <eos> <eop> deep cropping via attention box prediction aesthetics assessment <eos> model photo cropping problem cascade attention box regression aesthetic quality classification based deep learning <eos> neural network designed two branches predicting attention bounding box analyzing aesthetics respectively <eos> predicted attention box treated initial crop window set cropping candidates generated around without missing important information <eos> then aesthetics assessment employed select final crop one best aesthetic quality <eos> network cropping candidates share feature within full image convolutional feature maps thus avoiding repeated feature computation leading higher computation efficiency <eos> via leveraging rich data attention prediction aesthetics assessment proposed method produces high quality cropping result even limited availability training data photo cropping <eos> experimental result demonstrate competitive result fast processing speed fps all steps <eos> <eop> raster vector revisiting floorplan transformation <eos> paper addresses problem converting rasterized floorplan image into vector graphics representation <eos> unlike existing approaches rely sequence low level image processing heuristics adopt learning based approach <eos> neural architecture first transforms rasterized image set junctions represent low level geometric semantic information <eos> wall corners door end point <eos> integer programming then formulated aggregate junctions into set simple primitives <eos> wall lines door lines icon boxes produce vectorized floorplan while ensuring topologically geometrically consistent result <eos> algorithm significantly outperforms existing method achieves around precision recall getting range production ready performance <eos> vector representation allows three dimensional model popup better indoor scene visualization direct model manipulation architectural remodeling further computational applications such data analysis <eos> system efficient converted hundred thousand production level floorplan image into vector representation generated three dimensional popup models <eos> <eop> deep textspotter end end trainable scene text localization recognition framework <eos> method scene text localization recognition proposed <eos> novelties include training both text detection recognition single end end pass structure recognition cnn geometry its input layer preserves aspect text adapts its resolution data <eos> proposed method achieves state art accuracy end end text recognition two standard datasets icdar icdar whilst being order magnitude faster than competing method whole pipeline runs frames per second nvidia gpu <eos> <eop> playing benchmarks <eos> present benchmark suite visual perception <eos> benchmark based more than high resolution video frames all annotated ground truth data both low level high level vision tasks including optical flow semantic instance segmentation object detection tracking object level three dimensional scene layout visual odometry <eos> ground truth data all tasks available every frame <eos> data was collected while driving riding walking total kilometers diverse ambient conditions realistic virtual world <eos> create benchmark developed new approach collecting ground truth data simulated worlds without access their source code content <eos> conduct statistical analyses show composition scenes benchmark closely matches composition corresponding physical environments <eos> realism collected data further validated via perceptual experiments <eos> analyze performance state art method multiple tasks providing reference baselines highlighting challenges future research <eos> <eop> unpaired image image translation using cycle consistent adversarial network <eos> image image translation class vision graphics problems goal learn mapping between input image output image using training set aligned image pairs <eos> however many tasks paired training data will available <eos> present approach learning translate image source domain target domain absence paired examples <eos> goal learn mapping such distribution image indistinguishable distribution using adversarial loss <eos> because mapping highly under constrained couple inverse mapping introduce cycle consistency loss push vice versa <eos> qualitative result presented several tasks paired training data exist including collection style transfer object transfiguration season transfer photo enhancement etc <eos> quantitative comparisons against several prior method demonstrate superiority approach <eos> <eop> gans biological image synthesis <eos> paper propose novel application generative adversarial network gan synthesis cells imaged fluorescence microscopy <eos> compared natural image cells tend simpler more geometric global structure facilitates image generation <eos> however correlation between spatial pattern different fluorescent proteins reflects important biological functions synthesized image capture relationships relevant biological applications <eos> adapt gans task hand propose new models casual dependencies between image channels generate multi channel image would impossible obtain experimentally <eos> evaluate approach using two independent techniques compare against sensible baselines <eos> finally demonstrate interpolating across latent space mimic known changes protein localization occur through time during cell cycle allowing predict temporal evolution static image <eos> <eop> learning synthesize rgbd light field single image <eos> present machine learning algorithm takes input rgb image synthesizes rgbd light field color depth scene each ray direction <eos> training introduce largest public light field dataset consisting over plenoptic camera light fields scenes containing flowers plants <eos> synthesis pipeline consists convolutional neural network cnn estimates scene geometry stage renders lambertian light field using geometry second cnn predicts occluded rays non lambertian effects <eos> algorithm builds recent view synthesis method but unique predicting rgbd each light field ray improving unsupervised single image depth estimation enforcing consistency ray depths should intersect same scene point <eos> <eop> neural epi volume network shape light field <eos> paper presents novel deep regression network extract geometric information light field lf data <eos> network builds upon shaped network architectures <eos> network involve two symmetric parts encoding decoding part <eos> first part network encodes relevant information given input into set high level feature maps <eos> second part generated feature maps then decoded desired output <eos> predict reliable robust depth information proposed network examines three dimensional subsets lf called epipolar plane image epi volumes <eos> important aspect network use three dimensional convolutional layer allow propagate information two spatial dimensions one directional dimension lf <eos> compared previous work allows additional spatial regularization reduces depth artifacts simultaneously maintains clear depth discontinuities <eos> experimental result show approach allows create high quality reconstruction result outperform current state art shape light field sflf techniques <eos> main advantage proposed approach ability provide high quality reconstructions low computation time <eos> <eop> material editing using physically based rendering network <eos> ability edit materials object image desirable many content creators <eos> however extremely challenging task requires disentangle intrinsic physical properties image <eos> propose end end network architecture replicates forward image formation process accomplish task <eos> specifically given single image network first predicts intrinsic properties <eos> shape illumination material then provided rendering layer <eos> layer performs network image synthesis thereby enabling network understand physics behind image formation process <eos> proposed rendering layer fully differentiable supports both diffuse specular materials thus applicable variety problem settings <eos> demonstrate rich set visually plausible material editing examples provide extensive comparative study <eos> <eop> turning corners into cameras principles method <eos> show walls other obstructions edges exploited naturally occurring cameras reveal hidden scenes beyond them <eos> particular demonstrate method using subtle spatio temporal radiance variations arise ground base edges construct one dimensional video hidden scene <eos> resulting technique used variety applications diverse physical settings <eos> standard rgb video recordings variations intensity use edge cameras recover video reveals number trajectories people moving occluded scene <eos> further show adjacent vertical edges such arise case open doorway yield stereo camera location hidden moving object recovered <eos> demonstrate technique number indoor outdoor environments involving varied surfaces illumination conditions <eos> <eop> linear differential constraints photo polarimetric height estimation <eos> paper present differential approach photo polarimetric shape estimation <eos> propose several alternative differential constraints based polarisation photometric shading information show how express them unified partial differential system <eos> method uses image ratios technique combine shading polarisation information order directly reconstruct surface height without first computing surface normal vectors <eos> moreover able remove non linearities so problem reduces solving linear differential problem <eos> also introduce new method estimating polarisation image multichannel data finally show possible estimate illumination directions two source setup extending method into uncalibrated scenario <eos> numerical point view use least squares formulation discrete version problem <eos> best knowledge first work consider unified differential approach solve photo polarimetric shape estimation directly height <eos> numerical result synthetic real world data confirm effectiveness proposed method <eos> <eop> polynomial solvers saturated ideals <eos> paper present new method creating polynomial solvers problems possibly infinite subset solutions undesirable uninteresting <eos> solutions typically arise simplifications made during modeling but also come degeneracies inherent geometry original problem <eos> proposed approach extends standard action matrix method saturated ideals <eos> allows add constraints some polynomials should non zero solutions <eos> only offer possibility improved performance removing superfluous solutions but makes larger class problems tractable <eos> previously problems infinitely many solutions could solved directly using action matrix method requires zero dimensional ideal <eos> contrast only require after removing unwanted solutions only finitely many remain <eos> evaluate method three applications optimal triangulation time arrival self calibration optimal vanishing point estimation <eos> <eop> shape inpainting using three dimensional generative adversarial network recurrent convolutional network <eos> recent advances convolutional neural network shown promising result three dimensional shape completion <eos> but due gpu memory limitations method only produce low resolution outputs <eos> inpaint three dimensional models semantic plausibility contextual details introduce hybrid framework combines three dimensional encoder decoder generative adversarial network three dimensional ed gan long term recurrent convolutional network lrcn <eos> three dimensional ed gan three dimensional convolutional neural network trained generative adversarial paradigm fill missing three dimensional data low resolution <eos> lrcn adopts recurrent neural network architecture minimize gpu memory usage incorporates encoder decoder pair into long short term memory network <eos> handling three dimensional model sequence slices lrcn transforms coarse three dimensional shape into more complete higher resolution volume <eos> while three dimensional ed gan captures global contextual structure three dimensional shape lrcn localizes fine grained details <eos> experimental result both real world synthetic data show reconstructions corrupted models result complete high resolution three dimensional object <eos> <eop> surfacenet end end three dimensional neural network multiview stereopsis <eos> paper proposes end end learning framework multiview stereopsis <eos> term network surfacenet <eos> takes set image their corresponding camera parameters input directly infers three dimensional model <eos> key advantage framework both photo consistency well geometric relations surface structure directly learned purpose multiview stereopsis end end fashion <eos> surfacenet fully three dimensional convolutional network achieved encoding camera parameters together image three dimensional voxel representation <eos> evaluate surfacenet large scale dtu benchmark <eos> <eop> making minimal solvers absolute pose estimation compact robust <eos> paper present new techniques constructing compact robust minimal solvers absolute pose estimation <eos> focus pfr problem but method propose applicable more general setting <eos> previous approaches pfr suffer artificial degeneracies come their formulation geometry original problem <eos> paper show how avoid false degeneracies create more robust solvers <eos> combined recently published techniques grobner basis solvers also able construct solvers significantly smaller <eos> evaluate solvers both real synthetic data show improved performance compared competing solvers <eos> finally show techniques directly applied <eos> pf problem get non degenerate solver competitive current state art <eos> <eop> surface detail enhancement single normal map <eos> three dimensional reconstruction obtained surface details mainly limited visual sensor due sampling quantization digitalization process <eos> how get fine grained three dimensional surface low cost still challenging obstacle terms experience equipment easy obtain <eos> work introduces novel framework enhancing surfaces reconstructed normal map assumptions hardware <eos> photometric stereo setup reflection model <eos> lambertion reflection necessarily needed <eos> propose use new measure angle profile infer hidden micro structure existing surfaces <eos> addition inferred result further improved domain discrete geometry processing dgp able achieve stable surface structure under selectable enhancement setting <eos> extensive simulation result show proposed method obtains significantly improvements over uniform sharpening method terms both subjective visual assessment objective quality metric <eos> <eop> rmpe regional multi person pose estimation <eos> multi person pose estimation wild challenging <eos> although state art human detectors demonstrated good performance small errors localization recognition inevitable <eos> errors cause failures single person pose estimator sppe especially method solely depend human detection result <eos> paper propose novel regional multi person pose estimation rmpe framework facilitate pose estimation presence inaccurate human bounding boxes <eos> framework consists three components symmetric spatial transformer network sstn parametric pose non maximum suppression nms pose guided proposals generator pgpg <eos> method able handle inaccurate bounding boxes redundant detections allowing achieve <eos> map mpii multi person dataset <eos> model source codes made publicly available <eos> <eop> online video object detection using association lstm <eos> video object detection fundamental tool many applications <eos> since direct application image based object detection cannot leverage rich temporal information inherent video data advocate detection long range video object pattern <eos> while long short term memory lstm de facto choice such detection currently lstm cannot fundamentally model object association between consecutive frames <eos> paper propose association lstm address fundamental association problem <eos> association lstm only regresses classifiy directly object locations categories but also associates feature represent each output object <eos> minimizing matching error between feature learn how associate object two consecutive frames <eos> additionally method works online manner important most video tasks <eos> compared traditional video object detection method approach outperforms them standard video datasets <eos> <eop> polyfit polygonal surface reconstruction point clouds <eos> propose novel framework reconstructing lightweight polygonal surfaces point clouds <eos> unlike traditional method focus either extracting good geometric primitives obtaining proper arrangements primitives emphasis work lies intersecting primitives planes only seeking appropriate combination them obtain manifold polygonal surface model without boundary <eos> show reconstruction point clouds cast binary labeling problem <eos> method based hypothesizing selection strategy <eos> first generate reasonably large set face candidates intersecting extracted planar primitives <eos> then optimal subset candidate faces selected through optimization <eos> optimization based binary linear programming formulation under hard constraints enforce final polygonal surface model manifold watertight <eos> experiments point clouds various sources demonstrate method generate lightweight polygonal surface models arbitrary piecewise planar object <eos> besides method capable recovering sharp feature robust noise outliers missing data <eos> <eop> progressive large scale invariant image matching scale space <eos> power modern image matching approaches still fundamentally limited abrupt scale changes image <eos> paper propose scale invariant image matching approach tackling very large scale variation views <eos> drawing inspiration scale space theory start encoding image scale space into compact multi scale representation <eos> then rather than trying find exact feature matches all one step propose progressive two stage approach <eos> first determine related scale levels scale space enclosing inlier feature correspondences based optimal exhaustive matching limited scale space <eos> second produce both image similarity measurement feature correspondences simultaneously after restricting matching between related scale levels robust way <eos> matching performance intensively evaluated vision tasks including image retrieval feature matching structure motion sfm <eos> successful integration challenging fusion high aerial low ground level views significant scale differences manifests superiority proposed approach <eos> <eop> efficient global three dimensional matching camera localization large scale three dimensional map <eos> given image street scene city paper develops new method quickly precisely pinpoint location well viewing direction image was taken against pre stored large scale three dimensional point cloud map city <eos> adopt recently developed three dimensional direct feature matching framework task <eos> challenging task especially large scale problems <eos> map size grows bigger many three dimensional point wider geographical area visually very similar even identical causing severe ambiguities three dimensional feature matching <eos> key quickly unambiguously find correct matches between query image large three dimensional map <eos> existing method solve problem mainly via comparing individual feature visual similarities local per feature manner thus only local solutions found inadequate large scale applications <eos> paper introduce global method harnesses global contextual information exhibited both within query image among all three dimensional point map <eos> achieved novel global ranking algorithm applied markov network built upon three dimensional map takes account only visual similarities between individual three dimensional matches but also their global compatibilities measured co visibility among all matching pairs found scene <eos> tests standard benchmark datasets show method achieved both higher precision comparable recall compared state art <eos> <eop> multi view non rigid refinement normal selection high quality three dimensional reconstruction <eos> recent years there variety proposals high quality three dimensional reconstruction fusion depth normal maps contain good low high frequency information respectively <eos> typically method create initial mesh representation complete object scene being scanned <eos> subsequently normal estimates assigned each mesh vertex mesh normal fusion step carried out <eos> paper present complete pipeline such depth normal fusion <eos> key innovations pipeline twofold <eos> firstly introduce global multi view non rigid refinement step corrects non rigid misalignment present depth normal maps <eos> demonstrate such correction crucial preserving fine scale three dimensional feature final reconstruction <eos> secondly despite adequate care averaging multiple normals invariably result blurring three dimensional detail <eos> mitigate problem propose approach selects one out many available normals <eos> global cost normal selection incorporates variety desirable properties efficiently solved using graph cuts <eos> demonstrate efficacy approach generating high quality three dimensional reconstructions both synthetic real three dimensional models compare existing method literature <eos> <eop> multi stage multi recursive input fully convolutional network neuronal boundary detection <eos> field connectomics neuroscientists seek identify cortical connectivity comprehensively <eos> neuronal boundary detection electron microscopy em image often done assist automatic reconstruction neuronal circuit <eos> but segmentation em image challenging problem requires detector able detect both filament like thin blob like thick membrane while suppressing ambiguous intracellular structure <eos> paper propose multi stage multi recursiveinput fully convolutional network address problem <eos> multiple recursive inputs one stage <eos> multiple side outputs different receptive field sizes learned lower stage provide multi scale contextual boundary information consecutive learning <eos> design biologically plausible likes human visual system compare different possible segmentation solutions address ambiguous boundary issue <eos> multi stage network trained end end <eos> achieves promising result two public available em segmentation datasets mouse piriform cortex dataset isbi em dataset <eos> <eop> depth image restoration light field scattering medium <eos> traditional imaging method computer vision algorithms often ineffective when image acquired scattering media such underwater fog biological tissue <eos> here explore use light field imaging algorithms image restoration depth estimation address image degradation medium <eos> towards end make following three contributions <eos> first present new single image restoration algorithm removes backscatter attenuation image better than existing method apply each view light field <eos> second combine novel transmission based depth cue existing correspondence defocus cues improve light field depth estimation <eos> densely scattering media transmission depth cue critical depth estimation since image low signal noise ratios significantly degrades performance correspondence defocus cues <eos> finally propose shearing refocusing multiple views light field recover single image higher quality than possible single view <eos> demonstrate benefits method through extensive experimental result water tank <eos> <eop> video reflection removal through spatio temporal optimization <eos> reflections obstruct content during video capture hence their removal desirable <eos> current removal techniques designed still image extracting only one reflection foreground one background layer input <eos> when extended video unpleasant artifacts such temporal flickering incomplete separation generated <eos> present technique video reflection removal jointly solving motion separation <eos> novelty work optimization formulation well motion initialization strategy <eos> present novel spatio temporal optimization takes frames input directly estimates frames output each layer <eos> aim fully utilize spatio temporal information objective terms <eos> motion initialization based iterative frame frame alignment instead direct alignment used current approaches <eos> compare against advanced video extensions state art significantly reduce temporal flickering improve separation <eos> addition reduce image blur recover moving object more accurately <eos> validate approach through subjective objective evaluations real controlled data <eos> <eop> efficient online local metric adaptation via negative sample person re identification <eos> many existing person re identification prid method typically attempt train faithful global metric offline cover enormous visual appearance variations so directly use online various probes identity matching <eos> however their need huge set positive training pairs very demanding practice <eos> contrast method paper advocates different paradigm part learning performed online but nominal costs so achieve online metric adaptation different input probes <eos> major challenge here no positive training pairs available probe anymore <eos> only exploiting easily available negative sample propose novel solution achieve local metric adaptation effectively efficiently <eos> each probe test time learns strictly positive semi definite dedicated local metric <eos> comparing offline global metric learning its computational cost negligible <eos> insight new method local hard negative sample actually provide tight constraints fine tune metric locally <eos> new local metric adaptation method generally applicable used top any global metric enhance its performance <eos> addition paper gives depth theoretical analysis justification new method <eos> prove new method guarantees reduction classification error asymptotically prove actually learns optimal local metric best approximate asymptotic case finite number training data <eos> extensive experiments comparative studies almost all major benchmarks viper qmul grid cuhk campus cuhk market confirmed effectiveness superiority method <eos> <eop> stepwise metric promotion unsupervised video person re identification <eos> intensive annotation cost rich but unlabeled data contained video motivate propose unsupervised video based person re identification re id method <eos> start two assumptions different video tracklets typically contain different persons given tracklets taken distinct places long intervals within each tracklet frames mostly same person <eos> based assumptions paper propose stepwise metric promotion approach estimate identities training tracklets iterates between cross camera tracklet association feature learning <eos> specifically use each training tracklet query perform retrieval cross camera training set <eos> method built reciprocal nearest neighbor search eliminate hard negative label matches <eos> cross camera nearest neighbors false matches initial rank list <eos> tracklet passes reciprocal nearest neighbor check considered same id query <eos> experimental result prid ilids vid mars datasets show proposed method achieves very competitive re id accuracy compared its supervised counterparts <eos> <eop> beyond face rotation global local perception gan photorealistic identity preserving frontal view synthesis <eos> photorealistic frontal view synthesis single face image wide range applications field face recognition <eos> although data driven deep learning method proposed address problem seeking solutions ample face data problem still challenging because intrinsically ill posed <eos> paper proposes two pathway generative adversarial network tp gan photorealistic frontal view synthesis simultaneously perceiving global structures local details <eos> four landmark located patch network proposed attend local textures addition commonly used global encoder decoder network <eos> except novel architecture make ill posed problem well constrained introducing combination adversarial loss symmetry loss identity preserving loss <eos> combined loss function leverages both frontal face distribution pre trained discriminative deep face models guide identity preserving inference frontal views profiles <eos> different previous deep learning method mainly rely intermediate feature recognition method directly leverages synthesized identity preserving image downstream tasks like face recognition attribution estimation <eos> experimental result demonstrate method only presents compelling perceptual result but also outperforms state art result large pose face recognition <eos> <eop> group re identification via unsupervised transfer sparse feature encoding <eos> person re identification best known problem associating single person observed one more disjoint cameras <eos> existing literature mainly addressed such issue neglecting fact people usually move groups like crowded scenarios <eos> believe additional information carried neighboring individuals provides relevant visual context exploited obtain more robust match single persons within group <eos> despite re identifying groups people compound common single person re identification problems introducing changes relative position persons within group severe self occlusions <eos> paper propose solution group re identification grounds transferring knowledge single person re identification group re identification exploiting sparse dictionary learning <eos> first dictionary sparse atoms learned using patches extracted single person image <eos> then learned dictionary exploited obtain sparsity driven residual group representation finally matched perform re identification <eos> extensive experiments lids groups two newly collected datasets show proposed solution outperforms state art approaches <eos> <eop> visual transformation aided contrastive learning video based kinship verification <eos> automatic kinship verification facial information relatively new open research problem computer vision <eos> paper explores possibility learning efficient facial representation video based kinship verification exploiting visual transformation between facial appearance kin pairs <eos> end siamese like coupled convolutional encoder decoder network proposed <eos> reveal resemblance patterns kinship while discarding similarity patterns also observed between people who kin relationship novel contrastive loss function defined visual appearance space <eos> further optimization learned representation fine tuned using feature based contrastive loss <eos> expression matching procedure employed model minimize negative influence expression differences between kin pairs <eos> each kin video analyzed sliding temporal window leverage short term facial dynamics <eos> effectiveness proposed method assessed seven different kin relationships using smile video kin pairs <eos> verification accuracy achieved improving state art <eos> <eop> decoder network over lightweight reconstructed feature fast semantic style transfer <eos> recently community style transfer trying incorporate semantic information into traditional system <eos> practice achieves better perceptual result transferring style between semantically corresponding region <eos> yet few efforts invested address computation bottleneck back propagation <eos> paper propose new framework fast semantic style transfer <eos> method decomposes semantic style transfer problem into feature reconstruction part feature decoder part <eos> reconstruction part tactfully solves optimization problem content loss style loss feature space particularly reconstructed feature <eos> significantly reduces computation propagating loss through whole network <eos> decoder part transforms reconstructed feature into stylized image <eos> through careful bridging two modules proposed approach only achieves competitive result backward optimization method but also about two orders magnitude faster <eos> <eop> blind image deblurring outlier handling <eos> deblurring image outliers attracted considerable attention recently <eos> however existing algorithms usually involve complex operations increase difficulty blur kernel estimation <eos> paper propose simple yet effective blind image deblurring algorithm handle blurred image outliers <eos> proposed method motivated observation outliers blurred image significantly affect goodness fit function approximation <eos> therefore propose algorithm model data fidelity term so outliers little effect kernel estimation <eos> proposed algorithm require any heuristic outlier detection step critical state art blind deblurring method image outliers <eos> analyze relationship between proposed algorithm other blind deblurring method outlier handling show how estimate intermediate latent image blur kernel estimation principally <eos> show proposed method applied generic image deblurring well non uniform deblurring <eos> experimental result demonstrate proposed algorithm performs favorably against state art blind image deblurring method both synthetic real world image <eos> <eop> paying attention descriptions generated image captioning models <eos> bridge gap between humans machines image understanding describing need further insight into how people describe perceived scene <eos> paper study agreement between bottom up saliency based visual attention object referrals scene description constructs <eos> investigate properties human written descriptions machine generated ones <eos> then propose saliency boosted image captioning model order investigate benefits low level cues language models <eos> learn humans mention more salient object earlier than less salient ones their descriptions better captioning model performs better attention agreement human descriptions proposed saliency boosted model compared its baseline form improve significantly ms coco database indicating explicit bottom up boosting help when task well learnt tuned data better generalization however observed saliency boosted model unseen data <eos> <eop> fast image processing fully convolutional network <eos> present approach accelerating wide variety image processing operators <eos> approach uses fully convolutional network trained input output pairs demonstrate operator action <eos> after training original operator need run all <eos> trained network operates full resolution runs constant time <eos> investigate effect network architecture approximation accuracy runtime memory footprint identify specific architecture balances considerations <eos> evaluate presented approach ten advanced image processing operators including multiple variational models multiscale tone detail manipulation photographic style transfer nonlocal dehazing nonphotorealistic stylization <eos> all operators approximated same model <eos> experiments demonstrate presented approach significantly more accurate than prior approximation schemes <eos> increases approximation accuracy measured psnr across evaluated operators <eos> db mit adobe dataset <eos> db reduces dssim multiplicative factor compared most accurate prior approximation scheme while being fastest <eos> show models generalize across datasets across resolutions investigate number extensions presented approach <eos> <eop> robust video super resolution learned temporal dynamics <eos> video super resolution sr aims generate high resolution hr frame multiple low resolution lr frames <eos> inter frame temporal relation crucial intra frame spatial relation tackling problem <eos> however how utilize temporal information efficiently effectively remains challenging since complex motion difficult model introduce adverse effects if handled properly <eos> address problem two aspects <eos> first propose temporal adaptive neural network adaptively determine optimal scale temporal dependency <eos> filters various temporal scales applied input lr sequence before their responses adaptively aggregated <eos> second reduce complexity motion between neighboring frames using spatial alignment network much more robust efficient than competing alignment method jointly trained temporal adaptive network end end manner <eos> proposed models learned temporal dynamics systematically evaluated public video datasets achieve state art sr result compared other recent video sr approaches <eos> both temporal adaptation spatial alignment modules demonstrated considerably improve sr quality over their plain counterparts <eos> <eop> should encode rain streaks video deterministic stochastic <eos> video taken wild sometimes contain unexpected rain streaks brings difficulty subsequent video processing tasks <eos> rain streak removal video rsrv thus important issue attracting much attention computer vision <eos> different previous rsrv method formulating rain streaks deterministic message work first encodes rains stochastic manner <eos> patch based mixture gaussians <eos> such modification makes proposed model capable finely adapting wider range rain variations instead certain types rain configurations traditional <eos> integrating spatiotemporal smoothness configuration moving object low rank structure background scene propose concise model rsrv containing one likelihood term imposed rain streak layer two prior terms moving object background scene layer video <eos> experiments implemented video synthetic real rains verify superiority proposed method com pared state art method both visually quantitatively various performance metrics <eos> <eop> joint bi layer optimization single image rain streak removal <eos> present novel method removing rain streaks single input image decomposing into rain free background layer rain streak layer <eos> joint optimization process used alternates between removing rain streak details removing non streak details <eos> process assisted three novel image priors <eos> observing rain streaks typically span narrow range directions first analyze local gradient statistics rain image identify image region dominated rain streaks <eos> region estimate dominant rain streak direction extract collection rain dominated patches <eos> next define two priors background layer one based centralized sparse representation another based estimated rain direction <eos> third prior defined rain streak layer based similarity patches extracted rain patches <eos> both visual quantitative comparisons demonstrate method outperforms state art <eos> <eop> low dimensionality calibration through local anisotropic scaling robust hand model personalization <eos> present robust algorithm personalizing sphere mesh tracking model user collection depth measurements <eos> core contribution demonstrate how simple geometric reasoning exploited build shape space how its performance comparable shape spaces constructed datasets carefully calibrated models <eos> achieve goal first re parameterizing geometry tracking template introducing multi stage calibration optimization <eos> novel parameterization decouples degrees freedom pose shape resulting improved convergence properties <eos> analytically differentiable multi stage calibration pipeline optimizes model natural low dimensional space local anisotropic scalings leading effective solution easily embedded other tracking calibration algorithms <eos> compared existing sphere mesh calibration algorithms quantitative experiments assess algorithm possesses larger convergence basin personalized models allows perform motion tracking superior accuracy <eos> code data available github <eos> com edoremelli hadjust <eop> non markovian globally consistent multi object tracking <eos> many state art approaches multi object tracking rely detecting them each frame independently grouping detections into short but reliable trajectory segments then further grouping them into full trajectories <eos> grouping typically relies imposing local smoothness constraints but almost never enforcing more global ones trajectories <eos> paper propose non markovian approach imposing global consistency using behavioral patterns guide tracking algorithm <eos> when used conjunction state art tracking algorithms further increases their already good performance multiple challenging datasets <eos> show significant improvements both supervised settings ground truth available behavioral patterns learned completely unsupervised settings <eos> <eop> crest convolutional residual learning visual tracking <eos> discriminative correlation filters dcfs ryn shown perform superiorly visual tracking <eos> they ryn only need small set training sample initial frame generate appearance model <eos> however existing dcfs learn filters separately feature extraction update filters using moving average operation empirical weight <eos> dcf trackers hardly benefit end end training <eos> paper propose crest algorithm reformulate dcfs one layer convolutional neural network <eos> method integrates feature extraction response map generation well model update into neural network end end training <eos> reduce model degradation during online update apply residual learning take appearance changes into account <eos> extensive experiments benchmark datasets demonstrate crest tracker performs favorably against state art trackers <eos> <eop> volumetric flow estimation incompressible fluids using stationary stokes equations <eos> experimental fluid dynamics flow volume fluid observed injecting high contrast tracer particles tracking them multi view video <eos> fluid dynamics researchers developed variants space carving reconstruct three dimensional particle distribution given time step then use relatively simple local matching recover motion over time <eos> contrary estimating optical flow between two consecutive image long standing standard problem computer vision but only little work exists about volumetric three dimensional flow <eos> here propose variational method three dimensional fluid flow estimation multi view data <eos> start three dimensional version standard variational flow model investigate different regularization schemes ensure divergence free flow fields account physics incompressible fluids <eos> moreover propose semi dense formulation cope computational demands large volumetric datasets <eos> flow estimated regularized lower spatial resolution while data term evaluated full resolution preserve discriminative power geometric precision local particle distribution <eos> extensive experiments reveal simple sum squared differences ssd most suitable data term application <eos> regularization energy whose euler lagrange equations correspond stationary stokes equations leads best result <eos> strictly enforces divergence free flow additionally penalizes squared gradient flow <eos> <eop> bounding boxes segmentations object coordinates how important recognition three dimensional scene flow estimation autonomous driving scenarios <eos> existing method three dimensional scene flow estimation often fail presence large displacement local ambiguities <eos> texture less reflective surfaces <eos> however challenges omnipresent dynamic road scenes focus work <eos> main contribution overcome three dimensional motion estimation problems exploiting recognition <eos> particular investigate importance recognition granularity coarse bounding box estimates over instance segmentations fine grained three dimensional object part predictions <eos> compute cues using cnn trained newly annotated dataset stereo image integrate them into crf based model robust three dimensional scene flow estimation approach term instance scene flow <eos> analyze importance each recognition cue ablation study observe instance segmentation cue far strongest setting <eos> demonstrate effectiveness method challenging kitti scene flow benchmark achieve state art performance time submission <eos> <eop> performance guaranteed network acceleration via high order residual quantization <eos> input binarization shown effective way network acceleration <eos> however previous binarization scheme could regarded simple pixel wise thresholding operations <eos> order one approximation suffers big accuracy loss <eos> paper propose high order binarization scheme achieves more accurate approximation while still possesses advantage binary operation <eos> particular proposed scheme recursively performs residual quantization yields series binary input image decreasing magnitude scales <eos> accordingly propose high order binary filtering gradient propagation operations both forward backward computations <eos> theoretical analysis shows approximation error guarantee property proposed method <eos> extensive experimental result demonstrate proposed scheme yields great recognition accuracy while being accelerated <eos> <eop> deep metric learning angular loss <eos> modern image search system requires semantic understanding image key yet under addressed problem learn good metric measuring similarity between image <eos> while deep metric learning yielded impressive performance gains extracting high level abstractions image data proper objective loss function becomes central issue boost performance <eos> paper propose novel angular loss takes angle relationship into account learning better similarity metric <eos> whereas previous metric learning method focus optimizing similarity contrastive loss relative similarity triplet loss image pairs proposed method aims constraining angle negative point triplet triangles <eos> several favorable properties observed when compared conventional method <eos> first scale invariance introduced improving robustness objective against feature variance <eos> second third order geometric constraint inherently imposed capturing additional local structure triplet triangles than contrastive loss triplet loss <eos> third better convergence demonstrated experiments three publicly available datasets <eos> <eop> compositional human pose regression <eos> regression based method performing well detection based method human pose estimation <eos> central problem structural information pose well exploited previous regression method <eos> work propose structure aware regression approach <eos> adopts reparameterized pose representation using bones instead joints <eos> exploits joint connection structure define compositional loss function encodes long range interactions pose <eos> simple effective general both three dimensional pose estimation unified setting <eos> comprehensive evaluation validates effectiveness approach <eos> significantly advances state art human <eos> competitive state art result mpii <eos> <eop> mutan multimodal tucker fusion visual question answering <eos> bilinear models provide appealing framework mixing merging information visual question answering vqa tasks <eos> they help learn high level associations between question meaning visual concepts image but they suffer huge dimensionality issues <eos> introduce mutan multimodal tensor based tucker decomposition efficiently parametrize bilinear interactions between visual textual representations <eos> additionally tucker framework design low rank matrix based decomposition explicitly constrain interaction rank <eos> mutan control complexity merging scheme while keeping nice interpretable fusion relations <eos> show how tucker decomposition framework generalizes some latest vqa architectures providing state art result <eos> <eop> revisiting im gps deep learning era <eos> image geolocalization inferring geographic location image challenging computer vision problem many potential applications <eos> recent state art approach problem deep image classification approach world spatially divided into bins deep network trained predict correct bin given image <eos> propose combine approach original im gps approach query image matched against database geotagged image location inferred retrieved set <eos> estimate geographic location query image applying kernel density estimation locations its nearest neighbors reference database <eos> interestingly find best feature retrieval task derived network trained classification loss even though use classification approach test time <eos> training classification loss outperforms several deep feature learning method <eos> siamese network contrastive triplet loss more typical retrieval applications <eos> simple approach achieves state art geolocalization accuracy while also requiring significantly less training data <eos> <eop> scene parsing global context embedding <eos> present scene parsing method utilizes global context information based both parametric non parametric models <eos> compared previous method only exploit local relationship between object train context network based scene similarities generate feature representations global contexts <eos> addition learned feature utilized generate global spatial priors explicit classes inference <eos> then design modules embed feature representations priors into segmentation network additional global context cues <eos> show proposed method eliminate false positives compatible global context representations <eos> experiments both mit ade pascal context datasets show proposed method performs favorably against existing method <eos> <eop> simple yet effective baseline three dimensional human pose estimation <eos> following success deep convolutional network state art method human pose estimation focused deep end end systems predict joint locations given raw image pixels <eos> despite their excellent performance often easy understand whether their remaining error stems limited pose visual understanding failure map poses into dimensional positions <eos> goal understanding sources error set out build system given joint locations predicts positions <eos> much surprise found current technology lifting ground truth joint locations space task solved remarkably low error rate relatively simple deep feed forward network outperforms best reported result about human <eos> largest publicly available pose estimation benchmark <eos> furthermore training system output off shelf state art detector <eos> using image input yields state art result includes array systems trained end end specifically task <eos> result indicate large portion error modern deep pose estimation systems stems their visual analysis suggests directions further advance state art human pose estimation <eos> <eop> dual glance model deciphering social relationships <eos> since beginning early civilizations social relationships derived each individual fundamentally form basis social structure daily life <eos> computer vision literature much progress made scene understanding such object detection scene parsing <eos> recent research focuses relationship between object based its functionality geometrical relations <eos> work aim study problem social relationship recognition still image <eos> proposed dual glance model social relationship recognition first glance fixates individual pair interest second glance deploys attention mechanism explore contextual cues <eos> also collected new large scale people social context pisc dataset comprises image annotated sample types social relationship <eos> provide benchmark result pisc dataset qualitatively demonstrate efficacy proposed model <eos> <eop> sketching style visual search sketches aesthetic context <eos> propose novel measure visual similarity image retrieval incorporates both structural aesthetic style constraints <eos> algorithm accepts query sketched shape set one more contextual image specifying desired visual aesthetic <eos> triplet network used learn feature embedding capable measuring style similarity independent structure delivering significant gains over previous network style discrimination <eos> incorporate model within hierarchical triplet network unify learn joint space two discriminatively trained streams style structure <eos> demonstrate space enables first time style constrained sketch search over diverse domain digital artwork comprising graphics paintings drawings <eos> also briefly explore alternative query modalities <eos> <eop> point set registration global local correspondence transformation estimation <eos> present new point set registration method global local correspondence transformation estimation gl cate <eos> geometric structures point set exploited combining global feature point point euclidean distance local feature shape distance sd based histograms generated elliptical gaussian soft count strategy <eos> using bi directional deterministic annealing scheme directly control searching ranges two feature mixture feature gaussian mixture model mgmm constructed recover correspondences point set <eos> new vector based structure constraint term formulated regularize transformation <eos> accuracy transformation updating improved constraining spatial structure both global local scales <eos> annealing scheme applied progressively decrease strength regularization achieve maximum overlap <eos> both aforementioned processes incorporated em algorithm unified optimization framework <eos> test performances gl cate contour registration sequence image real image medical image fingerprint image remote sensing image compare eight state art method method shows favorable performances most scenarios <eos> <eop> scenenet rgb synthetic image beat generic imagenet pre training indoor segmentation <eos> introduce scenenet rgb dataset providing pixel perfect ground truth scene understanding problems such semantic segmentation instance segmentation object detection <eos> also provides perfect camera poses depth data allowing investigation into geometric computer vision problems such optical flow camera pose estimation three dimensional scene labelling tasks <eos> random sampling permits virtually unlimited scene configurations here provide rendered rgb image randomly generated three dimensional trajectories synthetic layouts random but physically simulated object configurations <eos> compare semantic segmentation performance network weights produced pre training rgb image dataset against generic vgg imagenet weights <eos> after fine tuning sun rgb nyuv real world datasets find both cases synthetically pre trained network outperforms vgg weights <eos> when synthetic pre training includes depth channel something imagenet cannot natively provide performance greater still <eos> suggests large scale high quality synthetic rgb datasets task specific labels more useful pre training than real world generic pre training such imagenet <eos> host dataset robotvault <eos> io scenenet rgbd <eos> html <eop> unified model near remote sensing <eos> propose novel convolutional neural network architecture estimating geospatial functions such population density land cover land use <eos> approach combine overhead ground level image end end trainable neural network uses kernel regression density estimation convert feature extracted ground level image into dense feature map <eos> output network dense estimate geospatial function form pixel level labeling overhead image <eos> evaluate approach created large dataset overhead ground level image major urban area three set labels land use building function building age <eos> find approach more accurate all tasks some cases dramatically so <eos> <eop> directionally convolutional network three dimensional shape segmentation <eos> previous approaches three dimensional shape segmentation mostly rely heuristic processing hand tuned geometric descriptors <eos> paper propose novel three dimensional shape representation learning approach directionally convolutional network dcn solve shape segmentation problem <eos> dcn extends convolution operations image surface mesh three dimensional shapes <eos> dcn learn effective shape representations raw geometric feature <eos> face normals distances achieve robust segmentation <eos> more specifically two stream segmentation framework proposed one stream made up proposed dcn face normals input other stream implemented neural network face distance histogram input <eos> learned shape representations two streams fused element wise product <eos> finally conditional random field crf applied optimize segmentation <eos> through extensive experiments conducted benchmark datasets demonstrate approach outperforms current state arts both classic deep learning based large variety three dimensional shapes <eos> <eop> amat medial axis transform natural image <eos> introduce appearance mat amat generalization medial axis transform natural image framed weighted geometric set cover problem <eos> make following contributions extend previous medial point detection method color image associating each medial point local scale ii inspired invertibility property binary mat also associate each medial point local encoding allows invert amat reconstructing input image iii describe clustering scheme takes advantage additional scale appearance information group individual point into medial branches providing shape decomposition underlying image region <eos> experiments show state art performance medial point detection berkeley medial axes bmax new dataset medial axes based bsds database good generalization sk wh symmax datasets <eos> also measure quality reconstructed image bmax obtained inverting their computed amat <eos> approach delivers significantly better reconstruction quality wrt three baselines using just image pixels <eos> code annotations available github <eos> com tsogkas amat <eos> <eop> deep dual learning semantic image segmentation <eos> deep neural network advanced many computer vision tasks because their compelling capacities learn large amount labeled data <eos> however their performances fully exploited semantic image segmentation scale training set limited per pixel labelmaps expensive obtain <eos> reduce labeling efforts natural solution collect additional image internet associated image level tags <eos> unlike existing works treated labelmaps tags independent supervisions present novel learning setting namely dual image segmentation dis consists two complementary learning problems jointly solved <eos> one predicts labelmaps tags image other reconstructs image using predicted labelmaps <eos> dis three appealing properties <eos> given image tags only its labelmap inferred leveraging image tags constraints <eos> estimated labelmaps capture accurate object classes boundaries used ground truths training boost performance <eos> dis able clean tags noises <eos> dis significantly reduces number per pixel annotations training while still achieves state art performance <eos> extensive experiments demonstrate effectiveness dis outperforms existing best performing baseline <eos> pascal voc test set without any post processing such crf mrf smoothing <eos> <eop> regional interactive image segmentation network <eos> interactive image segmentation model allows users iteratively add new inputs refinement until satisfactory result finally obtained <eos> therefore ideal interactive segmentation model should learn capture user intention minimal interaction <eos> however existing models fail fully utilize valuable user input information segmentation refinement process thus offer unsatisfactory user experience <eos> order fully exploit user provided information propose new deep framework called regional interactive segmentation network ris net expand field view given inputs capture local regional information surrounding them local refinement <eos> additionally ris net adopts multiscale global contextual information augment each local region improving feature representation <eos> also introduce click discount factors develop novel optimization strategy more effective end end training <eos> comprehensive evaluations four challenging datasets well demonstrate superiority proposed ris net over other state art approaches <eos> <eop> learning efficient convolutional network through network slimming <eos> deployment deep convolutional neural network cnn many real world applications largely hindered their high computational cost <eos> paper propose novel learning scheme cnn simultaneously reduce model size decrease run time memory footprint lower number computing operations without compromising accuracy <eos> achieved enforcing channel level sparsity network simple but effective way <eos> different many existing approaches proposed method directly applies modern cnn architectures introduces minimum overhead training process requires no special software hardware accelerators resulting models <eos> call approach network slimming takes wide large network input models but during training insignificant channels automatically identified pruned afterwards yielding thin compact models comparable accuracy <eos> empirically demonstrate effectiveness approach several state art cnn models including vggnet resnet densenet various image classification datasets <eos> vggnet multi pass version network slimming gives reduction model size reduction computing operations <eos> <eop> cvae gan fine grained image generation through asymmetric training <eos> present variational generative adversarial network general learning framework combines variational auto encoder generative adversarial network synthesizing image fine grained categories such faces specific person object category <eos> approach models image composition label latent attributes probabilistic model <eos> varying fine grained category label fed into resulting generative model generate image specific category randomly drawn values latent attribute vector <eos> approach two novel aspects <eos> first adopt cross entropy loss discriminative classifier network but mean discrepancy objective generative network <eos> kind asymmetric loss function makes gan training more stable <eos> second adopt encoder network learn relationship between latent space real image space use pairwise feature matching keep structure generated image <eos> experiment natural image faces flowers birds demonstrate proposed models capable generating realistic diverse sample fine grained category labels <eos> further show models applied other tasks such image inpainting super resolution data augmentation training better face recognition models <eos> <eop> universal adversarial perturbations against semantic image segmentation <eos> while deep learning remarkably successful perceptual tasks was also shown vulnerable adversarial perturbations input <eos> perturbations denote noise added input was generated specifically fool system while being quasi imperceptible humans <eos> more severely there even exist universal perturbations input agnostic but fool network majority inputs <eos> while recent work focused image classification work proposes attacks against semantic image segmentation present approach generating universal adversarial perturbations make network yield desired target segmentation output <eos> show empirically there exist barely perceptible universal noise patterns result nearly same predicted segmentation arbitrary inputs <eos> furthermore also show existence universal noise removes target class <eos> all pedestrians segmentation while leaving segmentation mostly unchanged otherwise <eos> <eop> associative domain adaptation <eos> propose associative domain adaptation novel technique end end domain adaptation neural network task inferring class labels unlabeled target domain based statistical properties labeled source domain <eos> training scheme follows paradigm order effectively derive class labels target domain network should produce statistically domain invariant embeddings while minimizing classification error labeled source domain <eos> accomplish reinforcing associations between source target data directly embedding space <eos> method easily added any existing classification network no structural almost no computational overhead <eos> demonstrate effectiveness approach various benchmarks achieve state art result across board generic convolutional neural network architecture specifically tuned respective tasks <eos> finally show proposed association loss produces embeddings more effective domain adaptation compared method employing maximum mean discrepancy similarity measure embedding space <eos> <eop> introspective neural network generative modeling <eos> study unsupervised learning developing generative model built progressively learned deep convolutional neural network <eos> resulting generator additionally discriminator capable introspection sense being able self evaluate difference between its generated sample given training data <eos> through repeated discriminative learning desirable properties modern discriminative classifiers directly inherited generator <eos> specifically model learns sequence cnn classifiers using synthesis classification algorithm <eos> experiments observe encouraging result number applications including texture modeling artistic style transferring face modeling unsupervised feature learning <eos> <eop> towards unified compositional model visual pattern modeling <eos> compositional models represent visual patterns hierarchies meaningful reusable parts <eos> they attractive vision modeling due their ability decompose complex patterns into simpler ones resolve low level ambiguities high level image interpretations <eos> however current compositional models separate structure part discovery parameter estimation generally leads suboptimal learning fitting model <eos> moreover commonly adopted latent structural learning scalable deep architectures <eos> address difficult issues compositional models paper quests unified framework compositional pattern modeling inference learning <eos> represented graphs aogs jointly models compositional structure parts feature composition sub configuration relationships <eos> show inference algorithm proposed framework equivalent feed forward network <eos> thus all parameters learned efficiently via highly scalable back propagation bp end end fashion <eos> validate model via task handwritten digit recognition <eos> visualizing processes bottom up composition top down parsing show model fully interpretable being able learn hierarchical compositions visual primitives visual patterns increasingly higher levels <eos> apply new compositional model natural scene character recognition generic object detection <eos> experimental result demonstrated its effectiveness <eos> <eop> least squares generative adversarial network <eos> unsupervised learning generative adversarial network gans proven hugely successful <eos> regular gans hypothesize discriminator classifier sigmoid cross entropy loss function <eos> however found loss function may lead vanishing gradients problem during learning process <eos> overcome such problem propose paper least squares generative adversarial network lsgans adopt least squares loss function discriminator <eos> show minimizing objective function lsgan yields minimizing pearson chi divergence <eos> there two benefits lsgans over regular gans <eos> first lsgans able generate higher quality image than regular gans <eos> second lsgans perform more stable during learning process <eos> evaluate lsgans lsun cifar datasets experimental result show image generated lsgans better quality than ones generated regular gans <eos> also conduct two comparison experiments between lsgans regular gans illustrate stability lsgans <eos> <eop> centered weight normalization accelerating training deep neural network <eos> training deep neural network difficult pathological curvature problem <eos> re parameterization effective way relieve problem learning curvature approximately constraining solutions weights good properties optimization <eos> paper proposes re parameterize input weight each neuron deep neural network normalizing zero mean unit norm followed learnable scalar parameter adjust norm weight <eos> technique effectively stabilizes distribution implicitly <eos> besides improves conditioning optimization problem thus accelerates training deep neural network <eos> wrapped linear module practice plugged any architecture replace standard linear module <eos> highlight benefits method both multi layer perceptrons convolutional neural network demonstrate its scalability efficiency svhn cifar cifar imagenet datasets <eos> <eop> deep growing learning <eos> semi supervised learning ssl import paradigm make full use large amount unlabeled data machine learning <eos> bottleneck ssl overfitting problem when training over limited labeled data especially complex model like deep neural network <eos> get around bottleneck propose bio inspired ssl framework deep neural network namely deep growing learning dgl <eos> specifically formulate ssl em like process deep network alternately iterates between automatically growing convolutional layer selecting reliable pseudo labeled data training <eos> dgl guarantees shallow neural network trained labeled data while deeper neural network trained growing amount reliable pseudo labeled data so alleviate overfitting problem <eos> experiments different visual recognition tasks verified effectiveness dgl <eos> <eop> smart mining deep metric learning <eos> solve deep metric learning problems produce feature embeddings current methodologies will commonly use triplet model minimise relative distance between sample same class maximise relative distance between sample different classes <eos> though successful training convergence triplet model compromised fact vast majority training sample will produce gradients magnitudes close zero <eos> issue motivated development method explore global structure embedding other method explore hard negative positive mining <eos> effectiveness such mining method often associated intractable computational requirements <eos> paper propose novel deep metric learning method combines triplet model global structure embedding space <eos> rely smart mining procedure produces effective training sample low computational cost <eos> addition propose adaptive controller automatically adjusts smart mining hyper parameters speeds up convergence training process <eos> show empirically proposed method allows fast more accurate training triplet convnets than other competing mining method <eos> additionally show method achieves new state art embedding result cub cars datasets <eos> <eop> temporal generative adversarial nets singular value clipping <eos> paper propose generative model temporal generative adversarial nets tgan learn semantic representation unlabeled video capable generating video <eos> unlike existing generative adversarial nets gan based method generate video single generator consisting three dimensional deconvolutional layer model exploits two different types generators temporal generator image generator <eos> temporal generator takes single latent variable input outputs set latent variables each corresponds image frame video <eos> image generator transforms set such latent variables into video <eos> deal instability training gan such advanced network adopt recently proposed model wasserstein gan propose novel method train stably end end manner <eos> experimental result demonstrate effectiveness method <eos> <eop> sampling matters deep embedding learning <eos> deep embeddings answer one simple question how similar two image learning embeddings bedrock verification zero shot learning visual search <eos> most prominent approaches optimize deep convolutional network suitable loss function such contrastive loss triplet loss <eos> while rich line work focuses solely loss functions show paper selecting training examples plays equally important role <eos> propose distance weighted sampling selects more informative stable examples than traditional approaches <eos> addition show simple margin based loss sufficient outperform all other loss functions <eos> evaluate approach cub car stanford online products datasets image retrieval clustering lfw dataset face verification <eos> method achieves state art performance all them <eos> <eop> dualgan unsupervised dual learning image image translation <eos> conditional generative adversarial network gans cross domain image image translation made much progress recently <eos> depending task complexity thousands millions labeled image pairs needed train conditional gan <eos> however human labeling expensive even impractical large quantities data may always available <eos> inspired dual learning natural language translation develop novel mechanism enables image translators trained two set image two domains <eos> architecture primal gan learns translate image domain domain while dual gan learns invert task <eos> closed loop made primal dual tasks allows image either domain translated then reconstructed <eos> hence loss function accounts reconstruction error image used train translators <eos> experiments multiple image translation tasks unlabeled data show considerable performance gain dualgan over single gan <eos> some tasks dualgan even achieve comparable slightly better result than conditional gan trained fully labeled data <eos> <eop> learning view invariant feature person identification temporally synchronized video taken wearable cameras <eos> paper study problem cross view person identification cvpi aims identifying same person temporally synchronized video taken different wearable cameras <eos> basic idea utilize human motion consistency cvpi human motion computed optical flow <eos> however optical flow view variant same person optical flow different video very different due view angle change <eos> paper attempt utilize three dimensional human skeleton sequences learn model extract view invariant motion feature optical flows different views <eos> purpose use three dimensional mocap database build synthetic optical flow dataset train triplet network tn consisting three sub network two optical flow sequences different views one underlying three dimensional mocap skeleton sequence <eos> finally sub network optical flows used extract view invariant feature cvpi <eos> experimental result show using only motion information proposed method achieve comparable performance state art method <eos> further combination proposed method appearance based method achieves new state art performance <eos> <eop> marioqa answering questions watching gameplay video <eos> present framework analyze various aspects models video question answering videoqa using customizable synthetic datasets constructed automatically gameplay video <eos> work motivated fact existing models often tested only datasets require excessively high level reasoning mostly contain instances accessible through single frame inferences <eos> hence difficult measure capacity flexibility trained models existing techniques often rely ad hoc implementations deep neural network without clear insight into datasets models <eos> particularly interested understanding temporal relationships between video events solve videoqa problems because reasoning temporal dependency one most distinct components video image <eos> address objective automatically generate customized synthetic videoqa dataset using super mario bros <eos> gameplay video so contains events different levels reasoning complexity <eos> using dataset show properly constructed datasets events various complexity levels critical learn effective models improve overall performance <eos> <eop> sbgar semantics based group activity recognition <eos> activity recognition become important function many emerging computer vision applications <eos> automatic video surveillance system human computer interaction application video recommendation system etc <eos> paper propose novel semantics based group activity recognition scheme namely sbgar achieves higher accuracy efficiency than existing group activity recognition method <eos> sbgar consists two stages stage use lstm model generate caption each video frame stage ii another lstm model trained predict final activity categories based generated captions <eos> evaluate sbgar using two well known datasets collective activity dataset volleyball dataset <eos> experimental result show sbgar improves group activity recognition accuracy shorter computation time compared state art method <eos> <eop> trespassing boundaries labeling temporal bounds object interactions egocentric video <eos> manual annotations temporal bounds object interactions <eos> start end times typical training input recognition localization detection algorithms <eos> three publicly available egocentric datasets uncover inconsistencies ground truth temporal bounds within across annotators datasets <eos> systematically assess robustness state art approaches changes labeled temporal bounds object interaction recognition <eos> boundaries trespassed drop up observed both improved dense trajectories two stream convolutional neural network <eos> demonstrate such disagreement stems limited understanding distinct phases action propose annotating based rubicon boundaries inspired similarly named cognitive model consistent temporal bounds object interactions <eos> evaluated public dataset report increase overall accuracy increase accuracy classes when rubicon boundaries used temporal annotations <eos> <eop> unmasking abnormal events video <eos> propose novel framework abnormal event detection video requires no training sequences <eos> framework based unmasking technique previously used authorship verification text documents adapt task <eos> iteratively train binary classifier distinguish between two consecutive video sequences while removing each step most discriminant feature <eos> higher training accuracy rates intermediately obtained classifiers represent abnormal events <eos> best knowledge first work apply unmasking computer vision task <eos> compare method several state art supervised unsupervised method four benchmark data set <eos> empirical result indicate abnormal event detection framework achieve state art result while running real time frames per second <eos> <eop> chained multi stream network exploiting pose motion appearance action classification detection <eos> general human action recognition requires understanding various visual cues <eos> paper propose network architecture computes integrates most important visual cues action recognition pose motion raw image <eos> integration introduce markov chain model adds cues successively <eos> resulting approach efficient applicable action classification well spatial temporal action localization <eos> two contributions clearly improve performance over respective baselines <eos> overall approach achieves state art action classification performance hmdb hmdb ntu rgb datasets <eos> moreover yields state art spatio temporal action localization result ucf hmdb <eos> <eop> temporal action detection structured segment network <eos> detecting actions untrimmed video important yet challenging task <eos> paper present structured segment network ssn novel framework models temporal structure each action instance via structured temporal pyramid <eos> top pyramid further introduce decomposed discriminative model comprising two classifiers respectively classifying actions determining completeness <eos> allows framework effectively distinguish positive proposals background incomplete ones thus leading both accurate recognition localization <eos> components integrated into unified network efficiently trained end end fashion <eos> additionally simple yet effective temporal action proposal scheme dubbed temporal actionness grouping tag devised generate high quality action proposals <eos> two challenging benchmarks thumos activitynet method remarkably outperforms previous state art method demonstrating superior accuracy strong adaptivity handling actions various temporal structures <eos> <eop> jointly recognizing object fluents tasks egocentric video <eos> paper addresses problem jointly recognizing object fluents tasks egocentric video <eos> fluents changeable attributes object <eos> tasks goal oriented human activities interact object aim change some attributes object <eos> process executing task process change object fluents over time <eos> propose hierarchical model represent tasks concurrent sequential object fluents <eos> task different fluents closely interact each other both spatial temporal domains <eos> given egocentric video beam search algorithm applied jointly recognizing object fluents each frame task entire video <eos> collected large scale egocentric video dataset tasks fluents <eos> dataset contains categories tasks object classes categories object fluents video sequences approximately video frames <eos> experimental result dataset prove strength method <eos> <eop> transferring object joint inference container human pose <eos> transferring object one place another place common task performed human daily life <eos> during process usually intuitive humans choose object proper container use efficient pose carry object yet non trivial current computer vision machine learning algorithms <eos> paper propose approach jointly infer container human pose transferring object minimizing costs associated both object pose candidates <eos> approach predicts object choose container while reasoning about how humans interact physical surroundings accomplish task transferring object given visual input <eos> learning phase presented method learns how humans make rational choices containers poses transferring different object well physical quantities required transfer task <eos> compatibility between container containee energy cost carrying pose via structured learning approach <eos> inference phase given scanned three dimensional scene different object candidates dictionary human poses approach infers best object container together human pose transferring given object <eos> <eop> interpretable learning self driving cars visualizing causal attention <eos> deep neural perception control network likely key component self driving vehicles <eos> models need explainable they should provide easy interpret rationales their behavior so passengers insurance companies law enforcement developers etc <eos> understand triggered particular behavior <eos> here explore use visual explanations <eos> explanations take form real time highlighted region image causally influence network output steering control <eos> approach two stage <eos> first stage use visual attention model train convolution network end end image steering angle <eos> attention model highlights image region potentially influence network output <eos> some true influences but some spurious <eos> then apply causal filtering step determine input region actually influence output <eos> produces more succinct visual explanations more accurately exposes network behavior <eos> demonstrate effectiveness model three datasets totaling hours driving <eos> first show training attention degrade performance end end network <eos> then show network causally cues variety feature used humans while driving <eos> <eop> learning cooperative visual dialog agents deep reinforcement learning <eos> introduce first goal driven training visual question answering dialog agents <eos> specifically pose cooperative image guessing game between two agents qbot abot who communicate natural language dialog so qbot select unseen image lineup image <eos> use deep reinforcement learning rl end end learn policies agents pixels multi agent multi round dialog game reward <eos> demonstrate two experimental result <eos> first sanity check demonstration pure rl scratch show result synthetic world agents communicate ungrounded vocabulary ie symbols no pre specified meanings <eos> find two bots invent their own communication protocol start using certain symbols ask answer about certain visual attributes shape color size <eos> thus demonstrate emergence grounded language communication among visual dialog agents no human supervision all <eos> second conduct large scale real image experiments visdial dataset pretrain dialog data show rl fine tuned agents significantly outperform supervised pretraining <eos> interestingly rl qbot learns ask questions abot good ultimately resulting more informative dialog better team <eos> <eop> mask cnn <eos> present conceptually simple flexible general framework object instance segmentation <eos> approach efficiently detects object image while simultaneously generating high quality segmentation mask each instance <eos> method called mask cnn extends faster cnn adding branch predicting object mask parallel existing branch bounding box recognition <eos> mask cnn simple train adds only small overhead faster cnn running fps <eos> moreover mask cnn easy generalize other tasks <eos> allowing estimate human poses same framework <eos> show top result all three tracks coco suite challenges including instance segmentation bounding box object detection person keypoint detection <eos> without tricks mask cnn outperforms all existing single model entries every task including coco challenge winners <eos> hope simple effective approach will serve solid baseline help ease future research instance level recognition <eos> code will made available <eos> <eop> towards diverse natural image descriptions via conditional gan <eos> despite substantial progress recent years problem image captioning remains far being satisfactorily tackled <eos> sentences produced existing method <eos> based lstm often overly rigid lacking variability <eos> issue related learning principle widely used practice maximize likelihood training sample <eos> principle encourages high resemblance ground truths while suppressing other reasonable expressions <eos> conventional evaluation metrics <eos> bleu meteor also favor such restrictive method <eos> paper explore alternative approach aim improve naturalness diversity two essential properties human expressions <eos> specifically propose new framework based conditional generative adversarial network cgan jointly learns generator produce descriptions conditioned image evaluator assess how well description fits visual content <eos> noteworthy training sequence generator nontrivial <eos> overcome difficulty policy gradient strategy stemming reinforcement learning allows generator receive early feedbacks along way <eos> tested method two large datasets performed competitively against real people user study outperformed other method various tasks <eos> <eop> focal loss dense object detection <eos> highest accuracy object detectors date based two stage approach popularized cnn classifier applied sparse set candidate object locations <eos> contrast one stage detectors applied over regular dense sampling possible object locations potential faster simpler but trailed accuracy two stage detectors thus far <eos> paper investigate why case <eos> discover extreme foreground background class imbalance encountered during training dense detectors central cause <eos> propose address class imbalance reshaping standard cross entropy loss such down weights loss assigned well classified examples <eos> novel focal loss focuses training sparse set hard examples prevents vast number easy negatives overwhelming detector during training <eos> evaluate effectiveness loss design train simple dense detector call retinanet <eos> result show when trained focal loss retinanet able match speed previous one stage detectors while surpassing accuracy all existing state art two stage detectors <eos> <eop> inferring executing programs visual reasoning <eos> existing method visual reasoning attempt directly map inputs outputs using black box architectures without explicitly modeling underlying reasoning processes <eos> result black box models often learn exploit biases data rather than learning perform visual reasoning <eos> inspired module network paper proposes model visual reasoning consists program generator constructs explicit representation reasoning process performed execution engine executes resulting program produce answer <eos> both program generator execution engine implemented neural network trained using combination backpropagation reinforce <eos> using clevr benchmark visual reasoning show model significantly outperforms strong baselines generalizes better variety settings <eos> <eop> visual forecasting imitating dynamics natural sequences <eos> introduce general framework visual forecasting directly imitates visual sequences without additional supervision <eos> result model applied several semantic levels require any domain knowledge handcrafted feature <eos> achieve formulating visual forecasting inverse reinforcement learning irl problem directly imitate dynamics natural sequences their raw pixel values <eos> key challenge high dimensional continuous state action space prohibits application previous irl algorithms <eos> address computational bottleneck extending recent progress model free imitation trainable deep feature representations bypasses exhaustive state action pair visits dynamic programming using dual formulation avoids explicit state sampling gradient computation using deep feature reparametrization <eos> allows apply irl scale directly imitate dynamics high dimensional continuous visual sequences raw pixel values <eos> evaluate approach three different level abstraction low level pixels higher level semantics future frame generation action anticipation visual story forecasting <eos> all levels approach outperforms existing method <eos> <eop> torontocity seeing world million eyes <eos> paper introduce torontocity benchmark covers full greater toronto area gta <eos> km land km road around buildings <eos> benchmark provides different perspectives world captured airplanes drones cars driving around city <eos> manually labeling such large scale dataset infeasible <eos> instead propose utilize different sources high precision maps create ground truth <eos> towards goal develop algorithms allow align all data sources maps while requiring minimal human supervision <eos> designed wide variety tasks including building height estimation reconstruction road centerline curb extraction building instance segmentation building contour extraction reorganization semantic labeling scene type classification recognition <eos> pilot study shows most tasks still difficult modern convolutional neural network <eos> <eop> low shot visual recognition shrinking hallucinating feature <eos> low shot visual learning ability recognize novel object categories very few examples hallmark human visual intelligence <eos> existing machine learning approaches fail generalize same way <eos> make progress foundational problem present low shot learning benchmark complex image mimics challenges faced recognition systems wild <eos> then propose representation regularization techniques techniques hallucinate additional training examples data starved classes <eos> together method improve effectiveness convolutional network low shot learning improving one shot accuracy novel classes <eos> challenging imagenet dataset <eos> <eop> coarse fine network keypoint localization <eos> propose coarse fine network cfn exploits multi level supervisions keypoint localization <eos> recently convolutional neural network cnn based method achieved great success due powerful hierarchical feature cnn <eos> method typically use confidence maps generated ground truth keypoint locations supervisory signals <eos> however while some keypoints easily located high accuracy many them hard localize due appearance ambiguity <eos> thus using strict supervision often fails detect keypoints difficult locate accurately <eos> target problem develop keypoint localization network composed several coarse detector branches each built top feature layer cnn fine detector branch built top multiple feature layer <eos> supervise each branch specified label map explicate certain supervision strictness level <eos> all branches unified principally produce final accurate keypoint locations <eos> demonstrate efficacy efficiency generality method several benchmarks multiple tasks including bird part localization human body pose estimation <eos> especially method achieves <eos> ap coco keypoints challenge dataset improvement over winning entry <eos> <eop> detect track track detect <eos> recent approaches high accuracy detection tracking object categories video consist complex multistage solutions become more cumbersome each year <eos> paper propose convnet architecture jointly performs detection tracking solving task simple effective way <eos> contributions threefold set up convnet architecture simultaneous detection tracking using multi task objective frame based object detection across frame track regression ii introduce correlation feature represent object co occurrences across time aid convnet during tracking iii link frame level detections based across frame tracklets produce high accuracy detections video level <eos> convnet architecture spatiotemporal object detection evaluated large scale imagenet vid dataset achieves state art result <eos> approach provides better single model performance than winning method last imagenet challenge while being conceptually much simpler <eos> finally show increasing temporal stride dramatically increase tracker speed <eos> <eop> single shot text detector regional attention <eos> present novel single shot text detector directly outputs word level bounding boxes natural image <eos> propose attention mechanism roughly identifies text region via automatically learned attentional map <eos> substantially suppresses background interference convolutional feature key producing accurate inference words particularly extremely small sizes <eos> result single model essentially works coarse fine manner <eos> departs recent fcn based text detectors cascade multiple fcn models achieve accurate prediction <eos> furthermore develop hierarchical inception module efficiently aggregates multi scale inception feature <eos> enhances local details also encodes strong context information allowing detector work reliably multi scale multi orientation text single scale image <eos> text detector achieves measure icdar benchmark advancing state art result <eos> <eop> subunets end end hand shape continuous sign language recognition <eos> propose novel deep learning approach solve simultaneous alignment recognition problems referred sequence sequence learning <eos> decompose problem into series specialised expert systems referred subunets <eos> spatio temporal relationships between subunets then modelled solve task while remaining trainable end end <eos> approach mimics human learning educational techniques number significant advantages <eos> subunets allow inject domain specific expert knowledge into system regarding suitable intermediate representations <eos> they also allow implicitly perform transfer learning between different interrelated tasks also allows exploit wider range more varied data sources <eos> experiments demonstrate each properties serves significantly improve performance overarching recognition system better constraining learning problem <eos> proposed techniques demonstrated challenging domain sign language recognition <eos> demonstrate state art performance hand shape recognition outperforming previous techniques more than <eos> furthermore able obtain comparable sign recognition rates previous research without need alignment step segment out signs recognition <eos> <eop> spatiotemporal oriented energy network dynamic texture recognition <eos> paper presents novel hierarchical spatiotemporal orientation representation spacetime image analysis <eos> designed combine benefits multilayer architecture convnets more controlled approach spacetime analysis <eos> distinguishing aspect approach unlike most contemporary convolutional network no learning involved rather all design decisions specified analytically theoretical motivations <eos> approach makes possible understand information being extracted each stage layer processing well minimize heuristic choices design <eos> another key aspect network its recurrent nature whereby output each layer processing feeds back input <eos> keep network size manageable across layer novel cross channel feature pooling proposed <eos> multilayer architecture result systematically reveals hierarchical image structure terms multiscale multiorientation properties visual spacetime <eos> illustrate its utility network applied task dynamic texture recognition <eos> empirical evaluation multiple standard datasets shows set new state art <eos> <eop> probabilistic structure motion object psfmo <eos> paper deal problem recovering affine camera calibration object position occupancy multi view image using information image detections <eos> show remarkable object localisation volumetric occupancy recovered including both geometrical constraints prior information given object cad models shapenet dataset <eos> done recasting problem context probabilistic framework based probabilistic pca includes both object semantic priors together multi view geometrical constraints <eos> present result synthetic real datasets show validity approach improvements respect previous approaches <eos> particular statistical priors key obtain reliable three dimensional reconstruction especially when input detections noisy likely case real scenarios <eos> <eop> three dimensional morphable model craniofacial shape texture variation <eos> present fully automatic pipeline train three dimensional morphable models dmms contributions pose normalisation dense correspondence using both shape texture information high quality high resolution texture mapping <eos> propose dense correspondence system combining hierarchical parts based template morphing framework shape channel refining optical flow texture channel <eos> texture map generated using raw texture image five views <eos> employ pixel embedding method maintain texture map same high resolution raw texture image rather than using per vertex color maps <eos> high quality texture map then used statistical texture modelling <eos> headspace dataset used training includes demographic information about each subject allowing construction both global dmms models tailored specific gender age groups <eos> build both global craniofacial dmms demographic sub population dmms more than distinct identities <eos> knowledge present first public dmm full human head both shape texture liverpool york head model <eos> furthermore analyse dmms terms range performance metrics <eos> evaluations reveal training pipeline constructs state art models <eos> <eop> multi view dynamic shape refinement using local temporal integration <eos> consider shape reconstructions multi view environments investigate how exploit temporal redundancy precision refinement <eos> addition being beneficial many dynamic multi view scenarios also enables larger scenes such increased precision compensate reduced spatial resolution per image frame <eos> precision scalability mind propose symmetric non causal local time window geometric integration scheme over temporal sequences shape reconstructions refined framewise warping local reliable geometric region neighboring frames them <eos> contrast recent comparable approaches targeting different context more compact scenes real time applications <eos> usually use single dense volumetric update space geometric template they causally track update globally frame frame limitations scalability larger scenes topology precision template based strategy <eos> template less local approach first step towards temporal shape super resolution <eos> show improves reconstruction accuracy considering multiple frames <eos> purpose addition real data examples introduce multi camera synthetic dataset provides ground truth data mid scale dynamic scenes <eos> <eop> learning hand articulations hallucinating heat distribution <eos> propose robust hand pose estimation method learning hand articulations depth feature auxiliary modality feature <eos> additional modality depth data present function geometric properties surface hand described heat diffusion <eos> proposed heat distribution descriptor robust identify keypoints surface incorporates both local geometry hand global structural representation multiple time scales <eos> along line train heat distribution network learn geometrically descriptive representations proposed descriptors fingertip position labels <eos> then hallucination network guided mimic intermediate responses heat distribution modality paired depth image <eos> use resulting geometrically informed responses together discriminative depth feature estimated depth network regularize angle parameters refinement network <eos> end conduct extensive evaluations validate proposed framework powerful achieves state art performance <eos> <eop> intrinsic high quality three dimensional reconstruction joint appearance geometry optimization spatially varying lighting <eos> introduce novel method obtain high quality three dimensional reconstructions consumer rgb sensors <eos> core idea simultaneously optimize geometry encoded signed distance field sdf textures automatically selected keyframes their camera poses along material scene lighting <eos> end propose joint surface reconstruction approach based shape shading sfs techniques utilizes estimation spatially varying spherical harmonics svsh subvolumes reconstructed scene <eos> through extensive examples evaluations demonstrate method dramatically increases level detail reconstructed scene geometry contributes highly consistent surface texture recovery <eos> <eop> robust hand pose estimation during interaction unknown object <eos> paper proposes robust solution accurate three dimensional hand pose estimation presence external object interacting hands <eos> main insight shape object causes configuration hand form hand grasp <eos> along line simultaneously train deep neural network using paired depth image <eos> object oriented network learns functional grasps object perspective whereas hand oriented network explores details hand configurations hand perspective <eos> two network share intermediate observations produced different perspectives create more informed representation <eos> system then collaboratively classifies grasp types orientation hand further constrains pose space using estimates <eos> finally collectively refine unknown pose parameters reconstruct final hand pose <eos> end conduct extensive evaluations validate efficacy proposed collaborative learning approach comparing self generated baselines state art method <eos> <eop> detailed surface geometry albedo recovery rgb video under natural illumination <eos> paper present novel approach depth map enhancement rgb video sequence <eos> basic idea exploit photometric information color sequence <eos> instead making any assumption about surface albedo controlled object motion lighting use lighting variations introduced casual object movement <eos> effectively calculating photometric stereo moving object under natural illuminations <eos> key technical challenge establish correspondences over entire image set <eos> therefore develop lighting insensitive robust pixel matching technique out performs optical flow method presence lighting variations <eos> addition present expectation maximization framework recover surface normal albedo simultaneously without any regularization term <eos> validated method both synthetic real datasets show its superior performance both surface details recovery intrinsic decomposition <eos> <eop> monocular free head three dimensional gaze tracking deep learning geometry constraints <eos> free head three dimensional gaze tracking outputs both eye location gaze vector three dimensional space wide applications scenarios such driver monitoring advertisement analysis surveillance <eos> reliable low cost monocular solution critical pervasive usage areas <eos> noticing gaze vector composition head pose eyeball movement geometrically deterministic way propose novel gaze transform layer connect separate head pose eyeball movement models <eos> proposed decomposition suffer head gaze correlation overfitting makes possible use datasets existing other tasks <eos> add stronger supervision better network training propose two step training strategy first trains sub tasks rough labels then jointly trains accurate gaze labels <eos> enable good cross subject performance under various conditions collect large dataset full coverage head poses eyeball movements contains subjects diverse illumination conditions <eos> deep solution achieves state art gaze tracking accuracy reaching <eos> degrees cross subject prediction error using small network running fps ingle cpu excluding face alignment time <eos> degrees cross subject error deeper network <eos> <eop> filter selection hyperspectral estimation <eos> while recovery hyperspectral signals natural rgb image recent subject exploration little no consideration given camera response profiles used recovery process <eos> paper demonstrate optimal selection camera response filters may improve hyperspectral estimation accuracy over emphasizing importance considering selecting response profiles wisely <eos> additionally present evolutionary optimization methodology optimal filter set selection very large filter spaces approach facilitates practical selection families customizable filters filter optimization multispectral cameras more than channels <eos> <eop> microfacet based reflectance model photometric stereo highly specular surfaces <eos> precise stable invertible model surface reflectance key success photometric stereo real world materials <eos> recent developments field enabled shape recovery techniques surfaces various types but effective solution directly estimating surface normal presence highly specular reflectance remains elusive <eos> paper derive analytical isotropic microfacet based reflectance model based physically interpretable approximate tailored highly specular surfaces <eos> approximate identify equivalence between surface recovery problem ellipsoid revolution fitting problem latter described system polynomials <eos> additionally devise fast non iterative globally optimal solver problem <eos> experimental result both synthetic real image validate model demonstrate solution stably deliver superior performance its targeted application domain <eos> <eop> detecting faces using inside cascaded contextual cnn <eos> deep convolutional neural network cnn achieve substantial improvements face detection wild <eos> classical cnn based face detection method simply stack successive layer filters input sample should pass through all layer before reaching face non face decision <eos> inspired fact face detection filters deeper layer discriminate between difficult face non face sample while shallower layer efficiently reject simple non face sample propose inside cascaded structure introduces face non face classifiers different layer within same cnn <eos> training phase propose data routing mechanism enables different layer trained different types sample thus deeper layer focus handling more difficult sample compared traditional architecture <eos> addition introduce two stream contextual cnn architecture leverages body part information adaptively enhance face detection <eos> extensive experiments challenging fddb wider face benchmarks demonstrate method achieves competitive accuracy state art techniques while keeps real time performance <eos> <eop> novel space time representation positive semidefinite cone facial expression recognition <eos> paper study problem facial expression recognition using novel space time geometric representation <eos> describe temporal evolution facial landmarks parametrized trajectories riemannian manifold positive semidefinite matrices fixed rank <eos> representation advantage bring naturally second desirable quantity when comparing shapes spatial covariance addition conventional affine shape representation <eos> derive then geometric computational tools rate invariant analysis adaptive re sampling trajectories grounding riemannian geometry manifold <eos> specifically approach involves three steps facial landmarks first mapped into riemannian manifold positive semidefinite matrices rank build time parameterized trajectories temporal alignment performed trajectories providing geometry aware dis similarity measure between them finally pairwise proximity function svm ppfsvm used classify them incorporating latter dis similarity measure into kernel function <eos> show effectiveness proposed approach four publicly available benchmarks ck mmi oulu casia afew <eos> result proposed approach comparable better than state art method when involving only facial landmarks <eos> <eop> deepcoder semi parametric variational autoencoders automatic facial action coding <eos> human face exhibits inherent hierarchy its representations <eos> holistic facial expressions encoded via set facial action units aus their intensity <eos> variational deep auto encoders vae shown great result unsupervised extraction hierarchical latent representations large amounts image data while being robust noise other undesired artifacts <eos> potentially makes vaes suitable approach learning facial feature au intensity estimation <eos> yet most existing vae based method apply classifiers learned separately encoded feature <eos> contrast non parametric probabilistic approaches such gaussian processes gps typically outperform their parametric counterparts but cannot deal easily large amounts data <eos> end propose novel vae semi parametric modeling framework named deepcoder combines modeling power parametric convolutional non parametric ordinal gps vaes joint learning latent representations multiple levels task hierarchy classification multiple ordinal outputs <eos> show benchmark datasets au intensity estimation proposed deepcoder outperforms state art approaches related vaes deep learning models <eos> <eop> pose invariant face alignment single cnn <eos> face alignment witnessed substantial progress last decade <eos> one recent focuses aligning dense three dimensional face shape face image large head poses <eos> dominant technology used based cascade regressors <eos> cnn shown promising result <eos> nonetheless cascade cnn suffers several drawbacks <eos> lack end end training hand crafted feature slow training speed <eos> address issues propose new layer named visualization layer integrated into cnn architecture enables joint optimization different loss functions <eos> extensive evaluation proposed method multiple datasets demonstrates state art accuracy while reducing training time more than half compared typical cascade cnn <eos> addition compare across multiple cnn architectures all visualization layer further demonstrate advantage its utilization <eos> <eop> unsupervised domain adaptation face recognition unlabeled video <eos> despite rapid advances face recognition there remains clear gap between performance still image based face recognition video based face recognition due vast difference visual quality between domains difficulty curating diverse large scale video datasets <eos> paper addresses both challenges through image video feature level domain adaptation approach learn discriminative video frame representations <eos> framework utilizes large scale unlabeled video data reduce gap between different domains while transferring discriminative knowledge large scale labeled still image <eos> given face recognition network pretrained image domain adaptation achieved distilling knowledge network video adaptation network through feature matching ii performing feature restoration through synthetic data augmentation iii learning domain invariant feature through domain adversarial discriminator <eos> further improve performance through discriminator guided feature fusion boosts high quality frames while eliminating degraded video domain specific factors <eos> experiments youtube faces ijb datasets demonstrate each module contributes feature level domain adaptation framework substantially improves video face recognition performance achieve state art accuracy <eos> demonstrate qualitatively network learns suppress diverse artifacts video such pose illumination occlusion without being explicitly trained them <eos> <eop> deeply learned part aligned representations person re identification <eos> paper address problem person re identification refers associating persons captured different cameras <eos> propose simple yet effective human part aligned representation handling body part misalignment problem <eos> approach decomposes human body into region parts discriminative person matching accordingly computes representations over region aggregates similarities computed between corresponding region pair probe gallery image overall matching score <eos> formulation inspired attention models deep neural network modeling three steps together learnt through minimizing triplet loss function without requiring body part labeling information <eos> unlike most existing deep learning algorithms learn global spatial partition based local representation approach performs human body partition thus more robust pose changes various human spatial distributions person bounding box <eos> approach shows state art result over standard datasets market cuhk cuhk viper <eos> <eop> semantic line detection its applications <eos> semantic lines characterize layout image <eos> despite their importance image analysis scene understanding there no reliable research semantic line detection <eos> paper propose semantic line detector using convolutional neural network multi task learning regarding line detection combination classification regression tasks <eos> use convolution max pooling layer obtain multi scale feature maps input image <eos> then develop line pooling layer extract feature vector each candidate line feature maps <eos> next feed feature vector into parallel classification regression layer <eos> classification layer decides whether line candidate semantic <eos> case semantic line regression layer determines offset refining line location <eos> experimental result show proposed detector extracts semantic lines accurately reliably <eos> moreover demonstrate proposed detector used successfully three applications horizon estimation composition enhancement image simplification <eos> <eop> generic deep architecture single image reflection removal image smoothing <eos> paper proposes deep neural network structure exploits edge information addressing representative low level vision tasks such layer separation image filtering <eos> unlike most other deep learning strategies applied context approach tackles challenging problems estimating edges reconstructing image using only cascaded convolutional layer arranged such no handcrafted application specific image processing components required <eos> apply resulting transferrable pipeline two different problem domains both sensitive edges namely single image reflection removal image smoothing <eos> former using mild reflection smoothness assumption novel synthetic data generation method acts type weak supervision network able solve much more difficult reflection cases cannot handled previous method <eos> latter also exceed state art quantitative qualitative result wide margins <eos> all cases proposed framework simple fast easy transfer across disparate domains <eos> <eop> revisiting cross channel information transfer chromatic aberration correction <eos> image aberrations cause severe degradation image quality consumer level cameras especially under current tendency reduce complexity lens designs order shrink overall size modules <eos> simplified optical designs chromatic aberration one most significant causes degraded image quality quite difficult remove post processing since result strong blurs least some color channels <eos> work revisit pixel wise similarity between different color channels image accordingly propose novel algorithm correcting chromatic aberration based cross channel correlation <eos> contrast recent weak prior based models ours uses strong pixel wise fitting transfer lead significant quality improvements large chromatic aberrations <eos> experimental result both synthetic real world image captured different optical systems demonstrate chromatic aberration significantly reduced using approach <eos> <eop> high quality correspondence segmentation estimation dual lens smart phone portraits <eos> estimating correspondence between two image extracting foreground object two challenges computer vision <eos> dual lens smart phones such iphone plus huawei coming into market two image slightly different views provide new information unify two topics <eos> propose joint method tackle them simultaneously via joint fully connected conditional random field crf framework <eos> regional correspondence used handle textureless region matching make crf system computationally efficient <eos> method evaluated over new image pairs produces promising result challenging portrait image <eos> <eop> learning visual attention identify people autism spectrum disorder <eos> paper presents novel method quantitative objective diagnoses autism spectrum disorder asd using eye tracking deep neural network <eos> lack clinical resources early diagnoses long lasting issue <eos> work differentiates itself three unique feature first proposed approach data driven free assumptions important new discoveries understanding asd well other neurodevelopmental disorders <eos> second concentrate analyses differences eye movement patterns between healthy people asd <eos> image selection method based fisher scores allows feature learning most discriminative contents leading efficient accurate diagnoses <eos> third leverage recent advances deep neural network both prediction visualization <eos> experimental result show superior performance method terms multiple evaluation metrics used diagnostic tests <eos> <eop> dslr quality photos mobile devices deep convolutional network <eos> despite rapid rise quality built smartphone cameras their physical limitations small sensor size compact lenses lack specific hardware impede them achieve quality result dslr cameras <eos> work present end end deep learning approach bridges gap translating ordinary photos into dslr quality image <eos> propose learning translation function using residual convolutional neural network improves both color rendition image sharpness <eos> since standard mean squared loss well suited measuring perceptual image quality introduce composite perceptual error function combines content color texture losses <eos> first two losses defined analytically while texture loss learned adversarial fashion <eos> also present dped large scale dataset consists real photos captured three different phones one high end reflex camera <eos> quantitative qualitative assessments reveal enhanced image quality comparable dslr taken photos while methodology generalized any type digital camera <eos> <eop> non uniform blind deblurring reblurring <eos> present approach blind image deblurring handles non uniform blurs <eos> algorithm two main components new method recovering unknown blur field directly blurry image ii method deblurring image given recovered nonuniform blur field <eos> blur field estimation based analyzing spectral content blurry image patches re blurring them <eos> being unrestricted any training data handle large variety blur sizes yielding superior blur field estimation result compared training based deep learning method <eos> non uniform deblurring algorithm based internal image specific patch recurrence prior <eos> attempts recover sharp image one hand result blurry image under estimated blur field other hand maximizes internal recurrence patches within across scales recovered sharp image <eos> combination two components gives rise blind deblurring algorithm exceeds performance state art cnn based blind deblurring significant margin without need any training data <eos> <eop> misalignment robust joint filter cross modal image pairs <eos> although several powerful joint filters cross modal image pairs proposed existing joint filters generate severe artifacts when there misalignments between target guidance image <eos> goal generate artifact free output image even misaligned target guidance image <eos> propose novel misalignment robust joint filter based weight volume based image composition joint filter cost volume <eos> proposed method first generates set translated guidances <eos> next joint filter cost volume set filtered image computed target image set translated guidances <eos> then weight volume obtained joint filter cost volume while considering spatial smoothness label sparseness <eos> final output image composed fusing set filtered image weight volume filtered image <eos> key generate final output image directly set filtered image weighted averaging using weight volume obtained joint filter cost volume <eos> proposed framework widely applicable involve any kind joint filter <eos> experimental result show proposed method effective various applications including image denosing image up sampling haze removal depth map interpolation <eos> <eop> low rank tensor completion pseudo bayesian learning approach <eos> low rank tensor completion solves linear inverse problem principle parsimony powerful technique used many application domains computer vision pattern recognition <eos> surrogate function matrix rank non convex discontinuous nuclear norm often used instead derive efficient algorithms recovering missing information matrices higher order tensors <eos> however nuclear norm loose approximation matrix rank more tensor nuclear norm guaranteed tightest convex envelope multilinear rank <eos> alternative algorithms either require specifying tuning several parameters <eos> tensor rank performance far reaching theoretical limit number observed elements equals degree freedom unknown low rank tensor <eos> paper propose pseudo bayesian approach bayesian inspired cost function adjusted using appropriate approximations lead desirable attributes including concavity symmetry <eos> although deviating original bayesian model resulting non convex cost function proved ability recover true tensor low multilinear rank <eos> computational efficient algorithm derived solve resulting non convex optimization problem <eos> demonstrate superior performance proposed algorithm comparison state art alternatives conducting extensive experiments both synthetic data several visual data recovery tasks <eos> <eop> deepcd learning deep complementary descriptors patch representations <eos> paper presents deepcd framework learns pair complementary descriptors jointly patch employing deep learning techniques <eos> achieved taking any descriptor learning architecture learning leading descriptor augmenting architecture additional network stream learning complementary descriptor <eos> enforce complementary property new network layer called data dependent modulation ddm layer introduced adaptively learning augmented network stream emphasis training data well handled leading stream <eos> optimizing proposed joint loss function late fusion obtained descriptors complementary each other their fusion improves performance <eos> experiments several problems datasets show proposed method simple yet effective outperforming state art method <eos> <eop> beyond standard benchmarks parameterizing performance evaluation visual object tracking <eos> object camera motion produces variety apparent motion patterns significantly affect performance short term visual trackers <eos> despite being crucial designing robust trackers their influence poorly explored standard benchmarks due weakly defined biased overlapping attribute annotations <eos> paper propose go beyond pre recorded benchmarks post hoc annotations presenting approach utilizes omnidirectional video generate realistic consistently annotated short term tracking scenarios exactly parameterized motion patterns <eos> created evaluation system constructed fully annotated dataset omnidirectional video generators typical motion patterns <eos> provide depth analysis major tracking paradigms complementary standard benchmarks confirms expressiveness evaluation approach <eos> <eop> pose knows video forecasting generating pose futures <eos> current approaches video forecasting attempt generate video directly pixel space using generative adversarial network gans variational autoencoders vaes <eos> however since approaches try model all structure scene dynamics once unconstrained settings they often generate uninterpretable result <eos> insight forecasting needs done first higher level abstraction <eos> specifically exploit human pose detectors free source supervision break video forecasting problem into two discrete steps <eos> first explicitly model high level structure active object scene humans use vae model possible future movements humans pose space <eos> then use future poses generated conditional information gan predict future frames video pixel space <eos> using structured space pose intermediate representation sidestep problems gans generating video pixels directly <eos> show through quantitative qualitative evaluation method outperforms state art method video prediction <eos> <eop> will happen next forecasting player moves sports video <eos> large number very popular team sports involve act one team trying score goal against other <eos> during game play defending players constantly try predict next move attackers prevent them scoring whereas attackers constantly try predict next move defenders order defy them score <eos> such behavior prime example general human faculty make predictions about future important facet human intelligence <eos> algorithmic solution learning model external world sensory inputs order make forecasts important unsolved problem <eos> work develop generic framework forecasting future events team sports video directly visual inputs <eos> introduce water polo basketball datasets towards end compare predictions proposed method against expert non expert humans <eos> <eop> robust kronecker decomposable component analysis low rank modeling <eos> dictionary learning component analysis part one most well studied active research fields intersection signal image processing computer vision statistical machine learning <eos> dictionary learning current method choice arguably svd its variants learn dictionary <eos> decomposition sparse coding via singular value decomposition <eos> robust component analysis leading method derive principal component pursuit pcp recovers low rank matrix sparse corruptions unknown magnitude support <eos> however svd sensitive presence noise outliers training set <eos> additionally pcp provide dictionary respects structure data <eos> image requires expensive svd computations when solved convex relaxation <eos> paper introduce new robust decomposition image combining ideas sparse dictionary learning pcp <eos> propose novel kronecker decomposable component analysis robust gross corruption used low rank modeling leverages separability solve significantly smaller problems <eos> design efficient learning algorithm drawing links restricted form tensor factorization <eos> effectiveness proposed approach demonstrated real world applications namely background subtraction image denoising performing thorough comparison current state art <eos> <eop> recurrent topic transition gan visual paragraph generation <eos> natural image usually conveys rich semantic content viewed different angles <eos> existing image description method largely restricted small set biased visual paragraph annotations fail cover rich underlying semantics <eos> paper investigate semi supervised paragraph generative framework able synthesize diverse semantically coherent paragraph descriptions reasoning over local semantic region exploiting linguistic knowledge <eos> proposed recurrent topic transition generative adversarial network rtt gan builds adversarial framework between structured paragraph generator multi level paragraph discriminators <eos> paragraph generator generates sentences recurrently incorporating region based visual language attention mechanisms each step <eos> quality generated paragraph sentences assessed multi level adversarial discriminators two aspects namely plausibility sentence level topic transition coherence paragraph level <eos> joint adversarial training rtt gan drives model generate realistic paragraphs smooth logical transition between sentence topics <eos> extensive quantitative experiments image video paragraph datasets demonstrate effectiveness rtt gan both supervised semi supervised settings <eos> qualitative result telling diverse stories image verify interpretability rtt gan <eos> <eop> two streamed network estimating fine scaled depth maps single rgb image <eos> estimating depth single rgb image ill posed inherently ambiguous problem <eos> state art deep learning method now estimate accurate depth maps but when maps projected into three dimensional they lack local detail often highly distorted <eos> propose fast train two streamed cnn predicts depth depth gradients then fused together into accurate detailed depth map <eos> also define novel set loss over multiple image regularizing estimation between common set image network less prone over fitting achieves better accuracy than competing method <eos> experiments nyu depth dataset shows depth predictions competitive state art lead faithful three dimensional projections <eos> <eop> weakly supervised object localization using things stuff transfer <eos> propose help weakly supervised object localization classes location annotations available transferring things stuff knowledge source set available annotations <eos> source target classes might share similar appearance <eos> bear fur similar cat fur appear against similar background <eos> horse sheep appear against grass <eos> exploit acquire three types knowledge source set segmentation model trained both thing stuff classes similarity relations between target source classes co occurrence relations between thing stuff classes source <eos> segmentation model used generate thing stuff segmentation maps target image while class similarity co occurrence knowledge help refining them <eos> then incorporate maps new cues into multiple instance learning framework mil propagating transferred knowledge pixel level object proposal level <eos> extensive experiments conduct transfer pascal context dataset source ilsvrc coco pascal voc datasets targets <eos> evaluate transfer across widely different thing classes including some similar appearance but appear against similar background <eos> result demonstrate significant improvement over standard mil outperform state art transfer setting <eos> <eop> single image action recognition using semantic body part actions <eos> paper propose novel single image action recognition algorithm based idea semantic part actions <eos> unlike existing part based method argue there exists mid level semantic semantic part action human action combination semantic part actions context cues <eos> detail divide human body into seven parts head torso arms hands lower body <eos> each them define few semantic part actions <eos> finally exploit part actions infer entire body action <eos> make proposed idea practical propose deep network based framework consists two subnetworks one part localization other action prediction <eos> action prediction network jointly learns part level body level action semantics combines them final decision <eos> extensive experiments demonstrate proposal semantic part actions elements entire body action <eos> method reaches map <eos> pascal voc stanford outperforms state art <eos> <eop> incremental learning object detectors without catastrophic forgetting <eos> despite their success object detection convolutional neural network ill equipped incremental learning <eos> adapting original model trained set classes additionally detect object new classes absence initial training data <eos> they suffer catastrophic forgetting abrupt degradation performance original set classes when training objective adapted new classes <eos> present method address issue learn object detectors incrementally when neither original training data nor annotations original classes new training set available <eos> core proposed solution loss function balance interplay between predictions new classes new distillation loss minimizes discrepancy between responses old classes original updated network <eos> incremental learning performed multiple times new set classes each step moderate drop performance compared baseline network trained ensemble data <eos> present object detection result pascal voc coco datasets along detailed empirical analysis approach <eos> <eop> generative adversarial network conditioned brain signals <eos> recent advancements generative adversarial network gans using deep convolutional models supported development image generation techniques able reach satisfactory levels realism <eos> further improvements proposed condition gans generate image matching specific object category short text description <eos> work build latter class approaches investigate possibility driving conditioning image generation process means brain signals recorded through electroencephalograph eeg while users look image set imagenet object categories objective generating seen image <eos> accomplish task first demonstrate brain activity eeg signals encode visually related information allows accurately discriminate between visual object categories accordingly extract more compact class dependent representation eeg data using recurrent neural network <eos> afterwards use learned eeg manifold condition image generation employing gans during inference will read eeg signals convert them into image <eos> tested generative approach using eeg signals recorded six subjects while looking image aforementioned visual classes <eos> result show classes represented well defined visual patterns <eos> pandas airplane etc <eos> generated image realistic highly resemble evoking eeg signals used conditioning gans resulting actual reading mind process <eos> <eop> learning disambiguate asking discriminative questions <eos> ability ask questions powerful tool gather information order learn about world resolve ambiguities <eos> paper explore novel problem generating discriminative questions help disambiguate visual instances <eos> work seen complement new extension rich research studies image captioning question answering <eos> introduce first large scale dataset over carefully annotated image question tuples facilitate benchmarking <eos> particular each tuple consists pair image <eos> discriminative questions positive sample <eos> non discriminative questions negative sample average <eos> addition present effective method visual discriminative question generation <eos> method trained weakly supervised manner without discriminative image question tuples but just existing visual question answering datasets <eos> promising result shown against representative baselines through quantitative evaluations user studies <eos> <eop> interpretable explanations black boxes meaningful perturbation <eos> machine learning algorithms increasingly applied high impact yet high risk tasks such medical diagnosis autonomous driving critical researchers explain how such algorithms arrived their predictions <eos> recent years number image saliency method developed summarize highly complex neural network look image evidence their predictions <eos> however techniques limited their heuristic nature architectural constraints <eos> paper make two main contributions first propose general framework learning different kinds explanations any black box algorithm <eos> second specialise framework find part image most responsible classifier decision <eos> unlike previous works method model agnostic testable because grounded explicit interpretable image perturbations <eos> <eop> deeproadmapper extracting road topology aerial image <eos> creating road maps essential success many applications such autonomous driving city planning <eos> most approaches industry focus leveraging expensive sensors mounted top fleet cars <eos> result very accurate estimates when using techniques involve user loop <eos> however solutions very expensive small coverage <eos> contrast paper propose approach directly estimates road topology aerial image <eos> provides affordable solution large coverage <eos> towards goal take advantage latest developments deep learning initial segmentation aerial image <eos> then propose algorithm reasons about missing connections extracted road topology shortest path problem solved efficiently <eos> demonstrate effectiveness approach challenging torontocity dataset show very significant improvements over state art <eos> <eop> monocular three dimensional human pose estimation predicting depth joints <eos> paper aims estimating full body three dimensional human poses monocular image biggest challenge inherent ambiguity introduced lifting pose into three dimensional space <eos> propose novel framework focusing reducing ambiguity predicting depth human joints based human joint locations body part image <eos> approach built two level hierarchy long short term memory lstm network trained end end <eos> first level consists two components skeleton lstm learns depth information global human skeleton feature patch lstm utilizes local image evidence around joint locations <eos> both network tree structure defined kinematic relation human skeleton thus information different joints broadcast through whole skeleton top down fashion <eos> two network first pre trained separately different data sources then aggregated second layer final depth prediction <eos> empirical evaluation human <eos> hhoi dataset demonstrates advantage combining global skeleton local image patches depth prediction superior quantitative qualitative performance relative state art method <eos> <eop> large scale image retrieval attentive deep local feature <eos> propose attentive local feature descriptor suitable large scale image retrieval referred delf deep local feature <eos> new feature based convolutional neural network trained only image level annotations landmark image dataset <eos> identify semantically useful local feature image retrieval also propose attention mechanism keypoint selection shares most network layer descriptor <eos> framework used image retrieval drop replacement other keypoint detectors descriptors enabling more accurate feature matching geometric verification <eos> system produces reliable confidence scores reject false positives particular robust against queries no correct match database <eos> evaluate proposed descriptor introduce new large scale dataset referred google landmarks dataset involves challenges both database query such background clutter partial occlusion multiple landmarks object variable scales etc <eos> <eop> deep globally constrained mrfs human pose estimation <eos> work introduces novel convolutional network architecture convnet task human pose estimation localization body joints single static image <eos> propose coarse fine architecture addresses shortcomings baseline architecture stem fact large inaccuracies its coarse convnet cannot corrected refinement convnet refines estimation within small windows coarse prediction <eos> overcome introducing markov random field mrf based spatial model network between coarse refinement model introduces geometric constraints relative locations body joints <eos> propose architecture filters implement message passing mrf inference factored way constrains them low dimensional pose manifold projection estimated separate branch proposed convnet strengths pairwise joint constraints modeled weights jointly estimated other parameters network <eos> proposed network trained end end fashion <eos> experimental result show proposed method improves baseline model provides state art result very challenging benchmarks <eos> <eop> predicting visual exemplars unseen classes zero shot learning <eos> leveraging class semantic descriptions examples known object zero shot learning makes possible train recognition model object class whose examples available <eos> paper propose novel zero shot learning model takes advantage clustering structures semantic embedding space <eos> key idea impose structural constraint semantic representations must predictive locations their corresponding visual exemplars <eos> end reduces training multiple kernel based regressors semantic representation exemplar pairs labeled data seen object categories <eos> despite its simplicity approach significantly outperforms existing zero shot learning method three out four benchmark datasets including imagenet dataset more than unseen categories <eos> <eop> multi label learning part detectors heavily occluded pedestrian detection <eos> detecting pedestrians partially occluded remains challenging problem due variations uncertainties partial occlusion patterns <eos> following commonly used framework handling partial occlusions part detection propose multi label learning approach jointly learn part detectors capture partial occlusion patterns <eos> part detectors share set decision trees via boosting exploit part correlations also reduce computational cost applying part detectors <eos> learned decision trees capture overall distribution all parts <eos> when used pedestrian detector individually part detectors learned jointly show better performance than their counterparts learned separately different occlusion situations <eos> learned part detectors further integrated better detect partially occluded pedestrians <eos> experiments caltech dataset show state art performance approach detecting heavily occluded pedestrians <eos> <eop> sgn sequential grouping network instance segmentation <eos> paper propose sequential grouping network sgn tackle problem object instance segmentation <eos> sgns employ sequence neural network each solving sub grouping problem increasing semantic complexity order gradually compose object out pixels <eos> particular first network aims group pixels along each image row column predicting horizontal vertical object breakpoints <eos> breakpoints then used create line segments <eos> exploiting two directional information second network groups horizontal vertical lines into connected components <eos> finally third network groups connected components into object instances <eos> experiments show sgn significantly outperforms state art approaches both cityscapes dataset well pascal voc <eos> <eop> adaptive feeding achieving fast accurate detections adaptively combining object detectors <eos> object detection aims high speed accuracy simultaneously <eos> however fast models usually less accurate while accurate models cannot satisfy need speed <eos> fast model times faster but less accurate than accurate model <eos> paper propose adaptive feeding af combine fast but less accurate detector accurate but slow detector adaptively determining whether image easy hard choosing appropriate detector <eos> practice build cascade detectors including af classifier make easy vs <eos> hard decision two detectors <eos> af classifier tuned obtain different tradeoff between speed accuracy negligible training time requires no additional training data <eos> experimental result pascal voc ms coco caltech pedestrian datasets confirm af ability achieve comparable speed fast detector comparable accuracy accurate one same time <eos> example combining fast ssd accurate ssd detector af leads speedup over ssd same precision voc test set <eos> <eop> aesthetic critiques generation photos <eos> said picture worth thousand words <eos> thus there various ways describe image especially aesthetic quality analysis <eos> although aesthetic quality assessment generated great deal interest last decade most studies focus providing quality rating good bad image <eos> work extend task produce captions related photo aesthetics photography skills <eos> best knowledge first study deals aesthetics captioning instead aq scoring <eos> contrast common image captioning tasks depict object their relations picture approach select particular aesthetics aspect generate captions respect aspect chosen <eos> meanwhile proposed aspect fusion method further uses attention mechanism generate more abundant aesthetics captions <eos> also introduce new dataset aesthetics captioning called photo critique captioning dataset pccd contains pair wise image comment data professional photographers <eos> result experiments pccd demonstrate approaches outperform existing method generating aesthetic oriented captions image <eos> <eop> hide seek forcing network meticulous weakly supervised object action localization <eos> propose hide seek weakly supervised framework aims improve object localization image action localization video <eos> most existing weakly supervised method localize only most discriminative parts object rather than all relevant parts leads suboptimal performance <eos> key idea hide patches training image randomly forcing network seek other relevant parts when most discriminative part hidden <eos> approach only needs modify input image work any network designed object localization <eos> during testing need hide any patches <eos> hide seek approach obtains superior performance compared previous method weakly supervised object localization ilsvrc dataset <eos> also demonstrate framework easily extended weakly supervised action localization <eos> <eop> two phase learning weakly supervised object localization <eos> weakly supervised semantic segmentation localization problem focusing only most important parts image since they use only image level annotations <eos> paper solve problem fundamentally via two phase learning <eos> network trained two steps <eos> first step conventional fully convolutional network fcn trained find most discriminative parts image <eos> second step activations most salient parts suppressed inference conditional feedback then second learning performed find area next most important parts <eos> combining activations both phases entire portion target object captured <eos> proposed training scheme novel utilized well designed techniques weakly supervised semantic segmentation salient region detection object location prediction <eos> detailed experiments demonstrate effectiveness two phase learning each task <eos> <eop> curriculum dropout <eos> dropout very effective way regularizing neural network <eos> stochastically dropping out units certain probability discourages over specific co adaptations feature detectors preventing overfitting improving network generalization <eos> besides dropout interpreted approximate model aggregation technique exponential number smaller network averaged order get more powerful ensemble <eos> paper show using fixed dropout probability during training suboptimal choice <eos> thus propose time scheduling probability retaining neurons network <eos> induces adaptive regularization scheme smoothly increases difficulty optimization problem <eos> idea starting easy adaptively increasing difficulty learning problem its roots curriculum learning allows one train better models <eos> indeed prove optimization strategy implements very general curriculum scheme gradually adding noise both input intermediate feature representations within network architecture <eos> experiments seven image classification datasets different network architectures show method named curriculum dropout frequently yields better generalization worst performs just well standard dropout method <eos> <eop> predictor combination test time <eos> present algorithm test time combination set reference predictors unknown parametric forms <eos> existing multi task transfer learning algorithms focus training time transfer combination parametric forms predictors known shared <eos> however when parametric form predictor unknown <eos> human predictor predictor precompiled library existing algorithms applicable <eos> instead empirically evaluate predictors sampled data point measure distances between different predictors <eos> embeds set reference predictors into riemannian manifold upon perform manifold denoising obtain refined predictor <eos> allows approach make no assumptions about underlying predictor forms <eos> test time combination algorithm equals outperforms existing multi task transfer learning algorithms challenging real world datasets without introducing specific model assumptions <eos> <eop> guided perturbations self corrective behavior convolutional neural network <eos> convolutional neural network subject great importance over past decade great strides made their utility producing state art performance many computer vision problems <eos> however behavior deep network yet fully understood still active area research <eos> work present intriguing behavior pre trained cnn made improve their predictions structurally perturbing input <eos> observe perturbations referred guided perturbations enable trained network improve its prediction performance without any learning change network weights <eos> perform various ablative experiments understand how perturbations affect local context feature representations <eos> furthermore demonstrate idea improve performance several existing approaches semantic segmentation scene labeling tasks pascal voc dataset supervised classification tasks mnist cifar datasets <eos> <eop> learning robust visual semantic embeddings <eos> many existing method learning joint embedding image text use only supervised information paired image its textual attributes <eos> taking advantage recent success unsupervised learning deep neural network propose end end learning framework able extract more robust multi modal representations across domains <eos> proposed method combines representation learning models <eos> auto encoders together cross domain learning criteria <eos> maximum mean discrepancy loss learn joint embeddings semantic visual feature <eos> novel technique unsupervised data adaptation inference introduced construct more comprehensive embeddings both labeled unlabeled data <eos> evaluate method animals attributes caltech ucsd birds dataset wide range applications including zero few shot image recognition retrieval inductive transductive settings <eos> empirically show framework improves over current state art many considered tasks <eos> <eop> punda probabilistic unsupervised domain adaptation knowledge transfer across visual categories <eos> paper introduces probabilistic latent variable model address unsupervised domain adaptation problems <eos> achieved learning projections each domain latent space along classifier latent space simultaneously minimizing notion domain disparity while maximizing measure discriminatory power <eos> non parametric nature latent variable model makes possible infer latent space dimension automatically data <eos> also develop variational bayes vb algorithm parameter estimation <eos> evaluate contrast proposed model against state art method task visual domain adaptation using both handcrafted deep net feature <eos> experiments show even simple softmax classifier model outperform several state art method taking advantage more sophisticated classification schemes <eos> <eop> learning uncertain world representing ambiguity through multiple hypotheses <eos> many prediction tasks contain uncertainty <eos> some cases uncertainty inherent task itself <eos> future prediction example many distinct outcomes equally valid <eos> other cases uncertainty arises way data labeled <eos> example object detection many object interest often go unlabeled human pose estimation occluded joints often labeled ambiguous values <eos> work focus principled approach handling such scenarios <eos> particular propose framework reformulating existing single prediction models multiple hypothesis prediction mhp models associated meta loss optimization procedure train them <eos> demonstrate approach consider four diverse applications human pose estimation future prediction image classification segmentation <eos> find mhp models outperform their single hypothesis counterparts all cases mhp models simultaneously expose valuable insights into variability predictions <eos> <eop> cdts collaborative detection tracking segmentation online multiple object segmentation video <eos> novel online algorithm segment multiple object video sequence proposed work <eos> develop collaborative detection tracking segmentation cdts technique extract multiple segment tracks accurately <eos> first jointly use object detector tracker generate multiple bounding box tracks object <eos> second transform each bounding box into pixel wise segment employing alternate shrinking expansion ase segmentation <eos> third refine segment tracks detecting object disappearance reappearance cases merging overlapping segment tracks <eos> experimental result show proposed algorithm significantly surpasses state art conventional algorithms benchmark datasets <eos> <eop> temporal superpixels based proximity weighted patch matching <eos> temporal superpixel algorithm based proximity weighted patch matching ts ppm proposed work <eos> develop proximity weighted patch matching ppm estimates motion vector superpixel robustly considering patch matching distances neighboring superpixels well target superpixel <eos> each frame initialize superpixels transferring superpixel labels previous frame using ppm motion vectors <eos> then update superpixel labels boundary pixels based cost function composed color spatial contour temporal consistency terms <eos> finally execute superpixel splitting merging relabeling regularize superpixel sizes reduce incorrect labels <eos> experiments show proposed algorithm outperforms state art conventional algorithms significantly <eos> <eop> joint detection recounting abnormal events learning deep generic knowledge <eos> paper addresses problem joint detection recounting abnormal events video <eos> recounting abnormal events <eos> explaining why they judged abnormal unexplored but critical task video surveillance because helps human observers quickly judge if they false alarms <eos> describe events human understandable form event recounting learning generic knowledge about visual concepts <eos> object action crucial <eos> although convolutional neural network cnn achieved promising result learning such concepts remains open question how effectively use cnn abnormal event detection mainly due environment dependent nature anomaly detection <eos> paper tackle problem integrating generic cnn model environment dependent anomaly detectors <eos> approach first learns cnn multiple visual tasks exploit semantic information useful detecting recounting abnormal events <eos> appropriately plugging model into anomaly detectors detect recount abnormal events while taking advantage discriminative power cnn <eos> approach outperforms state art avenue ucsd ped benchmarks abnormal event detection also produces promising result abnormal event recounting <eos> <eop> turn tap temporal unit regression network temporal action proposals <eos> address problem temporal action proposal tap generation <eos> important problem fast extraction semantically important <eos> human actions segments untrimmed video important step large scale video analysis <eos> tackle problem propose novel temporal unit regression network turn model <eos> there two salient aspects turn turn jointly predicts action proposals refines temporal boundaries temporal coordinate regression contextual information fast computation enabled unit feature reuse long untrimmed video decomposed into video units reused basic building blocks temporal proposals <eos> turn outperforms state art method under average recall ar large margin thumos activitynet datasets runs over frames per second fps titan gpu <eos> further apply turn proposal generation stage existing temporal action localization pipelines outperforms state art performance thumos activitynet <eos> <eop> online real time multiple spatiotemporal action localisation prediction <eos> present deep learning framework real time multiple spatio temporal action localisation classification <eos> current state art approaches work offline too slow useful real world settings <eos> overcome their limitations introduce two major developments <eos> firstly adopt real time ssd single shot multibox detector cnn regress classify detection boxes each video frame potentially containing action interest <eos> secondly design original efficient online algorithm incrementally construct label action tubes ssd frame level detections <eos> result system only capable performing detection real time but also perform early action prediction online fashion <eos> achieve new state art result both action localisation early action prediction challenging ucf hmdb benchmarks even when compared top offline competitors <eos> best knowledge ours first real time up fps system able perform online action localisation untrimmed video ucf <eos> <eop> leveraging weak semantic relevance complex video event classification <eos> existing video event classification approaches suffer limited human labeled semantic annotations <eos> weak semantic annotations harvested web knowledge without involving any human interaction <eos> however such weak annotations noisy thus effectively utilized without distinguishing its reliability <eos> paper propose novel approach automatically maximize utility weak semantic annotations formalized semantic relevance video shots target event facilitate video event classification <eos> novel attention model designed determine attention scores video shots weak semantic relevance considered attentional guidance <eos> specifically model jointly optimizes two objectives different levels <eos> first one classification loss corresponding video level groundtruth labels second shot level relevance loss corresponding weak semantic relevance <eos> use long short term memory lstm layer capture temporal information carried shots video <eos> each timestep lstm employs attention model weight current shot under guidance its weak semantic relevance event interest <eos> thus automatically exploit weak semantic relevance assist video event classification <eos> extensive experiments conducted three complex large scale video event datasets <eos> medtest activitynet fcvid <eos> approach achieves state art classification performance all three datasets <eos> significant performance improvement upon conventional attention model also demonstrates effectiveness model <eos> <eop> weakly supervised summarization web video <eos> most prior works summarize video either exploring different heuristically designed criteria unsupervised way developing fully supervised algorithms leveraging human crafted training data form video summary pairs importance annotations <eos> however unsupervised method blind video category often fail produce semantically meaningful video summaries <eos> other hand acquisition large amount training data supervised approaches non trivial may lead biased model <eos> different existing method introduce weakly supervised approach requires only video level annotation summarizing web video <eos> casting problem weakly supervised learning problem propose flexible deep three dimensional cnn architecture learn notion importance using only video level annotation without any human crafted training data <eos> specifically main idea leverage multiple video category automatically learn parametric model categorizing video then adopt model find important segments given video ones maximum influence model output <eos> furthermore unleash full potential three dimensional cnn architecture also explored series good practices reduce influence limited training data while summarizing video <eos> experiments two challenging diverse datasets well demonstrate approach produces superior quality video summaries compared several recently proposed approaches <eos> <eop> fcn rlstm deep spatio temporal neural network vehicle counting city cameras <eos> paper develop deep spatio temporal neural network sequentially count vehicles low quality video captured city cameras citycams <eos> citycam video low resolution low frame rate high occlusion large perspective making most existing method lose their efficacy <eos> overcome limitations existing method incorporate temporal information traffic video design novel fcn rlstm network jointly estimate vehicle density vehicle count connecting fully convolutional neural network fcn long short term memory network lstm residual learning fashion <eos> such design leverages strengths fcn pixel level prediction strengths lstm learning complex temporal dynamics <eos> residual learning connection reformulates vehicle count regression learning residual functions reference sum densities each frame significantly accelerates training network <eos> preserve feature map resolution propose hyper atrous combination integrate atrous convolution fcn combine feature maps different convolution layer <eos> fcn rlstm enables refined feature representation novel end end trainable mapping pixels vehicle count <eos> extensively evaluated proposed method different counting tasks three datasets experimental result demonstrating their effectiveness robustness <eos> particular fcn rlstm reduces mean absolute error mae <eos> trancos reduces mae <eos> training process accelerated times average <eos> <eop> fast face swap using convolutional neural network <eos> consider problem face swapping image input identity transformed into target identity while preserving pose facial expression lighting <eos> perform mapping use convolutional neural network trained capture appearance target identity unstructured collection his her photographs <eos> approach enabled framing face swapping problem terms style transfer goal render image style another one <eos> building recent advances area devise new loss function enables network produce highly photorealistic result <eos> combining neural network simple pre post processing steps aim making face swap work real time no input user <eos> <eop> towards visual privacy advisor understanding predicting privacy risks image <eos> increasing number users sharing information online privacy implications entailing such actions major concern <eos> explicit content such user profile gps data devices <eos> mobile phones well web services <eos> facebook offer set privacy settings order enforce users privacy preferences <eos> propose first approach extends concept image content spirit visual privacy advisor <eos> first categorize personal information image into image attributes collect dataset allows train models predict such information directly image <eos> second run user study understand privacy preferences different users <eos> third propose models predict user specific privacy score image order enforce users privacy preferences <eos> model trained predict user specific privacy risk even outperforms judgment users who often fail follow their own privacy preferences image data <eos> <eop> first person activity forecasting online inverse reinforcement learning <eos> address problem incrementally modeling forecasting long term goals first person camera wearer user will they will go goal they seek <eos> contrast prior work trajectory forecasting algorithm darko goes further reason about semantic states will pick up object future goal states far both terms space time <eos> darko learns forecasts first person visual observations user daily behaviors via online inverse reinforcement learning irl approach <eos> classical irl discovers only rewards batch setting whereas darko discovers states transitions rewards goals user streaming data <eos> among other result show darko forecasts goals better than competing method both noisy ideal settings approach theoretically empirically no regret <eos> <eop> binarized convolutional landmark localizers human pose estimation face alignment limited resources <eos> goal design architectures retain groundbreaking performance cnn landmark localization same time lightweight compact suitable applications limited computational resources <eos> end make following contributions first study effect neural network binarization localization tasks namely human pose estimation face alignment <eos> exhaustively evaluate various design choices identify performance bottlenecks more importantly propose multiple orthogonal ways boost performance <eos> based analysis propose novel hierarchical parallel multi scale residual architecture yields large performance improvement over standard bottleneck block while having same number parameters thus bridging gap between original network its binarized counterpart <eos> perform large number ablation studies shed light properties performance proposed block <eos> present result experiments most challenging datasets human pose estimation face alignment reporting many cases state art performance <eos> code downloaded www <eos> com binary cnn landmarks <eop> mofa model based deep convolutional face autoencoder unsupervised monocular reconstruction <eos> work propose novel model based deep convolutional autoencoder addresses highly challenging problem reconstructing three dimensional human face single wild color image <eos> end combine convolutional encoder network expert designed generative model serves decoder <eos> core innovation differentiable parametric decoder encapsulates image formation analytically based generative model <eos> decoder takes input code vector exactly defined semantic meaning encodes detailed face pose shape expression skin reflectance scene illumination <eos> due new way combining cnn based model based face reconstruction cnn based encoder learns extract semantically meaningful parameters single monocular input image <eos> first time cnn encoder expert designed generative model trained end end unsupervised manner renders training very large unlabeled real world data feasible <eos> obtained reconstructions compare favorably current state art approaches terms quality richness representation <eos> <eop> rpan end end recurrent pose attention network action recognition video <eos> recent studies demonstrate effectiveness recurrent neural network rnns action recognition video <eos> however previous works mainly utilize video level category supervision train rnns may prohibit rnns learn complex motion structures along time <eos> paper propose recurrent pose attention network rpan address challenge introduce novel pose attention mechanism adaptively learn pose related feature every time step action prediction rnns <eos> more specifically make three main contributions paper <eos> firstly unlike previous works pose related action recognition rpan end end recurrent network exploit important spatial temporal evolutions human pose assist action recognition unified framework <eos> secondly instead learning individual human joint feature separately pose attention mechanism learns robust human part feature sharing attention parameters partially semantically related human joints <eos> human part feature then fed into human part pooling layer construct highly discriminative pose related representation temporal action modeling <eos> thirdly one important byproduct rpan pose estimation video used coarse pose annotation action video <eos> evaluate proposed rpan quantitatively qualitatively two popular benchmarks <eos> sub jhmdb pennaction <eos> experimental result show rpan outperforms recent state art method challenging datasets <eos> <eop> temporal non volume preserving approach facial age progression age invariant face recognition <eos> modeling long term facial aging process extremely challenging due presence large non linear variations during face development stages <eos> order efficiently address problem work first decomposes aging process into multiple short term stages <eos> then novel generative probabilistic model named temporal non volume preserving tnvp transformation presented model facial aging process each stage <eos> unlike generative adversarial network gans requires empirical balance threshold restricted boltzmann machines rbm intractable model proposed tnvp approach guarantees tractable density function exact inference evaluation embedding feature transformations between faces consecutive stages <eos> model shows its advantages only capturing non linear age related variance each stage but also producing smooth synthesis age progression across faces <eos> approach model any face wild provided only four basic landmark point <eos> moreover structure transformed into deep convolutional network while keeping advantages probabilistic models tractable log likelihood density estimation <eos> method evaluated both terms synthesizing age progressed faces cross age face verification consistently shows state art result various face aging databases <eos> fg net morph aging faces wild agfw cross age celebrity dataset cacd <eos> large scale face verification megaface challenge also performed further show advantages proposed approach <eos> <eop> attribute enhanced face recognition neural tensor fusion network <eos> deep learning achieved great success face recognition however deep learned feature still limited invariance strong intra personal variations such large pose <eos> observed some facial attributes <eos> eyebrow thickness gender invariant such variations <eos> present first work systematically explore how fusion face recognition feature frf facial attribute feature faf enhance face recognition performance various challenging scenarios <eos> despite helpfulness faf practice find existing fusion method cannot reliably improve recognition performance <eos> thus develop powerful tensor based framework formulates fusion low rank tensor optimisation problem <eos> non trivial directly optimise tensor due large number parameters optimise <eos> solve problem establish theoretical equivalence between tensor optimisation two stream gated neural network <eos> equivalence allows tractable computation use standard neural network optimisation tools leading accurate stable optimisation <eos> experimental result show fused feature works better than individual feature thus proving first time facial attributes aid face recognition <eos> achieve state art performance databases such multipie casia nir vir <eos> <eop> unlabeled sample generated gan improve person re identification baseline vitro <eos> main contribution paper simple semi supervised pipeline only uses original training set without collecting extra data <eos> challenging how obtain more training data only training set how use newly generated data <eos> work generative adversarial network gan used generate unlabeled sample <eos> propose label smoothing regularization outliers lsro <eos> method assigns uniform label distribution unlabeled image regularizes supervised model improves baseline <eos> verify proposed method practical problem person re identification re id <eos> task aims retrieve query person other cameras <eos> adopt deep convolutional generative adversarial network dcgan sample generation baseline convolutional neural network cnn representation learning <eos> experiments show adding gan generated data effectively improves discriminative ability learned cnn embeddings <eos> three large scale datasets market cuhk dukemtmc reid obtain <eos> improvement rank precision over baseline cnn respectively <eos> additionally apply proposed method fine grained bird recognition achieve <eos> improvement over strong baseline <eos> code available github <eos> com layumi person reid gan <eos> <eop> egocentric gesture recognition using recurrent three dimensional convolutional neural network spatiotemporal transformer modules <eos> gesture natural interface interacting wearable devices such vr ar helmet glasses <eos> main challenge gesture recognition egocentric vision arises global camera motion caused spontaneous head movement device wearer <eos> paper address problem novel recurrent three dimensional convolutional neural network end end learning <eos> specially design spatiotemporal transformer module recurrent connections between neighboring time slices actively transform three dimensional feature map into canonical view both spatial temporal dimensions <eos> validate method introduce new dataset sufficient size variation reality contains gestures designed interaction wearable devices more than rgb gesture sample subjects captured scenes <eos> dataset show proposed network outperforms competing state art algorithms <eos> moreover method achieve state art performance challenging gtea egocentric action dataset <eos> <eop> recursive spatial transformer rest alignment free face recognition <eos> convolutional neural network cnn led significant progress face recognition <eos> currently most cnn based face recognition method follow two step pipeline <eos> detected face first aligned canonical one pre defined mean face shape then fed into cnn extract feature recognition <eos> alignment step transforms all faces same shape cause loss geometrical information helpful distinguishing different subjects <eos> moreover hard define single optimal shape following recognition since faces large diversity facial feature <eos> poses illumination etc <eos> free above problems independent alignment step introduce recursive spatial transformer rest module into cnn allowing face alignment jointly learned face recognition end end fashion <eos> designed rest intrinsic recursive structure capable progressively aligning faces canonical one even large variations <eos> model non rigid transformation multiple rest modules organized hierarchical structure account different parts faces <eos> overall proposed rest handle large face variations non rigid transformation end end learnable adaptive input making effective alignment free face recognition solution <eos> extensive experiments performed lfw ytf datasets proposed rest outperforms two step method demonstrating its effectiveness <eos> <eop> learning discriminative aggregation network video based face recognition <eos> paper propose discriminative aggregation network dan video face recognition aims integrate information video frames effectively efficiently <eos> different existing aggregation method method aggregates raw video frames directly instead feature obtained complex processing <eos> combining idea metric learning adversarial learning learn aggregation network produces more discriminative synthesized image compared input frames <eos> framework reduces number frames processed greatly speed up recognition procedure <eos> furthermore low quality frames containing misleading information denoised during aggregation process making system more robust discriminative <eos> experimental result show framework generate discriminative image video clips improve overall recognition performance both speed accuracy three widely used datasets <eos> <eop> synergy between face alignment tracking via discriminative global consensus optimization <eos> open question facial landmark localization video whether one should perform tracking tracking detection <eos> tracking produces fittings high accuracy but prone drifting <eos> tracking detection drift free but result low accuracy fittings <eos> provide solution problem describe very first best knowledge synergistic approach between detection face alignment tracking completely eliminates drifting face tracking merely perform tracking detection <eos> first main contribution show one achieve synergy between detection tracking using principled optimization framework based theory global variable consensus optimization using admm second contribution show how proposed analytic framework integrated within state art discriminative method face alignment tracking based cascaded regression deeply learned feature <eos> overall call method discriminative global consensus model dgcm <eos> third contribution show dgcm achieves large performance improvement over currently best performing face tracking method most challenging category vw dataset <eos> <eop> svdnet pedestrian retrieval <eos> paper proposes svdnet retrieval problems focus application person re identification re id <eos> view each weight vector within fully connected fc layer convolutional neuron network cnn projection basis <eos> observed weight vectors usually highly correlated <eos> problem leads correlations among entries fc descriptor compromises retrieval performance based euclidean distance <eos> address problem paper proposes optimize deep representation learning process singular vector decomposition svd <eos> specifically restraint relaxation iteration rri training scheme able iteratively integrate orthogonality constraint cnn training yielding so called svdnet <eos> conduct experiments market cuhk duke datasets show rri effectively reduces correlation among projection vectors produces more discriminative fc descriptors significantly improves re id accuracy <eos> market dataset instance rank accuracy improved <eos> <eop> towards more accurate iris recognition using deeply learned spatially corresponding feature <eos> paper proposes accurate generalizable deep learning framework iris recognition <eos> proposed framework based fully convolutional network fcn generates spatially corresponding iris feature descriptors <eos> specially designed extended triplet loss etl function introduced incorporate bit shifting non iris masking found necessary learning discriminative spatial iris feature <eos> also developed sub network provide appropriate information identifying meaningful iris region serves essential input newly developed etl <eos> thorough experiments four publicly available databases suggest proposed framework consistently outperforms several classic state art iris recognition approaches <eos> more importantly model exhibits superior generalization capability unlike popular method literature essentially require database specific parameter tuning another key advantage over other approaches <eos> <eop> semantically informed multiview surface refinement <eos> present method jointly refine geometry semantic segmentation three dimensional surface meshes <eos> method alternates between updating shape semantic labels <eos> geometry refinement step mesh deformed variational energy minimization such simultaneously maximizes photo consistency compatibility semantic segmentations across set calibrated image <eos> label specific shape priors account interactions between geometry semantic labels three dimensional semantic segmentation step labels mesh updated mrf inference such they compatible semantic segmentations input image <eos> also step includes prior assumptions about surface shape different semantic classes <eos> priors induce tight coupling semantic information influences shape update vice versa <eos> specifically introduce priors favor adaptive smoothing depending class label ii straightness class boundaries iii semantic labels consistent surface orientation <eos> novel mesh based reconstruction evaluated series experiments real synthetic data <eos> compare both state art voxel based semantic three dimensional reconstruction purely geometric mesh refinement demonstrate proposed scheme yields improved three dimensional geometry well improved semantic segmentation <eos> <eop> bb scalable accurate robust partial occlusion method predicting three dimensional poses challenging object without using depth <eos> introduce novel method three dimensional object detection pose estimation color image only <eos> first use segmentation detect object interest even presence partial occlusions cluttered background <eos> contrast recent patch based method rely holistic approach apply detected object convolutional neural network cnn trained predict their three dimensional poses form projections corners their three dimensional bounding boxes <eos> however sufficient handling object recent less dataset object exhibit axis rotational symmetry similarity two image such object under two different poses makes training cnn challenging <eos> solve problem restricting range poses used training introducing classifier identify range pose run time before estimating <eos> also use optional additional step refines predicted poses <eos> improve state art linemod dataset <eos> correctly registered rgb frames <eos> also first report result occlusion dataset using color image only <eos> obtain frames passing pose criterion average several sequences less dataset compared state art same sequences uses both color depth <eos> full approach also scalable single network trained multiple object simultaneously <eos> <eop> modeling urban scenes pointclouds <eos> present method modeling urban scenes pointclouds musp <eos> contrast existing approaches musp robust scalable provides more complete description making manhattan world assumption modeling both buildings polyhedra well non planar ground using nurbs <eos> first segment scene into consistent patches using divide conquer based algorithm within nonparametric bayesian framework stick breaking construction <eos> patches often correspond meaningful structures such ground facades roofs roof superstructures <eos> use polygon sweeping fit predefined templates buildings ground nurbs surface fit uniformly tessellated <eos> finally apply boolean operations polygons buildings buildings parts tesselated ground clip unnecessary geometry <eos> facades protrusions below non planar ground leading final model <eos> explicit bayesian formulation scene segmentation makes approach suitable challenging datasets varying amounts noise outliers point density <eos> demonstrate robustness musp three dimensional pointclouds image matching well lidar <eos> <eop> parameter free lens distortion calibration central cameras <eos> core many computer vision applications stands need define mathematical model describing imaging process <eos> end pinhole model radial distortion probably most commonly used balances low complexity precision sufficient most applications <eos> other hand unconstrained non parametric models despite being originally proposed handle specialty cameras shown outperform pinhole model even simpler setups <eos> still notwithstanding higher accuracy inability describing imaging model simple linear projective operators severely limits use standard algorithms unconstrained models <eos> paper propose parameter free camera model each imaging ray constrained common optical center forcing camera central <eos> such model easily calibrated practical procedure provides convenient undistortion map used obtain virtual pinhole camera <eos> proposed method also used calibrate stereo rig displacement map simultaneously provides stereo rectification corrects lens distortion <eos> <eop> pose guided rgbd feature learning three dimensional object pose estimation <eos> paper examine effects using object poses guidance learning robust feature three dimensional object pose estimation <eos> previous works focused learning feature embeddings based metric learning triplet comparisons rely only qualitative distinction similar dissimilar pose labels <eos> contrast consider exact pose differences between training sample aim learn embeddings such distances pose label space proportional distances feature space <eos> however since less desirable force pose feature correlation when object symmetric propose data driven weights reflect object symmetry when measuring pose distances <eos> furthermore end end pose regression investigated shown further boost discriminative power f